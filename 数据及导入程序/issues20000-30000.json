{"number": 20000, "title": "[pytorch] exclude some caffe2 modules from libtorch mobile build", "time": "2019-05-01T07:43:33Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20000 [pytorch] exclude some caffe2 modules from libtorch mobile build**\n\nSummary:\nSkip unused modules to accelerate compile speed / reduce code size for\nlibtorch mobile build.\n\nTest Plan:\n- verified libtorch mobile library builds and links successfully;\n\nDifferential Revision: [D15169024](https://our.internmc.facebook.com/intern/diff/D15169024)"}
{"number": 20001, "title": "Fix bug in dumpNet", "time": "2019-05-01T08:01:01Z", "body": "Summary: att\n\nDifferential Revision: D15164116\n\n"}
{"number": 20002, "title": "Remove explicit checks for parallelism from TH", "time": "2019-05-01T09:47:28Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #20157 Support for Eigen thread pool\n* #20087 Native ATen/Parallel backend\n* #20057 Split ATen/Parallel into interface and backend\n* #20050 Move inter-op settings into ATen/Parallel\n* #20043 Port ATen/native to ATen/Parallel\n* #20032 Port THNN to ATen/Parallel\n* **#20002 Remove explicit checks for parallelism from TH**\n\nSummary:\nSince we moved TH from OpenMP to ATen/Parallel we don't need\nto explicitly check for parallelism because we check for it in\nATen/Parallel backend\n\nTest Plan:\nuse benchmark from https://github.com/pytorch/pytorch/pull/19997\n\nBLAS=MKL USE_MKLDNN=1 USE_OPENCV=1 USE_FFMPEG=1 python setup.py develop --cmake\ncd benchmarks\npython -m operator_benchmark.ops.th_intraop_test --framework PyTorch\n\nDifferential Revision: [D15232028](https://our.internmc.facebook.com/intern/diff/D15232028)"}
{"number": 20003, "title": "[DOC] Fix examples in jit#user-defined-types documentation", "time": "2019-05-01T10:25:05Z", "body": ""}
{"number": 20004, "title": "[JIT] cannot access to initialized attribute in script-annotated class __getitem__", "time": "2019-05-01T11:02:58Z", "body": "## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n```python\r\n@torch.jit.script\r\nclass Pair:\r\n    def __init__(self, first, second):\r\n        self.first = first\r\n        self.second = second\r\n\r\n    def sum(self):\r\n        return self.first + self.second\r\n\r\n    def __getitem__(self, k):\r\n        return self.first\r\n```\r\n```\r\n$ python test.py\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 4, in <module>\r\n    @torch.jit.script\r\n  File \"/Users/qbx2/anaconda3/lib/python3.7/site-packages/torch/jit/__init__.py\", line 819, in script\r\n    _jit_script_class_compile(ast, _rcb)\r\nRuntimeError: \r\nTried to access to nonexistent attribute first. Did you forget to initialize it in __init__()?:\r\ndef __getitem__(self, k):\r\n    return self.first\r\n           ~~~~~~~~~~ <--- HERE\r\n```\r\n## Expected behavior\r\n\r\nIt should compile the class.\r\n\r\n## Environment\r\n\r\n$ python collect_env.py \r\nCollecting environment information...\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: Could not collect\r\n\r\nOS: Mac OSX 10.14.3\r\nGCC version: Could not collect\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: 1.1.0\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda/lib/libcudnn.7.dylib\r\n/usr/local/cuda/lib/libcudnn.dylib\r\n/usr/local/cuda/lib/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.2\r\n[pip3] numpydoc==0.8.0\r\n[pip3] torch==1.1.0\r\n[pip3] torchvision==0.2.2\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.3                      199  \r\n[conda] mkl-include               2019.3                      199  \r\n[conda] mkl-service               1.1.2            py37hfbe908c_5  \r\n[conda] mkl_fft                   1.0.10           py37h5e564d8_0  \r\n[conda] mkl_random                1.0.2            py37h27c97d8_0  \r\n[conda] pytorch                   1.1.0                   py3.7_0    pytorch\r\n[conda] torch                     1.1.0a0+6732358           dev_0    <develop>\r\n[conda] torchvision               0.2.2                      py_3    pytorch"}
{"number": 20005, "title": "install error from source", "time": "2019-05-01T11:31:32Z", "body": "Hello,there is the output.I don't know how to deal with it.\r\n\r\n[1/1460] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/aten/aten_op.cc.o\r\nFAILED: caffe2/CMakeFiles/caffe2.dir/contrib/aten/aten_op.cc.o\r\n/data1/NLPRMNT/public/gcc485/bin/c++   -DCPUINFO_SUPPORTED_PLATFORM=1 -DHAVE_MALLOC_USABLE_SIZE=1 -DHAVE_MMAP=1 -DHAVE_SHM_OPEN=1 -DHAVE_SHM_UNLINK=1 -DNNP_CONVOLUTION_ONLY=0 -DNNP_INFERENCE_ONLY=0 -DONNX_NAMESPACE=onnx_torch -DTH_BLAS_MKL -DUSE_GCC_ATOMICS=1 -D_FILE_OFFSET_BITS=64 -Dcaffe2_EXPORTS -I../aten/src -I. -I../ -isystem third_party/gloo -isystem ../cmake/../third_party/gloo -isystem ../cmake/../third_party/googletest/googlemock/include -isystem ../cmake/../third_party/googletest/googletest/include -I../third_party/protobuf/src -isystem /data1/NLPRMNT/liguanjun/bliu/opt/anaconda3/include -isystem ../third_party/gemmlowp -isystem ../third_party/neon2sse -I../cmake/../third_party/benchmark/include -isystem ../cmake/../third_party/eigen -isystem /data1/NLPRMNT/liguanjun/bliu/opt/anaconda3/include/python3.6m -isystem /data1/NLPRMNT/liguanjun/bliu/opt/anaconda3/lib/python3.6/site-packages/numpy/core/include -isystem ../cmake/../third_party/pybind11/include -isystem /opt/openmpi/include -isystem ../cmake/../third_party/cub -Icaffe2/contrib/aten -I../third_party/onnx -Ithird_party/onnx -I../third_party/foxi -Ithird_party/foxi -isystem ../third_party/ideep/mkl-dnn/include -isystem ../third_party/ideep/include -Icaffe2/aten/src/TH -I../aten/src/TH -Icaffe2/aten/src -Iaten/src -I../aten/../third_party/catch/single_include -I../aten/src/ATen/.. -Icaffe2/aten/src/ATen -I../caffe2/core/nomnigraph/include -I../third_party/miniz-2.0.8 -isystem include -I/data1/NLPRMNT/liguanjun/software/cuda9.0/include -I../c10/.. -Ithird_party/ideep/mkl-dnn/include -I../third_party/ideep/mkl-dnn/src/../include -I../third_party/QNNPACK/include -I../third_party/pthreadpool/include -I../third_party/NNPACK/include -I../third_party/cpuinfo/include -I../third_party/FP16/include -fvisibility-inlines-hidden -fopenmp -O2 -fPIC -Wno-narrowing -Wall -Wextra -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -DHAVE_AVX_CPU_DEFINITION -O3  -fPIC   -DCAFFE2_USE_GLOO -DCUDA_HAS_FP16=1 -DHAVE_GCC_GET_CPUID -DUSE_AVX -DTH_HAVE_THREAD -fvisibility=hidden -DCAFFE2_BUILD_MAIN_LIB -O2 -pthread -std=gnu++11 -MMD -MT caffe2/CMakeFiles/caffe2.dir/contrib/aten/aten_op.cc.o -MF caffe2/CMakeFiles/caffe2.dir/contrib/aten/aten_op.cc.o.d -o caffe2/CMakeFiles/caffe2.dir/contrib/aten/aten_op.cc.o -c ../caffe2/contrib/aten/aten_op.cc\r\nIn file included from ../caffe2/contrib/aten/aten_op.cc:1:0:\r\n../caffe2/contrib/aten/aten_op.h:1:52: fatal error: caffe2/caffe2/contrib/aten/gen_aten_op.h: No such file or directory\r\n #include \"caffe2/caffe2/contrib/aten/gen_aten_op.h\"\r\n                                                    ^\r\ncompilation terminated.\r\n[18/1460] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/conv_op_eigen.cc.o\r\nninja: build stopped: subcommand failed.\r\n\r\n"}
{"number": 20006, "title": "RuntimeError: invalid argument 10: ldb should be at least max(1, 0), but have 0 at ../aten/src/TH/generic/THBlas.cpp:36", "time": "2019-05-01T11:52:32Z", "body": "## üêõ Bug\r\n\r\nRuntime error when running:\r\n\r\n```\r\nloss = loss_func(self.alpha, self.lam, f_loss, ds, x_hat_p, x_hat_n)\r\noptim.zero_grad()\r\nloss.backward()   # <- Error here\r\n```\r\n\r\nError Message:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Applications/PyCharm.app/Contents/helpers/pydev/pydevd.py\", line 1741, in <module>\r\n    main()\r\n  File \"/Applications/PyCharm.app/Contents/helpers/pydev/pydevd.py\", line 1735, in main\r\n    globals = debugger.run(setup['file'], None, None, is_module)\r\n  File \"/Applications/PyCharm.app/Contents/helpers/pydev/pydevd.py\", line 1135, in run\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"/Applications/PyCharm.app/Contents/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 20, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"/Users/zayd/repos/uo/double_decoder/driver.py\", line 104, in <module>\r\n    _main(_args, _pos_db, _unlabel_db, _enc_dim, [_args.pos_label])\r\n  File \"/Users/zayd/repos/uo/double_decoder/driver.py\", line 62, in _main\r\n    deep_pu.train_pu(p_db=pos_db, u_db=u_db, f_loss=DeepPU.simple_loss, quiet_mode=args.q)\r\n  File \"/Users/zayd/repos/uo/double_decoder/deep_pu.py\", line 473, in train_pu\r\n    loss.backward()\r\n  File \"/Users/zayd/.pyenv/versions/3.7.1/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/tensor.py\", line 107, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File \"/Users/zayd/.pyenv/versions/3.7.1/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 93, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\nRuntimeError: invalid argument 10: ldb should be at least max(1, 0), but have 0 at ../aten/src/TH/generic/THBlas.cpp:365\r\n```\r\n\r\n## To Reproduce\r\n\r\nI do not get this in all cases.  I get it on an autoencoder with two decoders.  I can dig into it more and try to make a streamlined MWE if the devs are unsure what could cause this.\r\n\r\nIt seems to happen when a linear block has an out dimension of zero.  That was being concatenated with another vector so other checks in my code did not catch it.\r\n\r\n## Expected behavior\r\n\r\nI think the error message could be clearer.  When I saw this error, it was very unclear what could cause.  I expect the better solution would be prevent linear blocks being created with an output dimension of zero.\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.14.4\r\nGCC version: Could not collect\r\nCMake version: version 3.13.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] msgpack-numpy==0.4.3.2\r\n[pip3] numpy==1.16.1\r\n[pip3] torch==1.1.0\r\n[pip3] torchfile==0.1.0\r\n[pip3] torchnet==0.0.4\r\n[pip3] torchtext==0.3.1\r\n[pip3] torchvision==0.2.1\r\n[conda] Could not collect\r\n\r\n## Additional context\r\n\r\nConfirmed this error can also occur in torch 1.0.0\r\n"}
{"number": 20007, "title": "[tensorboard] smoke test for add_graph", "time": "2019-05-01T12:54:53Z", "body": "Do tests with common models from torchvision.\r\n\r\n"}
{"number": 20008, "title": "[tensorboard] Clarify API and add examples for all methods", "time": "2019-05-01T14:12:28Z", "body": "As a part of supporting writing data into TensorBoard readable format, we show more example on how to use the function in addition to the API docs.\r\n"}
{"number": 20009, "title": "issue while exporting torch model to onnx format", "time": "2019-05-01T14:18:11Z", "body": "## üêõ Bug\r\n\r\ni am getting error while converting my saved torch model to onnx format\r\n\r\ntorch.onnx.export(trained_model, dummy_input, \"sentiment.onnx\")\r\nTypeError: forward() missing 1 required positional argument: 'hidden' \r\n\r\ntorch version - '1.0.1.post2'\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\nfrom torch.autograd import Variable\r\nimport torch.onnx\r\n# Load the trained model from file\r\nnet_save = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\r\ntrained_model = net_save\r\ntrained_model.load_state_dict(torch.load('sentiment.pth'))\r\n\r\n# Export the trained model to ONNX\r\ndummy_input = Variable(torch.randn(1, 1, 28, 28)) # one black and white 28 x 28 picture will be the input to the model\r\ntorch.onnx.export(trained_model, dummy_input, \"sentiment.onnx\")\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\ni should get the onnx format of my model but getting error\r\n\r\n\n\ncc @BowenBao @neginraoof"}
{"number": 20010, "title": "[WIP] Move set_requires_grad() to VariableType", "time": "2019-05-01T15:27:04Z", "body": ""}
{"number": 20011, "title": "Tensor To OpenCV - C++", "time": "2019-05-01T15:44:54Z", "body": "I'm using the pytorch with float image (C++). Currently, I'm not using GPU, but I'm having difficulties to convert the data between torch and opencv. \r\n\r\nBasically, I'm reading an image,\r\n\r\ncv::Mat image, image_bgr;\r\nimage_bgr = cv::imread(\"FILENAME\");\r\ncvtColor(image_bgr, image, cv::COLOR_BGR2RGB);\r\n\r\n// converting it to float\r\ncv::Mat bgr[3]; // destination array\r\ncv::split(image, bgr);\r\ncv::Mat channelsConcatenated;\r\nvconcat(bgr[0], bgr[1], channelsConcatenated);\r\nvconcat(channelsConcatenated, bgr[2], channelsConcatenated);\r\ncv::Mat channelsConcatenatedFloat;\r\nchannelsConcatenated.convertTo(channelsConcatenatedFloat, 21);\r\n\r\nstd::vector<int64_t> dims{ 1, static_cast<int64_t>(image.channels()),\r\n\t\t\t\t\t\t\t  static_cast<int64_t>(image.rows),\r\n\t\t\t\t\t\t\t  static_cast<int64_t>(image.cols) };\r\n\r\n// creating my tensor\r\nauto options = torch::TensorOptions().dtype(torch::kFloat).layout(torch::kStrided).device(torch::kCPU);\r\nat::Tensor tensor_image =\r\n\t\ttorch::from_blob(channelsConcatenatedFloat.data, at::IntList(dims),options);\r\n\r\n\r\n// Then I'm performing the inverse transform, and I want to see the image in an opencv window. \r\n cv::Mat testTensorInit(cv::Size(700, 395), 21,tensor_image.clone().squeeze(0).to(at::kByte).data<float>());\r\n\r\nBut it always crashing. Do you have any suggestion?\r\n\r\nIf convert to uint8_t it works, however the image result is not correct due to the number of bytes. \r\nPlease see the result. I'm trying to solve the issue, but the available information is limited. \r\n\r\n![veins](https://user-images.githubusercontent.com/44382268/57025917-6594ee80-6c30-11e9-9095-4acb9d16cafc.PNG)\r\n\r\n\r\n\r\n"}
{"number": 20012, "title": "Automatic update of fbcode/onnx to 7d7bc83d29a328233d3e8affa4c4ea8b3e3599ef", "time": "2019-05-01T15:54:01Z", "body": "Summary:\nPrevious import was f1311e74ec8a91cbf86094cd6f10157cbf00c536\n\nIncluded changes:\n- **[7d7bc83d](https://github.com/onnx/onnx/commit/7d7bc83d)**: fix shape inference (#1984) <Ashwini Khade>\n- **[68630bbd](https://github.com/onnx/onnx/commit/68630bbd)**: fixing some of Mod test cases (#1962) <Jeff Saremi>\n\nReviewed By: zrphercule\n\nDifferential Revision: D15160934\n\n"}
{"number": 20013, "title": "Update TensorBoard docs to specify data type (#19959)", "time": "2019-05-01T16:06:07Z", "body": "Merge doc changes from https://github.com/pytorch/pytorch/pull/19959 to v1.1.0\r\n\r\ncc @zou3519 @lanpa @soumith "}
{"number": 20014, "title": "Run clang-format on c10d bits", "time": "2019-05-01T16:15:14Z", "body": "Enable the clang-format checker as a Travis rule."}
{"number": 20015, "title": "Canonicalize order of If and Loop outputs", "time": "2019-05-01T16:25:59Z", "body": "Canonicalize the ordering of outputs of if and loop nodes based on their first usage. Previously we were able to canonicalize output order by sorting on variable name, but this breaks down with outputs added in an early return pass."}
{"number": 20016, "title": "Benchmark repeat op.", "time": "2019-05-01T16:38:22Z", "body": "Summary: PT's repeat op benchmark\n\nDifferential Revision: D15166941\n\n"}
{"number": 20017, "title": "torch.jit.trace returns unwrapped C type", "time": "2019-05-01T16:59:02Z", "body": "## üêõ Bug\r\n\r\n### With pytorch 1.1.0\r\n\r\n```\r\nimport torch\r\ndef f(x):\r\n  return x*2\r\nz = torch.jit.trace(f, (torch.zeros(10),))\r\nprint(type(z))\r\n```\r\n\r\nReturns `torch._C.Function` and cannot be saved:\r\n\r\n`Traceback (most recent call last):\r\n  File \"1.py\", line 7, in <module>\r\n    torch.jit.save(z, \"filename\")\r\n  File \"/home/vitalyf/local/miniconda/envs/wp_1/lib/python3.7/site-packages/torch/jit/__init__.py\", line 198, in save\r\n    m.save(f, _extra_files=_extra_files)\r\nAttributeError: 'torch._C.Function' object has no attribute 'save'\r\n`\r\n\r\n### Expected as in pytorch 1.0.1\r\n\r\n```\r\nimport torch\r\ndef f(x):\r\n  return x*2\r\nz = torch.jit.trace(f, (torch.zeros(10),))\r\nprint(type(z))\r\n```\r\n\r\nReturns `<class 'torch.jit.TopLevelTracedModule'>`\r\n\r\n"}
{"number": 20018, "title": "Add proper __repr__ to LogSoftMax", "time": "2019-05-01T17:13:20Z", "body": "Fixes #19961\r\n\r\n"}
{"number": 20019, "title": "Product of ones is zero on cuda", "time": "2019-05-01T17:45:19Z", "body": "## üêõ Bug\r\n\r\nProduct of ones is zero on cuda. This seems like a rather common issue, but i found no mentions of it here after a brief keyword search.\r\n\r\n## To reproduce\r\n```\r\nimport torch\r\nprint(torch.__version__)\r\nprint(torch.prod(torch.ones(10 ** 6, device='cuda')))\r\n```\r\nprints\r\n```\r\n1.0.1.post2\r\ntensor(0., device='cuda:0')\r\n```\r\n\r\n## Expected behavior\r\nI'd expect it to be 1 :)\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.1 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.2) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: \r\nGPU 0: Tesla M40\r\nGPU 1: Tesla M40\r\nGPU 2: Tesla M40\r\nGPU 3: Tesla M40\r\n\r\nNvidia driver version: 396.37\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.21\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.4\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.3\r\n[pip] numpydoc==0.7.0\r\n[pip] torch==1.0.1.post2\r\n[pip] torchvision==0.2.1\r\n[conda] blas                      1.0                         mkl  \r\n[conda] faiss-gpu                 1.4.0           py36_cuda8.0.61_1    pytorch\r\n[conda] mkl                       2018.0.1             h19d6760_4  \r\n[conda] mkl-service               1.1.2            py36h17a0993_4  \r\n[conda] pytorch                   1.0.1           py3.6_cuda8.0.61_cudnn7.1.2_2    pytorch\r\n[conda] torch                     1.0.1.post2               <pip>\r\n[conda] torchvision               0.2.1                      py_2    pytorch\r\n```\r\n\r\n## Additional context\r\nThis does not seem to be a floating point precision issue:\r\n```\r\nprint(torch.prod(torch.ones(130_560, device='cuda'), dim=0))\r\nprint(torch.prod(torch.ones(130_561, device='cuda'), dim=0))\r\n```\r\nprints\r\n```\r\ntensor(1., device='cuda:0')\r\ntensor(0., device='cuda:0')\r\n```"}
{"number": 20020, "title": "shape inference for learning rate op", "time": "2019-05-01T17:47:59Z", "body": "Summary: Add shape inference for LearningRate op. The output (lr) should have similar shape with input (iteration), but not the same type (float vs int).\n\nDifferential Revision: D15112300\n\n"}
{"number": 20021, "title": "Add ShapeInference for AtomicIter Op", "time": "2019-05-01T17:54:27Z", "body": "Summary: Add shape inference for AtomicIter operator. The operator takes two blobs iteration and iter_mutex as input and outputs iteration, which should have the same type and shape as the input.\n\nDifferential Revision: D15111643\n\n"}
{"number": 20022, "title": "Precision of sparse float embeddings differs from dense embeddings on CPU", "time": "2019-05-01T18:01:51Z", "body": "## üêõ Bug\r\n\r\nIn test/test_nn.py we skip 'backward' for low-precision types (float, half) because the precision is often too low to get reliable results on large embeddings. The same test doesn't fail for dense embeddings. There's a limit to how much precision we can expect with float and half types, but it would be preferable if they were consistent or if the difference was clearer.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nin test/test_nn.py run this with test_backward=True and dtype=torch.float.\r\nself._test_EmbeddingBag(False, 'sum', True, test_backward=test_backward, dtype=dtype)\r\n\r\nRun a number of times and it will occasionally fail. With the third parameter (sparse) set to False, we don't see failures.\r\n\r\n## Expected behavior\r\n\r\nLimitations on precision are consistent between sparse and dense implementations of Embedding/EmbeddingBag.\r\n\r\n## Environment\r\n\r\n[bvaughan@devgpu005.ash6 ~/repos/pytorch] ./collect_env.sh\r\nbash: ./collect_env.sh: No such file or directory\r\n[bvaughan@devgpu005.ash6 ~/repos/pytorch] python ./collect_env.py\r\nCollecting environment information...\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: N/A\r\nCUDA runtime version: 9.2.88\r\nGPU models and configuration:\r\nGPU 0: Tesla M40\r\nGPU 1: Tesla M40\r\nGPU 2: Tesla M40\r\nGPU 3: Tesla M40\r\nGPU 4: Tesla M40\r\nGPU 5: Tesla M40\r\nGPU 6: Tesla M40\r\nGPU 7: Tesla M40\r\n\r\nNvidia driver version: 396.69\r\ncuDNN version: /usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn.so.7.4.2\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.4\r\n[pip3] numpydoc==0.8.0\r\n[pip3] torch==1.1.0a0+3900816\r\n[pip3] torchvision==0.2.1\r\n[conda] magma-cuda92              2.4.0                         1    pytorch\r\n[conda] mkl                       2019.1                      144\r\n[conda] mkl-include               2019.1                      144\r\n[conda] mkl-service               1.1.2            py37h90e4bf4_5\r\n[conda] mkl_fft                   1.0.4            py37h4414c95_1\r\n[conda] mkl_random                1.0.1            py37h4414c95_1\r\n[conda] mkldnn                    0.16.1                        0    mingfeima\r\n[conda] torch                     1.0.0a0+aaf6e36           <pip>\r\n[conda] torch                     1.1.0a0+0676ba0           <pip>\r\n[conda] torch                     1.0.0a0+c2f1811           <pip>\r\n[conda] torch                     1.0.0a0+e387d94           <pip>\r\n[conda] torch                     1.0.0a0+298b775           <pip>\r\n[conda] torch                     1.0.0a0+8de9564           <pip>\r\n[conda] torch                     1.0.0a0+b15242f           <pip>\r\n[conda] torch                     1.0.0a0+df022f8           <pip>\r\n[conda] torch                     1.0.0a0+9c20546           <pip>\r\n[conda] torch                     1.0.0a0+35a24a9           <pip>\r\n[conda] torch                     1.0.0a0+d4f9dbf           <pip>\r\n[conda] torch                     1.0.0a0+4a4cc13           <pip>\r\n[conda] torch                     1.0.0a0+e03136f           <pip>\r\n[conda] torch                     1.0.0a0+c715fcc           <pip>\r\n[conda] torch                     1.0.0a0+b8da44d           <pip>\r\n[conda] torch                     1.0.0a0+5c51f65           <pip>\r\n[conda] torch                     1.1.0a0+227c4e9           <pip>\r\n[conda] torch                     1.0.0a0+66a0447           <pip>\r\n[conda] torch                     1.0.0a0+fb8745e           <pip>\r\n[conda] torch                     1.0.0a0+a7445ad           <pip>\r\n[conda] torch                     1.0.0a0+6e0c5a8           <pip>\r\n[conda] torch                     1.1.0a0+71bdfe8           <pip>\r\n[conda] torch                     1.1.0a0+3900816           <pip>\r\n[conda] torch                     1.0.0a0+607094c           <pip>\r\n[conda] torch                     1.0.0a0+3ff7071           <pip>\r\n[conda] torch                     1.0.0a0+24c43e2           <pip>\r\n[conda] torchvision               0.2.1                     <pip>\r\n[bvaughan@devgpu005.ash6 ~/repos/pytorch]\r\n\r\n\r\n## Additional context\r\n\r\nencountered while working on:\r\nhttps://github.com/pytorch/pytorch/pull/19695\r\n"}
{"number": 20023, "title": "Broken Documentation Strings for nn.MultiheadAttention", "time": "2019-05-01T18:47:14Z", "body": "## üìö Documentation\r\n\r\nhttps://pytorch.org/docs/stable/nn.html?highlight=attention#torch.nn.MultiheadAttention\r\n\r\nThe docstring for the inputs and outputs of nn.MultiheadAttention are broken since they are in the wrong format. They also light on description of each input.\r\n\r\n They display for inputs as,\r\nquery: [target length, batch size, embed dim] key: [sequence length, batch size, embed dim] value: [sequence length, batch size, embed dim] key_padding_mask: if True, mask padding based on batch size incremental_state: if provided, previous time steps are cashed need_weights: output attn_output_weights static_kv: key and value are static\r\n\r\nand for outputs as.\r\nattn_output: [target length, batch size, embed dim] attn_output_weights: [batch size, target length, sequence length]\r\n"}
{"number": 20024, "title": "New 'bool' type doesn't support inversion in 1.1.0", "time": "2019-05-01T19:54:20Z", "body": "```\r\n~torch.tensor(True).to(torch.bool)        \r\n```\r\n\r\nTypeError: ~ (operator.invert) is only implemented on byte tensors\r\n"}
{"number": 20025, "title": "Delete TensorImpl::GetDevice()", "time": "2019-05-01T19:57:27Z", "body": "Summary: Delete TensorImpl::GetDevice() and clean all its call sites.\n\nDifferential Revision: D15170917\n\n"}
{"number": 20026, "title": "Remove warnings on new_* constructors", "time": "2019-05-01T20:27:16Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* **#20026 Remove warnings on new_* constructors**\r\n\r\nRevert of #16770, fixes #19995\nDifferential Revision: [D15171691](https://our.internmc.facebook.com/intern/diff/15171691/)"}
{"number": 20027, "title": "Make IValue(\"string\") do the right thing", "time": "2019-05-01T20:53:53Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20027 Make IValue(\"string\") do the right thing**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15172409/)\n\nBefore this change, any pointer was converted to bool, i.e.\n\n    IValue(\"string\") == IValue(true)\n\nAfter this change, it does the right thing and creates a string.\n\nDifferential Revision: [D15172409](https://our.internmc.facebook.com/intern/diff/D15172409/)"}
{"number": 20028, "title": "CosineAnnealingWarmRestarts documentation poor and not appearing", "time": "2019-05-01T20:55:06Z", "body": "## üìö Documentation\r\n\r\nThe documentation for the newly introduced `CosineAnnealingWarmRestarts` learning rate scheduler (#17226) does not appear on the website (see [here](https://pytorch.org/docs/stable/optim.html); the location where it should be).\r\n\r\nFurthermore, looking at the [source code of the function](https://github.com/pytorch/pytorch/blob/master/torch/optim/lr_scheduler.py#L699), I am very confused about how to use the scheduler.\r\n\r\nSpecifically, if we look at the documentation of `.step()`, we first see a clear typo, where `CosineAnnealingWarmRestarts` is called `SGDR`. But the provided example is not sufficient to use as a reference. Where did the value 26 come from and why, and what does it have to do with 10 iterations? Also, [the paper](https://arxiv.org/abs/1608.03983) on which this scheduler is based states that the LR should be updated every _batch_, not epoch. I believe the statement in the documentation `SGDR.step(0.1), SGDR.step(0.2)` is referring to this, but I am lost in the example.\r\n\r\nIf I try to ignore the docstring and look at the source code, my best guess is that the correct message for the docstring in `.step` should be something like:\r\n\r\n```python\r\n\"\"\"Step should be called after every time a batch is processed in addition to after each epoch,\r\n   like in the following example:\r\n\r\n         >>> scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult)\r\n         >>> for epoch in range(N):\r\n         >>>     scheduler.step(epoch)\r\n         >>>     for inputs, labels in trainloader:\r\n         >>>         optimizer.zero_grad()\r\n         >>>         outputs = net(inputs)\r\n         >>>         loss = criterion(outputs, labels)\r\n         >>>         loss.backward()\r\n         >>>         optimizer.step()\r\n         >>>         scheduler.step()\r\n\"\"\"\r\n```\r\n\r\nIs this correct? Or have I misinterpreted how to apply this function?\r\n\r\nThanks for the great work and please let me know if I can help in some way."}
{"number": 20029, "title": "RuntimeError:CUDA Error:out of memory", "time": "2019-05-01T20:57:52Z", "body": "## üêõ Bug\r\n With a CUDA 10.1 and RTX GPU. \r\nWhen I try even the most simpliest code like\r\nnums = torch.randn(2,2).cuda(), it show the above error\r\nAnd torch.cuda.is_available() is True.\r\nAny idea?"}
{"number": 20030, "title": "I can't import PyTorch, libomp.dylib can't be loaded.", "time": "2019-05-01T21:19:45Z", "body": "## üêõ Bug\r\n\r\nI tried to install PyTorch, but I can't use it. It can't link dynamic library libomp.dylib\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. pipenv install torch torchvision\r\n2. `from torch.utils.data import Dataset` \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/kaspersapala/Documents/Airly/new-pollution-forecast/Datasets/__init__.py\", line 3, in <module>\r\n    from torch.utils.data import Dataset\r\n  File \"/Users/kaspersapala/.local/share/virtualenvs/new-pollution-forecast-BSetKF9E/lib/python3.7/site-packages/torch/__init__.py\", line 79, in <module>\r\n    from torch._C import *\r\nImportError: dlopen(/Users/kaspersapala/.local/share/virtualenvs/new-pollution-forecast-BSetKF9E/lib/python3.7/site-packages/torch/_C.cpython-37m-darwin.so, 9): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\r\n  Referenced from: /Users/kaspersapala/.local/share/virtualenvs/new-pollution-forecast-BSetKF9E/lib/python3.7/site-packages/torch/lib/libshm.dylib\r\n  Reason: image not found\r\n```\r\n\r\n## Expected behavior\r\n\r\nBeing able to use and import PyTorch\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.1.0\r\n - OS (e.g., Linux): Mac OS Mojave \r\n - How you installed PyTorch (`conda`, `pip`, source): pipenv\r\n - Python version: 3.7.1\r\n"}
{"number": 20031, "title": "Cannot convert PyTorch to ONNX using Pytorch 1.1 and ONNX 1.5.0", "time": "2019-05-01T22:03:50Z", "body": "## üêõ Bug\r\n\r\nCannot convert PyTorch to ONNX using the latest PyTorch and ONNX version\r\n\r\nSystem Config:\r\n```\r\npytorch                   1.1.0           py2.7_cuda10.0.130_cudnn7.5.1_0    pytorch\r\ntorchvision               0.2.2                      py_2    pytorch\r\ncudatoolkit               10.0.130                      0  \r\n```\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Use Ubuntu 16.04\r\n2. Install latest PyTorch conda package as mentioned on official website\r\n3. pip install onnx==1.5.0\r\n\r\nRun the following:\r\n\r\npytorch_export.py:\r\n```\r\nimport os\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.optim as optim\r\nfrom torchvision import datasets, transforms\r\nfrom torch.autograd import Variable\r\nimport torch.onnx as torch_onnx\r\n\r\nclass Model(nn.Module):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.conv = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3,3), stride=1, padding=0, bias=False)\r\n\r\n    def forward(self, inputs):\r\n        x = self.conv(inputs)\r\n        return torch.mean(x, dim=2)\r\n\r\ninput_shape = (3, 100, 100)\r\n\r\ncurr_dir = os.path.dirname(__file__)\r\nmodel_onnx_path = curr_dir + \"/pytorch_model.onnx\"\r\n\r\nmodel = Model()\r\nmodel.train(False)\r\n\r\ndummy_input = Variable(torch.randn(1, *input_shape))\r\ntorch_onnx.export(model, \r\n                  dummy_input, \r\n                  model_onnx_path, \r\n                  verbose=True)\r\n\r\n```\r\n\r\nverify_onnx_model.py\r\n```\r\nimport os\r\n\r\nimport onnx\r\n\r\ncurr_dir = os.path.dirname(__file__)\r\nmodel_proto = onnx.load(curr_dir + \"/pytorch_model.onnx\")\r\nonnx.checker.check_model(model_proto)\r\n\r\ngraph = model_proto.graph\r\ninputs = []\r\nfor i in graph.input:\r\n    inputs.append(i.name)\r\nassert inputs == ['0', '1']\r\n\r\nparams = []\r\nfor tensor_vals in graph.initializer:\r\n    params.append(tensor_vals.name)\r\nassert params == ['1']\r\n\r\nnodes = []\r\nfor node in graph.node:\r\n    nodes.append(node.op_type)\r\nassert nodes == ['Conv', 'ReduceMean']\r\n```\r\n\r\n```python pytorch_export.py```\r\n```python verify_onnx_model.py```\r\n\r\nFollowing error will be thrown:\r\n\r\n```\r\n++ python pytorch_export.py\r\ngraph(%0 : Float(1, 3, 100, 100),\r\n      %conv.weight : Float(32, 3, 3, 3)):\r\n  %2 : Float(1, 32, 98, 98) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%0, %conv.weight), scope: Model/Conv2d[conv]\r\n  %3 : Float(1, 32, 98) = onnx::ReduceMean[axes=[2], keepdims=0](%2), scope: Model\r\n  return (%3)\r\n\r\n+ pytho verify_onnx_model.py\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/src/bin/testScripts/onnx_pytorch/assets/verify_onnx_model.py\", line 13, in <module>\r\n    assert inputs == ['0', '1']\r\nAssertionError\r\n\r\n```\r\n\r\n## Expected behavior\r\n\r\nVerification step should pass with no errors. Observing this behavior since updating PyTorch to 1.1\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0):  1.1.0\r\n - OS (e.g., Linux): Ubuntu 16.04.6 LTS\r\n - How you installed PyTorch (`conda`, `pip`, source):  conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 2.7 & 3.6\r\n - CUDA/cuDNN version: 10.0.130\r\n - GPU models and configuration:\r\nGPU 0: Tesla K80\r\nGPU 1: Tesla K80\r\nGPU 2: Tesla K80\r\nGPU 3: Tesla K80\r\nGPU 4: Tesla K80\r\nGPU 5: Tesla K80\r\nGPU 6: Tesla K80\r\nGPU 7: Tesla K80\r\nGPU 8: Tesla K80\r\nGPU 9: Tesla K80\r\nGPU 10: Tesla K80\r\nGPU 11: Tesla K80\r\nGPU 12: Tesla K80\r\nGPU 13: Tesla K80\r\nGPU 14: Tesla K80\r\nGPU 15: Tesla K80\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n\r\n"}
{"number": 20032, "title": "Port THNN to ATen/Parallel", "time": "2019-05-01T22:07:59Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #20157 Support for Eigen thread pool\n* #20087 Native ATen/Parallel backend\n* #20057 Split ATen/Parallel into interface and backend\n* #20050 Move inter-op settings into ATen/Parallel\n* #20043 Port ATen/native to ATen/Parallel\n* **#20032 Port THNN to ATen/Parallel**\n\nSummary:\nRemove explicit usage of OpenMP from THNN\n\nTest Plan:\nBLAS=MKL USE_MKLDNN=1 USE_OPENCV=1 USE_FFMPEG=1 python setup.py develop --cmake\npytest -s -v test/test_torch.py::TestTorch\n\nDifferential Revision: [D15247960](https://our.internmc.facebook.com/intern/diff/D15247960)"}
{"number": 20033, "title": "Support operator overloading for UDT", "time": "2019-05-01T22:41:23Z", "body": "Support operator overloading for User Defined Types, which includes desugaring `a + b` and python builtin functions which call into a method if it is defined like `len(x)`. \r\n\r\nSee https://rszalski.github.io/magicmethods/ for list of magic methods. "}
{"number": 20034, "title": "Adding support for exporting models with variable length input/output to ONNX", "time": "2019-05-01T22:41:23Z", "body": "Proposal: https://gist.github.com/pk-g/cc45ff8c5891b5699bffd883a87f13ae?fbclid=IwAR17bRA7Fks4APoZRYiNa93UkLdoFCpRDuIYEx0lNVyPTyaDAShbEnytiQo"}
{"number": 20035, "title": "PyTorch Profiler Shape aggregation support", "time": "2019-05-01T22:50:52Z", "body": "Summary:\nThis is useful when you would like to understand performance\nbottlenecks of your model.  One can use the shape analysis in order to\nfit model to a roofline model of their hardware.\n\nPlease note that this feature can potentially skew profiling\nresults. Also timing for not nested events will become wrong. One\nshould only use timing for the bottom most events when shape analysis\nis used. Also for the case where people don't need shapes, profiling\nshould not be affected. As in this case we don't collect shapes, which\nis the default behavior and this diff doesn't change it.\n\nOne of the next steps\ncould be, for example, choosing best candidates for quantization. In\nthe scope of this diff I am just adding optional shapes collection\ninto the Even class. After that in python there is minor functionality\nfor providing groupping by shapes.\n\nIn the output tables shapes are being truncated but in groupping full\nshape string is used as a key.\n\nHere is an example output:\n\ntest_profiler_shapes (test_autograd.TestAutograd) ...\n```\n------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------\nName                Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     CUDA total %     CUDA total       CUDA time avg    Number of Calls  Input Shapes\n------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------\nunsigned short      2.30%            305.031us        2.30%            305.031us        305.031us        NaN              0.000us          0.000us          1                [[30, 20]]\naddmm               69.40%           9.199ms          69.40%           9.199ms          9.199ms          NaN              0.000us          0.000us          1                [[30], [128, 20], [20, 30], [], []]\nunsigned short      0.98%            129.326us        0.98%            129.326us        129.326us        NaN              0.000us          0.000us          1                [[40, 30]]\naddmm               27.32%           3.621ms          27.32%           3.621ms          3.621ms          NaN              0.000us          0.000us          1                [[40], [128, 30], [30, 40], [], []]\n------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------\nSelf CPU time total: 13.255ms\nCUDA time total: 0.000us\n\n------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------\nName                Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     CUDA total %     CUDA total       CUDA time avg    Number of Calls  Input Shapes\n------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------\nunsigned short      2.30%            305.031us        2.30%            305.031us        305.031us        NaN              0.000us          0.000us          1                [[30, 20]]\naddmm               69.40%           9.199ms          69.40%           9.199ms          9.199ms          NaN              0.000us          0.000us          1                [[30], [128, 20], [20, 30], [], []]\nunsigned short      0.98%            129.326us        0.98%            129.326us        129.326us        NaN              0.000us          0.000us          1                [[40, 30]]\naddmm               27.32%           3.621ms          27.32%           3.621ms          3.621ms          NaN              0.000us          0.000us          1                [[40], [128, 30], [30, 40], [], []]\n------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------\nSelf CPU time total: 13.255ms\nCUDA time total: 0.000us\n```\n\nAlso added this for older aggregation test:\n\n```\ntest_profiler_aggregation_lstm (test_autograd.TestAutograd) ...\n======================================================================================================================================================================================================\nTEST\n-----------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------\nName                     Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     CUDA total %     CUDA total       CUDA time avg    Number of Calls  Input Shapes\n-----------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------\nlstm                     0.69%            4.606ms          5.30%            35.507ms         35.507ms         NaN              0.000us          0.000us          1                [[5, 3, 10]]\nlstm                     0.67%            4.521ms          5.27%            35.340ms         35.340ms         NaN              0.000us          0.000us          1                [[5, 3, 10]]\nlstm                     0.66%            4.399ms          5.02%            33.638ms         33.638ms         NaN              0.000us          0.000us          1                [[5, 3, 10]]\nlstm                     0.65%            4.354ms          4.92%            32.958ms         32.958ms         NaN              0.000us          0.000us          1                [[5, 3, 10]]\nlstm                     0.65%            4.351ms          4.96%            33.241ms         33.241ms         NaN              0.000us          0.000us          1                [[5, 3, 10]]\nlstm                     0.65%            4.323ms          5.10%            34.163ms         34.163ms         NaN              0.000us          0.000us          1                [[5, 3, 10]]\nlstm                     0.64%            4.304ms          4.92%            32.938ms         32.938ms         NaN              0.000us          0.000us          1                [[5, 3, 10]]\nlstm                     0.64%            4.300ms          5.10%            34.172ms         34.172ms         NaN              0.000us          0.000us          1                [[5, 3, 10]]\nlstm                     0.64%            4.292ms          5.05%            33.828ms         33.828ms         NaN              0.000us          0.000us          1                [[5, 3, 10]]\nlstm                     0.64%            4.263ms          4.98%            33.357ms         33.357ms         NaN              0.000us          0.000us          1                [[5, 3, 10]]\n-----------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------\nSelf CPU time total: 670.120ms\nCUDA time total: 0.000us\n\n-----------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------\nName                     Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     CUDA total %     CUDA total       CUDA time avg    Number of Calls  Input Shapes\n-----------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------\nsigmoid                  15.32%           102.647ms        15.32%           102.647ms        171.078us        NaN              0.000us          0.000us          600              [[3, 20]]\nmul                      15.20%           101.854ms        15.20%           101.854ms        169.757us        NaN              0.000us          0.000us          600              [[3, 20], [3, 20]]\nlstm                     12.74%           85.355ms         100.00%          670.120ms        33.506ms         NaN              0.000us          0.000us          20               [[5, 3, 10]]\naddmm                    11.16%           74.808ms         11.16%           74.808ms         249.361us        NaN              0.000us          0.000us          300              [[80], [3, 20], [20, 80], [], []]\ntanh                     9.89%            66.247ms         9.89%            66.247ms         165.617us        NaN              0.000us          0.000us          400              [[3, 20]]\nsplit                    6.42%            43.019ms         6.42%            43.019ms         215.095us        NaN              0.000us          0.000us          200              [[3, 80]]\nadd                      5.67%            38.020ms         5.67%            38.020ms         190.101us        NaN              0.000us          0.000us          200              [[3, 80], [3, 80], []]\nadd                      4.81%            32.225ms         4.81%            32.225ms         161.124us        NaN              0.000us          0.000us          200              [[3, 20], [3, 20], []]\naddmm                    3.79%            25.380ms         3.79%            25.380ms         253.796us        NaN              0.000us          0.000us          100              [[80], [3, 10], [10, 80], [], []]\nunsigned short           3.72%            24.925ms         3.72%            24.925ms         83.083us         NaN              0.000us          0.000us          300              [[80, 20]]\n-----------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------\nSelf CPU time total: 670.120ms\nCUDA time total: 0.000us\n\nTotal time based on python measurements:  691.366ms\nCPU time measurement python side overhead: 3.17%\nok\n```\n\nDifferential Revision: D15174987\n\n"}
{"number": 20036, "title": "Refactor Tests for Multiple ONNX Opsets", "time": "2019-05-01T23:01:25Z", "body": "Refactor tests for https://github.com/pytorch/pytorch/pull/19294."}
{"number": 20037, "title": "Slow distributed training", "time": "2019-05-02T00:02:19Z", "body": "## üêõ Bug\r\n\r\nDistributed training of the nightly build (1.1.0.dev20190501) is much slower (5x) than that of the stable build (1.0.0).\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Run the following code using \"python -m torch.distributed.launch --nproc_per_node=8 main.py\"\r\n2. On 8 V100s, the running time of the stable build is 20 sec, but that of the nightly build is 103 sec.\r\n\r\n```\r\nimport argparse                                                                                                                                                                                                                                     \r\nimport os\r\nimport time\r\nimport numpy as np\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.utils.data.distributed import DistributedSampler\r\nfrom torch.utils import data\r\nimport torch.optim as optim\r\n \r\nbatch_size = 8\r\nnum_gpus = int(os.environ['WORLD_SIZE'])\r\n \r\nclass ToyDataset(data.Dataset):\r\n    def __init__(self):\r\n        super(ToyDataset, self).__init__()\r\n \r\n    def __len__(self):\r\n        return 3840\r\n \r\n    def __getitem__(self, index):\r\n        return np.random.rand(3, 800, 800).astype('f'), np.random.randint(0, 20, size=(800, 800)).astype('f')\r\n \r\n \r\nclass ToyNet(torch.nn.Module):\r\n    def __init__(self):\r\n        super(ToyNet, self).__init__()\r\n        self.conv = torch.nn.Conv2d(3, 32, 1)\r\n        self.final_conv = torch.nn.Conv2d(32, 20, 1)\r\n \r\n    def forward(self, x):\r\n        x = self.conv(x)\r\n        x = self.final_conv(x)\r\n        return x\r\n \r\n \r\ndef Main():\r\n    print('Pytorch version: {}'.format(torch.__version__))\r\n \r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--local_rank\", type=int)\r\n    args = parser.parse_args()\r\n \r\n    torch.distributed.init_process_group(backend='nccl')\r\n \r\n    model = ToyNet()\r\n    loss_function = nn.CrossEntropyLoss()\r\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\r\n \r\n    device = torch.device('cuda', args.local_rank)\r\n    model = model.to(device)\r\n    distrib_model = torch.nn.parallel.DistributedDataParallel(model,\r\n                                                              device_ids=[args.local_rank],\r\n                                                              output_device=args.local_rank)\r\n     \r\n    train_set = ToyDataset()\r\n     \r\n    sampler = DistributedSampler(train_set)\r\n    dataloader = data.DataLoader(\r\n                     dataset=train_set,\r\n                     batch_size=batch_size // num_gpus,\r\n                     sampler=sampler)\r\n     \r\n    start_time = time.time()\r\n    distrib_model.train()\r\n    for batch_idx, (inputs, labels) in enumerate(dataloader):\r\n        predictions = distrib_model(inputs.to(device))\r\n        loss = loss_function(predictions, labels.long().to(device))\r\n        loss.backward()\r\n        optimizer.step()\r\n        #print('batch_idx: {}'.format(batch_idx))\r\n    end_time = time.time()\r\n    print('Total time: {}'.format(end_time-start_time))\r\n     \r\n     \r\nif __name__ == \"__main__\":\r\n    Main()  \r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nAlmost the same speed.\r\n\r\n## Environment\r\n\r\nStable build\r\n```\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: Tesla V100-SXM2-16GB\r\nGPU 1: Tesla V100-SXM2-16GB\r\nGPU 2: Tesla V100-SXM2-16GB\r\nGPU 3: Tesla V100-SXM2-16GB\r\nGPU 4: Tesla V100-SXM2-16GB\r\nGPU 5: Tesla V100-SXM2-16GB\r\nGPU 6: Tesla V100-SXM2-16GB\r\nGPU 7: Tesla V100-SXM2-16GB\r\n\r\nNvidia driver version: 410.79\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.1\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.15.1\r\n[pip] numpydoc==0.8.0\r\n[pip] torch==1.0.0\r\n[pip] torchvision==0.2.1\r\n[conda] blas                      1.0                         mkl\r\n[conda] cuda100                   1.0                           0    pytorch\r\n[conda] mkl                       2018.0.3                      1\r\n[conda] mkl-service               1.1.2            py37h90e4bf4_5\r\n[conda] mkl_fft                   1.0.4            py37h4414c95_1\r\n[conda] mkl_random                1.0.1            py37h4414c95_1\r\n[conda] pytorch                   1.0.0           py3.7_cuda10.0.130_cudnn7.4.1_1  [cuda100]  pytorch\r\n[conda] torchvision               0.2.1                      py_2    pytorch\r\n```\r\n\r\nNightly Build\r\n```\r\nPyTorch version: 1.1.0.dev20190501\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: Tesla V100-SXM2-16GB\r\nGPU 1: Tesla V100-SXM2-16GB\r\nGPU 2: Tesla V100-SXM2-16GB\r\nGPU 3: Tesla V100-SXM2-16GB\r\nGPU 4: Tesla V100-SXM2-16GB\r\nGPU 5: Tesla V100-SXM2-16GB\r\nGPU 6: Tesla V100-SXM2-16GB\r\nGPU 7: Tesla V100-SXM2-16GB\r\n\r\nNvidia driver version: 410.79\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.1\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.3\r\n[pip] torch==1.1.0.dev20190501\r\n[pip] torchvision-nightly==0.2.3\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.3                      199\r\n[conda] mkl-service               1.1.2            py37he904b0f_5    anaconda\r\n[conda] mkl_fft                   1.0.10           py37ha843d7b_0\r\n[conda] mkl_random                1.0.2            py37hd81dba3_0\r\n[conda] pytorch-nightly           1.1.0.dev20190501 py3.7_cuda10.0.130_cudnn7.5.1_0    pytorch\r\n[conda] torchvision-nightly       0.2.3                     <pip>\r\n```\r\n\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"}
{"number": 20038, "title": "[tensorboard] Remove in-memory scalars and add comments", "time": "2019-05-02T00:14:20Z", "body": "This takes care of some outstanding review comments for https://github.com/pytorch/pytorch/pull/16196/\r\n\r\nSpecifically:\r\n1. Add comment about kind\r\n2. Add comment about GraphPy\r\n3. Remove ONNX version comment\r\n4. Remove scalar_dict from SummaryWriter and all history functions\r\n\r\ncc @lanpa @ezyang"}
{"number": 20039, "title": "[jit] dispatch and expose linear op", "time": "2019-05-02T00:46:39Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* #20284 make linear avaiable in tracing\r\n* **#20039 dispatch and expose linear op**\r\n* #19988 split canonicalize_ops, make a decompose pass\r\n* #19987 lower batchmm to non-diff optimization\r\n\r\nSummary:\r\nThis expose the linear directly when we called the functional interface, the ATen linear op already do the same thing with the functional interface, so there's really no need to duplicate the code. Also, this will expose the higher level `aten::linear` op up until the custom fusion, so that different backends could know the high level information\r\n\r\nTest Plan:\r\nTest the linear op is correctly decomposed in the decomposition pass, which also means it does not get decomposed before that pass. \r\n\r\n\r\nCurrently being blocked from landing by https://github.com/pytorch/pytorch/issues/19769 and https://github.com/pytorch/pytorch/issues/20734. \r\n\r\nDifferential Revision: [D15190354](https://our.internmc.facebook.com/intern/diff/D15190354)"}
{"number": 20040, "title": "Add the support of feature store example in pytorch model in fblearner", "time": "2019-05-02T00:52:22Z", "body": "Summary: Add the support of feature store example in fblearner pytorch predictor, end to end\n\nDifferential Revision: D15177897\n\n"}
{"number": 20041, "title": "Add quant-dequant nodes for weights", "time": "2019-05-02T01:04:07Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20041 Add quant-dequant nodes for weights**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15178086/)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/20041\n\nDifferential Revision: [D15178086](https://our.internmc.facebook.com/intern/diff/D15178086/)"}
{"number": 20042, "title": "[pt1][quant] Add torch.nn.quantized.functional namespace", "time": "2019-05-02T01:04:25Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20042 [pt1][quant] Add torch.nn.quantized.functional namespace**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15178099/)\n\nExposing torch.ops.quantized as torch.nn.quantized.functional\n\nDifferential Revision: [D15178099](https://our.internmc.facebook.com/intern/diff/D15178099/)"}
{"number": 20043, "title": "Port ATen/native to ATen/Parallel", "time": "2019-05-02T02:08:26Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #20157 Support for Eigen thread pool\n* #20087 Native ATen/Parallel backend\n* #20057 Split ATen/Parallel into interface and backend\n* #20050 Move inter-op settings into ATen/Parallel\n* **#20043 Port ATen/native to ATen/Parallel**\n* #20032 Port THNN to ATen/Parallel\n\nSummary:\nRemove explicit usage of OpenMP from ATen/native\n\nTest Plan:\nBLAS=MKL USE_MKLDNN=1 USE_OPENCV=1 USE_FFMPEG=1 python setup.py develop --cmake\npytest -s -v test/test_torch.py::TestTorch\n\nDifferential Revision: [D15248505](https://our.internmc.facebook.com/intern/diff/D15248505)"}
{"number": 20044, "title": "Add Gate Policy GateLearningRateOp", "time": "2019-05-02T02:10:22Z", "body": "Summary:\nWe do not have a gating functor. This diff adds it.\n* Since there are other policy in LearningRateOp which will be used as a union, I chose to add it as a LearningRateOp.\n\n* There are multiple uses for it,\n    * e.g. as a gating blob generator that is useful for turning off.\n   * e.g. as a learning rate switcher at certain iteration.\n* For generalizability, no regulation or constraint is applied on the range of value, nor on the range of base_lr.\n\n* see figure below for illustration\n\n{F157292712}\n\nDifferential Revision: D15178229\n\n"}
{"number": 20045, "title": "Add quant-dequant nodes for bias.", "time": "2019-05-02T02:38:43Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20045 Add quant-dequant nodes for bias.**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15179141/)\n\nThis pass adds quant-dequant nodes for bias. This pass requires\nquant-dequant pass for activations and weights to be done as it is required\nto compute the qparams for bias\n\nDifferential Revision: [D15179141](https://our.internmc.facebook.com/intern/diff/D15179141/)"}
{"number": 20046, "title": "Leaking Torch Files", "time": "2019-05-02T02:40:55Z", "body": "## Bug\r\n\r\nWhen I build from source and install to `/usr/local`, some headers leaks out to `/usr`.\r\n\r\n![image](https://user-images.githubusercontent.com/5203025/57054303-88cb9800-6c48-11e9-965c-8d404a5c5854.png)\r\n![image](https://user-images.githubusercontent.com/5203025/57054323-bfa1ae00-6c48-11e9-9181-844c28582d3f.png)\r\n\r\nThis issue may be caused by the following code:\r\nhttps://github.com/pytorch/pytorch/blob/725ef26f344588884f9f803d687711af5a7f300a/aten/src/ATen/core/CMakeLists.txt#L10-L20\r\nhttps://github.com/pytorch/pytorch/blob/725ef26f344588884f9f803d687711af5a7f300a/aten/src/ATen/CMakeLists.txt#L418-L422\r\n\r\n## To Reproduce\r\n\r\nSet `CMAKE_INSTALL_PREFIX`.\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): master\r\n - OS (e.g., Linux): CentOS-7\r\n - How you installed PyTorch (`conda`, `pip`, source): source\r\n - Build command you used (if compiling from source): cmake + ninja + gcc-8\r\n - Python version: 3.6\r\n"}
{"number": 20047, "title": "Libtorch's Interoperability with OpenGL/Cuda ", "time": "2019-05-02T03:38:04Z", "body": "## ‚ùì Questions and Help\r\nHello everyone, I want to convert tensor(at::kCuda) to OpenGL texture. But the IO between GPU and CPU is much too slow. Could libtorch interoperate with OpenGL texture or Cuda array in GPU layer?  \r\n\r\nI can not find solution in pytorch forum, so post it here for help.\r\n"}
{"number": 20048, "title": "Forking is not possible anymore when using any PyTorch version from about 2 months ago", "time": "2019-05-02T03:47:02Z", "body": "I have been building PyTorch from source in the past couple of months. Since around 1 (or maybe 2) month(s) ago I am unable to do forward passing to my models when using the `multiprocessing` package of PyTorch. If I'm not mistaken, the last version of PyTorch that I built and still allows me to use `fork`ing is [this](https://github.com/pytorch/pytorch/tree/bad4442) one; or more precisely, the version of PyTorch that I am using now and allows me to do `fork`ing is `1.1.0a0+3a01a45`.\r\n\r\nNot being able to do `fork`ing is pretty annoying cause I have written all of my work flow in a way that it depends on using `multiprocessing.Process` with `fork`ing. In one part of my work flow I start tens of processes and do forward passing on a model that is on the CPU memory. This is pretty useful and super efficient since the weights of the model are not being copied every time I start a new process. However, if I `spawn` processes the weights of the model needs to get copied and it's not gonna be efficient anymore. \r\n\r\nIn another part of my framework, I start a process while the model is still on CPU's memory and then do `model.cuda()` within the process and the model is then copied to the GPU memory. This is a bit inefficient but still allows me to start a 4-5 processes (depending on the GPU memory) and do forward-pass my inputs. \r\n\r\nHere's a pseudocode of what I do:\r\n\r\n```\r\nfrom torch.multiprocessing import Process\r\nmodel = loadModel() # somehow load a model (e.g. from torch vision)\r\ninputList = loadListOfInputs() # somehow get the list of input tensors\r\n\r\nprocesses = []\r\nfor i in range(100):\r\n    processes.append(Process(target=doForwardPass, kwargs={'input': inputList[i]}))\r\n    processes[-1].start()\r\n\r\nfor i in range(100):\r\n    processes[i].join()\r\n\r\ndef doForwardPass(input):\r\n    # model.cuda() # this, uncommented, used to work also\r\n    output = model(input)\r\n```\r\n\r\nWith the current versions of PyTorch (since 1-2 months ago) I cannot do this anymore. I'm afraid this is happening due to some indirect affects of some commits which did not intend to disable `fork`ing. So I wonder if you guys can either revert the changes that have caused this or enable this feature again.\r\n\r\nAlso, when using some of the PyTorch versions from the master branch (since 1-2 months ago) I might get the following if I do `model.cuda()` within an started process:\r\n\r\n`THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=51 error=3 : initialization error`\r\n\r\nThis might be somewhat relevant to [lazy initialization of CUDA](https://github.com/pytorch/pytorch/pull/2811)."}
{"number": 20049, "title": "profile on uses", "time": "2019-05-02T05:10:19Z", "body": ""}
{"number": 20050, "title": "Move inter-op settings into ATen/Parallel", "time": "2019-05-02T05:29:35Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #20480 Native TBB parallel backend\n* #20454 Restore TBB module\n* #20157 Support for Eigen thread pool\n* #20087 Native ATen/Parallel backend\n* #20057 Split ATen/Parallel into interface and backend\n* **#20050 Move inter-op settings into ATen/Parallel**\n\nSummary:\nMove inter-op settings into ATen/Parallel, abstract away explicit\nthread pool usage behind at::launch interface, python interface to\nsetting number of threads\n\nTest Plan:\nBLAS=MKL USE_MKLDNN=1 USE_OPENCV=1 USE_FFMPEG=1 python setup.py develop --cmake\n\n./build/bin/thread_init_test\n\npytest -s -v test/test_torch.py::TestTorch\n\nDifferential Revision: [D15248576](https://our.internmc.facebook.com/intern/diff/D15248576)"}
{"number": 20051, "title": "[wip] Remove symbolic variable part 2", "time": "2019-05-02T06:31:12Z", "body": ""}
{"number": 20052, "title": "SegFault and other errors on instantiating subclass of torch.FloatTensor and torch.Tensor", "time": "2019-05-02T06:45:41Z", "body": "## üêõ Bug\r\n\r\nInstantiating a subclass of `torch.FloatTensor`, `torch.ByteTensor`, or `torch.BoolTensor` causes a **segmentation fault**, a `c10::Error: Unrecognized Scalartype UNKNOWN_SCALAR`, and/or other odd errors.\r\n\r\nInstantiating a subclass of `torch.Tensor` with parameters and overriding `__init__` fails with a `TypeError`.\r\n\r\n## To Reproduce\r\n\r\nRun the following 4 lines on a fresh python REPL (or some variant):\r\n```python\r\nimport torch\r\nclass MyFloatTensor(torch.FloatTensor):\r\n    pass\r\nMyFloatTensor()\r\n```\r\n\r\nWith python2.7 and pytorch 1.1.0, I get\r\n`Segmentation fault (core dumped)`\r\n\r\n\r\nWith python3.5.2 and pytorch 1.1.0, it's a bit less predictable.\r\n\r\nSometimes I get a `RuntimeError`:\r\n```python\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-6-57316e947464> in <module>\r\n----> 1 MyFloatTensor()\r\n\r\nRuntimeError: Unknown backend\r\n```\r\n\r\nSometimes, I get a segmentation fault:\r\n```Segmentation fault (core dumped)```\r\n\r\nSometimes I get a `c10::Error`:\r\n```\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  Unrecognized Scalartype UNKNOWN_SCALAR (please report this error) (scalarTypeToTypeMeta at /pytorch/c10/core/ScalarType.h:136)\r\nframe #0: std::function<std::string ()>::operator()() const + 0x11 (0x7fb12efc9441 in /usr/people/myusername/python3env/lib/python3.5/site-packages/torch/lib/libc10.so)\r\nframe #1: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x2a (0x7fb12efc8d7a in /usr/people/myusername/python3env/lib/python3.5/site-packages/torch/lib/libc10.so)\r\nframe #2: <unknown function> + 0x2fb757 (0x7fb16e493757 in /usr/people/myusername/python3env/lib/python3.5/site-packages/torch/lib/libtorch_python.so)\r\nframe #3: <unknown function> + 0x3caee6 (0x7fb16e562ee6 in /usr/people/myusername/python3env/lib/python3.5/site-packages/torch/lib/libtorch_python.so)\r\nframe #4: torch::utils::legacy_tensor_ctor(at::Type const&, c10::ScalarType, _object*, _object*) + 0x210 (0x7fb16e6c0000 in /usr/people/myusername/python3env/lib/python3.5/site-packages/torch/lib/libtorch_python.so)\r\nframe #5: <unknown function> + 0x510d8b (0x7fb16e6a8d8b in /usr/people/myusername/python3env/lib/python3.5/site-packages/torch/lib/libtorch_python.so)\r\nframe #6: /usr/people/myusername/python3env/bin/python3.5() [0x57efe5]\r\nframe #7: PyObject_Call + 0x47 (0x5c1797 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #8: PyEval_EvalFrameEx + 0x4ec6 (0x53bba6 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #9: /usr/people/myusername/python3env/bin/python3.5() [0x540199]\r\nframe #10: PyEval_EvalCode + 0x1f (0x540e4f in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #11: /usr/people/myusername/python3env/bin/python3.5() [0x54a7c5]\r\nframe #12: PyCFunction_Call + 0x4f (0x4e9b7f in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #13: PyEval_EvalFrameEx + 0x614 (0x5372f4 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #14: _PyGen_Send + 0x133 (0x4ed7d3 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #15: PyEval_EvalFrameEx + 0x5ce5 (0x53c9c5 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #16: _PyGen_Send + 0x133 (0x4ed7d3 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #17: PyEval_EvalFrameEx + 0x5ce5 (0x53c9c5 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #18: _PyGen_Send + 0x133 (0x4ed7d3 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #19: PyEval_EvalFrameEx + 0x4ce6 (0x53b9c6 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #20: PyEval_EvalFrameEx + 0x4b04 (0x53b7e4 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #21: PyEval_EvalFrameEx + 0x4b04 (0x53b7e4 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #22: /usr/people/myusername/python3env/bin/python3.5() [0x540199]\r\nframe #23: PyEval_EvalFrameEx + 0x50b2 (0x53bd92 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #24: /usr/people/myusername/python3env/bin/python3.5() [0x540199]\r\nframe #25: PyEval_EvalFrameEx + 0x50b2 (0x53bd92 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #26: /usr/people/myusername/python3env/bin/python3.5() [0x540199]\r\nframe #27: PyEval_EvalFrameEx + 0x50b2 (0x53bd92 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #28: PyEval_EvalFrameEx + 0x4b04 (0x53b7e4 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #29: PyEval_EvalCodeEx + 0x13b (0x540f9b in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #30: /usr/people/myusername/python3env/bin/python3.5() [0x4ebe37]\r\nframe #31: PyObject_Call + 0x47 (0x5c1797 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #32: PyEval_EvalFrameEx + 0x252b (0x53920b in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #33: /usr/people/myusername/python3env/bin/python3.5() [0x540199]\r\nframe #34: PyEval_EvalFrameEx + 0x50b2 (0x53bd92 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #35: /usr/people/myusername/python3env/bin/python3.5() [0x540199]\r\nframe #36: PyEval_EvalCode + 0x1f (0x540e4f in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #37: /usr/people/myusername/python3env/bin/python3.5() [0x60c272]\r\nframe #38: PyRun_FileExFlags + 0x9a (0x60e71a in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #39: PyRun_SimpleFileExFlags + 0x1bc (0x60ef0c in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #40: Py_Main + 0x456 (0x63fb26 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #41: main + 0xe1 (0x4cfeb1 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #42: __libc_start_main + 0xf0 (0x7fb1e860b830 in /lib/x86_64-linux-gnu/libc.so.6)\r\nframe #43: _start + 0x29 (0x5d6049 in /usr/people/myusername/python3env/bin/python3.5)\r\n\r\nAborted (core dumped)\r\n```\r\n\r\nAnd occasionally, it just runs without crashing and outputs\r\n```\r\ntensor([], dtype=torch.uint8)\r\n```\r\nwhich is almost what I would expect except that this is a `ByteTensor` and not a `FloatTensor`.\r\n\r\nEach time I run it, I get a different one of these errors, with no discernible pattern. Sometimes running it two times in a row gets the same resulting error, sometimes not.\r\n\r\nThe same behavior occurs with other variants including\r\n- replacing `torch.FloatTensor` with another tensor datatype such as `torch.ByteTensor` and `torch.BoolTensor`\r\n- passing parameters to the instantiation.\r\n- adding a body to the class definition\r\n\r\nWhile I encountered/produced all these errors while running on a single machine (environment specification below), I have tried this on other machines as well and I get similarly erratic behavior, including on Google Colaboratory.\r\n\r\n-----------------\r\n\r\nA different issue occurs when instantiating a subclass of `torch.Tensor`.\r\n\r\nRunning the following code:\r\n```python\r\nimport torch\r\nclass MyTensor(torch.Tensor):\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\nMyTensor(2)\r\n```\r\nproduces a `TypeError`:\r\n```python\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-c115513591af> in <module>\r\n----> 1 MyTensor(2)\r\n\r\n<ipython-input-2-0187aa9fddd0> in __init__(self, *args, **kwargs)\r\n      1 class MyTensor(torch.Tensor):\r\n      2     def __init__(self, *args, **kwargs):\r\n----> 3         super().__init__(*args, **kwargs)\r\n\r\nTypeError: object.__init__() takes no parameters\r\n```\r\nThis error occurs for any nonzero number of parameters.\r\nNote that just `MyTensor()` works fine, as it does when I do not override `__init__`. But this is overriding `__init__` with its superclass's `__init__`, which should do nothing, and yet it causes it to fail. I understand that the `torch.Tensor` class is defined in the C backend, and that it only uses the `__new__` method to initialize. However, the `__init__` method of `torch.Tensor` should at least swallow its arguments before passing them on to `object` so that overriding `__init__` doesn't have to avoid calling its superclass (which is messy to deal with).\r\n\r\n## Expected behavior\r\n\r\nRunning\r\n```python\r\nimport torch\r\nclass MyFloatTensor(torch.FloatTensor):\r\n    pass\r\nMyFloatTensor()\r\n```\r\nshould instantiate a `MyFloatTensor` as subclass of a `torch.FloatTensor` and should output the same as calling\r\n```python\r\n>>> torch.FloatTensor()\r\ntensor([])\r\n```\r\n\r\nRunning\r\n```python\r\nimport torch\r\nclass MyTensor(torch.Tensor):\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\nMyTensor(2)\r\n```\r\nshould instantiate a 2-element `MyTensor` as subclass of a `torch.Tensor` and should output the same as calling\r\n```python\r\n>>> torch.Tensor(2)\r\ntensor([6.5783e-31, 4.5766e-41])\r\n```\r\n\r\nIt's possible that pytorch tensor types weren't intended to be extended, which would be quite unfortunate. I would argue that it should be possible to subclass them, and doing so would be quite useful for me in my research. But in any case, attempting to subclass them should never cause a Segmentation Fault, and that usually seems to be indicative of deeper problem.\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: Could not collect\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.1.85\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\nGPU 4: GeForce GTX 1080 Ti\r\nGPU 5: GeForce GTX 1080 Ti\r\nGPU 6: GeForce GTX 1080 Ti\r\nGPU 7: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 390.77\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.1\r\n[pip3] torch==1.1.0\r\n[pip3] torchfile==0.1.0\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] Could not collect\r\n```\r\n\r\n## Related GitHub Issues\r\nhttps://github.com/pytorch/pytorch/issues/17249 and https://github.com/pytorch/pytorch/issues/17716 are both related to subclassing PyTorch Tensors but don't mention these errors."}
{"number": 20053, "title": "RFC: accscalar_t for float on CPU", "time": "2019-05-02T06:46:32Z", "body": "## Issue\r\nCurrently `accscalar_t` for `float` is `double` on the CPU:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/master/aten/src/ATen/AccumulateType.h#L34\r\n\r\nI suggest to have a small discussion whether it'd be better to switch to float.\r\n\r\n## Motivation\r\n\r\nThis is prompted by @wanchaol asking about a [`UndefinedBehaviorSanitizer: float-cast-overflow`](https://gist.github.com/wanchaol/aceb0a1e8d3a93c8853689730b0f709f) error, but I think there are three possible reasons to consider changing this:\r\n- Consistency with GPU,\r\n- support platforms on which float is faster (e.g. arm32)\r\n- get rid of UB.\r\n\r\nI seem to recall @apaszke preferring the current behaviour a year ago or so back (in the context of #6855, which always dispatched a double auxilliary function on CPU or so).\r\n\r\n## Pitch\r\n\r\nSwitch to `float` generally.\r\n\r\n## Alternatives\r\n\r\nSwitch to `float` only for specific platforms (e.g. arm32),  somehow get rid of the UB warning.\r\n\r\n## Additional context\r\n\r\nWe (e.g. @ljk53 and me) might be interested in changing this for Android specifically if we don't generally.\r\n"}
{"number": 20054, "title": "Dockerfile submodule and unicode issue ", "time": "2019-05-02T07:18:40Z", "body": "## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\n#### 1. git clone missing\r\nI tried to build [Dockerfile](https://github.com/pytorch/pytorch/blob/master/docker/pytorch/Dockerfile).\r\nFirst, before `RUN git submodule update --init`, It is not fully functional independently.\r\n Is it true that the `git clone pytorch.git` was missing before `WORKDIR /opt/pytorch`?\r\n\r\n```shell\r\nWORKDIR /opt/pytorch\r\nCOPY . . # I think this is \r\n\r\nRUN git submodule update --init\r\n```\r\n\r\nI already see https://github.com/pytorch/pytorch/pull/16698\r\nbut I want to know why is it closed\r\n\r\n#### 2. ENV LANG missing \r\nThere are some language configurare missing.\r\n```shell\r\n# See http://bugs.python.org/issue19846\r\nENV LANG C.UTF-8\r\n```\r\n\r\nCan I fix it rightly by contribution?\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n## Environment\r\n\r\n`Docker version 18.03.0-ce, build 0520e24302` Window 10\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"}
{"number": 20055, "title": "[Caffe2] Fix the spelling of \"context\"", "time": "2019-05-02T07:52:11Z", "body": ""}
{"number": 20056, "title": "Creation of too big multidimensional array returns empty tensor.", "time": "2019-05-02T08:38:45Z", "body": "## üêõ Bug\r\n\r\nWhen trying to create a tensor of too many dimensions it simply returns an empty tensor with its shape is the dimensions I passed even though it should have contained the value `1`.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nNumber of 2's in the following example is `>63`.\r\n```Python\r\ntorch.ones((2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2),\r\ndtype=torch.uint8)\r\n```\r\nOutput: \r\n\r\n```Python\r\ntensor([],\r\n       size=(2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2),\r\n       dtype=torch.uint8)\r\n```\r\n\r\nAnother related issue\r\n\r\n```Python\r\ntorch.ones((2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2),\r\ndtype=torch.uint8)\r\n```\r\n\r\nThese are `62` 2's. \r\n\r\nOutput: \r\n\r\n```Python\r\nRuntimeError: $ Torch: not enough memory: you tried to allocate 0GB. Buy new RAM! at /opt/conda/conda-bld/pytorch_1549628766161/work/aten/src/TH/THGeneral.cpp:201\r\n```\r\n\r\nWhen they're `63` 2's. The output changes to:\r\n\r\n```Python\r\nRuntimeError: $ Torch: invalid memory size -- maybe an overflow? at /opt/conda/conda-bld/pytorch_1549628766161/work/aten/src/TH/THGeneral.cpp:188\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nI expected to see the aforementioned dimensions filled with `1` if it's possible, or an exception that says a value error.\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n```\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Arch Linux\r\nGCC version: (crosstool-NG 1.23.0.449-a04d0) 7.3.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.105\r\nGPU models and configuration: GPU 0: GeForce GTX 1050 Ti with Max-Q Design\r\nNvidia driver version: 418.56\r\ncuDNN version: /usr/lib/libcudnn.so.7.5.0\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.2\r\n[pip3] torch==1.0.1.post2\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] _tflow_select             2.3.0                       mkl  \r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.1                      144  \r\n[conda] mkl_fft                   1.0.10           py36ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py36hd81dba3_0  \r\n[conda] pytorch                   1.0.1           py3.6_cuda9.0.176_cudnn7.4.2_2    pytorch\r\n[conda] tensorflow                1.12.0          mkl_py36h69b6ba0_0  \r\n[conda] tensorflow-base           1.12.0          mkl_py36h3c3e929_0  \r\n[conda] torchvision               0.2.1                      py_2    pytorch\r\n```\r\n## Additional context\r\n\r\nI suspect that the desired behavior is not happening because tensors seem to be basing on NumPy `ndarray` which accepts only `32` dims.\r\n\r\nAlso, I have just found this [line](https://github.com/pytorch/pytorch/blob/7ddd5d06ed07a50b94aa6b2fdffa2f667d677c4b/aten/src/TH/THGeneral.cpp#L184).\r\n\r\n```cpp\r\nTHError(\"$ Torch: not enough memory: you tried to reallocate %dGB. Buy new RAM!\", size/1073741824);\r\n```\r\n\r\nI think the integer division of those two elements results in the error. "}
{"number": 20057, "title": "Split ATen/Parallel into interface and backend", "time": "2019-05-02T09:10:53Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* #20157 Support for Eigen thread pool\r\n* #20087 Native ATen/Parallel backend\r\n* **#20057 Split ATen/Parallel into interface and backend**\r\n* #20050 Move inter-op settings into ATen/Parallel\r\n\r\nSummary:\r\nSplit into ATen/Parallel into backend-independent part and\r\nspecific implementations (currently - OpenMP and native thread pool).\r\nAlso adding cmake option to choose corr. backend\r\n\r\nTest Plan:\r\nPARALLEL_BACKEND=OPENMP BLAS=MKL USE_MKLDNN=1 USE_OPENCV=1 USE_FFMPEG=1\r\npython setup.py develop --cmake\r\n\r\nDifferential Revision: [D15248618](https://our.internmc.facebook.com/intern/diff/D15248618)"}
{"number": 20058, "title": "Implement noise_shape keyword for Dropout layers", "time": "2019-05-02T12:11:20Z", "body": "## üöÄ Feature\r\nTensorflow has a noise_shape keyword for tf.nn.dropout, which specifies which dimensions should have dropout masks calculated independently and which dimensions should have shared dropout masks. PyTorch should have a similar feature too.\r\n\r\n## Motivation\r\n\r\nThis is a very useful feature to have (for example, if we process the same example multiple times, we may want to tie dropout masks for each time we see that example), and while easy to implement independently, would be good to have common functionality for.\r\n\r\n## Pitch\r\n\r\nInclude extra kwarg to torch.nn.Dropout called ``noise_shape``, with the same functionality as tf.nn.dropout.\r\n\r\n## Alternatives\r\n\r\nFairly easy to implement independently, but would save hassle for many people if there was a core implementation.\r\n"}
{"number": 20059, "title": "Initialize last_epoch in _LRScheduler.__init__()", "time": "2019-05-02T13:23:47Z", "body": "Class attributes preferably be explicitly initiated within\r\nthe __init__() call. Otherwise, overriding step() is\r\nprone to bugs.\r\n\r\nThis patch partially reverts #7889\r\n\r\n"}
{"number": 20060, "title": "[JIT] LSTMCell still not speedup with 1.1.0", "time": "2019-05-02T13:50:32Z", "body": "I've tried the [template provided in fastrnn](https://github.com/pytorch/benchmark/blob/master/rnns/fastrnns/custom_lstms.py#L94-L120) with latest PyTorch, however, it seems the speedup still not there yet. Here is the benchmark code\r\n\r\n```python\r\nfrom torch import jit\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\ntorch.manual_seed(0)\r\n\r\nclass LSTMCell(jit.ScriptModule):\r\n    def __init__(self, input_size, hidden_size):\r\n        super(LSTMCell, self).__init__()\r\n        self.input_size = input_size\r\n        self.hidden_size = hidden_size\r\n        self.weight_ih = nn.Parameter(torch.randn(4 * hidden_size, input_size))\r\n        self.weight_hh = nn.Parameter(torch.randn(4 * hidden_size, hidden_size))\r\n        self.bias_ih = nn.Parameter(torch.randn(4 * hidden_size))\r\n        self.bias_hh = nn.Parameter(torch.randn(4 * hidden_size))\r\n\r\n    @jit.script_method\r\n    def forward(self, input, state):\r\n        # type: (Tensor, Tuple[Tensor, Tensor]) -> Tuple[Tensor, Tuple[Tensor, Tensor]]\r\n        hx, cx = state\r\n        gates = (torch.mm(input, self.weight_ih.t()) + self.bias_ih +\r\n                 torch.mm(hx, self.weight_hh.t()) + self.bias_hh)\r\n        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\r\n\r\n        ingate = torch.sigmoid(ingate)\r\n        forgetgate = torch.sigmoid(forgetgate)\r\n        cellgate = torch.tanh(cellgate)\r\n        outgate = torch.sigmoid(outgate)\r\n\r\n        cy = (forgetgate * cx) + (ingate * cellgate)\r\n        hy = outgate * torch.tanh(cy)\r\n\r\n        return hy, (hy, cy)\r\n    \r\nimport time\r\n\r\nx = torch.randn(64, 512)\r\nh = torch.zeros(64, 512)\r\nc = torch.zeros_like(h)\r\n\r\nrnn1 = LSTMCell(512, 512)\r\nrnn2 = nn.LSTMCell(512, 512)\r\n\r\nt = time.perf_counter()\r\nrnn1(x, (h, c))\r\nprint(time.perf_counter() - t)\r\n\r\nt = time.perf_counter()\r\nrnn2(x, (h, c))\r\nprint(time.perf_counter() - t)\r\n\r\n>>> 0.06589111499488354\r\n>>> 0.006260981783270836\r\n```\r\n\r\n```\r\n - PyTorch Version: 1.1.0\r\n - OS: Ubuntu 18.04\r\n - How you installed PyTorch: pip\r\n - Python version: 3.7\r\n```"}
{"number": 20061, "title": "Sparse Half Embedding backward/sum on cpu", "time": "2019-05-02T14:52:46Z", "body": "Adds minimal support for sparse half embedding on CPU (backward() and sum()). Previous PR only allowed sum().backward() of sparse half embedding on CUDA. We can add more operators if this is useful, but currently skips most CPU operator implementations that depend on blas or vec256 since they don't have float16 support.\r\n\r\nAfter 19695,\r\n```\r\n>>> a = torch.nn.Embedding(3, 4, sparse=True).half()\r\n>>> a(torch.LongTensor([1, 0])).backward(torch.ones(2,4).half())\r\n\r\n```\r\nGave: `RuntimeError: _th_index_select not supported on CPUType for Half`\r\nand `a(torch.LongTensor([1, 0]).sum()` gave an error that sum_cpu was not available.\r\n\r\nBuilds on: https://github.com/pytorch/pytorch/pull/19695\r\nDon't need to re-review commits before 29a72ea\r\n"}
{"number": 20062, "title": "Rearrange stopping condition in CompositeReader", "time": "2019-05-02T14:53:48Z", "body": "Summary:\nPreviously, the batch counter is incremented even if none of the readers has data. In this diff,\n1) Limiter is applied to the last reader so that the batch counter is not incremented unless the first N-1 readers have data\n2) The stop blob of the last reader as the stop blob of the task so that it's checked before the counter is incremented\n\nDifferential Revision: D15099761\n\n"}
{"number": 20063, "title": "disable flaky test_proper_exit again, still occasionally failing", "time": "2019-05-02T15:36:47Z", "body": "test was disabled for being flaky, re-enabled in https://github.com/pytorch/pytorch/pull/19421 but still occasionally failing:\r\n\r\nhttps://circleci.com/gh/pytorch/pytorch/1520165?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link\r\n\r\n```\r\nApr 29 19:51:58 ======================================================================\r\nApr 29 19:51:58 FAIL: test_proper_exit (__main__.TestDataLoader)\r\nApr 29 19:51:58 There might be ConnectionResetError or leaked semaphore warning (due to dirty process exit), but they are all safe to ignore\r\nApr 29 19:51:58 ----------------------------------------------------------------------\r\nApr 29 19:51:58 Traceback (most recent call last):\r\nApr 29 19:51:58   File \"/var/lib/jenkins/workspace/test/common_utils.py\", line 129, in wrapper\r\nApr 29 19:51:58     fn(*args, **kwargs)\r\nApr 29 19:51:58   File \"test_dataloader.py\", line 847, in test_proper_exit\r\nApr 29 19:51:58     self.fail(fail_msg + ', and had exception {}'.format(loader_p.exception))\r\nApr 29 19:51:58 AssertionError: test_proper_exit with use_workers=True, pin_memory=False, hold_iter_reference=False, exit_method=worker_kill: loader process did not terminate, and had exception Traceback (most recent call last):\r\nApr 29 19:51:58   File \"test_dataloader.py\", line 227, in run\r\nApr 29 19:51:58     super(ErrorTrackingProcess, self).run()\r\nApr 29 19:51:58   File \"/opt/python/2.7.9/lib/python2.7/multiprocessing/process.py\", line 114, in run\r\nApr 29 19:51:58     self._target(*self._args, **self._kwargs)\r\nApr 29 19:51:58   File \"test_dataloader.py\", line 424, in _test_proper_exit\r\nApr 29 19:51:58     for i, _ in enumerate(it):\r\nApr 29 19:51:58   File \"/opt/python/2.7.9/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 545, in __next__\r\nApr 29 19:51:58     idx, batch = self._get_batch()\r\nApr 29 19:51:58   File \"/opt/python/2.7.9/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 522, in _get_batch\r\nApr 29 19:51:58     success, data = self._try_get_batch()\r\nApr 29 19:51:58   File \"/opt/python/2.7.9/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 480, in _try_get_batch\r\nApr 29 19:51:58     data = self.data_queue.get(timeout=timeout)\r\nApr 29 19:51:58   File \"/opt/python/2.7.9/lib/python2.7/multiprocessing/queues.py\", line 135, in get\r\nApr 29 19:51:58     res = self._recv()\r\nApr 29 19:51:58   File \"/opt/python/2.7.9/lib/python2.7/site-packages/torch/multiprocessing/queue.py\", line 22, in recv\r\nApr 29 19:51:58     return pickle.loads(buf)\r\nApr 29 19:51:58   File \"/opt/python/2.7.9/lib/python2.7/pickle.py\", line 1382, in loads\r\nApr 29 19:51:58     return Unpickler(file).load()\r\nApr 29 19:51:58   File \"/opt/python/2.7.9/lib/python2.7/pickle.py\", line 858, in load\r\nApr 29 19:51:58     dispatch[key](self)\r\nApr 29 19:51:58   File \"/opt/python/2.7.9/lib/python2.7/pickle.py\", line 1133, in load_reduce\r\nApr 29 19:51:58     value = func(*args)\r\nApr 29 19:51:58   File \"/opt/python/2.7.9/lib/python2.7/site-packages/torch/multiprocessing/reductions.py\", line 274, in rebuild_storage_fd\r\nApr 29 19:51:58     fd = multiprocessing.reduction.rebuild_handle(df)\r\nApr 29 19:51:58   File \"/opt/python/2.7.9/lib/python2.7/multiprocessing/reduction.py\", line 155, in rebuild_handle\r\nApr 29 19:51:58     conn = Client(address, authkey=current_process().authkey)\r\nApr 29 19:51:58   File \"/opt/python/2.7.9/lib/python2.7/multiprocessing/connection.py\", line 169, in Client\r\nApr 29 19:51:58     c = SocketClient(address)\r\nApr 29 19:51:58   File \"/opt/python/2.7.9/lib/python2.7/multiprocessing/connection.py\", line 304, in SocketClient\r\nApr 29 19:51:58     s.connect(address)\r\nApr 29 19:51:58   File \"/opt/python/2.7.9/lib/python2.7/socket.py\", line 224, in meth\r\nApr 29 19:51:58     return getattr(self._sock,name)(*args)\r\nApr 29 19:51:58 error: [Errno 111] Connection refused\r\n\r\n```"}
{"number": 20064, "title": "Quantized Conv operator", "time": "2019-05-02T16:12:30Z", "body": "Summary: Initial implementation of quantized convolution operator using fbgemm.\n\nDifferential Revision: D15178352\n\n"}
{"number": 20065, "title": "[DO NOT MERGE] Test whether we can keep !impl_->is_variable() invariant", "time": "2019-05-02T16:25:51Z", "body": ""}
{"number": 20066, "title": "Change devtoolset7 CUDA 9.0 nightlies to use a lower devtoolset", "time": "2019-05-02T16:27:53Z", "body": "\"CUDA 9.0 is not compatible with std::tuple from GCC version >= 6\"\r\nWe should use devtoolset5 for the CUDA 9.0 nightlies"}
{"number": 20067, "title": "Remove CUDA 8.0 nightlies", "time": "2019-05-02T16:29:01Z", "body": "We will only provide 9.0 and 10.0 for now."}
{"number": 20068, "title": "Removing CUDA 8.0 nightlies", "time": "2019-05-02T16:49:30Z", "body": "Resolves https://github.com/pytorch/pytorch/issues/20067"}
{"number": 20069, "title": "Fix smoke tests on binary builds", "time": "2019-05-02T17:01:01Z", "body": ""}
{"number": 20070, "title": "RuntimeError: _th_uniform_ not supported on CPUType for Long", "time": "2019-05-02T17:05:08Z", "body": "## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nRuntimeError: _th_uniform_ not supported on CPUType for Long\r\n\r\n## To Reproduce\r\nFollow 1st tutorial https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py\r\nThe error occurs when specifying the dtype:  \r\n`x = torch.zeros(5, 3, dtype=torch.long)`\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Install Nvidia driver\r\n2. Run the docker image nvidia/cuda:9.0-cudnn7-devel\r\n3. Install conda inside the image\r\n4. Use conda to install pytorch: \r\n    conda install pytorch torchvision cudatoolkit=9.0 -c pytorch\r\n5. Run python and follow the aforementioned tutorial\r\n\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\nNo error, print(x) yielding:  \r\ntensor([[0, 0, 0],\r\n        [0, 0, 0],\r\n        [0, 0, 0],\r\n        [0, 0, 0],\r\n        [0, 0, 0]])  \r\nAccording to said tutorial.  \r\n\r\n## Environment\r\n\r\n(base) root@4624f2f6cd3f:/# python collect_env.py \r\nCollecting environment information...\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: Could not collect\r\n\r\nPython version: 2.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 390.116\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.5.0\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.2\r\n[pip] numpydoc==0.8.0\r\n[pip] torch==1.1.0\r\n[pip] torchvision==0.2.2\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.3                      199  \r\n[conda] mkl-service               1.1.2            py27he904b0f_5  \r\n[conda] mkl_fft                   1.0.10           py27ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py27hd81dba3_0  \r\n[conda] pytorch                   1.1.0           py2.7_cuda9.0.176_cudnn7.5.1_0    pytorch\r\n[conda] torchvision               0.2.2                      py_3    pytorch\r\n\r\n\r\n## Additional context\r\n(base) root@4624f2f6cd3f:/# lscpu\r\nArchitecture:          x86_64\r\nCPU op-mode(s):        32-bit, 64-bit\r\nByte Order:            Little Endian\r\nCPU(s):                16\r\nOn-line CPU(s) list:   0-15\r\nThread(s) per core:    2\r\nCore(s) per socket:    8\r\nSocket(s):             1\r\nNUMA node(s):          1\r\nVendor ID:             AuthenticAMD\r\nCPU family:            23\r\nModel:                 1\r\nModel name:            AMD Ryzen 7 1800X Eight-Core Processor\r\nStepping:              1\r\nCPU MHz:               2675.483\r\nCPU max MHz:           3600.0000\r\nCPU min MHz:           2200.0000\r\nBogoMIPS:              7199.13\r\nVirtualization:        AMD-V\r\nHypervisor vendor:     vertical\r\nVirtualization type:   full\r\nL1d cache:             32K\r\nL1i cache:             64K\r\nL2 cache:              512K\r\nL3 cache:              8192K\r\nNUMA node0 CPU(s):     0-15\r\nFlags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb hw_pstate sme ssbd vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 xsaves clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov succor smca\r\n\r\n"}
{"number": 20071, "title": "Update MultiheadAttention documentations", "time": "2019-05-02T17:24:59Z", "body": "Add documentations to add_bias_kv, add_zero_attn, and attn_mask."}
{"number": 20072, "title": "[Do NOT Review] Export dynamic expand", "time": "2019-05-02T17:53:00Z", "body": ""}
{"number": 20073, "title": "[WIP] numpy arg translation proof of concept", "time": "2019-05-02T19:14:08Z", "body": "No need to review this in detail, since it is in flux -- proof of concept of the stuff mentioned in pytorch/pytorch#2228"}
{"number": 20074, "title": "Unable to compile an older version of PyTorch", "time": "2019-05-02T19:14:53Z", "body": "I just tired building PyTorch from source from [this commit](https://github.com/pytorch/pytorch/tree/bad4442) but I got the following messages and error:\r\n\r\n```\r\nexport BUILD_CAFFE2_OPS=0\r\ngit clone https://github.com/pytorch/pytorch\r\ncd pytorch\r\ngit checkout 3a01a45\r\n```\r\n\r\nThen I get the following messages:\r\n\r\n> warning: unable to rmdir third_party/foxi: Directory not empty\r\n> M\tthird_party/NNPACK\r\n> M\tthird_party/fbgemm\r\n> M\tthird_party/ideep\r\n> M\tthird_party/nccl/nccl\r\n> M\tthird_party/onnx\r\n> M\tthird_party/onnx-tensorrt\r\n> Note: checking out '3a01a45'.\r\n> \r\n> You are in 'detached HEAD' state. You can look around, make experimental\r\n> changes and commit them, and you can discard any commits you make in this\r\n> state without impacting any branches by performing another checkout.\r\n> \r\n> If you want to create a new branch to retain commits you create, you may\r\n> do so (now or later) by using -b with the checkout command again. Example:\r\n> \r\n>   git checkout -b <new-branch-name>\r\n> \r\n> HEAD is now at 3a01a45... Implement IRParser. (#16987)\r\n\r\nThen I do\r\n`git submodule update --init --recursive`\r\n\r\nand get the following messages:\r\n\r\n> Submodule 'third_party/ComputeLibrary' (https://github.com/ARM-software/ComputeLibrary.git) registered for path 'third_party/ComputeLibrary'\r\n> Cloning into 'third_party/ComputeLibrary'...\r\n> remote: Enumerating objects: 88692, done.\r\n> remote: Total 88692 (delta 0), reused 0 (delta 0), pack-reused 88692\r\n> Receiving objects: 100% (88692/88692), 121.26 MiB | 27.69 MiB/s, done.\r\n> Resolving deltas: 100% (76635/76635), done.\r\n> Checking connectivity... done.\r\n> Submodule path 'third_party/ComputeLibrary': checked out '292227986edb37b01061afcad6df18ba9d6ccbeb'\r\n> Submodule path 'third_party/NNPACK': checked out '1e005b0c2777f39972a4ac15bea03e0e315a3d92'\r\n> Submodule path 'third_party/fbgemm': checked out '83b462ec58e78311ba3b64b48cbd49db0485641f'\r\n> Submodule path 'third_party/ideep': checked out '689e38a124698261087b073068eb04588b9d1c55'\r\n> Submodule path 'third_party/ideep/mkl-dnn': checked out '0c3cb94999919d33e4875177fdef662bd9413dd4'\r\n> Submodule path 'third_party/nccl/nccl': checked out '3c6e25210bb1b544748937e5db74db0b9679b95e'\r\n> Submodule path 'third_party/onnx': checked out '15c33c945851907411619f599900c3852108e7e3'\r\n> fatal: reference is not a tree: 9884f286a236a3b4e3218e4afa17781752e048bd\r\n> Unable to checkout '9884f286a236a3b4e3218e4afa17781752e048bd' in submodule path 'third_party/onnx-tensorrt'\r\n\r\nI also tried with `git checkout bad4442` which is a bit older than `3a01a45`  but got the same error at the end.\r\n\r\nIn the past I used to compile older versions of PyTorch without an issue, but I had not done this for the past 2-3 months until today. I wonder what I should do to be able to compile an older version of PyTorch?"}
{"number": 20075, "title": "torch.arange always generate constant result in tracing", "time": "2019-05-02T19:20:37Z", "body": "We hope to get dynamic result, however, in the traced onnx graph, we alway get constant directly.\r\n\r\nFor a reproduction, please https://github.com/pytorch/pytorch/pull/20072"}
{"number": 20076, "title": "Add autograd wrapper registration to c10 dispatcher", "time": "2019-05-02T20:34:26Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #20077 Introduce c10 dispatch path for ATen\n* **#20076 Add autograd wrapper registration to c10 dispatcher**\n\nDifferential Revision: [D15188585](https://our.internmc.facebook.com/intern/diff/D15188585)"}
{"number": 20077, "title": "[test] Introduce c10 dispatch path for ATen", "time": "2019-05-02T20:34:37Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20077 Introduce c10 dispatch path for ATen**\n* #20076 Add autograd wrapper registration to c10 dispatcher\n\nDifferential Revision: [D15188586](https://our.internmc.facebook.com/intern/diff/D15188586)"}
{"number": 20078, "title": "[ONNX] Change the export of _dim_arange in ONNX", "time": "2019-05-02T20:35:54Z", "body": "Previously using ATen op, now fully switched to pure ONNX/Caffe2 ops."}
{"number": 20079, "title": "Add ninja to PyTorch README.md file.", "time": "2019-05-02T21:14:06Z", "body": "Fixes https://github.com/pytorch/pytorch/issues/17572"}
{"number": 20080, "title": "Adding ShufflenetV2 to caffe2's benchmark suite.", "time": "2019-05-02T21:19:56Z", "body": "Summary:\nAdding ShufflenetV2 (by Ma et. al. 2018) to the caffe2's benchmark\nsuite.\n\nTo run, use: `buck run mode/opt caffe2/caffe2/python/examples:imagenet_trainer -- --train_data null --batch_size 128 --epoch_size 3200 --num_epochs 2 --num_gpus 2 --model shufflenet`\n\nDifferential Revision: D15094282\n\n"}
{"number": 20081, "title": "Index into a tuple with non constant integer", "time": "2019-05-02T21:37:24Z", "body": "Fix for https://github.com/pytorch/pytorch/issues/16962\r\n\r\nThis needs fixing because we turn lists into tuples when constantify a module, so indexing into a Tuple of one type with a non-constant integer is quite common. "}
{"number": 20082, "title": "[JIT] SubgraphRewriter: Rename pattern fusion to subgraph rewrite.", "time": "2019-05-02T21:38:42Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #20084 [JIT] SubgraphRewriter: Add a support for arbitrary replacement graphs in subgraph rewriter.\n* #20083 [JIT] SubgraphRewriter: Expose runOnGraph and use it in tests.\n* **#20082 [JIT] SubgraphRewriter: Rename pattern fusion to subgraph rewrite.**\n\nDifferential Revision: [D15190193](https://our.internmc.facebook.com/intern/diff/D15190193)"}
{"number": 20083, "title": "[JIT] SubgraphRewriter: Expose runOnGraph and use it in tests.", "time": "2019-05-02T21:38:47Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #20084 [JIT] SubgraphRewriter: Add a support for arbitrary replacement graphs in subgraph rewriter.\n* **#20083 [JIT] SubgraphRewriter: Expose runOnGraph and use it in tests.**\n* #20082 [JIT] SubgraphRewriter: Rename pattern fusion to subgraph rewrite.\n\nDifferential Revision: [D15190192](https://our.internmc.facebook.com/intern/diff/D15190192)"}
{"number": 20084, "title": "[JIT] SubgraphRewriter: Add a support for arbitrary replacement graphs in subgraph rewriter.", "time": "2019-05-02T21:38:52Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20084 [JIT] SubgraphRewriter: Add a support for arbitrary replacement graphs in subgraph rewriter.**\n* #20083 [JIT] SubgraphRewriter: Expose runOnGraph and use it in tests.\n* #20082 [JIT] SubgraphRewriter: Rename pattern fusion to subgraph rewrite.\n\nPreviously, we could only replace a match with a single node. With this\ncahnge, we will be able to specify an arbitrary subgraph to insert\ninstead of the match. A special case would be an empty replacement\nsubgraph, which would essentially mean that we just want to remove\nmatches from the original graph.\n\nDifferential Revision: [D15190191](https://our.internmc.facebook.com/intern/diff/D15190191)"}
{"number": 20085, "title": "Reduce overhead of OnnxifiOp", "time": "2019-05-02T22:20:51Z", "body": "Summary: Reduce the overhead of OnnxifiOp from ~40us avg (~100us max) to ~25us avg (~40us max).\n\nDifferential Revision: D15191071\n\n"}
{"number": 20086, "title": "CosineAnnealingLR giving unexpected learning rates on PyTorch 1.1.", "time": "2019-05-02T22:50:33Z", "body": "## üêõ Bug\r\n\r\n`CosineAnnealingLR` (and probably others) are giving unexpected learning rates.\r\n\r\n## To Reproduce\r\n\r\nCode:\r\n```\r\nimport torch\r\nmodel = torch.nn.Linear(1, 1)\r\noptim = torch.optim.SGD(model.parameters(), lr=1.)\r\nl = torch.optim.lr_scheduler.CosineAnnealingLR(optim, 3)\r\n\r\nfor _ in range(10):\r\n    print(l.last_epoch, l.get_lr()[0])\r\n    l.step()\r\n```\r\n\r\nOutput of PyTorch 1.0.1:\r\n```\r\n-1 0.75\r\n0 1.0\r\n1 0.75\r\n2 0.2500000000000001\r\n3 0.0\r\n4 0.24999999999999978\r\n5 0.75\r\n6 1.0\r\n7 0.7500000000000002\r\n8 0.2500000000000004\r\n```\r\n\r\nOutput of PyTorch 1.1:\r\n```\r\n0 1.0\r\n1 0.5625\r\n2 0.08333333333333341\r\n3 0.0\r\n4 0.4999999999999999\r\n5 2.2500000000000036\r\n6 1.333333333333334\r\n7 0.5625000000000006\r\n8 0.0833333333333336\r\n9 0.0\r\n```\r\n\r\n## Expected behavior\r\n\r\nI was expecting almost the exact same output as PyTorch 1.0.1. The only difference would be that PyTorch 1.1 should start with `last_epoch = 0`.\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.1 and 1.0.1\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source): -\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: 10\r\n - GPU models and configuration: Titan Xp\r\n - Any other relevant information:"}
{"number": 20087, "title": "Native ATen/Parallel backend", "time": "2019-05-03T01:38:54Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #20157 Support for Eigen thread pool\n* #20480 Native TBB parallel backend\n* #20454 Restore TBB module\n* **#20087 Native ATen/Parallel backend**\n\nSummary:\nImplementing ATen/Parallel withtout OpenMP using native thread pool\n\nTest Plan:\nPARALLEL_BACKEND=NATIVE BLAS=MKL USE_MKLDNN=1 USE_OPENCV=1 USE_FFMPEG=1\npython setup.py develop --cmake\npytest -s -v test/test_torch.py::TestTorch\n\nDifferential Revision: [D15248663](https://our.internmc.facebook.com/intern/diff/D15248663)"}
{"number": 20088, "title": "Update README.md", "time": "2019-05-03T02:13:33Z", "body": "Sometimes people need to checkout an older version and build PyTorch. In that case, they need to do `git submodule sync` and maybe `git submodule update --init` as mentioned [here](https://github.com/pytorch/pytorch/issues/20074)."}
{"number": 20089, "title": "[Undesirable behaviour] Allocating a nn.Parameter on gpu inside a nn.Module makes it not to be enlisted as network parameter", "time": "2019-05-03T04:04:36Z", "body": "\r\nShort explanation:\r\nDefining a parameter inside a nn.Module produces different a behaviour than a nn.Module if it's allocated on gpu after defined.\r\n\r\nLong explanation:\r\nAs far as I know registering a parameter inside a nn.Module, in the init method, expects to register that tensor as a parameter of the network. Those parameters are leaf tensors, thus, they get optimized through learning.\r\n\r\nMain problem is that if you define a module and a paremeter within, but afterwards this parameter is allocated on gpu, it does not appear as a model parameter since what the attribute registers is a copy of the tensor allocated on gpu. It points to a cpu version and updates ctx version and sets backward function properly good. \r\n\r\nAfter thinking about it, if user would like to allocate it into gpu, it would be better to define it in cpu and to allocate during the forward pass. Nevertheless, this is not the case with nn.Modules, as they can be allocated of gpu during the init method.\r\n\r\n\r\n\r\n\r\n\r\n## To Reproduce\r\n\r\n```\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch\r\n\r\nclass Gpu(nn.Module):\r\n    def __init__(self):\r\n        super(Gpu,self).__init__()\r\n        self.w = nn.Parameter(torch.ones(10)).cuda()\r\n\r\ngpu = Gpu()\r\n# list(gpu.parameters()) --> empty\r\n# gpu.w.grad_fn = <CopyBackwards at 0x7fa0ff2890b8>\r\n# gpu.w._version = 1\r\n# gpu.isleaf = False\r\n\r\nclass Cpu(nn.Module):\r\n    def __init__(self):\r\n        super(Cpu,self).__init__()\r\n        self.w = nn.Parameter(torch.ones(10))\r\ncpu = Cpu()\r\n\r\n# list(cpu.parameters())\r\n# [Parameter containing:\r\n # tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)]\r\n# cpu.w.grad_fn = None\r\n# cpu.w._version = 0\r\n# cpu.w.isleaf = True\r\n\r\n\"\"\"\r\nThis is obviously expected, however it behaves totally different in case of nn modules\r\n\"\"\"\r\n\r\nclass Wrapped(nn.Module):\r\n    def __init__(self):\r\n        super(Wrapped,self).__init__()\r\n        self.w = Cpu().cuda()\r\n\r\nwrp= Wrapped()\r\n\"\"\"\r\nHere, as the nn.Parameter is wrapped by a nn.Module it behaves as expected.\r\nmodel contains all the parameters and they appear when calling wrp.parameters() and so on.\r\nHowever I find confusing to be able to allocate nn.Modules while defining them but not parameters.\r\n This may generate problems as users wouldn't realize parameters are not enlisted in model.parameters(), thus, not passed to optimizer.\r\n\r\n\"\"\"\r\n```\r\n\r\nJust write this to check this is the desired behaviour, but considering all tutorials show you can allocate nn.Modules on gpu while defining them, a different behaviour for parameters should at least be warned.\r\n\r\nIt's possible to bypass this behaviour preallocating the tensor on gpu before setting it as a parameter but I find it's a bad style. \r\n\r\n"}
{"number": 20090, "title": "How to add dynamically allocated strings to Pickler?", "time": "2019-05-03T04:43:08Z", "body": "The following code prints `111` and `111`, instead of `222` and `111`, because `222` is skipped [here](https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/pickler.cpp#L68). Is this by design as Pickler only works for statically allocated strings? Or is there a way to correctly add dynamically allocated strings? (and all other types listed [here](https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/pickler.cpp#L104-L114))? \r\n\r\n```c++\r\n  std::string str1 = \"111\";\r\n  std::string str2 = \"222\";\r\n\r\n  std::vector<at::Tensor> tensor_table;\r\n  torch::jit::Pickler pickler(&tensor_table);\r\n  pickler.start();\r\n  pickler.addIValue(str1);\r\n  pickler.addIValue(str2);\r\n  pickler.finish();\r\n\r\n  auto buffer = new char[pickler.stack().size()];\r\n  memcpy(buffer, pickler.stack().data(), pickler.stack().size());\r\n\r\n  torch::jit::Unpickler unpickler(buffer, pickler.stack().size(), &tensor_table);\r\n  auto values = unpickler.parse_ivalue_list();\r\n  std::cout << values.back().toStringRef() << std::endl;\r\n  values.pop_back();\r\n  std::cout << values.back().toStringRef() << std::endl;\r\n  values.pop_back();\r\n```\r\n\r\ncc @zdevito "}
{"number": 20091, "title": "DOC: Update web documentation of geometric_ to be consistent with Tensor behaviour", "time": "2019-05-03T05:39:56Z", "body": "Fix #19940 by updating web doc to reflect Tensor behaviour which will reflect [here](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.geometric_)"}
{"number": 20092, "title": "new tensorboard add_scalar not allowing merged scalars on same chart", "time": "2019-05-03T06:06:20Z", "body": "## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. call add_scalar('\\<groupname\\>/\\<id\\>', scalar_value, epoch)\r\n2. open tensorboard \r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nWhen using '\\<group\\>/\\<id\\>' as the tag for a scalar we should see the scalars plotted on the same chart.  This allows for train and test loss to be plotted on same chart for instance.  Instead we get a new header \\<group\\> with two charts \\<id\\>.\r\n\r\nNote: My assumption is that this is the way tensorflowX used to work.\r\n\r\n## Environment\r\n - PyTorch Version (e.g., 1.0): 1.1\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.6\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\nI also tried the add_scalars using multiple variation of tag names and dict keys to try to get the grouping to work as expected.  None of these worked.\r\n"}
{"number": 20093, "title": "[ONNX] Export randn_like in ONNX exporter", "time": "2019-05-03T06:14:13Z", "body": "As a work around for dynamic shape case."}
{"number": 20094, "title": "[jit] fix compilation order for class methods", "time": "2019-05-03T06:42:11Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #20106 [jit] allow classes to be used in their own methods\n* **#20094 [jit] fix compilation order for class methods**\n\nIt turns out that methods are reported in Python in name order instead\nof definition order. This means that potentially we can try to compile a\nmethod before __init__, which will break if we wanted to use any\nattributes that get defined in __init__.\n\nThis PR fixes that by special-casing __init__ so that its always\ncompiled first. Fixes #20004.\n\nDifferential Revision: [D15197079](https://our.internmc.facebook.com/intern/diff/D15197079)"}
{"number": 20095, "title": "[jit] cannot recursively construct a class inside itself", "time": "2019-05-03T06:56:48Z", "body": "This should be possible:\r\n```\r\n@jit.script\r\nclass LSTMStateStack:\r\n    def __init__(self, num_layers: int, hidden_size: int):\r\n        self.num_layers = num_layers\r\n        self.hidden_size = hidden_size\r\n        self.last_state = (\r\n            torch.zeros(num_layers, 1, hidden_size),\r\n            torch.zeros(num_layers, 1, hidden_size),\r\n        )\r\n        # Stack of hidden state, element\r\n        self.stack = [(self.last_state[0][-1], self.last_state[0][-1])]\r\n\r\n    def copy(self):\r\n        other = LSTMStateStack(self.num_layers, self.hidden_size)\r\n        other.stack = list(self.stack)\r\n        return other\r\n```"}
{"number": 20096, "title": "Can't ONNX export torch.jit.load-ed model", "time": "2019-05-03T07:15:08Z", "body": "## üêõ Bug\r\n\r\n```\r\n  for j in range(seqlen):\r\nTraceback (most recent call last):\r\n  File \"onnx_full_export.py\", line 74, in <module>\r\n    main()\r\n  File \"onnx_full_export.py\", line 70, in main\r\n    beam_search.save_to_db(args.output_file)\r\n  File \"/home/raden/translate/pytorch_translate/ensemble_export.py\", line 1170, in save_to_db\r\n    self.onnx_export(tmp_file)\r\n  File \"/home/raden/translate/pytorch_translate/ensemble_export.py\", line 1130, in onnx_export\r\n    export_type=ExportTypes.ZIP_ARCHIVE,\r\n  File \"/home/raden/anaconda3/lib/python3.7/site-packages/torch/onnx/__init__.py\", line 19, in _export\r\n    result = utils._export(*args, **kwargs)\r\n  File \"/home/raden/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py\", line 363, in _export\r\n    _retain_param_name, do_constant_folding)\r\n  File \"/home/raden/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py\", line 256, in _model_to_graph\r\n    method.graph, tuple(args) + tuple(params), example_outputs, False, propagate)\r\nRuntimeError: retval->inputs().size() == inputs.size() ASSERT FAILED at /pytorch/torch/csrc/jit/script/init.cpp:744, please report a bug to PyTorch. (_propagate_and_assign_input_and_output_shapes at /pytorch/torch/csrc/jit/script/init.cpp:744)\r\nframe #0: std::function<std::string ()>::operator()() const + 0x11 (0x7f6b9c93b441 in /home/raden/anaconda3/lib/python3.7/site-packages/torch/lib/libc10.so)\r\nframe #1: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x2a (0x7f6b9c93ad7a in /home/raden/anaconda3/lib/python3.7/site-packages/torch/lib/libc10.so)\r\nframe #2: <unknown function> + 0x452376 (0x7f6bdbf5c376 in /home/raden/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\r\nframe #3: <unknown function> + 0x4793af (0x7f6bdbf833af in /home/raden/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\r\nframe #4: <unknown function> + 0x130fac (0x7f6bdbc3afac in /home/raden/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\r\n<omitting python frames>\r\nframe #31: __libc_start_main + 0xf0 (0x7f6beb660830 in /lib/x86_64-linux-gnu/libc.so.6)\r\n\r\n\r\n```\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nRun onnx_full_export.py from https://github.com/pytorch/translate\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\nPytorch 1.1 stable\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"}
{"number": 20097, "title": "Manually sync fbcode/caffe2 and xplat/caffe2", "time": "2019-05-03T09:33:14Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20097 Manually sync fbcode/caffe2 and xplat/caffe2**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15196242/)\n\n-\n\nDifferential Revision: [D15196242](https://our.internmc.facebook.com/intern/diff/D15196242/)"}
{"number": 20098, "title": "Prevent accidental casting to bool in IValue", "time": "2019-05-03T09:36:15Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20098 Prevent accidental casting to bool in IValue**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15184661/)\n\nThis is a follow-up to D15172409\nThe change to Dict::erase() is necessary because otherwise the compiler complains about changing IValue.\n\nDifferential Revision: [D15184661](https://our.internmc.facebook.com/intern/diff/D15184661/)"}
{"number": 20099, "title": "RuntimeError: input->node()->kind() == prim::Constant ASSERT FAILED at /opt/conda/conda-bld/pytorch-cpu_1556653093101/work/torch/csrc/jit/passes/graph_fuser.cpp:483, please report a bug to PyTorch. (mergeNodeIntoGroup at /opt/conda/conda-bld/pytorch-cpu_1556653093101/work/torch/csrc/jit/passes/graph_fuser.cpp:483)", "time": "2019-05-03T10:24:12Z", "body": "## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\n```\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch\r\n\r\nclass Net(torch.jit.ScriptModule):\r\n\r\n    def __init__(self):\r\n        super(Net,self).__init__()\r\n\r\n        kernel_size = 3\r\n        stride = 1\r\n        filters = 1\r\n\r\n        self.conv1 = nn.Conv2d(1, filters, kernel_size, stride)\r\n        self.bn1 = nn.BatchNorm2d(filters)\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, x):\r\n        x = self.bn1(F.relu(self.conv1(x)))\r\n        return x\r\n\r\n\r\nm = Net()\r\n\r\nm.eval()\r\nwith torch.no_grad():\r\n    output = m(torch.zeros((1,1,10,10)))\r\n\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nReturn a tensor of rank (1,1,8,8).\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-16ubuntu3) 7.3.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.14.2\r\n[pip3] numpydoc==0.7.0\r\n[pip3] torch==1.1.0\r\n[pip3] torchvision==0.2.2\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2018.0.1             h19d6760_4\r\n[conda] mkl-dnn                   0.14                          2    intel\r\n[conda] mkl-service               1.1.2            py36h17a0993_4\r\n[conda] pytorch-cpu               1.1.0               py3.6_cpu_0    pytorch\r\n[conda] torchvision-cpu           0.2.2                      py_3    pytorch\r\n\r\n## Additional context\r\n\r\nWorks fine without `@torch.jit.script_method`\r\n<!-- Add any other context about the problem here. -->\r\n"}
{"number": 20100, "title": "MultivariateNormal fails with CUDA", "time": "2019-05-03T11:50:16Z", "body": "## üêõ Bug\r\n\r\nMultivariateNormal fails on CUDA\r\n\r\n## To Reproduce\r\n\r\nFor example, cpu version works fine\r\n\r\n```\r\nx = MultivariateNormal(t.ones(5, 3), t.eye(3))\r\nx.log_prob(x.sample())\r\ntensor([ -2.9737,  -4.3816,  -3.1320, -10.9442,  -3.1390])\r\n```\r\n\r\nBut if I try to create  MultivariateNormal instance with CUDA tensors then:\r\n\r\n```\r\nx = MultivariateNormal(t.ones(5, 3).cuda(), t.eye(3).cuda())\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributions/multivariate_normal.py\", line 142, in __init__\r\n    self._unbroadcasted_scale_tril = torch.cholesky(self.covariance_matrix)\r\nRuntimeError: CUDA error: invalid device function\r\n```\r\n\r\nI tried to workaround this issue by creating instance not from covariance matrix, but from `scale_tril`, but in this case `log_prob` method fails\r\n\r\n```\r\nx = MultivariateNormal(t.ones(5, 3).cuda(), scale_tril=t.eye(3).cuda())\r\nx.sample()\r\n\r\ntensor([[0.8211, 1.6837, 2.3296],\r\n        [0.1134, 3.0275, 0.6402],\r\n        [0.5820, 1.3581, 1.0628],\r\n        [0.6996, 1.1009, 0.7085],\r\n        [1.6969, 0.0917, 1.7386]], device='cuda:0')\r\n\r\nx.log_prob(x.sample())\r\n\r\nTHCudaCheck FAIL file=/opt/pytorch/aten/src/THC/generic/THCTensorMathPointwise.cu line=324 error=98 : invalid device function\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributions/multivariate_normal.py\", line 199, in log_prob\r\n    M = _batch_mahalanobis(self._unbroadcasted_scale_tril, diff)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributions/multivariate_normal.py\", line 57, in _batch_mahalanobis\r\n    M_swap = torch.triangular_solve(flat_x_swap, flat_L, upper=False)[0].pow(2).sum(-2)  # shape = b x c\r\nRuntimeError: cuda runtime error (98) : invalid device function at /opt/pytorch/aten/src/THC/generic/THCTensorMathPointwise.cu:324\r\n```\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.105\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04) 7.4.0\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.105\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX TITAN X\r\nGPU 1: GeForce GTX TITAN X\r\nGPU 2: GeForce GTX TITAN X\r\nGPU 3: GeForce GTX TITAN X\r\n\r\nNvidia driver version: 418.43\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.5.0\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.3\r\n[pip3] torch==1.1.0\r\n[conda] blas                      1.0                         mkl\r\n[conda] magma-cuda100             2.5.0                         1    soumith\r\n[conda] mkl                       2019.3                      199\r\n[conda] mkl-include               2019.3                      199\r\n[conda] mkl_fft                   1.0.12           py37ha843d7b_0\r\n[conda] mkl_random                1.0.2            py37hd81dba3_0\r\n[conda] mkldnn                    0.16.1                        0    mingfeima\r\n[conda] torch                     1.1.0                    pypi_0    pypi\r\n```\r\n\r\n[Here](https://pastebin.com/5FBN6W9e) is Dockerfile where I build Pytorch"}
{"number": 20101, "title": "jit tracing error for nn.Sequential with nn.Conv2d in torch 1.1.0 ", "time": "2019-05-03T12:56:35Z", "body": "## üêõ Bug\r\n\r\n```python\r\nRuntimeError: Cannot insert a Tensor that requires grad as a constant. Consider making it a parameter or input, or detaching the gradient\r\n``` \r\nwhen tracing nn.Sequential with nn.Conv2d in torch 1.1.0 \r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nfrom torch import nn\r\nimport torch.jit\r\n\r\nmodel = nn.Sequential(nn.Conv2d(2, 2, 1, 1, 1))\r\n\r\ntorch.jit.trace(model.forward, torch.randn(1, 1, 2, 2))\r\n```\r\n\r\nRaises the following error \r\n```python\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-3-5e9a2f5de8a5> in <module>\r\n      4 model = nn.Sequential(nn.Conv2d(2, 2, 1, 1, 1))\r\n      5 \r\n----> 6 torch.jit.trace(model.forward, torch.randn(1, 1, 2, 2))\r\n\r\n~/.virtualenvs/test/lib/python3.6/site-packages/torch/jit/__init__.py in trace(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, _force_outplace, _module_class)\r\n    693         traced = torch._C._create_function_from_trace(name, func, example_inputs,\r\n    694                                                       var_lookup_fn,\r\n--> 695                                                       _force_outplace)\r\n    696 \r\n    697     # Check the trace against new traces created from user-specified inputs\r\n\r\n~/.virtualenvs/test/lib/python3.6/site-packages/torch/nn/modules/container.py in forward(self, input)\r\n     90     def forward(self, input):\r\n     91         for module in self._modules.values():\r\n---> 92             input = module(input)\r\n     93         return input\r\n     94 \r\n\r\n~/.virtualenvs/test/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    489             hook(self, input)\r\n    490         if torch._C._get_tracing_state():\r\n--> 491             result = self._slow_forward(*input, **kwargs)\r\n    492         else:\r\n    493             result = self.forward(*input, **kwargs)\r\n\r\n~/.virtualenvs/test/lib/python3.6/site-packages/torch/nn/modules/module.py in _slow_forward(self, *input, **kwargs)\r\n    479         tracing_state._traced_module_stack.append(self)\r\n    480         try:\r\n--> 481             result = self.forward(*input, **kwargs)\r\n    482         finally:\r\n    483             tracing_state.pop_scope()\r\n\r\n~/.virtualenvs/test/lib/python3.6/site-packages/torch/nn/modules/conv.py in forward(self, input)\r\n    336                             _pair(0), self.dilation, self.groups)\r\n    337         return F.conv2d(input, self.weight, self.bias, self.stride,\r\n--> 338                         self.padding, self.dilation, self.groups)\r\n    339 \r\n    340 \r\n\r\nRuntimeError: Cannot insert a Tensor that requires grad as a constant. Consider making it a parameter or input, or detaching the gradient\r\nTensor:\r\n(1,1,.,.) = \r\n  0.3550\r\n\r\n(2,1,.,.) = \r\n 0.01 *\r\n  9.7722\r\n\r\n(1,2,.,.) = \r\n -0.5052\r\n\r\n(2,2,.,.) = \r\n  0.5900\r\n[ Variable[CPUType]{2,2,1,1} ]\r\n```\r\n\r\n## Expected behavior\r\n\r\nExpected to convert without issues\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.1.85\r\ncuDNN version: /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.0.5\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.2\r\n[pip3] numpy-image-widget==2019.1.6\r\n[pip3] torch==1.1.0\r\n[pip3] torchfile==0.1.0\r\n[pip3] torchvision==0.2.1\r\n[conda] Could not collect\r\n"}
{"number": 20102, "title": "LSTM forget bias must be initialized properly", "time": "2019-05-03T14:31:09Z", "body": "## üöÄ Feature\r\nLSTM forget bias must be initialized to 1 or 2 for better training.\r\n\r\n## Motivation\r\n\r\nPlease see:\r\nhttps://pdfs.semanticscholar.org/1154/0131eae85b2e11d53df7f1360eeb6476e7f4.pdf\r\nhttp://proceedings.mlr.press/v37/jozefowicz15.pdf\r\n\r\n\"This problem is addressed by simply initializing the forget\r\ngates bf to a large value such as 1 or 2. By doing so, the\r\nforget gate will be initialized to a value that is close to 1,\r\nenabling gradient flow. This idea was present in Gers et al.\r\n(2000), but we reemphasize it since we found many practitioners to not be familiar with it.\"\r\n"}
{"number": 20103, "title": "[FR] magma for cuda 10.1 ", "time": "2019-05-03T15:03:11Z", "body": ""}
{"number": 20104, "title": "[DO NOT MERGE] testing builder change", "time": "2019-05-03T16:21:18Z", "body": ""}
{"number": 20105, "title": "conv2d / avg_pool2d implmentation for LongTensor", "time": "2019-05-03T16:47:42Z", "body": "Summary:\n- enable generation of conv2d for LongTensor\n- enable generation of avg_pool2d for LongTensor\n\nDifferential Revision: D15192353\n\n"}
{"number": 20106, "title": "[jit] allow classes to be used in their own methods", "time": "2019-05-03T18:14:32Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20106 [jit] allow classes to be used in their own methods**\n* #20094 [jit] fix compilation order for class methods\n\nRecent changes to how class names are resolved broke this functionality.\nWe switched to  use the python resolver to find classes, but a class is not\ndefined in python until *after* the compilation for that class\ncomplete.\n\nThis PR gives PythonResolver the knowledge of the currently compiling\nclass so it can resolve that name correctly.\n\nDifferential Revision: [D15202261](https://our.internmc.facebook.com/intern/diff/D15202261)"}
{"number": 20107, "title": "[pt1][quant] Add dequantize_linear for JIT pass", "time": "2019-05-03T18:16:26Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20656 [pt1][quant] int_repr for different quantized types&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15398134/)\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20107 [pt1][quant] Add dequantize_linear for JIT pass**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15202187/)\n\natt\n\nDifferential Revision: [D15202187](https://our.internmc.facebook.com/intern/diff/D15202187/)"}
{"number": 20108, "title": "add c2 benchmark runs in cpp", "time": "2019-05-03T19:55:48Z", "body": "Summary:\nAdd cpp runs for c2, hooked up via pybinds. Print output to terminal. This is not hooked up with the pep output yet because I'd like to verify the numbers first.\n\nNote that this isn't quite the same mechanism as the pytorch cpp hookup, which uses cpp_python_extensions. If I can use the same mechanism to pull all the inputs for c2 through cpp and do FeedBlobs in cpp, then I'll switch to that.\n\nDifferential Revision: D15155976\n\n"}
{"number": 20109, "title": "[ONNX] Update logic for folding onnx::Constant nodes.", "time": "2019-05-03T20:32:21Z", "body": "Currently, constant folding pass during ONNX conversion removes all onnx::Constant nodes that are parents of nodes that are folded. In situations where the parent onnx::Constant node is other subscribers downstream this could be a problem. This change updates the removal logic to remove to only those onnx::Constant nodes that do not have other subscribers downstream"}
{"number": 20110, "title": "Formula typo fix", "time": "2019-05-03T20:44:23Z", "body": "T_{cur + 1} -> T_{cur} + 1\r\n\r\n"}
{"number": 20111, "title": "Split libtorch binary build CI job into separate variants", "time": "2019-05-03T20:46:41Z", "body": "We should move this loop https://github.com/pytorch/builder/blob/master/manywheel/build_common.sh#L120 into separate jobs.\r\n\r\nRight now the libtorch job can take a ridiculous amount of time. This should help greatly."}
{"number": 20112, "title": "dont make alias for none value", "time": "2019-05-03T21:34:09Z", "body": "Don't make an alias value for a value that is known to be None. This was preventing constant propagation from running the `out is None` check in nn.functional.normalize, and thus preventing the if statement from being inlined. "}
{"number": 20113, "title": "[JIT][script] JIT for torch.norm ignores argument defaults", "time": "2019-05-03T21:40:45Z", "body": "## üêõ Bug\r\n\r\ntorch.norm `p` argument default is ignored by JIT\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.\r\n```python\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef norm_test():\r\n  t = torch.ones(10, 5)\r\n  return torch.norm(t, dim=1, keepdim=True)\r\n```\r\n\r\nfails with:\r\n\r\n> RuntimeError:\r\n> arguments for call are not valid:\r\n>   \r\n>   for operator aten::norm(Tensor self, Scalar p=<default>) -> Tensor:\r\n>   keyword argument dim unknown\r\n>   \r\n>   for operator aten::norm(Tensor self, Scalar? p, *, int dtype) -> Tensor:\r\n>   argument p not provided.\r\n>   @torch.jit.script\r\n>   def norm_test():\r\n>     t = torch.ones(10, 5)\r\n>     return torch.norm(t, dim=1, keepdim=True)\r\n>            ~~~~~~~~~~ <--- HERE\r\n\r\n2.\r\n```python\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef norm_test():\r\n  t = torch.ones(10, 5)\r\n  return torch.norm(t, p=2, dim=1)\r\n```\r\nWorks fine\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0\r\n\r\nOS: Microsoft Windows 10 Pro\r\nGCC version: Could not collect\r\nCMake version: version 3.9.0-rc5\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: GPU 0: GeForce GTX 1080 Ti\r\nNvidia driver version: 419.67\r\ncuDNN version: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin\\cudnn64_7.dll\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.14.3\r\n[pip] numpydoc==0.8.0\r\n[pip] pytorch-ignite==0.1.0\r\n[pip] torch==1.1.0\r\n[pip] torchvision==0.2.2\r\n[conda] blas                      1.0                         mkl\r\n[conda] cuda91                    1.0                           0    pytorch\r\n[conda] cuda92                    1.0                           0    pytorch\r\n[conda] mkl                       2018.0.2                      1\r\n[conda] mkl-service               1.1.2            py36h57e144c_4\r\n[conda] mkl_fft                   1.0.1            py36h452e1ab_0\r\n[conda] mkl_random                1.0.1            py36h9258bd6_0\r\n[conda] pytorch                   1.1.0           py3.6_cuda90_cudnn7_1    pytorch\r\n[conda] pytorch-ignite            0.1.0                    pypi_0    pypi\r\n[conda] torchvision               0.2.1                    pypi_0    pypi\r\n\n\ncc @suo"}
{"number": 20114, "title": "[Caffe2] Expose test utils", "time": "2019-05-03T21:44:31Z", "body": "Some functions were not decorated with `CAFFE2_API`, makes them unusable when creating unit tests for custom ops outside Caffe2 repo."}
{"number": 20115, "title": "[tensorboard] Add logging import and failing MLP", "time": "2019-05-03T21:48:17Z", "body": "Add logging import and a failed MLP model that confirms that we don't fail `add_graph` when graph optimization fails.\r\n\r\nThis addresses part of https://github.com/pytorch/pytorch/issues/18903\r\n\r\ncc @lanpa @ezyang @natalialunova "}
{"number": 20116, "title": "bug fix 19374 - fix for upsample export", "time": "2019-05-03T21:48:52Z", "body": ""}
{"number": 20117, "title": "[FR] [RFC] add Sequential.append & .extend", "time": "2019-05-03T22:17:40Z", "body": "A common pattern people use in `__init__` of a `nn.Module` is to build a `list` first and then feed it into `nn.Sequential` because `Sequential` doesn't support many handy methods existing on `list`. E.g., \r\n\r\n```py\r\ndef __init__(self, logres):\r\n    super().__init__():\r\n    layers = [\r\n          nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\r\n          nn.ReLU(inplace=True),\r\n          nn.MaxPool2d(kernel_size=3, stride=2),\r\n    ]\r\n\r\n    for _ in range(logres):\r\n         layers.extend([\r\n              nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\r\n              nn.ReLU(inplace=True),\r\n              nn.MaxPool2d(kernel_size=3, stride=2),\r\n        ])\r\n    self.layers = nn.Sequential(*layers)\r\n```\r\n\r\nThis is totally unnecessary if we can just provide `.append` and `.extend` on `Sequential`."}
{"number": 20118, "title": "Unable to import 1.1 when installing with pip", "time": "2019-05-04T00:06:56Z", "body": "## üêõ Bug\r\n\r\nIt seems there is an issue with the pip installation of 1.1. After upgrading, I get an error when calling `import torch`.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Install torch==1.1.0 with pip\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/anaconda3/envs/torch/lib/python3.6/site-packages/torch/__init__.py\", line 79, in <module>\r\n    from torch._C import *\r\nImportError: dlopen(/anaconda3/envs/torch/lib/python3.6/site-packages/torch/_C.cpython-36m-darwin.so, 9): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\r\n  Referenced from: /anaconda3/envs/torch/lib/python3.6/site-packages/torch/lib/libshm.dylib\r\n  Reason: image not found\r\n\r\n## Expected behavior\r\n\r\nThe torch package should import...\r\n\r\n## Environment\r\n\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\n\r\nOS: Mac OSX 10.14.4\r\nGCC version: Could not collect\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: N/A\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.3\r\n[pip3] torch==1.1.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.3                      199  \r\n[conda] mkl-service               1.1.2            py37hfbe908c_5  \r\n[conda] mkl_fft                   1.0.10           py37h5e564d8_0  \r\n[conda] mkl_random                1.0.2            py37h27c97d8_0\r\n"}
{"number": 20119, "title": "[JIT] Source highlighting doesn't line up when tabs are used for indentation", "time": "2019-05-04T00:10:36Z", "body": "```\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef foo(x):\r\n\t\treturn torch.neg(x).foo()\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"tabs.py\", line 3, in <module>\r\n    @torch.jit.script\r\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/jit/__init__.py\", line 826, in script\r\n    fn = torch._C._jit_script_compile(ast, _rcb, get_default_args(obj))\r\nRuntimeError:\r\nunknown builtin op: aten::foo\r\nHere are some suggestions:\r\n\taten::pop\r\n\taten::to\r\n\taten::cos\r\n\taten::pow\r\n\taten::dot\r\n\taten::floor\r\n\taten::fmod\r\n\taten::log\r\n\taten::fft\r\n:\r\n@torch.jit.script\r\ndef foo(x):\r\n\t\treturn torch.neg(x).foo()\r\n         ~~~~~~~~~~~~~~~~ <--- HERE\r\n```\n\ncc @suo"}
{"number": 20120, "title": "Fix NameError with PYTORCH_JIT=0", "time": "2019-05-04T00:11:06Z", "body": "Right now using `PYTORCH_JIT=0` gives this error:\r\n\r\n`NameError: name '_CachedForward' is not defined`\nDifferential Revision: [D15210046](https://our.internmc.facebook.com/intern/diff/15210046/)"}
{"number": 20121, "title": "open source intra_op_parallel (#19175)", "time": "2019-05-04T01:06:30Z", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/19175\n\nOpen source operators with intra-op parallelism\n\nTODO: gate tbb dependent code\n\nDifferential Revision: D14417823\n\n"}
{"number": 20122, "title": "Is the `device=` parameter required in torch.FloatTensor and similar ones", "time": "2019-05-04T04:31:07Z", "body": "Hey! \r\n\r\n```\r\n>>> import torch\r\n>>> torch.FloatTensor(3, 2, 2, device='cuda')\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-3-c0613bab77d9> in <module>()\r\n----> 1 torch.FloatTensor(3, 2, 2, device='cuda')\r\n\r\nRuntimeError: legacy constructor for device type: cpu was passed device type: cuda, but device type must be: cpu\r\n```\r\n\r\nBut,\r\n```\r\n>>> import torch\r\n>>> torch.FloatTensor(3, 2, 2).to('cuda')\r\n```\r\nworks!\r\n\r\nAlso, \r\n```\r\n>>> import torch\r\n>>> torch.cuda.FloatTensor(3, 2, 2)\r\n```\r\nworks!\r\n\r\n---\r\n\r\nSo, is the `device=` really needed in `torch.FloatTensor` ? \r\nOr is this some device compatibility issues or like that? "}
{"number": 20123, "title": "RuntimeError cudnn error cudnn_status_alloc_failed", "time": "2019-05-04T05:19:53Z", "body": "python version: 3.6.4\r\npytorch version: 1.1.0\r\ncuda version: 9.0\r\ncudnn version: 7\r\nwin10"}
{"number": 20124, "title": "[FR] Warn if scheduler.step() is called but optim.step has not been called", "time": "2019-05-04T06:01:46Z", "body": "In 1.1 we made a major BC breaking change, where the order of calling lr schedulers should be changed from \r\n```py\r\nfor e in range(nepochs):\r\n  scheduler.step()\r\n  train()\r\n```\r\nto\r\n```py\r\nfor e in range(nepochs):\r\n  train()\r\n  scheduler.step()\r\n```\r\n\r\nThis silently breaks many code, and makes it impossible to write consistent code for 1.0.1 and 1.1. So I propose to add a warning in `scheduler.step` where it looks at the corresponding optimizer, and checks if its `.step` has been called.\r\n\r\nIf it has not been called, this is a sign that the user is using `scheduler.step()` with the old pattern. I can't think of a reasonable case where this would detect a false positive."}
{"number": 20125, "title": "NCCL hang in PyTorch Distributed Data Parallel for Mixed Precision Training", "time": "2019-05-04T06:07:34Z", "body": "## üêõ Bug\r\n\r\nOn an AWS p3.16xlarge machine, I'm using the mixed precision training from https://github.com/NVIDIA/Megatron-LM checked in at https://github.com/cybertronai/transformer-xl\r\nWhen I run with [adaptive softmax](https://github.com/cybertronai/transformer-xl/blob/master/mem_transformer.py#L477) with 8 GPUs and [apex mixed precision](https://github.com/cybertronai/transformer-xl/blob/master/fp16_opt.py), NCCL hangs. When I remove `--adaptive`, it works as expected (after reducing batch size to avoid OOM). If I remove `--fp16` it also works fine, so it's definitely a mixed precision issue.\r\n\r\nSorry if this is a dup of #11672 but I haven't been able to figure it out.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\nOn a p3.16xlarge\r\n```sh\r\ngit clone https://github.com/cybertronai/transformer-xl\r\ncd transformer-xl\r\npip install -r requirements.txt\r\n# Download dataset to ./data/wikitext-103/ then\r\nNCCL_DEBUG_FILE=log.%h.%p NCCL_DEBUG_SUBSYS=COLL NCCL_DEBUG=INFO NCCL_MIN_NRINGS=16 NCCL_MAX_NRINGS=16  python -m tor\r\nch.distributed.launch --nproc_per_node=8 --nnodes=1 --node_rank=0 --master_addr=172.31.20.228 --master_port=6016 train.py --seed 1111 --data data/wikitext-103\r\n --dataset wt103 --adaptive --log_interval 100 --eval_interval 500 --max_tokens 1500000000 --logdir /ncluster/runs/ben-no-bpe.13 --distributed --lr 0.0005 --b\r\natch_size 4 --eta_min 5e-05 --n_layer 18 --d_model 1024 --n_head 16 --d_head 64 --d_inner 4096 --dropout 0.2 --dropatt 0.2 --optim lamb --warmup_tokens 300000\r\n0 --tgt_len 128 --mem_len 128 --eval_tgt_len 128 --fp16 --dynamic_loss_scale  --init_std 0.005 --div_val 4 \r\n```\r\n\r\nIn the [logs](https://github.com/NVIDIA/nccl/files/3144052/combined.txt) I found this:\r\n```\r\nip-172-31-20-228:22326:22707 [3] NCCL INFO AllReduce: opCount 5 sendbuff 0x7fcf6b400000 recvbuff 0x7fcf6b400000 count 1322459 datatype 6 op 0 root 0 comm 0x7f\r\ncfc403a800 [nranks=8] stream 0x563efa445460\r\nip-172-31-20-228:22327:22327 [4] NCCL INFO AllReduce: opCount 5 sendbuff 0x7fb11f71c200 recvbuff 0x7fb11f71c200 count 1 datatype 4 op 0 root 0 comm 0x7fb09c03\r\na800 [nranks=8] stream 0x55c779123600\r\n```\r\nHow do I figure out where this is coming from?  Is this something my code should account for or is it a bug in DDP or apex?\r\n\r\n## Expected behavior\r\nDoesn't hang silently.\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\n```\r\nPyTorch version: 1.0.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: Tesla V100-SXM2-16GB\r\nGPU 1: Tesla V100-SXM2-16GB\r\nGPU 2: Tesla V100-SXM2-16GB\r\nGPU 3: Tesla V100-SXM2-16GB\r\nGPU 4: Tesla V100-SXM2-16GB\r\nGPU 5: Tesla V100-SXM2-16GB\r\nGPU 6: Tesla V100-SXM2-16GB\r\nGPU 7: Tesla V100-SXM2-16GB\r\n\r\nNvidia driver version: 410.104\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.4\r\n[conda] blas                      1.0                         mkl\r\n[conda] cuda100                   1.0                           0    pytorch\r\n[conda] mkl                       2019.3                      199\r\n[conda] mkl-include               2019.3                      199\r\n[conda] mkl-service               1.1.2            py36he904b0f_5\r\n[conda] mkl_fft                   1.0.10           py36ha843d7b_0\r\n[conda] mkl_random                1.0.2            py36hd81dba3_0\r\n[conda] pytorch                   1.0.1           py3.6_cuda10.0.130_cudnn7.4.2_0    pytorch\r\n[conda] pytorch-lamb              0.0.0                     <pip>\r\n[conda] pytorch-pretrained-bert   0.6.2                     <pip>\r\n[conda] torchvision               0.2.1                      py_2    pytorch\r\n```\r\n## Additional context\r\n\r\n"}
{"number": 20126, "title": "[ONNX] Standard gamma's export", "time": "2019-05-04T06:39:18Z", "body": ""}
{"number": 20127, "title": "Improve nn.ActivationCls repr of inplace", "time": "2019-05-04T07:14:21Z", "body": ""}
{"number": 20128, "title": "[FR] make IncompatibleKeys print nicer when there is no error", "time": "2019-05-04T07:37:29Z", "body": "Now it prints `IncompatibleKeys(missing_keys=[], unexpected_keys=[])` upon a successful `load_state_dict`, and could be quite confusing. This can happen a lot, e.g., in jupyter notebooks.\r\n\r\nWe can just subclass and overwrite `__repr__`."}
{"number": 20129, "title": "GRUcell has a wrong formula", "time": "2019-05-04T07:46:59Z", "body": "## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nhttps://pytorch.org/docs/stable/nn.html?highlight=grucell#torch.nn.GRUCell\r\n$$\r\n\\begin{aligned} r &=\\sigma\\left(W_{i r} x+b_{i r}+W_{h r} h+b_{h r}\\right) \\\\ z &=\\sigma\\left(W_{i z} x+b_{i z}+W_{h z} h+b_{h z}\\right) \\\\ n &=\\tanh \\left(W_{i n} x+b_{i n}+r *\\left(W_{h n} h+b_{h n}\\right)\\right) \\\\ h^{\\prime} &=(1-z) * n+z * h \\end{aligned}\r\n$$\r\n\r\nFor the last one , It should be (1-z) * h+z * n. \r\n$$\r\nh^{\\prime}=(1-z) * h+z * n\r\n$$\r\n"}
{"number": 20130, "title": "Need to include ATen/Parallel.h for get/set_num_threads", "time": "2019-05-04T19:14:24Z", "body": "## üêõ Bug\r\n\r\nPrior to 1.1.0, the c++ interface was able to reference\r\n```\r\ntorch::get_num_threads\r\ntorch::set_num_threads\r\n```\r\nwithout additional includes beyond: _torch/torch.h_\r\n\r\nnow, it looks like it is necessary to explicitly include:\r\nATen/Parallel.h\r\n\r\nthis is easy enough to do, but seems like this should be part of the overall include,\r\nyes(?)\r\n\r\nthanks\r\n"}
{"number": 20131, "title": "update nn.init.calculate_gain doc example", "time": "2019-05-04T21:09:25Z", "body": ""}
{"number": 20132, "title": "nn.MultiheadAttention has no `_{get,set}_input_buffer` internals", "time": "2019-05-05T00:46:11Z", "body": "## üêõ Bug\r\n\r\nPretty clear from the title: there are no internal methods `_{get,set}_input_buffer` in the new `nn.MultiheadAttention` module,  so you get the following error when passing an `incremental_state` to the module call\r\n\r\n```\r\nAttributeError: 'MultiheadAttention' object has no attribute '_get_input_buffer'\r\n```\r\n\r\nIt would seem that these were lost when porting the [Fairseq implementation](https://github.com/pytorch/fairseq/blob/master/fairseq/modules/multihead_attention.py).\r\n\r\n(As a side note: it's completely not obvious what `incremental_state` state is or how to use it.)\r\n"}
{"number": 20133, "title": "THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp", "time": "2019-05-05T02:32:15Z", "body": "Hello, my friend are trying her new RTX 2080 8GB, with nvidia driver 410.57\r\n\r\nShe's currently running some legacy deep learning model using pytorch 0.4.1 (incompatible with pytorch 1.0) due to NMS and RoI align still using ffi instead of cpp. The nvidia driver 410.57, and still using cuda 9.0.\r\nShe gets THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp silent error, while the model still running. But some of the case after she break current process, she need to logout first before running another model because of CUDA:OutOfMemory error.\r\nDo you have some trick to it?"}
{"number": 20134, "title": "ERROR: Command \"python setup.py egg_info\" when dockerfile build", "time": "2019-05-05T04:00:55Z", "body": "## üêõ Bug\r\n\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n`docker build -t pytorch -f Dockerfile .`\r\n```shell\r\n    ['cmake',\r\n     '-GNinja',\r\n     '-DBUILDING_WITH_TORCH_LIBS=ON',\r\n     '-DBUILD_BINARY=False',\r\n     '-DBUILD_CAFFE2_OPS=True',\r\n     '-DBUILD_PYTHON=True',\r\n     '-DBUILD_SHARED_LIBS=ON',\r\n     '-DBUILD_TEST=True',\r\n     '-DBUILD_TORCH=ON',\r\n     '-DCAFFE2_STATIC_LINK_CUDA=False',\r\n     '-DCMAKE_BUILD_TYPE=Release',\r\n     '-DCMAKE_CXX_FLAGS= ',\r\n     '-DCMAKE_C_FLAGS= ',\r\n     '-DCMAKE_EXE_LINKER_FLAGS=',\r\n     '-DCMAKE_INSTALL_PREFIX=/tmp/pip-req-build-keauxx9z/torch',\r\n     '-DCMAKE_PREFIX_PATH=/opt/conda/bin/../',\r\n     '-DCMAKE_SHARED_LINKER_FLAGS=',\r\n     '-DINSTALL_TEST=True',\r\n     '-DNCCL_EXTERNAL=True',\r\n     '-DNCCL_INCLUDE_DIR=/usr/include',\r\n     '-DNCCL_ROOT_DIR=/usr/',\r\n     '-DNCCL_SYSTEM_LIB=/usr/lib/x86_64-linux-gnu/libnccl.so.2.4.2',\r\n     '-DNUMPY_INCLUDE_DIR=/opt/conda/lib/python3.6/site-packages/numpy/core/include',\r\n     '-DONNX_ML=False',\r\n     '-DONNX_NAMESPACE=onnx_torch',\r\n     '-DPYTHON_EXECUTABLE=/opt/conda/bin/python',\r\n     '-DPYTHON_INCLUDE_DIR=/opt/conda/include/python3.6m',\r\n     '-DPYTHON_LIBRARY=/opt/conda/lib/libpython3.6m.so.1.0',\r\n     '-DTHD_SO_VERSION=1',\r\n     '-DTORCH_BUILD_VERSION=1.1.0a0+fc00bfd',\r\n     '-DUSE_CUDA=True',\r\n     '-DUSE_DISTRIBUTED=True',\r\n     '-DUSE_FBGEMM=True',\r\n     '-DUSE_FFMPEG=False',\r\n     '-DUSE_LEVELDB=False',\r\n     '-DUSE_LMDB=False',\r\n     '-DUSE_MKLDNN=True',\r\n     '-DUSE_NCCL=True',\r\n     '-DUSE_NNPACK=True',\r\n     '-DUSE_NUMPY=True',\r\n     '-DUSE_OPENCV=False',\r\n     '-DUSE_QNNPACK=True',\r\n     '-DUSE_ROCM=False',\r\n     '-DUSE_SYSTEM_EIGEN_INSTALL=OFF',\r\n     '-DUSE_SYSTEM_NCCL=True',\r\n     '-DUSE_TENSORRT=False',\r\n     '-DMKLDNN_ENABLE_CONCURRENT_EXEC=ON',\r\n     '/tmp/pip-req-build-keauxx9z']\r\nCleaning up...\r\n  Removing source in /tmp/pip-req-build-keauxx9z\r\nRemoved file:///opt/pytorch from build tracker '/tmp/pip-req-tracker-27yyn7rz'\r\nRemoved build tracker '/tmp/pip-req-tracker-27yyn7rz'\r\nERROR: Command \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-req-build-keauxx9z/\r\nException information:\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.6/site-packages/pip/_internal/cli/base_command.py\", line 178, in main\r\n    status = self.run(options, args)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pip/_internal/commands/install.py\", line 352, in run\r\n    resolver.resolve(requirement_set)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pip/_internal/resolve.py\", line 131, in resolve\r\n    self._resolve_one(requirement_set, req)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pip/_internal/resolve.py\", line 294, in _resolve_one\r\n    abstract_dist = self._get_abstract_dist_for(req_to_install)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pip/_internal/resolve.py\", line 242, in _get_abstract_dist_for\r\n    self.require_hashes\r\n  File \"/opt/conda/lib/python3.6/site-packages/pip/_internal/operations/prepare.py\", line 368, in prepare_linked_requirement\r\n    abstract_dist.prep_for_dist(finder, self.build_isolation)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pip/_internal/operations/prepare.py\", line 177, in prep_for_dist\r\n    self.req.prepare_metadata()\r\n  File \"/opt/conda/lib/python3.6/site-packages/pip/_internal/req/req_install.py\", line 539, in prepare_metadata\r\n    self.run_egg_info()\r\n  File \"/opt/conda/lib/python3.6/site-packages/pip/_internal/req/req_install.py\", line 617, in run_egg_info\r\n    command_desc='python setup.py egg_info')\r\n  File \"/opt/conda/lib/python3.6/site-packages/pip/_internal/utils/misc.py\", line 776, in call_subprocess\r\n    % (command_desc, proc.returncode, cwd))\r\npip._internal.exceptions.InstallationError: Command \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-req-build-keauxx9z/\r\nThe command '/bin/sh -c TORCH_CUDA_ARCH_LIST=\"3.5 5.2 6.0 6.1 7.0+PTX\" TORCH_NVCC_FLAGS=\"-Xfatbin -compress-all\"     CMAKE_PREFIX_PATH=\"$(dirname $(which conda))/../\"     pip install -v .' returned a non-zero code: 1\r\n```\r\n\r\n## Expected behavior\r\n\r\nThis will be issue, How i can fix it?\r\n```dockerfile\r\nRUN TORCH_CUDA_ARCH_LIST=\"3.5 5.2 6.0 6.1 7.0+PTX\" TORCH_NVCC_FLAGS=\"-Xfatbin -compress-all\" \\\r\n    CMAKE_PREFIX_PATH=\"$(dirname $(which conda))/../\" \\\r\n    pip install -v .\r\n```\r\n\r\n## Environment\r\n\r\n\r\n - PyTorch Version (e.g., 1.0): lastest\r\n - OS (e.g., Linux): Window10 Home\r\n - How you installed PyTorch (`conda`, `pip`, source): source\r\n - Build command you used (if compiling from source): `docker build -t pytorch -f Dockerfile .`\r\n - Python version: master\r\n - CUDA/cuDNN version: None\r\n - GPU models and configuration: None\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\nMy dockerfile has been modified very little.\r\n```dockerfile\r\nFROM nvidia/cuda:10.0-cudnn7-devel-ubuntu16.04\r\nARG PYTHON_VERSION=3.6\r\nRUN apt-get update && apt-get install -y --no-install-recommends \\\r\n         build-essential \\\r\n         cmake \\\r\n         git \\\r\n         curl \\\r\n         vim \\\r\n         ca-certificates \\\r\n         libjpeg-dev \\\r\n         libpng-dev &&\\\r\n     rm -rf /var/lib/apt/lists/*\r\n\r\n\r\nRUN curl -o ~/miniconda.sh -O  https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh  && \\\r\n     chmod +x ~/miniconda.sh && \\\r\n     ~/miniconda.sh -b -p /opt/conda && \\\r\n     rm ~/miniconda.sh && \\\r\n     /opt/conda/bin/conda install -y python=$PYTHON_VERSION numpy pyyaml scipy ipython mkl mkl-include cython typing && \\\r\n     /opt/conda/bin/conda install -y -c pytorch magma-cuda100 && \\\r\n     /opt/conda/bin/conda clean -ya\r\nENV PATH /opt/conda/bin:$PATH\r\nRUN pip install ninja\r\nWORKDIR /opt\r\nRUN git clone https://github.com/pytorch/pytorch\r\n\r\n# This must be done before pip so that requirements.txt is available\r\nWORKDIR /opt/pytorch\r\nCOPY . .\r\n\r\nRUN git submodule update --init\r\nRUN TORCH_CUDA_ARCH_LIST=\"3.5 5.2 6.0 6.1 7.0+PTX\" TORCH_NVCC_FLAGS=\"-Xfatbin -compress-all\" \\\r\n    CMAKE_PREFIX_PATH=\"$(dirname $(which conda))/../\" \\\r\n    pip install -v .\r\n\r\nRUN git clone https://github.com/pytorch/vision.git && cd vision && pip install -v .\r\n\r\nWORKDIR /workspace\r\nRUN chmod -R a+w /workspace\r\n```\r\n"}
{"number": 20135, "title": "Use ignore=dirty in submodules.", "time": "2019-05-05T04:00:59Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* **#20135 Use ignore=dirty in submodules.**\r\n\r\nSummary: This makes it so that leaving build files in submodule\r\nfolders not longer show '-dirty' in the diffs, which makes it hard\r\nto see if you are about to commit accidental submodule changes.\r\n\r\nTest Plan:\r\n\r\nReviewers:\r\n\r\nSubscribers:\r\n\r\nTasks:\r\n\r\nTags:\n\nDifferential Revision: [D15214187](https://our.internmc.facebook.com/intern/diff/D15214187)"}
{"number": 20136, "title": "Implementation of MatMul with bias operator for MKL-DNN", "time": "2019-05-05T07:13:09Z", "body": "  Matrix multiplication with bias $Y = A * B + Bias$, where `A` has size (M x K) or (Batch_size x M x k), `B` has size (K x N), `Bias` has size (n), and `Y` will have a size (M x N) or (Batch_size x M x N). To transpose `A` or `B` before multiplication, pass 1 to the `trans_a` and/or `trans_b` arguments, which separate the first and second dimensions of the respective matrices using `axis_a` and `axis_b`.\r\n\r\n  This patch depend on #19955.\r\nChange-Id: Iea33d609f814a062d4e2b1b48a96e8a1da9f1072\r\n\r\n"}
{"number": 20137, "title": "fix build with python-2.7.5", "time": "2019-05-05T08:06:50Z", "body": "pytorch failed to build with the following error, complaining about the first regex match\r\nIt may be caused by a bug in python 2.7.5\r\nThis change proposed is a workaround for building pytorch with python 2.7.5\r\nSince the '*' star notation is greedy in python regex, the new expression shall produce the identical result with the old one.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/data2/nihuini/pytorch/cmake/../aten/src/ATen/gen.py\", line 14, in <module>\r\n    import preprocess_declarations\r\n  File \"/data2/nihuini/pytorch/aten/src/ATen/preprocess_declarations.py\", line 3, in <module>\r\n    from function_wrapper import TYPE_FORMAL_GENERIC\r\n  File \"/data2/nihuini/pytorch/aten/src/ATen/function_wrapper.py\", line 5, in <module>\r\n    from code_template import CodeTemplate\r\n  File \"/data2/nihuini/pytorch/aten/src/ATen/code_template.py\", line 13, in <module>\r\n    class CodeTemplate(object):\r\n  File \"/data2/nihuini/pytorch/aten/src/ATen/code_template.py\", line 23, in CodeTemplate\r\n    subtitution = re.compile(substitution_str, re.MULTILINE)\r\n  File \"/usr/lib64/python2.7/re.py\", line 190, in compile\r\n    return _compile(pattern, flags)\r\n  File \"/usr/lib64/python2.7/re.py\", line 242, in _compile\r\n    raise error, v # invalid expression\r\nsre_constants.error: nothing to repeat\r\n-- \r\nCMake Error at cmake/Codegen.cmake:162 (message):\r\n  Failed to get generated_cpp list\r\nCall Stack (most recent call first):\r\n  caffe2/CMakeLists.txt:2 (include)\r\n\r\n\r\n```"}
{"number": 20138, "title": "Inconsistant values of lr_scheduler.get_lr and lr in optimizer.param_groups", "time": "2019-05-05T09:13:24Z", "body": "## üêõ Bug\r\n\r\nAfter upgrading to 1.1.0, the value returned by `lr_scheduler.get_lr` is confusing comparing to the lr value inside `optimizer.param_groups`.\r\n\r\n## To Reproduce\r\n\r\nHere I follow the new convention putting the `lr_scheduler.step()` at the end of each iteration, see the new documents and #7889 (which is probably the root of this issue).\r\n\r\nCode:\r\n\r\n```python\r\n# 1.1.0\r\nimport torch\r\n\r\nnet = torch.nn.Conv2d(1, 1, 1)\r\noptimizer = torch.optim.SGD(net.parameters(), lr=0.1)\r\nlr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\r\n\r\nfor i in range(5):\r\n    print(i, lr_scheduler.get_lr(), optimizer.param_groups[0]['lr'])\r\n    lr_scheduler.step()\r\n```\r\n\r\nOutput:\r\n\r\n```\r\n0 [0.1] 0.1\r\n1 [0.08100000000000002] 0.09000000000000001\r\n2 [0.07290000000000002] 0.08100000000000002\r\n3 [0.06561000000000002] 0.07290000000000002\r\n4 [0.05904900000000002] 0.06561000000000002\r\n```\r\n\r\nWe got inconsistant values. The two values are the **same in the first line**, but **different in following lines** by lr decay factor `gamma = 0.9`. \r\n\r\n## Expected behavior\r\n\r\nAt least we should have consistant values of the two, right? \r\n\r\nIn the old version 1.0.1, if we follow the previous convention putting the `lr_scheduler.step()` at the beginning of each iteration, the output values are reasonable and consistant:\r\n\r\nCode:\r\n```python\r\n# 1.0.1\r\nimport torch\r\n\r\nnet = torch.nn.Conv2d(1, 1, 1)\r\noptimizer = torch.optim.SGD(net.parameters(), lr=0.1)\r\nlr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\r\n\r\nfor i in range(5):\r\n    lr_scheduler.step()\r\n    print(i, lr_scheduler.get_lr(), optimizer.param_groups[0]['lr'])\r\n```\r\n\r\nOutput:\r\n\r\n```\r\n0 [0.1] 0.1\r\n1 [0.09000000000000001] 0.09000000000000001\r\n2 [0.08100000000000002] 0.08100000000000002\r\n3 [0.0729] 0.0729\r\n4 [0.06561] 0.06561\r\n```\r\n\r\n## Environment\r\n\r\nI'll skip this part since it can be easily reproduced from a fresh 1.1.0 installation."}
{"number": 20139, "title": "Fix crash issue in conv+sum fusion for MKLDNN on caffe2", "time": "2019-05-05T09:35:26Z", "body": "The isConvFusion(...) is only for Conv op.\r\nIf non-Conv op, the crash takes place.\r\n"}
{"number": 20140, "title": "TensorBoard logging requires TensorBoard with Python summary writer installed. This should be available in 1.14 or above", "time": "2019-05-05T10:11:21Z", "body": "Traceback (most recent call last):\r\n  File \"/home/tian/.conda/envs/jiqi/lib/python3.6/site-packages/torch/utils/tensorboard/__init__.py\", line 2, in <module>\r\n    from tensorboard.summary.writer.record_writer import RecordWriter  # noqa F401\r\n  File \"/home/tian/Desktop/ai/machine-learn/Image_recognition/tensorboard.py\", line 3, in <module>\r\n    from torch.utils.tensorboard import SummaryWriter\r\nImportError: cannot import name 'SummaryWriter'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/tian/Desktop/ai/machine-learn/Image_recognition/tensorboard.py\", line 3, in <module>\r\n    from torch.utils.tensorboard import SummaryWriter\r\n  File \"/home/tian/.conda/envs/jiqi/lib/python3.6/site-packages/torch/utils/tensorboard/__init__.py\", line 4, in <module>\r\n    raise ImportError('TensorBoard logging requires TensorBoard with Python summary writer installed. '\r\nImportError: TensorBoard logging requires TensorBoard with Python summary writer installed. This should be available in 1.14 or above"}
{"number": 20141, "title": "AvgPool2d: when kernel is NxN while Input is MxN w/o padding : not work right", "time": "2019-05-05T13:21:51Z", "body": "## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nself.model = nn.Sequential((nn.AvgPool2d(kernel_size = 6 )))(x)\r\n1.x = [16,1,3,6]\r\n1.No error message despite the fact that no padding was allowed by configuration\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. --> I was expecting an error message like in TF\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux):\r\n - How you installed PyTorch (`conda`, `pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"}
{"number": 20142, "title": "Convolution over tensor with different batch-sizes gets different results on RTX 2080Ti", "time": "2019-05-05T14:11:29Z", "body": "## Convolution over tensor with different batch-sizes gets different results on RTX 2080Ti\r\n\r\n\r\n## To Reproduce\r\n\r\nI tried to use both pytorch 1.0.0 and the recently released pytorch 1.1.0 and run the following test codes.\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nx1 = torch.Tensor(1, 3, 8, 256, 256).uniform_().cuda()\r\nx2 = torch.cat((x1, x1, x1),dim=0)\r\nx2 = x2.cuda()\r\nconv = nn.Conv3d(3, 64, kernel_size=(5,7,7)).cuda()\r\ny1 = conv(x1)\r\ny2 = conv(x2)\r\nprint(torch.mean(torch.abs(y2[0,...] - y1[0,...])))\r\nx1 = torch.Tensor(4, 3, 8, 256, 256).uniform_().cuda()\r\nx2 = torch.cat((x1, x1, x1),dim=0)\r\nx2 = x2.cuda()\r\nconv = nn.Conv3d(3, 64, kernel_size=(5,7,7)).cuda()\r\ny1 = conv(x1)\r\ny2 = conv(x2)\r\nprint(torch.mean(torch.abs(y2[0,...] - y1[0,...])))\r\nx1 = torch.Tensor(1, 3, 256, 256).uniform_().cuda()\r\nx2 = torch.cat((x1, x1, x1),dim=0)\r\nx2 = x2.cuda()\r\nconv = nn.Conv2d(3, 64, kernel_size=(7,7)).cuda()\r\ny1 = conv(x1)\r\ny2 = conv(x2)\r\nprint(torch.mean(torch.abs(y2[0,...] - y1[0,...])))\r\nx1 = torch.Tensor(10, 3, 256, 256).uniform_().cuda()\r\nx2 = torch.cat((x1, x1, x1),dim=0)\r\nx2 = x2.cuda()\r\nconv = nn.Conv2d(3, 64, kernel_size=(7,7)).cuda()\r\ny1 = conv(x1)\r\ny2 = conv(x2)\r\nprint(torch.mean(torch.abs(y2[0,...] - y1[0,...])))\r\n```\r\n\r\n\r\n## Expected behavior\r\n\r\nI expected all of the outputs to be zeros. However, on RTX 2080Ti, I get some output like below:\r\n```\r\ntensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\r\ntensor(1.2948e-07, device='cuda:0', grad_fn=<MeanBackward0>)\r\ntensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\r\ntensor(9.3088e-08, device='cuda:0', grad_fn=<MeanBackward0>)\r\n```\r\nAs a reference, on Titan Xp, the results is \r\n```\r\ntensor(0., device='cuda:0', grad_fn=<MeanBackward1>)\r\ntensor(0., device='cuda:0', grad_fn=<MeanBackward1>)\r\ntensor(0., device='cuda:0', grad_fn=<MeanBackward1>)\r\ntensor(0., device='cuda:0', grad_fn=<MeanBackward1>)\r\n```\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: GeForce RTX 2080 Ti\r\nGPU 1: GeForce RTX 2080 Ti\r\nGPU 2: GeForce RTX 2080 Ti\r\nGPU 3: GeForce RTX 2080 Ti\r\nGPU 4: GeForce RTX 2080 Ti\r\nGPU 5: GeForce RTX 2080 Ti\r\nGPU 6: GeForce RTX 2080 Ti\r\nGPU 7: GeForce RTX 2080 Ti\r\n\r\nNvidia driver version: 418.43\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.3\r\n[pip] torch==1.1.0\r\n[pip] torchvision==0.2.2.post3\r\n[conda] torch                     1.1.0                     <pip>\r\n[conda] torchvision               0.2.2.post3               <pip>\r\n```\r\n"}
{"number": 20143, "title": "TripletMarginLoss example isn't clear", "time": "2019-05-05T16:00:24Z", "body": "## üìö Documentation\r\n\r\nThe example in https://pytorch.org/docs/stable/nn.html#tripletmarginloss isn't very clear. It doesn't specify which input is the anchor, positive, and negative. \r\nCurrent:\r\n>>> triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\r\n>>> input1 = torch.randn(100, 128, requires_grad=True)\r\n>>> input2 = torch.randn(100, 128, requires_grad=True)\r\n>>> input3 = torch.randn(100, 128, requires_grad=True)\r\n>>> output = triplet_loss(input1, input2, input3)\r\n>>> output.backward()\r\n\r\nProposed:\r\n>>> triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\r\n>>> anchor = torch.randn(100, 128, requires_grad=True)\r\n>>> positive = torch.randn(100, 128, requires_grad=True)\r\n>>> negative = torch.randn(100, 128, requires_grad=True)\r\n>>> output = triplet_loss(anchor, positive, negative)\r\n>>> output.backward()\r\n\r\n\r\n"}
{"number": 20144, "title": "maybe_group && maybe_group == fused_cat ASSERT FAILED", "time": "2019-05-05T16:46:26Z", "body": "## üêõ Bug\r\n\r\nI get the following error during the backwards pass of a JITted model that I'm training. The trace is as follows:\r\n\r\n```sh\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 157, in <module>\r\n    (observation_loss + reward_loss + kl_loss).backward()\r\n  File \"/home/kai/miniconda3/lib/python3.6/site-packages/torch/tensor.py\", line 107, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File \"/home/kai/miniconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 93, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\nRuntimeError: maybe_group && maybe_group == fused_cat ASSERT FAILED at /opt/conda/conda-bld/pytorch-cpu_1556653093101/work/torch/csrc/jit/passes/graph_fuser.cpp:1223, please report a bug to PyTorch.\r\n```\r\n\r\n## To Reproduce\r\n\r\nThis is from my [PlaNet repo](https://github.com/Kaixhin/PlaNet), where I'm trying to replace `nn.GRUCell` with my own JIT variant:\r\n\r\n```py\r\nclass GRUCell(jit.ScriptModule):\r\n  __constants__ = ['hidden_size']\r\n\r\n  def __init__(self, input_size, hidden_size):\r\n    super().__init__()\r\n    self.hidden_size = hidden_size\r\n    self.weight = nn.Parameter(torch.randn(input_size + hidden_size, 3 * hidden_size))\r\n    self.bias = nn.Parameter(torch.randn(3 * hidden_size))\r\n\r\n  @jit.script_method\r\n  def forward(self, input, hidden):\r\n    update, reset = torch.chunk(torch.sigmoid(torch.addmm(self.bias[:2 * self.hidden_size], torch.cat([input, hidden], dim=1), self.weight[:, :2 * self.hidden_size])), 2, dim=1)\r\n    candidate = torch.tanh(torch.addmm(self.bias[2 * self.hidden_size:], torch.cat([input, reset * hidden], dim=1), self.weight[:, 2 * self.hidden_size:]))\r\n    return update * hidden + (1 - update) * candidate\r\n```\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.3\r\n[pip] torch==1.1.0\r\n[pip] torchvision==0.2.2\r\n[conda] blas                      1.0                         mkl  \r\n[conda] cuda91                    1.0                  h4c16780_0    pytorch\r\n[conda] faiss-cpu                 1.5.1            py36h6bb024c_1    pytorch\r\n[conda] mkl                       2019.3                      199  \r\n[conda] mkl_fft                   1.0.12           py36ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py36hd81dba3_0  \r\n[conda] pytorch-cpu               1.1.0               py3.6_cpu_0    pytorch\r\n[conda] torchvision-cpu           0.2.2                      py_3    pytorch\r\n```"}
{"number": 20145, "title": " #20143 TripletMarginLoss example isn't clear", "time": "2019-05-06T01:01:21Z", "body": "Pull Request for TripletMarginLoss example isn't clear #20143\r\n@SsnL "}
{"number": 20146, "title": "CPU Memory leak when using weight_decay in libtorch", "time": "2019-05-06T03:24:11Z", "body": "## Bug\r\nWhen using weight_decay in libtorch, CPU Memory usage is slowly increasing.\r\n\r\n## To Reproduce\r\nI used docker container \"nvidia/cuda:9.2-cudnn7-devel-ubuntu18.04\", stable libtorch(1.1) for cuda9.0. This phenomenon can be reproduced using the mnist example in the pytorch/example repository.\r\n\r\nrewrite examples/cpp/mnist/mnist.cpp l.148\r\n\r\n```cpp\r\n- model.parameters(), torch::optim::SGDOptions(0.01).momentum(0.5));\r\n+ model.parameters(), torch::optim::SGDOptions(0.01).momentum(0.5).weight_decay(1e-4));\r\n```\r\n\r\nBecause the speed of increase is very slow, it may be better to increase the number of epochs. This happens with both CPU learning and GPU learning.\r\n\r\n## Environment\r\n- OS: Ubuntu 18.04.2 LTS\r\n- GCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n- CMake version: version 3.10.2\r\n- CUDA runtime version: 9.2.148\r\n- GPU models and configuration: GPU 0: GeForce RTX 2080 Ti\r\n- Nvidia driver version: 410.104\r\n- cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.1\r\n\r\n## Additional context\r\nThis phenomenon happens also in cuda10.0 and libtorch nightly build for cuda10.0."}
{"number": 20147, "title": "Revert \"Fix lr_scheduler's last_epoch value at the time of initialization (BC BREAKING!) (#7889)\"", "time": "2019-05-06T03:52:33Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20147 Revert \"Fix lr_scheduler's last_epoch value at the time of initialization (BC BREAKING!) (#7889)\"**\n\nThis reverts commit 36084908e4d31d70d1c8022fb979059e6984ecc6."}
{"number": 20148, "title": "Fix small typo T_mul->T_mult", "time": "2019-05-06T04:30:08Z", "body": ""}
{"number": 20149, "title": "Advanced indexing with uint8 tensor versus int64 tensor is inconsistent", "time": "2019-05-06T04:33:39Z", "body": "## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\ndifferent behavior of slicing tensor with slices in different tensor type\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n```\r\nimport torch\r\n\r\na = torch.randint(low=1, high=10, size=(2, 3))\r\nprint(a)\r\nb = torch.randint(2, (3,), dtype=torch.uint8)\r\nprint(b)\r\na[0, b] = 0\r\nprint(a)\r\n\r\n```\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nif dtype of b is torch.uint8, it works normally like:\r\n```\r\ntensor([[9, 5, 3],\r\n        [1, 3, 9]])\r\ntensor([1, 0, 1], dtype=torch.uint8)\r\ntensor([[0, 5, 0],\r\n        [1, 3, 9]])\r\n```\r\nif I change dtype to torch.int64, it woks differently like:\r\n```\r\ntensor([[3, 9, 9],\r\n        [8, 7, 7]])\r\ntensor([1, 0, 1])\r\ntensor([[0, 0, 9],\r\n        [8, 7, 7]])\r\n```\r\nit seems that the value of slices act as slices instead of the position of no-zero elements\r\n## Environment\r\n\r\n\r\n - PyTorch Version (e.g., 1.0): 1.1.0\r\n - OS (e.g., Linux): Mac os 10.14.3\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: None\r\n - GPU models and configuration:  None\r\n - Any other relevant information:  None\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"}
{"number": 20150, "title": "DataLoader: add error detection for worker_init_fn", "time": "2019-05-06T05:43:19Z", "body": "This is an attempt to isolate unrelated changes from #19228 for easier review.\r\n\r\n\r\n"}
{"number": 20151, "title": "RuntimeError: Given input size: (2048x1x1). Calculated output size: (2048x-5x-5). Output size is too small at /pytorch/aten/src/THNN/generic/SpatialAveragePooling.c:48", "time": "2019-05-06T05:48:24Z", "body": "```\r\nimport torch\r\nimport torchvision\r\nfrom torch.utils.tensorboard import SummaryWriter\r\nfrom torchvision import datasets, transforms\r\n```\r\n\r\n  # Writer will output to ./runs/ directory by default\r\n```\r\nwriter = SummaryWriter()\r\n\r\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\r\ntrainset = datasets.MNIST('mnist', train=True, download=True, transform=transform)\r\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\r\nmodel = torchvision.models.resnet50(False)\r\n  # Have ResNet model take in grayscale rather than RGB\r\nmodel.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\r\nimages, labels = next(iter(trainloader))\r\ngrid = torchvision.utils.make_grid(images)\r\nwriter.add_image('images', grid, 0)\r\nwriter.add_graph(model, images)\r\nwriter.close()\r\n\r\n```\r\n\r\n> Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\r\n> Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\r\n> Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\r\n> Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\r\n> Processing...\r\n> Done!\r\n> Error occurs, No graph saved\r\n> Traceback (most recent call last):\r\n>   File \"/home/tian/.conda/envs/jiqi/lib/python3.6/site-packages/torch/utils/tensorboard/_pytorch_graph.py\", line 276, in graph\r\n>     trace, _ = torch.jit.get_trace_graph(model, args)\r\n>   File \"/home/tian/.conda/envs/jiqi/lib/python3.6/site-packages/torch/jit/__init__.py\", line 231, in get_trace_graph\r\n>     return LegacyTracedModule(f, _force_outplace, return_inputs)(*args, **kwargs)\r\n>   File \"/home/tian/.conda/envs/jiqi/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 493, in __call__\r\n>     result = self.forward(*input, **kwargs)\r\n>   File \"/home/tian/.conda/envs/jiqi/lib/python3.6/site-packages/torch/jit/__init__.py\", line 294, in forward\r\n>     out = self.inner(*trace_inputs)\r\n>   File \"/home/tian/.conda/envs/jiqi/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 491, in __call__\r\n>     result = self._slow_forward(*input, **kwargs)\r\n>   File \"/home/tian/.conda/envs/jiqi/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 481, in _slow_forward\r\n>     result = self.forward(*input, **kwargs)\r\n>   File \"/home/tian/.conda/envs/jiqi/lib/python3.6/site-packages/torchvision/models/resnet.py\", line 149, in forward\r\n>     x = self.avgpool(x)\r\n>   File \"/home/tian/.conda/envs/jiqi/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 491, in __call__\r\n>     result = self._slow_forward(*input, **kwargs)\r\n>   File \"/home/tian/.conda/envs/jiqi/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 481, in _slow_forward\r\n>     result = self.forward(*input, **kwargs)\r\n>   File \"/home/tian/.conda/envs/jiqi/lib/python3.6/site-packages/torch/nn/modules/pooling.py\", line 563, in forward\r\n>     self.padding, self.ceil_mode, self.count_include_pad)\r\n> RuntimeError: Given input size: (2048x1x1). Calculated output size: (2048x-5x-5). Output size is too small at /pytorch/aten/src/THNN/generic/SpatialAveragePooling.c:48\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n> Traceback (most recent call last):\r\n>   File \"/home/tian/pycharm-2018.3.3/helpers/pydev/pydevd.py\", line 1741, in <module>\r\n>     main()\r\n>   File \"/home/tian/pycharm-2018.3.3/helpers/pydev/pydevd.py\", line 1735, in main\r\n>     globals = debugger.run(setup['file'], None, None, is_module)\r\n>   File \"/home/tian/pycharm-2018.3.3/helpers/pydev/pydevd.py\", line 1135, in run\r\n>     pydev_imports.execfile(file, globals, locals)  # execute the script\r\n>   File \"/home/tian/pycharm-2018.3.3/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n>     exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n>   File \"/home/tian/Desktop/ai/machine-learn/Image_recognition/tensorboard_test.py\", line 18, in <module>\r\n>     writer.add_graph(model, images)\r\n>   File \"/home/tian/.conda/envs/jiqi/lib/python3.6/site-packages/torch/utils/tensorboard/writer.py\", line 534, in add_graph\r\n>     self._get_file_writer().add_graph(graph(model, input_to_model, verbose, **kwargs))\r\n>   File \"/home/tian/.conda/envs/jiqi/lib/python3.6/site-packages/torch/utils/tensorboard/_pytorch_graph.py\", line 279, in graph\r\n>     _ = model(*args)  # don't catch, just print the error message\r\n>   File \"/home/tian/.conda/envs/jiqi/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 493, in __call__\r\n>     result = self.forward(*input, **kwargs)\r\n> TypeError: forward() takes 2 positional arguments but 65 were given\r\n\r\n- \r\n![2019-05-06 14-14-41 ÁöÑÂ±èÂπïÊà™Âõæ](https://user-images.githubusercontent.com/37957822/57209028-52579980-7009-11e9-9e16-4c694ae08c33.png)\r\n"}
{"number": 20152, "title": "tweak scripts/build_android.sh for ABI and header install", "time": "2019-05-06T07:35:47Z", "body": "We now can build libtorch for Android.\r\nThis patch aims to provide two improvements to the build\r\n- Make the architecture overridable by providing an environment variable `ANDROID_ABI`.\r\n- Use `--target install` when calling cmake to actually get the header files nicely in one place.\r\n\r\nI ran the script without options to see if the caffe2 builds are affected (in particularly by the install), but they seem to run OK and probably only produce a few files in build_android/install.\r\n"}
{"number": 20153, "title": "JIT scripted indexing with ellipsis and None is not working correctly", "time": "2019-05-06T07:49:45Z", "body": "## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. use torch.chunk on torch.jit.script_method\r\n\r\n```python\r\nimport torch\r\n\r\nclass BatchMod2(torch.jit.ScriptModule):\r\n    def __init__(self, x_dim, z_dim):\r\n        super().__init__()\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, x, z):\r\n        mean_scale = z.repeat(1, 2)\r\n        mean, scale = torch.chunk(mean_scale[..., None, None], 2, 1)\r\n        y = x * (scale + 1) + mean\r\n        return y\r\n\r\n\r\nb = BatchMod2(6, 6)\r\na1 = torch.ones(2,6,4,4)\r\na2 = torch.ones(2,6)\r\nc=b(a1, a2)\r\n```\r\n\r\n## Expected behavior\r\n\r\nNo exception.\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.1.0\r\n - OS (e.g., Linux): win10\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7.1\r\n - CUDA/cuDNN version: 10.0\r\n - GPU models and configuration:gtx970m\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\nWhen I comment @torch.jit.script_method, it can work normally.\r\n"}
{"number": 20154, "title": "convert the model from pytorch to onnx to caffe2, but get a lower accuracy than before", "time": "2019-05-06T08:14:10Z", "body": "When I convert the model from pytorch into caffe2, the top1 and top5 accuracy rate on ILSVRC12 dataset became smaller.\r\n\r\nThe procedure for converting is:\r\n\r\n1.convert pytorch into onnx IR\r\n2.convert onnx IR into caffe2\r\n\r\nI have tested many pretrained models. All of the accuracy decreased a lot.\r\n\r\n![EDFhQJ.png](https://s2.ax1x.com/2019/05/06/EDFhQJ.png)\r\n\r\nI don't know what caused the problem. May the internal differences between the pytorch and caffe2 make ?"}
{"number": 20155, "title": "No module named _msvccompiler", "time": "2019-05-06T10:49:09Z", "body": "## ‚ùì Questions and Help\r\nI tried to install caffe2 in Windows10, vs2017 following [this](https://caffe2.ai/docs/getting-started.html?platform=windows&configuration=compile) and got this problem.\r\n![image](https://user-images.githubusercontent.com/31846112/57220828-60b8ac00-702f-11e9-9cf8-12a952064f91.png)\r\n\r\ndoes anyone known how to solve that?\r\nthanks.\r\n"}
{"number": 20156, "title": "libtorch does not initialize OpenMP/MKL by default", "time": "2019-05-06T11:14:55Z", "body": "I find that matrix multiplication is slower in C++ API, so I write the same code in C++ and python and record their execution times, code is as following:\r\n\r\n**C++:**\r\n```\r\n#include<torch/torch.h>\r\n#include<iostream>\r\n#include <chrono>\r\n\r\nint main(){\r\n\ttorch::Tensor tensor = torch::randn({2708, 1433});\r\n\ttorch::Tensor weight = torch::randn({1433, 16});\r\n\tauto start = std::chrono::high_resolution_clock::now();\r\n\ttensor.mm(weight);\r\n\tauto end = std::chrono::high_resolution_clock::now();\r\n\tstd::cout<< \"C++ Operation Time(s) \" << std::chrono::duration<double>(end - start).count() << \"s\" << \tstd::endl;\r\n\treturn 0;\r\n}\r\n```\r\n\r\n**Result**:\r\n```\r\nC++ Operation Time(s) 0.082496s\r\n```\r\n\r\n**python:**\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\ntensor = torch.randn(2708, 1433)\r\nweight = torch.randn(1433, 16)\r\nt0 = time.time()\r\ntensor.mm(weight)\r\nt1 = time.time()\r\nprint(\"Python Operation Time(s) {:.4f}\".format(t1 - t0))\r\n```\r\n\r\n**Result**:\r\n```\r\nPython Operation Time(s) 0.0114\r\n```\r\n\r\n**Testing Environment:**\r\n```\r\nubuntu 16.04\r\ngcc version 5.4.0\r\npython version 3.7.3\r\npytorch version 1.0.1\r\n```\r\n\r\nIt's not a small difference, why is it happen???\r\n"}
{"number": 20157, "title": "Support for Eigen thread pool", "time": "2019-05-06T11:42:19Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* **#20157 Support for Eigen thread pool**\r\n* #20087 Native ATen/Parallel backend\r\n* #20057 Split ATen/Parallel into interface and backend\r\n* #20050 Move inter-op settings into ATen/Parallel\r\n* #20043 Port ATen/native to ATen/Parallel\r\n* #20032 Port THNN to ATen/Parallel\r\n* #20002 Remove explicit checks for parallelism from TH\r\n* #19105 Port TH library to ATen/Parallel\r\n* #19997 Intra-op parallel microbenchmarks for PT\r\n* #19993 Support for non-contiguous tensors and arbitrary dtypes in PT benchmarks\r\n* #19980 Initialize Caffe2 only when running Caffe2 benchmarks\r\n* #19749 Allow a non-OpenMP based build\r\n\r\nSummary:\r\nAdding support for Eigen thread pool as a replacement\r\nfor the current native thread pool\r\n\r\nTest Plan:\r\nUSE_EIGEN_THREADPOOL=1 USE_CUDA=0 PARALLEL_BACKEND=NATIVE BLAS=MKL\r\nUSE_MKLDNN=1 USE_OPENCV=1 USE_FFMPEG=1 python setup.py develop --cmake\r\npytest -s -v test/test_torch.py::TestTorch\r\npytest -s -v test/test_jit.py\n\nDifferential Revision: [D15248708](https://our.internmc.facebook.com/intern/diff/D15248708)"}
{"number": 20158, "title": "Fix the warning if the wrong gcc is used with nvcc", "time": "2019-05-06T12:08:53Z", "body": "Fixes https://github.com/pytorch/pytorch/issues/11886"}
{"number": 20159, "title": "[TESTING] ghexport smoketest", "time": "2019-05-06T13:19:34Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20160 [TESTING] ghexport smoketest 2&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15217826/)\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20159 [TESTING] ghexport smoketest**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15217827/)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/20159\n\nDifferential Revision: [D15217827](https://our.internmc.facebook.com/intern/diff/D15217827/)"}
{"number": 20160, "title": "[TESTING] ghexport smoketest 2", "time": "2019-05-06T13:19:40Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20160 [TESTING] ghexport smoketest 2**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15217826/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20159 [TESTING] ghexport smoketest&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15217827/)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/20160\n\nDifferential Revision: [D15217826](https://our.internmc.facebook.com/intern/diff/D15217826/)"}
{"number": 20161, "title": "Get error when do the backward:‚ÄúRuntimeError: Tensor: invalid storage offset‚Äù?", "time": "2019-05-06T13:21:53Z", "body": "When I use pytorch to do the backward grad, get error: \r\n`RuntimeError: Tensor: invalid storage offset at c:\\users\\administrator\\downloads\\new-builder\\win-wheel\\pytorch\\aten\\src\\th\\generic/THTensor.cpp:761`\r\n\r\nand here are part of codes\r\n```\r\nH_grad = torch.autograd.grad(outputs=prediction.sum(), inputs=database_choose, create_graph=True)[0] Ht=H_grad[:,1].reshape(choose,1) Htt=torch.autograd.grad(outputs=Ht.sum(), inputs=database_choose,create_graph=True)[0][:,1].reshape(choose,1) \r\nloss.backward()\r\n```\r\n"}
{"number": 20162, "title": "Add USE_NAMEDTENSOR compilation flag.", "time": "2019-05-06T13:24:13Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #20168 [namedtensor] Add test/test_namedtensor.py\n* #20163 Add namedtensor build and test to the CI\n* **#20162 Add USE_NAMEDTENSOR compilation flag.**\n\nSets the NAMEDTENSOR_ENABLED macro (in cpp) and\ntools.setup_helpers.env.NAMEDTENSOR_ENABLED.\n\nTest Plan:\n- Compile with NAMEDTENSOR_ENABLED=1. Verify that\n`torch.__config__.show()` has -DNAMEDTENSOR_ENABLED in the CXX flags.\n- Compile without the flag. Verify that `-DNAMEDTENSOR_ENABLED` is not\npresent in `torch.__config__.show()`.\n\ngh-metadata: pytorch pytorch 20162 gh/zou3519/32/head\n\nDifferential Revision: [D15278211](https://our.internmc.facebook.com/intern/diff/D15278211)"}
{"number": 20163, "title": "Add namedtensor build and test to the CI", "time": "2019-05-06T15:11:26Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #20168 [namedtensor] Add test/test_namedtensor.py\n* **#20163 Add namedtensor build and test to the CI**\n* #20162 Add USE_NAMEDTENSOR compilation flag.\n\nThese always run on master. In addition, these will only run on pull\nrequests if they have namedtensor in their title.\n\nRight now, the build + test are equivalent to the normal pytorch\nbuild with USE_NAMEDTENSOR=1 + test, if they run.\n\nTest Plan:\n- Check that the new namedtensor build + test runs on this PR and exits with success through `circleci step halt`\n- Inspect the generated config.yml\n\ngh-metadata: pytorch pytorch 20163 gh/zou3519/33/head\n\nIf BUILD_ENVIRONMENT contains\n\nDifferential Revision: [D15278216](https://our.internmc.facebook.com/intern/diff/D15278216)"}
{"number": 20164, "title": "More stable computation of KL between two Bernoulli distributions", "time": "2019-05-06T15:59:23Z", "body": "I propose to use `Bernoulli.logits` instead of `Bernoulli.probs` when computing the KL between two Bernoulli distributions. The two formulations are equivalent but since usually neural networks output logits, the second is more stable."}
{"number": 20165, "title": "torch.nn.threshold cannot accept tensor as a threshold", "time": "2019-05-06T16:18:13Z", "body": "## üöÄ Feature\r\nI think it will be useful if we can pass threshold as a tensor to the threshold function. in this way we can compute the backprop of the threshold tensor and use it in training process.\r\n\r\nright now the threshold function just takes non-tensor threshold"}
{"number": 20166, "title": "Improve test_proper_exit error printing", "time": "2019-05-06T16:18:35Z", "body": "This doesn't have `strace` yet. But still have `faulthandler` to print stack traces at hanging. Also part of an attempt to isolate changes from #19228 ."}
{"number": 20167, "title": "What kind of cuda version needed for Ubuntu 18.04, and pytorch version", "time": "2019-05-06T16:22:43Z", "body": "## ‚ùì Questions and Help\r\nCuda 10.1 seems not working\r\n### Please note that this issue tracker is not a help form and this issue will be closed.\r\n\r\nWe have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:\r\n\r\n- [Discussion Forum](https://discuss.pytorch.org/)\r\n"}
{"number": 20168, "title": "[namedtensor] Add test/test_namedtensor.py", "time": "2019-05-06T16:25:03Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20168 [namedtensor] Add test/test_namedtensor.py**\n* #20163 Add namedtensor build and test to the CI\n* #20162 Add USE_NAMEDTENSOR compilation flag.\n\nTests are skipped if pytorch is compiled without named tensor support\n(default).\n\nTest Plan\n\nCompile without namedtensor support\n- python test/run_test.py -v --include namedtensor\n- Check the output for \"skipped because not compiled with named tensor\n\nCompile with namedtensor support\n- python test/run_test.py -v --include namedtensor\n- Check the output for success on TestNamedTensor.test_trivial\n\nThe OSS CI should compile with namedtensor support for this PR.\n\ngh-metadata: pytorch pytorch 20168 gh/zou3519/34/head\n\nDifferential Revision: [D15278222](https://our.internmc.facebook.com/intern/diff/D15278222)"}
{"number": 20169, "title": "[extension-cpp] extra_compile_args inplace operations", "time": "2019-05-06T16:30:35Z", "body": "## üêõ Bug\r\n\r\nProblem occurs when building multiple extension, and giving the same dictionnary for `extra_compile_args`\r\n\r\n## To Reproduce\r\n\r\nVery easy to reproduce with https://github.com/pytorch/extension-cpp/tree/master/cuda\r\n\r\nchange the setup.py to have a second fake extension and add extra_compile_args.\r\nthis version of `setup.py` won't work, but the second proposed version will:\r\n\r\n```python\r\nargs = {'nvcc':[], 'cxx':[]}\r\n\r\nsetup(\r\n    name='lltm_cuda',\r\n    ext_modules=[\r\n        CUDAExtension('lltm_cuda', [\r\n            'lltm_cuda.cpp',\r\n            'lltm_cuda_kernel.cu',\r\n        ], extra_compile_args=args),\r\n        CUDAExtension('lltm_cuda2', [\r\n            'lltm_cuda.cpp',\r\n            'lltm_cuda_kernel.cu',\r\n        ], extra_compile_args=args),\r\n    ],\r\n    cmdclass={\r\n        'build_ext': BuildExtension\r\n    })\r\n```\r\nThis outputs (although it compiles)\r\n```\r\n<command-line>:0:0: warning: \"TORCH_EXTENSION_NAME\" redefined\r\n<command-line>:0:0: note: this is the location of the previous definition\r\n```\r\nand importing first module gets the error\r\n```\r\nIn [1]: import lltm_cuda                                                                                                                                                                 \r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-15b747285754> in <module>\r\n----> 1 import lltm_cuda\r\n\r\nImportError: dynamic module does not define module export function (PyInit_lltm_cuda)\r\n```\r\n-------\r\n```python\r\nargs = {'nvcc':[], 'cxx':[]}\r\n\r\nsetup(\r\n    name='lltm_cuda',\r\n    ext_modules=[\r\n        CUDAExtension('lltm_cuda', [\r\n            'lltm_cuda.cpp',\r\n            'lltm_cuda_kernel.cu',\r\n        ], extra_compile_args=args),\r\n        CUDAExtension('lltm_cuda2', [\r\n            'lltm_cuda.cpp',\r\n            'lltm_cuda_kernel.cu',\r\n        ], extra_compile_args=copy.deepcopy(args)),\r\n    ],\r\n    cmdclass={\r\n        'build_ext': BuildExtension\r\n    })\r\n```\r\nThis works fine and both modules can be imported.\r\n\r\n## Expected behavior\r\n\r\nSame behaviour between the two versions of the install script.\r\n\r\n## Additional context\r\n\r\nThe problem comes from the fact that a simple shallow copy of dictionnary is not enough, since compile args are nested into two lists, one for C++ and one for cuda.\r\nRecommended actions should be either change this line \r\nhttps://github.com/pytorch/pytorch/blob/master/torch/utils/cpp_extension.py#L373\r\nto make a `copy.deepcopy` instead of a simple `copy.copy`, or change this line https://github.com/pytorch/pytorch/blob/master/torch/utils/cpp_extension.py#L376 to make a second `copy.copy` .\r\n\r\nFirst solution is less verbose but might be less secure, since we can't control the depth of the copy. I'm personally more inclined to first solution as I don't see a way to unintentionally copy too much with a simple `extra_compile_args` dictionnary."}
{"number": 20170, "title": "nn.Transformer", "time": "2019-05-06T16:32:24Z", "body": "Create a PR for comments. The model is still WIP but I want to have some feedbacks before moving too far. The transformer model depends on several modules, like MultiheadAttention (landed).\r\n\r\nTransformer is implemented based on the paper (https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf). Users have the flexibility to build a transformer with self-defined and/or built-in components (i.e encoder, decoder, encoder_layer, decoder_layer). Users could use Transformer class to build a standard transformer model and modify sub-layers as needed.\r\n\r\nAdd a few unit tests for the transformer module, as follow:\r\nTestNN.test_Transformer_cell\r\nTestNN.test_transformerencoderlayer\r\nTestNN.test_transformerdecoderlayer\r\nTestNN.test_transformer_args_check\r\nTestScript.test_scriptmodule_transformer_cuda\r\n\r\nThere is another demonstration example for applying transformer module on the word language problem. https://github.com/pytorch/examples/pull/555"}
{"number": 20171, "title": "Extract feature length information from SigridTransforms op", "time": "2019-05-06T17:22:05Z", "body": "Summary: Extract feature length information from SigridTransforms op\n\nDifferential Revision: D15219408\n\n"}
{"number": 20172, "title": "Disable worker_kill & holder_iter_reference combination in test_proper_exit", "time": "2019-05-06T17:31:32Z", "body": "cc @nairbv \r\nAll failures I have seen are of this combination. So let's just disable it for all cases. After #20063 I find it failing for py3 once."}
{"number": 20173, "title": "Enable operator profiling via command line", "time": "2019-05-06T17:33:41Z", "body": "Summary:\nEnabled op profiling even when net type is not dag or prof dag. Also added\nengine type info to summary.\n\nDifferential Revision: D15177813\n\n"}
{"number": 20174, "title": "temporarily disable devtoolset7 nightlies", "time": "2019-05-06T18:35:49Z", "body": "toward issue #20066\r\n\r\n"}
{"number": 20175, "title": "Autograd profile recording in c10 custom ops", "time": "2019-05-06T19:03:14Z", "body": "Summary: This ensures that custom operators registered through c10::RegisterOperators are recorded in autograd profile traces.\n\nDifferential Revision: D15221311\n\n"}
{"number": 20176, "title": "Use torch::get/set_num_threads without additional includes beyond torch/torch.h", "time": "2019-05-06T19:46:03Z", "body": "Fixes https://github.com/pytorch/pytorch/issues/20130."}
{"number": 20177, "title": "Disable incremental_state function in MultiheadAttention module. Add more tests for MultiheadAttention function.", "time": "2019-05-06T20:03:53Z", "body": "To fully support incremental_state function, it requires several additional utils available in fairseq. However, we lack a problem for the unit test. Therefore, the incremental_state function will be disable for now. If it is needed in the future, a feature request could be created. Fixed #20132\r\n\r\nAdd some unit tests to cover the arguments of MultiheadAttention module, including bias, add_bias_kv, add_zero_attn, key_padding_mask, need_weights, attn_mask. "}
{"number": 20178, "title": "Tensor.__floordiv__ missing from pyi", "time": "2019-05-06T20:13:15Z", "body": "## üêõ Bug\r\n\r\n`Tensor.__floordiv__` is not provided in pyi files.\r\n\r\n## To Reproduce\r\n\r\nDivide 2 tensors (or a tensor and an int) with `//` in PyCharm."}
{"number": 20179, "title": "Avoid log(0.0) in the 'gumbel_softmax' function", "time": "2019-05-06T21:19:49Z", "body": "tensor.exponential_() on cuda device may generate 0.0; below is the code for reproducing the error.\r\n\r\n```\r\nimport torch\r\ntorch.manual_seed(0) \r\ntorch.cuda.set_device(0)\r\n\r\ncnt = 0 \r\nwhile True:\r\n    randns = torch.empty((10000000,), device=torch.cuda.current_device()).exponential_() \r\n    #randns = torch.empty((10000000,)).exponential_() \r\n    gumbel = -randns.log() \r\n\r\n    cnt += 1\r\n    idxes = torch.isinf(gumbel)\r\n    if idxes.any():\r\n        _, idx = torch.max(idxes, 0)\r\n        print('{} is sampled in the {}-th entry in the {}-th sampling'.format(randns[idx], idx, cnt))\r\n        break\r\n    else:\r\n        print('{}'.format(cnt))\r\n```\r\noutput: -0.0 is sampled in the 376731-th entry in the 1-th sampling"}
{"number": 20180, "title": "Adding ShufflenetV2 to caffe2's benchmark suite. (#20080)", "time": "2019-05-06T21:31:02Z", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/20080\n\nAdding ShufflenetV2 (by Ma et. al. 2018) to the caffe2's benchmark\nsuite.\n\nTo run, use: `buck run mode/opt caffe2/caffe2/python/examples:imagenet_trainer -- --train_data null --batch_size 128 --epoch_size 3200 --num_epochs 2 --num_gpus 2 --model shufflenet`\n\nDifferential Revision: D15094282\n\n"}
{"number": 20181, "title": "Add a FAQ entry to explain `Cannot insert a Tensor that requires grad as a constant`", "time": "2019-05-06T21:49:12Z", "body": ""}
{"number": 20182, "title": "fix WAR race", "time": "2019-05-06T21:49:20Z", "body": "was flagged by racecheck. "}
{"number": 20183, "title": "Latex Errors when Compiling documentation to latexpdf", "time": "2019-05-06T21:59:45Z", "body": "## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nI raise this issue because I encountered many warnings and errors compiling the documentation.\r\nI checked out version 1.1.0, and followed the instruction of \"Building the Documentation\" by running pip install -r requirements, and then make latexpdf. I already have texlive, latexmk, and other necessary latex packages installed. My system is Ubuntu 19.04.\r\n\r\nErrors include missing references, missing \"$\", extra alignment, etc. Given how verbose latex's error list is, I am not listing everything here. I can upload a log file of the errors if that is desired.\r\n\r\nIs there any specific version of latex that is required? \r\n\r\nThe reason I try to get a PDF is that PDF is so much better as a single file format when I don't get internet. PDF readers can handle very long PDF files smoothly, while my browser had a hard time to load a large single-file html is hard to handle by my browser. In this use case, PDF is overall a better solution.\r\n\r\nOn the other hand, I don't really know where the errors originated (the docs, Sphinx, or latex version). So, **maybe a better solution would be providing a downloadable PDF file of every release on the website**? I suppose that if the pytorch team already have a testing pipeline for documentation generation, it wouldn't be too much work to post the pdf file on the website. However, I don't really know if that fits into your pipeline.\r\n\r\nThank you very much!"}
{"number": 20184, "title": "Allow Dict type in c10 operators", "time": "2019-05-06T22:03:13Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20185 Extend testAvailableArgTypes&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15227621/)\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20184 Allow Dict type in c10 operators**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15227620/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #19976 Dict&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15156384/)\n\n- Add support for Dict<Key, Value> arguments and returns to c10 operators\n- Add support for std::unordered_map<Key, Value> to the legacy API (but not to c10 kernels)\n\nDifferential Revision: [D15227620](https://our.internmc.facebook.com/intern/diff/D15227620/)"}
{"number": 20185, "title": "Extend testAvailableArgTypes", "time": "2019-05-06T22:03:23Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20185 Extend testAvailableArgTypes**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15227621/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20184 Allow Dict type in c10 operators&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15227620/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #19976 Dict&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15156384/)\n\nThis test case now also tests that the argument type works correctly in kernels that\n - don't return outputs\n - return multiple outputs\n\nDifferential Revision: [D15227621](https://our.internmc.facebook.com/intern/diff/D15227621/)"}
{"number": 20186, "title": "Add USE_CUDA macro in THD DataChannelGloo for non-GPU use case", "time": "2019-05-06T22:29:19Z", "body": "Summary:\nWe're missing two USE_CUDA macro for GPU-related code in THD's DataChannelGloo.\nAlso adding GlooCache back to compilation.\n\nDifferential Revision: D15227502\n\n"}
{"number": 20187, "title": "[ONNX] Support exporting optional args from scripting", "time": "2019-05-06T22:30:35Z", "body": "Unlike tracing, which records the default argument inside the trace. Scripting inserts `prim::Constant()` for each unprovided optional argument. The ONNX exporter has no idea that this actually stands for argument not provided.\r\n\r\nThis PR is the first step of adding the support for exporting these optional arguments. "}
{"number": 20188, "title": "add str builtin support", "time": "2019-05-06T22:45:20Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20188 add str builtin support**\n\nSummary: adds str builtin cast to turn values into strings\nTest Plan: python test/test_jit.py\n\nDifferential Revision: [D15228812](https://our.internmc.facebook.com/intern/diff/D15228812)"}
{"number": 20189, "title": "Unify Caffe2 and PyTorch OpenMP initialization", "time": "2019-05-06T23:12:28Z", "body": "At the moment we have two separate OpenMP initialization procedures, one in Caffe2 in `caffe2/core/init_omp.cc`, executed when `GlobalInit` is called. Another is in PyTorch in `at::init_num_threads()`\r\n\r\nThere're some differences in default settings (e.g. single-thread setting in Caffe2 and default settings in PyTorch) as well as potential conflict if two initialization procedures are called in one process."}
{"number": 20190, "title": "[jit] Fix reflection on weak modules, copy attributes", "time": "2019-05-06T23:14:40Z", "body": "* Constructs a new type at runtime so that `isinstance` checks work for\r\nweak modules assigned to `ScriptModule`s\r\n* Fix some extraneous names in `__constants__`\r\n* Add `in_features` and `out_features` to `nn.Linear` `__constants__`\r\n\r\nFixes #19363\n\nDifferential Revision: [D15302350](https://our.internmc.facebook.com/intern/diff/15302350/)"}
{"number": 20191, "title": "[jit] Add all list specializations to pickler", "time": "2019-05-06T23:51:25Z", "body": "TensorList, DoubleList, and BoolList were missing from the pickler, so\r\nthis adds them.\r\n\r\nAs a follow up a lot of the code for these could be templated and cut\r\ndown\r\n\r\n\n\nDifferential Revision: [D15299106](https://our.internmc.facebook.com/intern/diff/15299106/)"}
{"number": 20192, "title": "eps parameter of torch.nn.utils.spectral_norm doesn't work", "time": "2019-05-07T00:03:18Z", "body": "## üêõ Bug\r\n\r\nThe `eps` parameter of `torch.nn.utils.spectral_norm` doesn't have the intended effect if a weight matrix is zero.\r\n\r\n## To Reproduce\r\n\r\n```\r\n>>> import torch\r\n>>> from torch.nn.utils import spectral_norm\r\n>>> from torch import nn\r\n>>> a = torch.randn(3,3)\r\n>>> \r\n>>> m = nn.Linear(3,3)\r\n>>> m(a)\r\ntensor([[-0.3986, -0.1544, -0.2359],\r\n        [-0.8249,  0.7046,  0.4226],\r\n        [-0.0013, -0.0395,  0.3294]], grad_fn=<AddmmBackward>)\r\n>>> m.weight.data.zero_()\r\n>>> m(a)\r\ntensor([[-0.1742,  0.3715,  0.3060],\r\n        [-0.1742,  0.3715,  0.3060],\r\n        [-0.1742,  0.3715,  0.3060]], grad_fn=<AddmmBackward>)\r\n>>> spectral_norm(m)\r\nLinear(in_features=3, out_features=3, bias=True)\r\n>>> m(a)\r\ntensor([[nan, nan, nan],\r\n        [nan, nan, nan],\r\n        [nan, nan, nan]], grad_fn=<AddmmBackward>)\r\n```\r\n\r\n## Expected behavior\r\n\r\nThe output of `m(a)` should not be `nan`s, since the default `eps` is `1e-12`.\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.1.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Python version: 3.7"}
{"number": 20193, "title": "[wip] Produce JUnit XML output", "time": "2019-05-07T00:12:49Z", "body": ""}
{"number": 20194, "title": "[jit][AD] Don't do dropout if training=True", "time": "2019-05-07T01:20:44Z", "body": "\nDifferential Revision: [D15232342](https://our.internmc.facebook.com/intern/diff/15232342/)"}
{"number": 20195, "title": "caffe2 Build error on windows", "time": "2019-05-07T01:22:54Z", "body": "Windows build of caffe2 fails. Errors in running build_windows.bat:\r\n[1121/1204] Linking CXX executable bin\\data_filler_test.exe\r\nFAILED: bin/data_filler_test.exe\r\nCmd.exe /C \"cd . && \"C:\\Program Files\\CMake\\bin\\cmake.exe\" -E vs_link_exe --intdir=caffe2\\CMakeFiles\\data_filler_test.dir --manifests -- C:\\PROGRA~2\\ MIB055~1\\2017\\ENTERP~1\\VC\\Tools\\MSVC\\1416~1.270\\bin\\Hostx64\\x64\\link.exe /nologo caffe2\\CMakeFiles\\data_filler_test.dir\\predictor\\emulator\\data_filler_test.cc.obj /out :bin\\data_filler_test.exe /implib:lib\\data_filler_test.lib /pdb:bin\\data_filler_test.pdb /version:0.0 /INCREMENTAL:NO /subsystem:console lib\\gtest_main.lib -WHOLEARCHIVE:E:/opencv_build/_caffe2_/try /pytorch/lib/caffe2.lib -WHOLEARCHIVE:E:/opencv_build/_caffe2_/try/pytorch/lib/caffe2_protos.lib lib\\cpuinfo.lib lib\\clog.lib lib\\foxi_loader.lib -WHOLEARCHIVE:E:/opencv_build/ _caffe2_/try/pytorch/lib/onnx.lib lib\\onnx_proto.lib lib\\libprotobuf.lib -WHOLEARCHIVE:E:/opencv_build/_caffe2_/try/pytorch/lib/Caffe2_perfkernels_avx.lib -WHOLEARCHIVE:E:/opencv_build/_caffe2_/ Try/pytorch/lib/Caffe2_perfkernels_avx2.lib lib\\c10.lib -WHOLEARCHIVE:E:/opencv_build/_caffe2_/try/pytorch/lib/C Affe2_perfkernels_avx512.lib lib\\gtest.lib kernel32.lib user32.lib gdi32.lib winspool.lib shell32.lib ole32.lib oleaut32.lib uuid.lib comdlg32.lib advapi32.lib && cd .\"\r\nLINK: command \"C:\\PROGRA~2\\MIB055~1\\2017\\ENTERP~1\\VC\\Tools\\MSVC\\1416~1.270\\bin\\Hostx64\\x64\\link.exe /nologo caffe2\\CMakeFiles\\data_filler_test.dir\\ Predictor\\emulator\\data_filler_test.cc.obj /out:bin\\data_filler_test.exe /implib:lib\\data_filler_test.lib /pdb:bin\\data_filler_test.pdb /version:0.0 /INCREMENTAL:NO /subsystem:console lib\\gtest_main.lib -WHOLEARCHIVE:E:/opencv_build/_caffe2_/try/pytorch/lib/caffe2.lib -WHOLEARCHIVE:E:/opencv_build/_caffe2_/try/pytorch/lib/caffe2_protos.lib lib\\cpuinfo.lib lib\\clog.lib lib\\ Foxi_loader.lib -WHOLEARCHIVE:E:/opencv_build/_caffe2_/try/pytorch/lib/onnx.lib lib\\onnx_proto.lib lib\\libprotobuf.lib -WHOLEARCHIVE:E:/opencv_build/_caffe2_/try/pytorch/lib/Caffe2_perfkernels_avx. Lib -WHOLEARCHIVE:E:/opencv_build/_caffe2_/try/pytorch/lib/Caffe2_perfkernels_avx2.lib lib\\c10.lib -WHOLEARCHIVE:E:/opencv_build/_caffe2_/try/pytorch/lib/Caffe2_perfkernels_avx512.lib lib\\gtest.lib kernel32 .lib user32.lib gdi32.lib winspool.lib shell32.lib ole32.lib oleaut32.lib u Uid.lib comdlg32.lib advapi32.lib /MANIFEST /MANIFESTFILE:bin\\data_filler_test.exe.manifest\" failed (exit code 1120) with the following output:\r\nLINK : warning LNK4098: The default library \"MSVCRT\" conflicts with the use of other libraries; please use /NODEFAULTLIB:library\r\nCaffe2.lib(miniz.c.obj) : warning LNK4217: Locally defined symbol _localtime64_s is imported in function localtime_s\r\nCaffe2.lib(miniz.c.obj) : warning LNK4217: Locally defined symbol _time64 is imported in function mz_zip_writer_add_mem_ex_v2\r\nCaffe2.lib(miniz.c.obj) : warning LNK4217: Locally defined symbol _wassert is imported in function mz_zip_array_ensure_capacity\r\nCaffe2.lib(miniz.c.obj) : warning LNK4217: Locally defined symbol free Imported in function miniz_def_free_func\r\nCaffe2.lib(miniz.c.obj) : warning LNK4217: The locally defined symbol malloc is imported in the function miniz_def_alloc_func\r\nCaffe2.lib(miniz.c.obj) : warning LNK4217: Locally defined symbol realloc is imported in function miniz_def_realloc_func\r\nCaffe2.lib(miniz.c.obj) : error LNK2019: Unresolved external symbol __imp__mktime64, this symbol is referenced in function mktime\r\nBin\\data_filler_test.exe : fatal error LNK1120: 1 external command that cannot be resolved\r\nNinja: build stopped: subcommand failed.\r\nTraceback (most recent call last):\r\n¬†¬†File \"tools\\build_libtorch.py\", line 22, in <module>\r\n¬†¬†¬†¬†Build_python=False, rerun_cmake=True, build_dir='.')\r\n¬†¬†File \"E:\\opencv_build\\_caffe2_\\try\\pytorch\\tools\\build_pytorch_libs.py\", line 268, in build_caffe2\r\n¬†¬†¬†¬†Check_call(build_cmd, cwd=build_dir, env=my_env)\r\n¬†¬†File \"C:\\Users\\84206\\Anaconda3\\lib\\subprocess.py\", line 311, in check_call\r\n¬†¬†¬†¬†Raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['cmake', '--build', '.', '--target', 'install', '--config', 'Release', '--', '-j' , '7']' returned non-zero exit status 1.\r\n\"Caffe2 building failed\"\r\n\r\n"}
{"number": 20196, "title": "[ROCm CI] test_LPPool1d_cuda occassional SIGIOT", "time": "2019-05-07T01:23:10Z", "body": "```\r\n23:57:08 test_LPPool1d_cuda (__main__.TestNN) ... Memory access fault by GPU node-5 (Agent handle: 0x25b3ed0) on address 0x7fa7236ca000. Reason: Page not present or supervisor privilege.\r\n23:57:20 Traceback (most recent call last):\r\n23:57:20   File \"test/run_test.py\", line 439, in <module>\r\n23:57:20     main()\r\n23:57:20   File \"test/run_test.py\", line 431, in main\r\n23:57:20     raise RuntimeError(message)\r\n23:57:20 RuntimeError: test_nn failed! Received signal: SIGIOT\r\n```\r\nhttps://ci.pytorch.org/jenkins/job/pytorch-builds/job/py2-devtoolset7-rocmrpm-centos7.5-test/10657/console"}
{"number": 20197, "title": "optimize pytorch layer_norm on CPU", "time": "2019-05-07T01:36:08Z", "body": "Summary:\noptimize pytorch layer_norm on CPU\nboth forward and backward will be 4x faster than before\n\nDifferential Revision: D15194600\n\n"}
{"number": 20198, "title": "test_namedtuple_return is flakey", "time": "2019-05-07T05:32:21Z", "body": "```\r\nMay 07 00:00:01 ======================================================================\r\nMay 07 00:00:01 ERROR: test_namedtuple_return (__main__.TestNamedTupleAPI)\r\nMay 07 00:00:01 ----------------------------------------------------------------------\r\nMay 07 00:00:01 Traceback (most recent call last):\r\nMay 07 00:00:01   File \"test_namedtuple_return_api.py\", line 93, in test_namedtuple_return\r\nMay 07 00:00:01     ret = b.pstrf()\r\nMay 07 00:00:01   File \"/opt/python/nightly/lib/python3.7/site-packages/torch/tensor.py\", line 259, in pstrf\r\nMay 07 00:00:01     return super(Tensor, self).pstrf(upper=upper)\r\nMay 07 00:00:01 RuntimeError: Lapack Error pstrf : matrix is rank deficient or not positive semidefinite at /var/lib/jenkins/workspace/aten/src/TH/generic/THTensorLapack.cpp:617\r\nMay 07 00:00:01 \r\nMay 07 00:00:01 ----------------------------------------------------------------------\r\n```"}
{"number": 20199, "title": "load_state_dict creates reference cycle, is there a python garbage collect operation before raise CUDA out-of-memory error?", "time": "2019-05-07T06:49:30Z", "body": "## üêõ Bug\r\n\r\nPython's garbage collection invokes every 700 object creations (by default), so there should be a situation when the object is not referred but still occupies the CUDA memory (when reference cycles happen), so I'm asking for an additional garbage collecting operation if the CUDA memory is out."}
{"number": 20200, "title": "Distributed Communication with openmpi fails", "time": "2019-05-07T08:10:35Z", "body": "## üêõ Bug\r\n\r\nWhen trying to run a multi-process training with mpi backend the program fails at communication. Otherwise runs perfectly fine.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Install CUDA's latest toolkit 10.1 and equivalent CuDNN 7.5.1\r\n2. Install Openmpi v3.1.2 with CUDA support\r\n3. Build / install pytroch from source.\r\n4. Test any communication for a process group with mpi backend.\r\n\r\n## Expected behavior\r\n\r\nCarry out communication as it would normally. Send / Receive / Reduce operations.\r\n\r\n\r\nThe following error occurs when trying to use reduce function of the distributed class :\r\n```\r\n[g2-nasp:24252] *** An error occurred in MPI_Reduce\r\n[g2-nasp:24252] *** reported by process [1911816193,0]\r\n[g2-nasp:24252] *** on communicator MPI_COMM_WORLD\r\n[g2-nasp:24252] *** MPI_ERR_ROOT: invalid root\r\n[g2-nasp:24252] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\r\n[g2-nasp:24252] ***    and potentially your MPI job)\r\n[g2-nasp:24247] 1 more process has sent help message help-mpi-errors.txt / mpi_errors_are_fatal\r\n```\r\n\r\nThis only seems to be occur when trying to establish communication between two nodes. A different error occurs for send / receive function calls.\r\n\r\nI get following errors when trying to use isend / irecv functions of the distributed module:\r\n```\r\n--------------------------------------------------------------------------\r\n[[18472,1],1]: A high-performance Open MPI point-to-point messaging module\r\nwas unable to find any relevant network interfaces:\r\n\r\nModule: OpenFabrics (openib)\r\n  Host: g9-nasp\r\n\r\nAnother transport will be used instead, although this may result in\r\nlower performance.\r\n\r\nNOTE: You can disable this warning by setting the MCA parameter\r\nbtl_base_warn_component_unused to 0.\r\n--------------------------------------------------------------------------\r\nI am rank:  1  and my tensor value is:  tensor(0., device='cuda:0')\r\nI am rank:  0  and my tensor value is:  tensor(0., device='cuda:0')\r\nTensor value before receive:  tensor(0., device='cuda:0')\r\n[g9-nasp:12139:0] Caught signal 11 (Segmentation fault: invalid permissions for mapped object at address 0x13011e0000)\r\n==== backtrace ====\r\n    0  /opt/ucx/lib/libucs.so.0(+0x12d68) [0x7f29c32b3d68]\r\n    1  /opt/ucx/lib/libucs.so.0(+0x12f02) [0x7f29c32b3f02]\r\n    2  /lib/x86_64-linux-gnu/libc.so.6(+0x18eb64) [0x7f2a35a60b64]\r\n    3  /opt/ucx/lib/libuct.so.0(uct_mm_ep_am_short+0x88) [0x7f29c2e74e08]\r\n    4  /opt/ucx/lib/libucp.so.0(ucp_tag_send_nb+0x7e) [0x7f29c34dd1ee]\r\n    5  /home/usama/.openmpi/lib/openmpi/mca_pml_ucx.so(mca_pml_ucx_isend+0x95) [0x7f29c36efc45]\r\n    6  /home/usama/.openmpi/lib/libmpi.so.40(MPI_Isend+0x105) [0x7f2a0d675355]\r\n    7  /home/usama/anaconda3/lib/python3.6/site-packages/torch/lib/libtorch_python.so(_ZN4c10d15ProcessGroupMPI4sendERSt6vectorIN2at6TensorESaIS3_EEii+0xee) [0x7f2a26e1ab3e]\r\n    8  /home/usama/anaconda3/lib/python3.6/site-packages/torch/lib/libtorch_python.so(+0x654887) [0x7f2a26d5c887]\r\n    9  /home/usama/anaconda3/lib/python3.6/site-packages/torch/lib/libtorch_python.so(+0x11f9f6) [0x7f2a268279f6]\r\n   10  python(_PyCFunction_FastCallDict+0x154) [0x55649c111744]\r\n   11  python(+0x19857e) [0x55649c19857e]\r\n   12  python(_PyEval_EvalFrameDefault+0x30a) [0x55649c1bd38a]\r\n   13  python(+0x1918e4) [0x55649c1918e4]\r\n   14  python(+0x192771) [0x55649c192771]\r\n   15  python(+0x198505) [0x55649c198505]\r\n   16  python(_PyEval_EvalFrameDefault+0x10c7) [0x55649c1be147]\r\n   17  python(+0x19253b) [0x55649c19253b]\r\n   18  python(+0x198505) [0x55649c198505]\r\n   19  python(_PyEval_EvalFrameDefault+0x30a) [0x55649c1bd38a]\r\n   20  python(PyEval_EvalCodeEx+0x329) [0x55649c193289]\r\n   21  python(PyEval_EvalCode+0x1c) [0x55649c19401c]\r\n   22  python(+0x2163c4) [0x55649c2163c4]\r\n   23  python(PyRun_FileExFlags+0xa1) [0x55649c2167c1]\r\n   24  python(PyRun_SimpleFileExFlags+0x1c3) [0x55649c2169c3]\r\n   25  python(Py_Main+0x613) [0x55649c21a4b3]\r\n   26  python(main+0xee) [0x55649c0e302e]\r\n   27  /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xe7) [0x7f2a358f3b97]\r\n   28  python(+0x1c3e0e) [0x55649c1c3e0e]\r\n===================\r\n[g9-nasp:12139:0] Process frozen...\r\n[g9-nasp:12134] 1 more process has sent help message help-mpi-btl-base.txt / btl:no-nics\r\n[g9-nasp:12134] Set MCA parameter \"orte_base_help_aggregate\" to 0 to see all help / error messages\r\n```\r\n\r\n## Environment\r\n```\r\nPyTorch version: 1.1.0a0+8cd6d2f                                                                                  \r\nIs debug build: No                                                                                                \r\nCUDA used to build PyTorch: 10.1.105                                                                              \r\n                                                                                                                  \r\nOS: Ubuntu 16.04.6 LTS                                                                                            \r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609                                                      \r\nCMake version: version 3.14.0                                                                                     \r\n                                                                                                                  \r\nPython version: 3.6                                                                                               \r\nIs CUDA available: Yes                                                                                            \r\nCUDA runtime version: 10.1.105                                                                                    \r\nGPU models and configuration:                                                                                     \r\nGPU 0: Tesla K40c                                                                                                 \r\nGPU 1: Tesla K40c                                                                                                 \r\n                                                                                                                  \r\nNvidia driver version: 418.39                                                                                     \r\ncuDNN version: /usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudnn.so.7                                        \r\n                                                                                                                  \r\nVersions of relevant libraries:                                                                                   \r\n[pip3] numpy==1.15.4                                                                                              \r\n[conda] blas                      1.0                         mkl                                                 \r\n[conda] magma-cuda100             2.5.0                         1    pytorch                                      \r\n[conda] mkl                       2019.3                      199  \r\n[conda] mkl-include               2019.3                      199  \r\n[conda] mkl-service               1.1.2            py36he904b0f_5  \r\n[conda] mkl_fft                   1.0.12           py36ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py36hd81dba3_0  \r\n[conda] pytorch                   1.0.1           cuda100py36he554f03_0  \r\n[conda] torch                     1.1.0a0+8cd6d2f          pypi_0    pypi\r\n[conda] torchvision               0.2.1                    py36_0\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.1\r\n - OS (e.g., Linux): Ubuntu 16.04\r\n - How you installed PyTorch (`conda`, `pip`, source): installed from source\r\n - Build command you used (if compiling from source): python setup.py install\r\n - Python version: 3.6.8\r\n - CUDA/cuDNN version: 10.1.105 / 7.5.1\r\n - GPU models and configuration: Tesla K40c [2 slots]\r\n - Any other relevant information: Openmpi v3.1.2\r\n\r\n## Additional context\r\n\r\nThis seems to occur for older version of openmpi i.e. 3.1.2... I have the latest version of openmpi v4.0.1 on my local machine where the same code works perfectly fine."}
{"number": 20201, "title": "In-place modification with double slicing (using boolean mask)", "time": "2019-05-07T08:47:44Z", "body": "## üêõ Bug\r\n\r\nCannot do in-place modification of a tensor with a double slicing using `torch.uint8` (boolean mask) tensor.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```\r\nimport torch\r\n\r\na = torch.tensor([[1., 2.], [13., 4.], [8., 14.]])\r\n\r\n# Creating a mask for those elements of a that are greater than 10\r\nb = a > 10\r\nprint(\"b is: \", b)\r\nprint(\"a[b] is: \", a[b])\r\n\r\n# Creating a mask for those elements of a[b] (i.e. [13., 14.]) that are greater than 13.5\r\nc = a[b] > 13.5\r\nprint(\"c is: \", c)\r\nprint(\"a[b][c] is: \", a[b][c])\r\n\r\n# HERE IS THE BUG\r\na[b][c] = torch.tensor([17.], dtype=a.dtype)\r\n# a[b][c] = torch.tensor([17.])\r\n# a[b][c] = 17.\r\n\r\nprint(\"a is: \", a)\r\nprint(\"a[b] is: \", a[b])\r\n\r\n```\r\n\r\nNo error is reported when running this code.\r\n\r\n## Expected behavior\r\n\r\nI think that it is expected that, no matter which of the three ways I described to do the in-place modification of the tensor, `a` should have been modified while it is not. If I use a single mask, then everything works just fine.\r\nIf I try something else such as `a[0][0] = 17.` or `a[0, :][0] = 17.` then everything is working (but notice that the slicing is not done with a mask (or `torch.unit8` tensor) in these two cases.\r\n\r\nOne way to work around this issue is using another tensor:\r\n\r\n```\r\nk = a[b]\r\nprint(\"k is: \", k)\r\nk[c] = 42\r\n\r\n# k as been modified\r\nprint(\"k is: \", k)\r\n\r\na[b] = k\r\n\r\n# a has been modified\r\nprint(\"a is: \", a)\r\nprint(\"a[b] is: \",  a[b])\r\n```\r\n\r\n## Environment\r\n\r\n - PyTorch Version: 1.0.1\r\n - OS: Ubuntu 18.04\r\n - How I installed PyTorch: `conda install pytorch cudatoolkit=9.0 -c pytorch`\r\n - Python version: 3.7\r\n"}
{"number": 20202, "title": "cuDNN arch mismatch - software or hardware error?", "time": "2019-05-07T09:03:58Z", "body": "Hello, I had a lot of fun with training neural networks on my GPU but then I tried LSTM.\r\n\r\n```Python\r\n    x = self.lstm(x)\r\n  File \"D:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 489, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"D:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\", line 179, in forward\r\n    self.dropout, self.training, self.bidirectional, self.batch_first)\r\nRuntimeError: cuDNN error: CUDNN_STATUS_ARCH_MISMATCH\r\n```\r\n\r\nMy GPU is `GeForce GTX 950` and it's `NVIDIA Compute Capability` is `5.2`.\r\n\r\n```\r\n>>> import torch\r\n>>> torch.backends.cudnn.version()\r\n7501\r\n```\r\nI would really appreciate quick guess, is the hardware too old or could that be an environment issue?"}
{"number": 20203, "title": "Fixes #20124", "time": "2019-05-07T09:07:14Z", "body": "Fixes #20124\r\n\r\nDescription:\r\nCode wraps `optimizer.step()` method to detect whether user is following new pattern or old pattern. In case of old pattern detected, a UserWarning is raised. Documentation is also updated to reflect the change:\r\n\r\n![Screen Shot 2019-05-07 at 11 05 17](https://user-images.githubusercontent.com/2459423/57287527-04e63580-70b8-11e9-9ddd-5d159ef0ed2f.png)\r\n \r\n\r\ncc @SsnL, @bado-lee"}
{"number": 20204, "title": "SyncBatchNorm should support 2D input (B, C)", "time": "2019-05-07T10:03:39Z", "body": "## üêõ Bug\r\n\r\nIn the implementation of SyncBatchNorm, I just found that there is a `_check_input_dim` to make sure the input dimension > 2. But actually it should support the input with shape (B, C), just like the BatchNorm1d. \r\n\r\nSo is there any other reasons for the condition `input.dim() <=2`? If no, could you relax the input check to be `input.dim() < 2` of [SyncBatchNorm here](https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#SyncBatchNorm) ?"}
{"number": 20205, "title": "Fix cuda and cudnn libraries search process on Windows", "time": "2019-05-07T10:10:07Z", "body": "Fixes #20202 "}
{"number": 20206, "title": "Failed to build with system protobuf", "time": "2019-05-07T12:43:06Z", "body": "## üêõ Bug\r\n\r\nFailed to build (link) `caffe2_benchmark` target.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. `git submodule update --init --recursive`\r\n2. `... python setup.py build`\r\n3. Get error:\r\n```bash\r\n[ 94%] Linking CXX executable ../bin/caffe2_benchmark\r\n/usr/bin/ld: ../../../../../lib/libpthreadpool.a(threadpool-pthreads.c.o): in function `pthreadpool_create':\r\nthreadpool-pthreads.c:(.text+0x520): multiple definition of `pthreadpool_create'; ../../../../../lib/libcaffe2.a(pthreadpool_impl.cc.o):pthreadpool_impl.cc:(.text+0x140): first defined here\r\n/usr/bin/ld: ../../../../../lib/libpthreadpool.a(threadpool-pthreads.c.o): in function `pthreadpool_get_threads_count':\r\nthreadpool-pthreads.c:(.text+0x670): multiple definition of `pthreadpool_get_threads_count'; ../../../../../lib/libcaffe2.a(pthreadpool_impl.cc.o):pthreadpool_impl.cc:(.text+0x120): first defined here\r\n/usr/bin/ld: ../../../../../lib/libpthreadpool.a(threadpool-pthreads.c.o): in function `pthreadpool_compute_1d':\r\nthreadpool-pthreads.c:(.text+0x680): multiple definition of `pthreadpool_compute_1d'; ../../../../../lib/libcaffe2.a(pthreadpool_impl.cc.o):pthreadpool_impl.cc:(.text+0x50): first defined here\r\n/usr/bin/ld: ../../../../../lib/libpthreadpool.a(threadpool-pthreads.c.o): in function `pthreadpool_destroy':\r\nthreadpool-pthreads.c:(.text+0x17d0): multiple definition of `pthreadpool_destroy'; ../../../../../lib/libcaffe2.a(pthreadpool_impl.cc.o):pthreadpool_impl.cc:(.text+0x1f0): first defined here\r\ncollect2: error: ld returned 1 exit status\r\nmake[2]: *** [caffe2/torch/lib/c10d/test/CMakeFiles/ProcessGroupGlooTest.dir/build.make:119: bin/ProcessGroupGlooTest] Error 1\r\nmake[1]: *** [CMakeFiles/Makefile2:10591: caffe2/torch/lib/c10d/test/CMakeFiles/ProcessGroupGlooTest.dir/all] Error 2\r\nmake[1]: *** Waiting for unfinished jobs....\r\n```\r\n\r\n## Expected behavior\r\n\r\nExpected to build and link fine\r\n\r\n## Environment\r\n`collect_env.py`:\r\n```\r\nCollecting environment information...\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\n\r\nOS: Arch Linux\r\nGCC version: (GCC) 8.3.0\r\nCMake version: version 3.14.3\r\n\r\nPython version: 3.7\r\nIs CUDA available: N/A\r\nCUDA runtime version: 10.1.105\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: /usr/lib/libcudnn.so.7.5.1\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n```\r\nManual spec:\r\n\r\n - PyTorch Version (e.g., 1.0): 1.1.0\r\n - OS (e.g., Linux): ArchLinux x86_64\r\n - How you installed PyTorch (`conda`, `pip`, source): source\r\n - Build command you used (if compiling from source):\r\n```bash\r\n# Build command\r\n  export CC=gcc\r\n  export CXX=g++\r\n  export PYTORCH_BUILD_VERSION=\"1.1.0\"\r\n  export PYTORCH_BUILD_NUMBER=1\r\n  export BUILD_CUSTOM_PROTOBUF=0\r\n  export BUILD_SHARED_LIBS=0\r\n  export NO_CUDA=1\r\n  export WITH_CUDNN=0\r\n  export USE_OPENCV=1\r\n  export BUILD_BINARY=1\r\n  python setup.py build\r\n```\r\n - Python version: 3.7.3\r\n - CUDA/cuDNN version: N/A\r\n - GPU models and configuration: N/A\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\nFull config log:\r\n```\r\n-- The CXX compiler identification is GNU 8.3.0\r\n-- The C compiler identification is GNU 8.3.0\r\n-- Check for working CXX compiler: /usr/bin/g++\r\n-- Check for working CXX compiler: /usr/bin/g++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Check for working C compiler: /usr/bin/gcc\r\n-- Check for working C compiler: /usr/bin/gcc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Not forcing any particular BLAS to be found\r\n-- Performing Test COMPILER_WORKS\r\n-- Performing Test COMPILER_WORKS - Success\r\n-- Performing Test SUPPORT_GLIBCXX_USE_C99\r\n-- Performing Test SUPPORT_GLIBCXX_USE_C99 - Success\r\n-- Performing Test CAFFE2_EXCEPTION_PTR_SUPPORTED\r\n-- Performing Test CAFFE2_EXCEPTION_PTR_SUPPORTED - Success\r\n-- std::exception_ptr is supported.\r\n-- Performing Test CAFFE2_IS_NUMA_AVAILABLE\r\n-- Performing Test CAFFE2_IS_NUMA_AVAILABLE - Failed\r\n-- NUMA is not available\r\n-- Performing Test CAFFE2_NEED_TO_TURN_OFF_DEPRECATION_WARNING\r\n-- Performing Test CAFFE2_NEED_TO_TURN_OFF_DEPRECATION_WARNING - Success\r\n-- Performing Test CAFFE2_COMPILER_SUPPORTS_AVX2_EXTENSIONS\r\n-- Performing Test CAFFE2_COMPILER_SUPPORTS_AVX2_EXTENSIONS - Success\r\n-- Current compiler supports avx2 extension. Will build perfkernels.\r\n-- Performing Test CAFFE2_COMPILER_SUPPORTS_AVX512_EXTENSIONS\r\n-- Performing Test CAFFE2_COMPILER_SUPPORTS_AVX512_EXTENSIONS - Success\r\n-- Current compiler supports avx512f extension. Will build fbgemm.\r\n-- Performing Test COMPILER_SUPPORTS_HIDDEN_VISIBILITY\r\n-- Performing Test COMPILER_SUPPORTS_HIDDEN_VISIBILITY - Success\r\n-- Performing Test COMPILER_SUPPORTS_HIDDEN_INLINE_VISIBILITY\r\n-- Performing Test COMPILER_SUPPORTS_HIDDEN_INLINE_VISIBILITY - Success\r\n-- Performing Test COMPILER_SUPPORTS_RDYNAMIC\r\n-- Performing Test COMPILER_SUPPORTS_RDYNAMIC - Success\r\n-- Caffe2: Found protobuf with new-style protobuf targets.\r\n-- Caffe2 protobuf include directory: /usr/include\r\n-- Found Threads: TRUE  \r\n-- Trying to find preferred BLAS backend of choice: MKL\r\n-- Looking for sys/types.h\r\n-- Looking for sys/types.h - found\r\n-- Looking for stdint.h\r\n-- Looking for stdint.h - found\r\n-- Looking for stddef.h\r\n-- Looking for stddef.h - found\r\n-- Check size of void*\r\n-- Check size of void* - done\r\nCMake Warning at cmake/Dependencies.cmake:120 (message):\r\n  MKL could not be found.  Defaulting to Eigen\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:236 (include)\r\n\r\n\r\nCMake Warning at cmake/Dependencies.cmake:139 (message):\r\n  Preferred BLAS (MKL) cannot be found, now searching for a general BLAS\r\n  library\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:236 (include)\r\n\r\n\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_core - pthread - m - dl]\r\n--   Library mkl_intel_lp64: not found\r\n-- Checking for [mkl_intel - mkl_core - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_core - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_core - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl - guide - pthread - m]\r\n--   Library mkl: not found\r\n-- MKL library not found\r\n-- Checking for [Accelerate]\r\n--   Library Accelerate: BLAS_Accelerate_LIBRARY-NOTFOUND\r\n-- Checking for [vecLib]\r\n--   Library vecLib: BLAS_vecLib_LIBRARY-NOTFOUND\r\n-- Checking for [openblas]\r\n--   Library openblas: BLAS_openblas_LIBRARY-NOTFOUND\r\n-- Checking for [openblas - pthread]\r\n--   Library openblas: BLAS_openblas_LIBRARY-NOTFOUND\r\n-- Checking for [goto2 - gfortran]\r\n--   Library goto2: BLAS_goto2_LIBRARY-NOTFOUND\r\n-- Checking for [goto2 - gfortran - pthread]\r\n--   Library goto2: BLAS_goto2_LIBRARY-NOTFOUND\r\n-- Checking for [acml - gfortran]\r\n--   Library acml: BLAS_acml_LIBRARY-NOTFOUND\r\n-- Checking for [ptf77blas - atlas - gfortran]\r\n--   Library ptf77blas: BLAS_ptf77blas_LIBRARY-NOTFOUND\r\n-- Checking for [blas]\r\n--   Library blas: /usr/lib/libblas.so\r\n-- Looking for sgemm_\r\n-- Looking for sgemm_ - found\r\n-- Performing Test BLAS_F2C_DOUBLE_WORKS\r\n-- Performing Test BLAS_F2C_DOUBLE_WORKS - Failed\r\n-- Performing Test BLAS_F2C_FLOAT_WORKS\r\n-- Performing Test BLAS_F2C_FLOAT_WORKS - Success\r\n-- Performing Test BLAS_USE_CBLAS_DOT\r\n-- Performing Test BLAS_USE_CBLAS_DOT - Failed\r\n-- Found a library with BLAS API (generic).\r\n-- The ASM compiler identification is GNU\r\n-- Found assembler: /usr/bin/gcc\r\n-- Check if compiler accepts -pthread\r\n-- Check if compiler accepts -pthread - yes\r\n-- Brace yourself, we are building NNPACK\r\n-- Performing Test NNPACK_ARCH_IS_X86_32\r\n-- Performing Test NNPACK_ARCH_IS_X86_32 - Failed\r\n-- Found PythonInterp: /usr/bin/python (found version \"3.7.3\") \r\n-- NNPACK backend is x86-64\r\n-- Failed to find LLVM FileCheck\r\n-- Found Git: /usr/bin/git (found version \"2.21.0\") \r\n-- git Version: v1.4.0-505be96a\r\n-- Version: 1.4.0\r\n-- Performing Test HAVE_CXX_FLAG_STD_CXX11\r\n-- Performing Test HAVE_CXX_FLAG_STD_CXX11 - Success\r\n-- Performing Test HAVE_CXX_FLAG_WALL\r\n-- Performing Test HAVE_CXX_FLAG_WALL - Success\r\n-- Performing Test HAVE_CXX_FLAG_WEXTRA\r\n-- Performing Test HAVE_CXX_FLAG_WEXTRA - Success\r\n-- Performing Test HAVE_CXX_FLAG_WSHADOW\r\n-- Performing Test HAVE_CXX_FLAG_WSHADOW - Success\r\n-- Performing Test HAVE_CXX_FLAG_WERROR\r\n-- Performing Test HAVE_CXX_FLAG_WERROR - Success\r\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC\r\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC - Success\r\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC_ERRORS\r\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC_ERRORS - Success\r\n-- Performing Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32\r\n-- Performing Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32 - Failed\r\n-- Performing Test HAVE_CXX_FLAG_WFLOAT_EQUAL\r\n-- Performing Test HAVE_CXX_FLAG_WFLOAT_EQUAL - Success\r\n-- Performing Test HAVE_CXX_FLAG_FSTRICT_ALIASING\r\n-- Performing Test HAVE_CXX_FLAG_FSTRICT_ALIASING - Success\r\n-- Performing Test HAVE_CXX_FLAG_WNO_DEPRECATED_DECLARATIONS\r\n-- Performing Test HAVE_CXX_FLAG_WNO_DEPRECATED_DECLARATIONS - Success\r\n-- Performing Test HAVE_CXX_FLAG_WSTRICT_ALIASING\r\n-- Performing Test HAVE_CXX_FLAG_WSTRICT_ALIASING - Success\r\n-- Performing Test HAVE_CXX_FLAG_WD654\r\n-- Performing Test HAVE_CXX_FLAG_WD654 - Failed\r\n-- Performing Test HAVE_CXX_FLAG_WTHREAD_SAFETY\r\n-- Performing Test HAVE_CXX_FLAG_WTHREAD_SAFETY - Failed\r\n-- Performing Test HAVE_CXX_FLAG_COVERAGE\r\n-- Performing Test HAVE_CXX_FLAG_COVERAGE - Success\r\n-- Performing Test HAVE_STD_REGEX\r\n-- Performing Test HAVE_STD_REGEX\r\n-- Performing Test HAVE_STD_REGEX -- success\r\n-- Performing Test HAVE_GNU_POSIX_REGEX\r\n-- Performing Test HAVE_GNU_POSIX_REGEX\r\n-- Performing Test HAVE_GNU_POSIX_REGEX -- failed to compile\r\n-- Performing Test HAVE_POSIX_REGEX\r\n-- Performing Test HAVE_POSIX_REGEX\r\n-- Performing Test HAVE_POSIX_REGEX -- success\r\n-- Performing Test HAVE_STEADY_CLOCK\r\n-- Performing Test HAVE_STEADY_CLOCK\r\n-- Performing Test HAVE_STEADY_CLOCK -- success\r\n-- Performing Test COMPILER_SUPPORTS_AVX512\r\n-- Performing Test COMPILER_SUPPORTS_AVX512 - Success\r\n-- Found OpenMP_C: -fopenmp (found version \"4.5\") \r\n-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \r\n-- Found OpenMP: TRUE (found version \"4.5\")  \r\n-- Downloading asmjit to /build/python-pytorch/src/pytorch-1.1.0/build/third_party/fbgemm/third_party/asmjit\r\n      (define ASMJIT_SRC_DIR to avoid it)\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /build/python-pytorch/src/pytorch-1.1.0/build/third_party/fbgemm/asmjit-download\r\nScanning dependencies of target asmjit\r\n[ 11%] Creating directories for 'asmjit'\r\n[ 22%] Performing download step (git clone) for 'asmjit'\r\nCloning into 'asmjit'...\r\nNote: checking out '673dcefaa048c5f5a2bf8b85daf8f7b9978d018a'.\r\n\r\nYou are in 'detached HEAD' state. You can look around, make experimental\r\nchanges and commit them, and you can discard any commits you make in this\r\nstate without impacting any branches by performing another checkout.\r\n\r\nIf you want to create a new branch to retain commits you create, you may\r\ndo so (now or later) by using -b with the checkout command again. Example:\r\n\r\n  git checkout -b <new-branch-name>\r\n\r\nHEAD is now at 673dcef Fixed #188\r\n[ 33%] No patch step for 'asmjit'\r\n[ 44%] Performing update step for 'asmjit'\r\n[ 55%] No configure step for 'asmjit'\r\n[ 66%] No build step for 'asmjit'\r\n[ 77%] No install step for 'asmjit'\r\n[ 88%] No test step for 'asmjit'\r\n[100%] Completed 'asmjit'\r\n[100%] Built target asmjit\r\nCMake Warning (dev) at build/third_party/fbgemm/third_party/asmjit/CMakeLists.txt:34 (set):\r\n  implicitly converting 'BOOLEAN' to 'STRING' type.\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\nCMake Warning (dev) at build/third_party/fbgemm/third_party/asmjit/CMakeLists.txt:35 (set):\r\n  implicitly converting 'BOOLEAN' to 'STRING' type.\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\nCMake Warning (dev) at build/third_party/fbgemm/third_party/asmjit/CMakeLists.txt:36 (set):\r\n  implicitly converting 'BOOLEAN' to 'STRING' type.\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\nCMake Warning (dev) at build/third_party/fbgemm/third_party/asmjit/CMakeLists.txt:37 (set):\r\n  implicitly converting 'BOOLEAN' to 'STRING' type.\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\nCMake Warning (dev) at build/third_party/fbgemm/third_party/asmjit/CMakeLists.txt:38 (set):\r\n  implicitly converting 'BOOLEAN' to 'STRING' type.\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\n-- Performing Test __CxxFlag__msse\r\n-- Performing Test __CxxFlag__msse - Success\r\n-- Performing Test __CxxFlag__msse2\r\n-- Performing Test __CxxFlag__msse2 - Success\r\n-- Performing Test __CxxFlag__msse3\r\n-- Performing Test __CxxFlag__msse3 - Success\r\n-- Performing Test __CxxFlag__mssse3\r\n-- Performing Test __CxxFlag__mssse3 - Success\r\n-- Performing Test __CxxFlag__msse4_1\r\n-- Performing Test __CxxFlag__msse4_1 - Success\r\n-- Performing Test __CxxFlag__msse4_2\r\n-- Performing Test __CxxFlag__msse4_2 - Success\r\n-- Performing Test __CxxFlag__mavx\r\n-- Performing Test __CxxFlag__mavx - Success\r\n-- Performing Test __CxxFlag__mavx2\r\n-- Performing Test __CxxFlag__mavx2 - Success\r\n-- Performing Test __CxxFlag__std_c__17\r\n-- Performing Test __CxxFlag__std_c__17 - Success\r\n-- Performing Test __CxxFlag__std_c__14\r\n-- Performing Test __CxxFlag__std_c__14 - Success\r\n-- Performing Test __CxxFlag__std_c__11\r\n-- Performing Test __CxxFlag__std_c__11 - Success\r\n-- Performing Test __CxxFlag__std_c__0x\r\n-- Performing Test __CxxFlag__std_c__0x - Success\r\n-- Performing Test __CxxFlag__fno_tree_vectorize\r\n-- Performing Test __CxxFlag__fno_tree_vectorize - Success\r\n-- Performing Test __CxxFlag__fvisibility_hidden\r\n-- Performing Test __CxxFlag__fvisibility_hidden - Success\r\n-- Performing Test __CxxFlag__Winconsistent_missing_override\r\n-- Performing Test __CxxFlag__Winconsistent_missing_override - Failed\r\n-- Performing Test __CxxFlag__O2\r\n-- Performing Test __CxxFlag__O2 - Success\r\n-- Performing Test __CxxFlag__fno_keep_static_consts\r\n-- Performing Test __CxxFlag__fno_keep_static_consts - Success\r\n-- Performing Test __CxxFlag__fmerge_all_constants\r\n-- Performing Test __CxxFlag__fmerge_all_constants - Success\r\n-- [asmjit]\r\n   BuildMode=Static\r\n   BuildTest=Off\r\n   ASMJIT_DIR=/build/python-pytorch/src/pytorch-1.1.0/build/third_party/fbgemm/third_party/asmjit\r\n   ASMJIT_DEPS=pthread;rt\r\n   ASMJIT_LIBS=asmjit;pthread;rt\r\n   ASMJIT_CFLAGS=-DASMJIT_STATIC\r\n   ASMJIT_SOURCE_DIR=/build/python-pytorch/src/pytorch-1.1.0/build/third_party/fbgemm/third_party/asmjit/src\r\n   ASMJIT_INCLUDE_DIR=/build/python-pytorch/src/pytorch-1.1.0/build/third_party/fbgemm/third_party/asmjit/src\r\n   ASMJIT_PRIVATE_CFLAGS=\r\n     -DASMJIT_STATIC\r\n     -std=c++17\r\n     -fno-tree-vectorize\r\n     -fvisibility=hidden\r\n     -O2 [RELEASE]\r\n     -fno-keep-static-consts [RELEASE]\r\n     -fmerge-all-constants [RELEASE]\r\n-- Could NOT find Numa (missing: Numa_INCLUDE_DIR Numa_LIBRARIES) \r\nCMake Warning at cmake/Dependencies.cmake:487 (message):\r\n  Not compiling with NUMA.  Suppress this warning with -DUSE_NUMA=OFF\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:236 (include)\r\n\r\n\r\n-- OpenCV found (/usr/lib64/cmake/opencv4)\r\n-- Using third party subdirectory Eigen.\r\nPython 3.7.3\r\n-- Found PythonInterp: /usr/bin/python (found suitable version \"3.7.3\", minimum required is \"2.7\") \r\n-- Found PythonLibs: /usr/lib/libpython3.7m.so.1.0 (found suitable version \"3.7.3\", minimum required is \"2.7\") \r\n-- Found PythonInterp: /usr/bin/python (found version \"3.7.3\") \r\n-- Found PythonLibs: /usr/lib/libpython3.7m.so.1.0\r\n-- System pybind11 found\r\n-- pybind11 include dirs: /usr/include;/usr/include/python3.7m\r\n-- Could NOT find MPI_C (missing: MPI_C_LIB_NAMES MPI_C_HEADER_DIR MPI_C_WORKS) \r\n-- Could NOT find MPI_CXX (missing: MPI_CXX_LIB_NAMES MPI_CXX_HEADER_DIR MPI_CXX_WORKS) \r\n-- Could NOT find MPI (missing: MPI_C_FOUND MPI_CXX_FOUND) \r\nCMake Warning at cmake/Dependencies.cmake:714 (message):\r\n  Not compiling with MPI.  Suppress this warning with -DUSE_MPI=OFF\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:236 (include)\r\n\r\n\r\n-- Adding OpenMP CXX_FLAGS: -fopenmp\r\n-- Will link against OpenMP libraries: /usr/lib/libgomp.so;/usr/lib/libpthread.so\r\nCMake Warning (dev) at third_party/gloo/CMakeLists.txt:21 (option):\r\n  Policy CMP0077 is not set: option() honors normal variables.  Run \"cmake\r\n  --help-policy CMP0077\" for policy details.  Use the cmake_policy command to\r\n  set the policy and suppress this warning.\r\n\r\n  For compatibility with older versions of CMake, option is clearing the\r\n  normal variable 'BUILD_BENCHMARK'.\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\nCMake Warning at cmake/Dependencies.cmake:981 (message):\r\n  Metal is only used in ios builds.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:236 (include)\r\n\r\n\r\n-- \r\n-- ******** Summary ********\r\n--   CMake version         : 3.14.3\r\n--   CMake command         : /usr/bin/cmake\r\n--   System                : Linux\r\n--   C++ compiler          : /usr/bin/g++\r\n--   C++ compiler version  : 8.3.0\r\n--   CXX flags             : -march=x86-64 -mtune=generic -O2 -pipe -fno-plt -D_FORTIFY_SOURCE=2 -fvisibility-inlines-hidden -fopenmp -Wnon-virtual-dtor\r\n--   Build type            : Release\r\n--   Compile definitions   : \r\n--   CMAKE_PREFIX_PATH     : /usr/lib/python3.7/site-packages\r\n--   CMAKE_INSTALL_PREFIX  : /build/python-pytorch/src/pytorch-1.1.0/torch\r\n--   CMAKE_MODULE_PATH     : /build/python-pytorch/src/pytorch-1.1.0/cmake/Modules;/usr/share/cmake/pybind11\r\n-- \r\n--   ONNX version          : 1.5.0\r\n--   ONNX NAMESPACE        : onnx_torch\r\n--   ONNX_BUILD_TESTS      : OFF\r\n--   ONNX_BUILD_BENCHMARKS : OFF\r\n--   ONNX_USE_LITE_PROTO   : OFF\r\n--   ONNXIFI_DUMMY_BACKEND : OFF\r\n--   ONNXIFI_ENABLE_EXT    : OFF\r\n-- \r\n--   Protobuf compiler     : /usr/bin/protoc\r\n--   Protobuf includes     : /usr/include\r\n--   Protobuf libraries    : /usr/lib/libprotobuf.so;-lpthread\r\n--   BUILD_ONNX_PYTHON     : OFF\r\n-- \r\n-- ******** Summary ********\r\n--   CMake version         : 3.14.3\r\n--   CMake command         : /usr/bin/cmake\r\n--   System                : Linux\r\n--   C++ compiler          : /usr/bin/g++\r\n--   C++ compiler version  : 8.3.0\r\n--   CXX flags             : -march=x86-64 -mtune=generic -O2 -pipe -fno-plt -D_FORTIFY_SOURCE=2 -fvisibility-inlines-hidden -fopenmp -Wnon-virtual-dtor\r\n--   Build type            : Release\r\n--   Compile definitions   : \r\n--   CMAKE_PREFIX_PATH     : /usr/lib/python3.7/site-packages\r\n--   CMAKE_INSTALL_PREFIX  : /build/python-pytorch/src/pytorch-1.1.0/torch\r\n--   CMAKE_MODULE_PATH     : /build/python-pytorch/src/pytorch-1.1.0/cmake/Modules;/usr/share/cmake/pybind11\r\n-- \r\n--   ONNX version          : 1.4.1\r\n--   ONNX NAMESPACE        : onnx_torch\r\n--   ONNX_BUILD_TESTS      : OFF\r\n--   ONNX_BUILD_BENCHMARKS : OFF\r\n--   ONNX_USE_LITE_PROTO   : OFF\r\n--   ONNXIFI_DUMMY_BACKEND : OFF\r\n-- \r\n--   Protobuf compiler     : /usr/bin/protoc\r\n--   Protobuf includes     : /usr/include\r\n--   Protobuf libraries    : /usr/lib/libprotobuf.so;-lpthread\r\n--   BUILD_ONNX_PYTHON     : OFF\r\n-- Found gcc >=5 and CUDA <= 7.5, adding workaround C++ flags\r\n-- Could not find CUDA with FP16 support, compiling without torch.CudaHalfTensor\r\n-- Removing -DNDEBUG from compile flags\r\n-- MAGMA not found. Compiling without MAGMA support\r\n-- Could not find hardware support for NEON on this machine.\r\n-- No OMAP3 processor on this machine.\r\n-- No OMAP4 processor on this machine.\r\n-- Looking for cpuid.h\r\n-- Looking for cpuid.h - found\r\n-- Performing Test HAVE_GCC_GET_CPUID\r\n-- Performing Test HAVE_GCC_GET_CPUID - Success\r\n-- Performing Test NO_GCC_EBX_FPIC_BUG\r\n-- Performing Test NO_GCC_EBX_FPIC_BUG - Success\r\n-- Performing Test C_HAS_AVX_1\r\n-- Performing Test C_HAS_AVX_1 - Failed\r\n-- Performing Test C_HAS_AVX_2\r\n-- Performing Test C_HAS_AVX_2 - Success\r\n-- Performing Test C_HAS_AVX2_1\r\n-- Performing Test C_HAS_AVX2_1 - Failed\r\n-- Performing Test C_HAS_AVX2_2\r\n-- Performing Test C_HAS_AVX2_2 - Success\r\n-- Performing Test CXX_HAS_AVX_1\r\n-- Performing Test CXX_HAS_AVX_1 - Failed\r\n-- Performing Test CXX_HAS_AVX_2\r\n-- Performing Test CXX_HAS_AVX_2 - Success\r\n-- Performing Test CXX_HAS_AVX2_1\r\n-- Performing Test CXX_HAS_AVX2_1 - Failed\r\n-- Performing Test CXX_HAS_AVX2_2\r\n-- Performing Test CXX_HAS_AVX2_2 - Success\r\n-- AVX compiler support found\r\n-- AVX2 compiler support found\r\n-- Performing Test HAS_C11_ATOMICS\r\n-- Performing Test HAS_C11_ATOMICS - Success\r\n-- Atomics: using C11 intrinsics\r\n-- Looking for cheev_\r\n-- Looking for cheev_ - found\r\n-- Found a library with LAPACK API (generic).\r\ndisabling CUDA because NOT USE_CUDA is set\r\n-- CuDNN not found. Compiling without CuDNN support\r\ndisabling ROCM because NOT USE_ROCM is set\r\n-- MIOpen not found. Compiling without MIOpen support\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100   621    0   621    0     0   1584      0 --:--:-- --:--:-- --:--:--  1588\r\n100 66.4M  100 66.4M    0     0  19.6M      0  0:00:03  0:00:03 --:--:-- 28.2M\r\nDownloaded and unpacked Intel(R) MKL small libraries to /build/python-pytorch/src/pytorch-1.1.0/third_party/ideep/mkl-dnn/external\r\n-- This is a product build\r\n-- Detecting Intel(R) MKL: trying mklml_intel\r\n-- Intel(R) MKL: include /build/python-pytorch/src/pytorch-1.1.0/third_party/ideep/mkl-dnn/external/mklml_lnx_2019.0.3.20190220/include\r\n-- Intel(R) MKL: lib /build/python-pytorch/src/pytorch-1.1.0/third_party/ideep/mkl-dnn/external/mklml_lnx_2019.0.3.20190220/lib/libmklml_intel.so\r\n-- Found OpenMP_C: -fopenmp  \r\n-- Found OpenMP_CXX: -fopenmp  \r\n-- Found OpenMP: TRUE   \r\n-- OpenMP lib: /build/python-pytorch/src/pytorch-1.1.0/third_party/ideep/mkl-dnn/external/mklml_lnx_2019.0.3.20190220/lib/libiomp5.so\r\n-- Could NOT find Doxygen (missing: DOXYGEN_EXECUTABLE) \r\n-- VTune profiling environment is unset\r\nCMake Warning (dev) at third_party/ideep/mkl-dnn/cmake/utils.cmake:120 (target_link_libraries):\r\n  Policy CMP0023 is not set: Plain and keyword target_link_libraries\r\n  signatures cannot be mixed.  Run \"cmake --help-policy CMP0023\" for policy\r\n  details.  Use the cmake_policy command to set the policy and suppress this\r\n  warning.\r\n\r\n  The plain signature for target_link_libraries has already been used with\r\n  the target \"mkldnn\".  All uses of target_link_libraries with a target\r\n  should be either all-keyword or all-plain.\r\n\r\n  The uses of the plain signature are here:\r\n\r\n   * third_party/ideep/mkl-dnn/cmake/utils.cmake:111 (target_link_libraries)\r\n\r\nCall Stack (most recent call first):\r\n  third_party/ideep/mkl-dnn/src/CMakeLists.txt:108 (target_link_libraries_install)\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\n-- Found MKL-DNN: TRUE\r\n-- Looking for clock_gettime in rt\r\n-- Looking for clock_gettime in rt - found\r\n-- Looking for mmap\r\n-- Looking for mmap - found\r\n-- Looking for shm_open\r\n-- Looking for shm_open - found\r\n-- Looking for shm_unlink\r\n-- Looking for shm_unlink - found\r\n-- Looking for malloc_usable_size\r\n-- Looking for malloc_usable_size - found\r\n-- Performing Test C_HAS_THREAD\r\n-- Performing Test C_HAS_THREAD - Success\r\n-- GCC 8.3.0: Adding gcc and gcc_s libs to link line\r\n-- don't use NUMA\r\ndisabling CUDA because USE_CUDA is set false\r\n-- Check size of long double\r\n-- Check size of long double - done\r\n-- Performing Test COMPILER_SUPPORTS_LONG_DOUBLE\r\n-- Performing Test COMPILER_SUPPORTS_LONG_DOUBLE - Success\r\n-- Performing Test COMPILER_SUPPORTS_FLOAT128\r\n-- Performing Test COMPILER_SUPPORTS_FLOAT128 - Success\r\n-- Performing Test COMPILER_SUPPORTS_SSE2\r\n-- Performing Test COMPILER_SUPPORTS_SSE2 - Success\r\n-- Performing Test COMPILER_SUPPORTS_SSE4\r\n-- Performing Test COMPILER_SUPPORTS_SSE4 - Success\r\n-- Performing Test COMPILER_SUPPORTS_AVX\r\n-- Performing Test COMPILER_SUPPORTS_AVX - Success\r\n-- Performing Test COMPILER_SUPPORTS_FMA4\r\n-- Performing Test COMPILER_SUPPORTS_FMA4 - Success\r\n-- Performing Test COMPILER_SUPPORTS_AVX2\r\n-- Performing Test COMPILER_SUPPORTS_AVX2 - Success\r\n-- Performing Test COMPILER_SUPPORTS_SVE\r\n-- Performing Test COMPILER_SUPPORTS_SVE - Failed\r\n-- Performing Test COMPILER_SUPPORTS_AVX512F\r\n-- Performing Test COMPILER_SUPPORTS_AVX512F - Success\r\n-- Performing Test COMPILER_SUPPORTS_OPENMP\r\n-- Performing Test COMPILER_SUPPORTS_OPENMP - Success\r\n-- Performing Test COMPILER_SUPPORTS_WEAK_ALIASES\r\n-- Performing Test COMPILER_SUPPORTS_WEAK_ALIASES - Success\r\n-- Performing Test COMPILER_SUPPORTS_BUILTIN_MATH\r\n-- Performing Test COMPILER_SUPPORTS_BUILTIN_MATH - Success\r\n-- Configuring build for SLEEF-v3.2\r\n   Target system: Linux-5.0.10-arch1-1-ARCH\r\n   Target processor: x86_64\r\n   Host system: Linux-5.0.10-arch1-1-ARCH\r\n   Host processor: x86_64\r\n   Detected C compiler: GNU @ /usr/bin/gcc\r\n-- Using option `-Wall -Wno-unused -Wno-attributes -Wno-unused-result -Wno-psabi -ffp-contract=off -fno-math-errno -fno-trapping-math` to compile libsleef\r\n-- Building shared libs : OFF\r\n-- MPFR : /usr/lib/libmpfr.so\r\n-- MPFR header file in /usr/include\r\n-- GMP : /usr/lib/libgmp.so\r\n-- RUNNING_ON_TRAVIS : 0\r\n-- COMPILER_SUPPORTS_OPENMP : 1\r\n-- pytorch is compiling with OpenMP. \r\nOpenMP CXX_FLAGS: -fopenmp. \r\nOpenMP libraries: /usr/lib/libgomp.so;/usr/lib/libpthread.so.\r\n-- /usr/bin/g++ /build/python-pytorch/src/pytorch-1.1.0/torch/abi-check.cpp -o /build/python-pytorch/src/pytorch-1.1.0/build/abi-check\r\n-- Determined _GLIBCXX_USE_CXX11_ABI=1\r\n-- Performing Test HAS_THREAD_LOCAL\r\n-- Performing Test HAS_THREAD_LOCAL - Success\r\n-- Could NOT find MPI_C (missing: MPI_C_LIB_NAMES MPI_C_HEADER_DIR MPI_C_WORKS) \r\n-- Could NOT find MPI_CXX (missing: MPI_CXX_LIB_NAMES MPI_CXX_HEADER_DIR MPI_CXX_WORKS) \r\n-- Could NOT find MPI (missing: MPI_C_FOUND MPI_CXX_FOUND) \r\n-- ignoring CUDA\r\n-- Building C10D without CUDA support\r\n-- Could NOT find MPI_C (missing: MPI_C_LIB_NAMES MPI_C_HEADER_DIR MPI_C_WORKS) \r\n-- Could NOT find MPI_CXX (missing: MPI_CXX_LIB_NAMES MPI_CXX_HEADER_DIR MPI_CXX_WORKS) \r\n-- Could NOT find MPI (missing: MPI_C_FOUND MPI_CXX_FOUND) \r\n-- Not able to find MPI, will compile c10d without MPI support\r\n-- NCCL operators skipped due to no CUDA support\r\n-- Including IDEEP operators\r\n-- Including image processing operators\r\n-- Excluding video processing operators due to no opencv\r\n-- MPI operators skipped due to no MPI support\r\n-- Include Observer library\r\n-- Caffe2 is compiling with OpenMP. \r\nOpenMP CXX_FLAGS: -fopenmp. \r\nOpenMP libraries: /usr/lib/libgomp.so;/usr/lib/libpthread.so.\r\n-- Using lib/python3.7/site-packages as python relative installation path\r\nCMake Warning at CMakeLists.txt:435 (message):\r\n  Generated cmake files are only fully tested if one builds with system glog,\r\n  gflags, and protobuf.  Other settings may generate files that are not well\r\n  tested.\r\n\r\n\r\nCMake Warning at CMakeLists.txt:486 (message):\r\n  Generated cmake files are only available when building shared libs.\r\n\r\n\r\n-- \r\n-- ******** Summary ********\r\n-- General:\r\n--   CMake version         : 3.14.3\r\n--   CMake command         : /usr/bin/cmake\r\n--   System                : Linux\r\n--   C++ compiler          : /usr/bin/g++\r\n--   C++ compiler id       : GNU\r\n--   C++ compiler version  : 8.3.0\r\n--   BLAS                  : MKL\r\n--   CXX flags             : -march=x86-64 -mtune=generic -O2 -pipe -fno-plt -D_FORTIFY_SOURCE=2 -fvisibility-inlines-hidden -fopenmp -D_FORCE_INLINES -D_MWAITXINTRIN_H_INCLUDED -D__STRICT_ANSI__ -DUSE_FBGEMM -O2 -fPIC -Wno-narrowing -Wall -Wextra -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Wno-stringop-overflow\r\n--   Build type            : Release\r\n--   Compile definitions   : ONNX_NAMESPACE=onnx_torch;USE_C11_ATOMICS=1;HAVE_MMAP=1;_FILE_OFFSET_BITS=64;HAVE_SHM_OPEN=1;HAVE_SHM_UNLINK=1;HAVE_MALLOC_USABLE_SIZE=1\r\n--   CMAKE_PREFIX_PATH     : /usr/lib/python3.7/site-packages\r\n--   CMAKE_INSTALL_PREFIX  : /build/python-pytorch/src/pytorch-1.1.0/torch\r\n-- \r\n--   TORCH_VERSION         : 1.1.0\r\n--   CAFFE2_VERSION        : 1.1.0\r\n--   BUILD_ATEN_MOBILE     : OFF\r\n--   BUILD_ATEN_ONLY       : OFF\r\n--   BUILD_BINARY          : True\r\n--   BUILD_CUSTOM_PROTOBUF : OFF\r\n--     Protobuf compiler   : /usr/bin/protoc\r\n--     Protobuf includes   : /usr/include\r\n--     Protobuf libraries  : /usr/lib/libprotobuf.so;-lpthread\r\n--   BUILD_DOCS            : OFF\r\n--   BUILD_PYTHON          : True\r\n--     Python version      : 3.7.3\r\n--     Python executable   : /usr/bin/python\r\n--     Pythonlibs version  : 3.7.3\r\n--     Python library      : /usr/lib/libpython3.7m.so.1.0\r\n--     Python includes     : /usr/include/python3.7m\r\n--     Python site-packages: lib/python3.7/site-packages\r\n--   BUILD_CAFFE2_OPS      : True\r\n--   BUILD_SHARED_LIBS     : 0\r\n--   BUILD_TEST            : True\r\n--   USE_ASAN              : OFF\r\n--   USE_CUDA              : False\r\n--   USE_ROCM              : False\r\n--   USE_EIGEN_FOR_BLAS    : ON\r\n--   USE_FBGEMM            : ON\r\n--   USE_FFMPEG            : False\r\n--   USE_GFLAGS            : OFF\r\n--   USE_GLOG              : OFF\r\n--   USE_LEVELDB           : False\r\n--   USE_LITE_PROTO        : OFF\r\n--   USE_LMDB              : False\r\n--   USE_METAL             : OFF\r\n--   USE_MKL               : OFF\r\n--   USE_MKLDNN            : ON\r\n--   USE_NCCL              : False\r\n--   USE_NNPACK            : True\r\n--   USE_NUMPY             : ON\r\n--   USE_OBSERVERS         : ON\r\n--   USE_OPENCL            : OFF\r\n--   USE_OPENCV            : True\r\n--     OpenCV version      : 4.1.0\r\n--   USE_OPENMP            : ON\r\n--   USE_PROF              : OFF\r\n--   USE_QNNPACK           : True\r\n--   USE_REDIS             : OFF\r\n--   USE_ROCKSDB           : OFF\r\n--   USE_ZMQ               : OFF\r\n--   USE_DISTRIBUTED       : True\r\n--     USE_MPI             : OFF\r\n--     USE_GLOO            : ON\r\n--     USE_GLOO_IBVERBS    : OFF\r\n--   Public Dependencies  : Threads::Threads;caffe2::mkldnn\r\n--   Private Dependencies : qnnpack;nnpack;cpuinfo;fbgemm;opencv_core;opencv_highgui;opencv_imgproc;opencv_imgcodecs;opencv_optflow;opencv_videoio;opencv_video;fp16;gloo;aten_op_header_gen;foxi_loader;rt;gcc_s;gcc;dl\r\n-- Configuring done\r\nCMake Warning at torch/lib/c10d/test/CMakeLists.txt:9 (add_executable):\r\n  Cannot generate a safe linker search path for target TCPStoreTest because\r\n  files in some directories may conflict with libraries in implicit\r\n  directories:\r\n\r\n    link library [libiomp5.so] in /usr/lib may be hidden by files in:\r\n      /build/python-pytorch/src/pytorch-1.1.0/third_party/ideep/mkl-dnn/external/mklml_lnx_2019.0.3.20190220/lib\r\n\r\n  Some of these libraries may not be found correctly.\r\nCall Stack (most recent call first):\r\n  torch/lib/c10d/test/CMakeLists.txt:17 (c10d_add_test)\r\n\r\n\r\nCMake Warning at torch/lib/c10d/test/CMakeLists.txt:9 (add_executable):\r\n  Cannot generate a safe linker search path for target ProcessGroupGlooTest\r\n  because files in some directories may conflict with libraries in implicit\r\n  directories:\r\n\r\n    link library [libiomp5.so] in /usr/lib may be hidden by files in:\r\n      /build/python-pytorch/src/pytorch-1.1.0/third_party/ideep/mkl-dnn/external/mklml_lnx_2019.0.3.20190220/lib\r\n\r\n  Some of these libraries may not be found correctly.\r\nCall Stack (most recent call first):\r\n  torch/lib/c10d/test/CMakeLists.txt:26 (c10d_add_test)\r\n\r\n\r\nCMake Warning at torch/lib/c10d/test/CMakeLists.txt:9 (add_executable):\r\n  Cannot generate a safe linker search path for target FileStoreTest because\r\n  files in some directories may conflict with libraries in implicit\r\n  directories:\r\n\r\n    link library [libiomp5.so] in /usr/lib may be hidden by files in:\r\n      /build/python-pytorch/src/pytorch-1.1.0/third_party/ideep/mkl-dnn/external/mklml_lnx_2019.0.3.20190220/lib\r\n\r\n  Some of these libraries may not be found correctly.\r\nCall Stack (most recent call first):\r\n  torch/lib/c10d/test/CMakeLists.txt:16 (c10d_add_test)\r\n\r\n\r\nCMake Warning at torch/lib/libshm/CMakeLists.txt:32 (ADD_EXECUTABLE):\r\n  Cannot generate a safe linker search path for target torch_shm_manager\r\n  because files in some directories may conflict with libraries in implicit\r\n  directories:\r\n\r\n    link library [libiomp5.so] in /usr/lib may be hidden by files in:\r\n      /build/python-pytorch/src/pytorch-1.1.0/third_party/ideep/mkl-dnn/external/mklml_lnx_2019.0.3.20190220/lib\r\n\r\n  Some of these libraries may not be found correctly.\r\n\r\n\r\nCMake Warning at torch/lib/libshm/CMakeLists.txt:25 (ADD_LIBRARY):\r\n  Cannot generate a safe linker search path for target shm because files in\r\n  some directories may conflict with libraries in implicit directories:\r\n\r\n    link library [libiomp5.so] in /usr/lib may be hidden by files in:\r\n      /build/python-pytorch/src/pytorch-1.1.0/third_party/ideep/mkl-dnn/external/mklml_lnx_2019.0.3.20190220/lib\r\n\r\n  Some of these libraries may not be found correctly.\r\n\r\n\r\n-- Generating done\r\nCMake Warning:\r\n  Manually-specified variables were not used by the project:\r\n\r\n    THD_SO_VERSION\r\n```"}
{"number": 20207, "title": "Remove CPU_tensor_parallel_kernel_apply2", "time": "2019-05-07T14:12:34Z", "body": "This code is unused and has been superseded by TensorIterators."}
{"number": 20208, "title": "GPU histc returns tensor of wrong dtype", "time": "2019-05-07T14:19:11Z", "body": "cc @xvdp @jaciefan \r\n\r\nOriginal report from @xvdp: https://github.com/pytorch/pytorch/issues/1382#issuecomment-490092747\r\n\r\nThank you for the implementation, it is present in latest release, but there's an inconsistency in behaviour you may want to fix: cuda and cpu return different types\r\n\r\n```\r\nimport torch\r\nimport sys\r\nprint(sys.version_info)\r\nprint(torch.__version__)\r\nnormalcpu = torch.zeros(100, dtype=torch.float32, device=\"cpu\", requires_grad=False).normal_(0., 1.)\r\nh_cpu = torch.histc(normalcpu, bins=10)\r\nnormalcuda = normalcpu.to(device=\"cuda\")\r\nh_cuda = torch.histc(normalcuda, bins=10)\r\nprint(h_cpu, h_cpu.dtype)\r\nprint(h_cuda, h_cuda.dtype)\r\n```\r\n\r\n```\r\nsys.version_info(major=3, minor=7, micro=3, releaselevel='final', serial=0)\r\n1.1.0\r\ntensor([ 2.,  5., 15., 26., 17., 18., 10.,  4.,  2.,  1.]) torch.float32\r\ntensor([ 2,  5, 15, 26, 17, 18, 10,  4,  2,  1], device='cuda:0') torch.int64\r\n```"}
{"number": 20209, "title": "[Please ignore][xla ci] testing", "time": "2019-05-07T14:25:37Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20209 [Please ignore][xla ci] testing**\n\ngh-metadata: pytorch pytorch 20209 gh/zou3519/35/head"}
{"number": 20210, "title": "[TESTING] ghimport smoketest", "time": "2019-05-07T14:59:55Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #20211 [TESTING] ghimport smoketest 2\n* **#20210 [TESTING] ghimport smoketest**\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nDifferential Revision: [D15240983](https://our.internmc.facebook.com/intern/diff/D15240983)"}
{"number": 20211, "title": "[TESTING] ghimport smoketest 2", "time": "2019-05-07T15:00:02Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20211 [TESTING] ghimport smoketest 2**\n* #20210 [TESTING] ghimport smoketest\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nDifferential Revision: [D15240984](https://our.internmc.facebook.com/intern/diff/D15240984)"}
{"number": 20212, "title": "Fix test_namedtuple_return", "time": "2019-05-07T15:03:53Z", "body": "Fixes: https://github.com/pytorch/pytorch/issues/20198\r\n\r\nInput matrix created randomly might be rank deficient or not positive semidefinite\r\n\r\nNot tested yet, will look at CI."}
{"number": 20213, "title": "Run 'checkout' before 'setup ci environment' on pytorch linux tests", "time": "2019-05-07T15:23:28Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #20231 Error out if `git log` fails in setup_ci_environment\n* **#20213 Run 'checkout' before 'setup ci environment' on pytorch linux tests**\n\nRight now, the xla ci and the slow test pull request test triggers don't seem\nto work (I'm currently running a test on this but the CI turnaround is\nslow). The problem is as follows:\n\nDuring the pytorch_linux_test_defaults, the CI runs setup_ci_environment\nwithout running `checkout`. There is this [line\nhere](https://github.com/pytorch/pytorch/blob/b8256280ce45f02a7e105d3b3db4a547990e683d/.circleci/config.yml#L169-L173)\nthat suggests that we need to run checkout. Indeed, all the `git log`\ncalls should fail because there is no git checkout; the git checkout\nexists inside the docker container that has yet to be pulled by the\ncircleci machine.\nThis means that the xla ci and the slow test tests should fail with \"no\ngit repo found\".\n\nTest Plan\n- Run tests\n\ngh-metadata: pytorch pytorch 20213 gh/zou3519/36/head\n\nDifferential Revision: [D15244428](https://our.internmc.facebook.com/intern/diff/D15244428)"}
{"number": 20214, "title": "[TESTING] is slow test working", "time": "2019-05-07T15:33:48Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20214 [TESTING] is slow test working**\n\n[slow ci]\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>"}
{"number": 20215, "title": "The JIT sometimes does not repeat values in the output_size argument.", "time": "2019-05-07T15:36:46Z", "body": "**Background**\r\n\r\nThis issue came up during the port of `adaptive_max_pool2d()` to `ATen`. The relevant function signature in `native_functions.yaml` is:\r\n\r\n   `func: adaptive_max_pool2d(Tensor self, int[2] output_size) -> (Tensor, Tensor)`\r\n\r\n\r\n**The issue**\r\n\r\nWhen `output_size` is a single `int` on the Python level (square image H x H), `adaptive_max_pool2d()` receives an `output_size` argument with `output_size.size()==1` instead of `2`.\r\n\r\nCurrently `adaptive_max_pool2d()` has a workaround that also accepts `output_size.size()==1`  and repeats `H` inside the function.\r\n\r\n**How to reproduce**\r\n\r\nThe cause is difficult to isolate since the tests are generated and quite complex. I think also a module export/import is involved.\r\n\r\n1) Apply this diff:\r\n\r\n```\r\ndiff --git a/aten/src/ATen/native/AdaptiveMaxPooling2d.cpp b/aten/src/ATen/native/AdaptiveMaxPooling2d.cpp\r\nindex b3b77c5..ec16c5e 100644\r\n--- a/aten/src/ATen/native/AdaptiveMaxPooling2d.cpp\r\n+++ b/aten/src/ATen/native/AdaptiveMaxPooling2d.cpp\r\n@@ -322,6 +322,7 @@ std::tuple<Tensor, Tensor> adaptive_max_pool2d_cpu(\r\n {\r\n   Tensor output = at::empty({0}, input.options());\r\n   Tensor indices = at::empty({0}, input.options().dtype(kLong));\r\n+  assert(output_size.size() == 2);\r\n   adaptive_max_pool2d_out_cpu_template(\r\n     output,\r\n     indices,\r\n```\r\n\r\n2) Run the relevant test case:\r\n\r\n`python3 -m pytest -v test_jit.py::TestJitGeneratedModule::test_nn_AdaptiveMaxPool2d_single`\r\n\r\n\r\n**Backtrace**\r\n\r\n```\r\n(gdb) f 9\r\n#9  0x00007fffcdeb2127 in torch::jit::(anonymous namespace)::<lambda(torch::jit::Stack&)>::operator()(torch::jit::Stack &) const (__closure=0x44ba5580, \r\n    stack=std::vector of length 2, capacity 2 = {...}) at /home/stefan/pytorch/torch/csrc/jit/generated/register_aten_ops_1.cpp:2948\r\n2948              );\r\n(gdb) f 8\r\n#8  0x00007fffcde9aa4d in at::adaptive_max_pool2d (self=..., output_size=...) at /home/stefan/pytorch/build/aten/src/ATen/Functions.h:5728\r\n5728        return detail::infer_type(self).adaptive_max_pool2d(self, output_size);\r\n(gdb) p output_size.size()\r\n$1 = 1\r\n```\r\n\r\n"}
{"number": 20216, "title": "[TESTING] ghimport smoketest", "time": "2019-05-07T15:38:01Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #20217 [TESTING] ghimport smoketest 2\n* **#20216 [TESTING] ghimport smoketest**\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nDifferential Revision: [D15241239](https://our.internmc.facebook.com/intern/diff/D15241239)"}
{"number": 20217, "title": "[TESTING] ghimport smoketest 2", "time": "2019-05-07T15:38:07Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20217 [TESTING] ghimport smoketest 2**\n* #20216 [TESTING] ghimport smoketest\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nDifferential Revision: [D15241238](https://our.internmc.facebook.com/intern/diff/D15241238)"}
{"number": 20218, "title": "[TESTING] ghimport smoketest", "time": "2019-05-07T15:51:17Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #20219 [TESTING] ghimport smoketest 2\n* **#20218 [TESTING] ghimport smoketest**\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nDifferential Revision: [D15241369](https://our.internmc.facebook.com/intern/diff/D15241369)"}
{"number": 20219, "title": "[TESTING] ghimport smoketest 2", "time": "2019-05-07T15:51:23Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20219 [TESTING] ghimport smoketest 2**\n* #20218 [TESTING] ghimport smoketest\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nDifferential Revision: [D15241371](https://our.internmc.facebook.com/intern/diff/D15241371)"}
{"number": 20220, "title": "Mention JIT workaround in the comments.", "time": "2019-05-07T16:11:26Z", "body": "I've opened #20215 for the `JIT` issue.  This is just the comment update that mentions the issue number."}
{"number": 20221, "title": "Make a deep copy of extra_compile_flag dictionnary", "time": "2019-05-07T16:12:46Z", "body": "See issue #20169\r\n\r\n"}
{"number": 20222, "title": "Mention issue number in the JIT workaround comments", "time": "2019-05-07T16:26:32Z", "body": "This just updates the `JIT` comments with the issue number #20215.  Hopefully this will stop the proliferation of the workaround. :)"}
{"number": 20223, "title": "[tensorboard] Have add_video use NamedTemporaryFile directly", "time": "2019-05-07T16:40:48Z", "body": "address comment in #16196\r\nhttps://github.com/pytorch/pytorch/pull/16196/files#r278676986\r\n\r\ncc @orionr "}
{"number": 20224, "title": "[please ignore][slow ci] testing", "time": "2019-05-07T17:12:02Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20224 [please ignore][slow ci] testing**\n\ngh-metadata: pytorch pytorch 20224 gh/zou3519/37/head"}
{"number": 20225, "title": "Remove flake8 E303 (too many blank lines)", "time": "2019-05-07T17:31:36Z", "body": "similar to too few blank lines, I feel like this is not important enough to warrant breaking signal for all linters when it's violated.\r\n\r\n"}
{"number": 20226, "title": "[jit] Convenience APIs for script objects", "time": "2019-05-07T17:45:34Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20226 [jit] Convenience APIs for script objects**\n\nThe compiler should be emitting static lookups to object slots, but\npeople using the C++ API should have some sugar for doing attribute\nlookup.\n\nDifferential Revision: [D15243625](https://our.internmc.facebook.com/intern/diff/D15243625)"}
{"number": 20227, "title": "Lite JIT executor, the high level save flow.", "time": "2019-05-07T17:54:59Z", "body": "This first diff of the lite JIT executor project. It's focused on the high-level save flow: from script::module, how could we save the instructions compiled in JIT. \r\nThe user-facing idea in this pull is that for PyTorch script, we provide the option for users to dump instruction-level byte codes. If the user wants to have inference on a method, they could dump instructions using this feature. On device the light executor could load and execute them. \r\nThe unit test is in src/pytorch/test/cpp/jit/test_lite_executor.h. \r\nThe two major TODO parts:\r\n1) A parser to serialize/deserialize the instructions\r\n2) A lite executor to load and execute the instructions\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack):\r\n* **#20227 Lite JIT executor, the high level save flow.**\r\n\r\nDifferential Revision: [D15244231](https://our.internmc.facebook.com/intern/diff/D15244231)"}
{"number": 20228, "title": "Remove brew libomp from binary mac machines", "time": "2019-05-07T18:01:32Z", "body": "Resolves https://github.com/pytorch/pytorch/issues/20030"}
{"number": 20229, "title": "[JIT] String parser does not handle trailing commas in apply expression arg list", "time": "2019-05-07T18:06:35Z", "body": "Repro:\r\n\r\n```\r\nimport torch\r\n\r\ncode = '''\r\ndef forward(self, x):\r\n    return torch.neg(x,)\r\n'''\r\n\r\ninvoke = '''\r\nprint(forward(None, torch.rand(3, 4)))\r\n'''\r\n\r\nexec(code + invoke, {'torch': torch}, {})\r\n\r\nclass SM(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n        super(SM, self).__init__()\r\n        self.define(code)\r\n\r\nsm = SM()\r\nprint(sm(torch.rand(3, 4)))\r\n```\r\n\r\nOutput:\r\n\r\n```\r\ntensor([[-0.1211, -0.3662, -0.7884, -0.8946],\r\n        [-0.1497, -0.8419, -0.8527, -0.5679],\r\n        [-0.4483, -0.1271, -0.9466, -0.0418]])\r\n<snip>\r\nRuntimeError: expected ident but found ')' here:\r\n\r\ndef forward(self, x):\r\n    return torch.neg(x,)\r\n                       ~ <--- HERE\r\n```"}
{"number": 20230, "title": "[jit] torch.tensor doesn't support list of tuples", "time": "2019-05-07T18:10:08Z", "body": "## üêõ Bug\r\n\r\nCannot create a tensor using `torch.tensor` from a list of tuples.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef test():\r\n  li = [(1, 2, 3, 4), (5, 6, 7, 8), (9, 10, 11, 12)]\r\n  return torch.tensor(li)\r\n```\r\n\r\ngives the following error:\r\n```\r\nRuntimeError: \r\nInput list to torch.tensor must be of ints, floats, or bools, got (int, int, int, int):\r\n@torch.jit.script\r\ndef test():\r\n  li = [(1, 2, 3, 4), (5, 6, 7, 8), (9, 10, 11, 12)]\r\n  return torch.tensor(li)\r\n         ~~~~~~~~~~~~ <--- HERE\r\n```\r\n\r\n## Expected behavior\r\n\r\nShould not give an error. Works in pure python.\r\n\r\n## Environment\r\nPytorch 1.1.0\r\n\r\n## Additional context\r\n\r\nA workaround is to use a list of lists instead.\r\n"}
{"number": 20231, "title": "Error out if `git log` fails in setup_ci_environment", "time": "2019-05-07T18:11:17Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20231 Error out if `git log` fails in setup_ci_environment**\n* #20213 Run 'checkout' before 'setup ci environment' on pytorch linux tests\n\nThis PR makes setup_ci_environment more robust. If the topmost commit\ndoesn't exist, the xla_test and slow_test CI used to erroneously report\nsuccess,\nnow it should report failure.\n\nTest Plan:\n- run slow tests [slow ci]\n\ngh-metadata: pytorch pytorch 20231 gh/zou3519/38/head\n\nDifferential Revision: [D15244497](https://our.internmc.facebook.com/intern/diff/D15244497)"}
{"number": 20232, "title": "Automatic update of fbcode/onnx to 5bde6371620b76302864bce90f521d72eda95d0e", "time": "2019-05-07T18:41:51Z", "body": "Summary:\nPrevious import was 7d7bc83d29a328233d3e8affa4c4ea8b3e3599ef\n\nIncluded changes:\n- **[5bde6371](https://github.com/onnx/onnx/commit/5bde6371)**: Add shape inference for legacy auto_pad modes (#1988) <stevenlix>\n- **[6c9b3407](https://github.com/onnx/onnx/commit/6c9b3407)**: Move Quantization working group to completed state (#1980) <Prasanth Pulavarthi>\n- **[8eba124e](https://github.com/onnx/onnx/commit/8eba124e)**: Define the IR acronym (#1985) <Senja Filipi>\n\nDifferential Revision: D15244357\n\n"}
{"number": 20233, "title": "Adds quantized addition and renames sum to add", "time": "2019-05-07T19:24:26Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20233 Adds quantized addition and renames sum to add**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15245791/)\n\nAdding a quantized addition (without relu)\n\nDifferential Revision: [D15245791](https://our.internmc.facebook.com/intern/diff/D15245791/)"}
{"number": 20234, "title": "Add c10d::broadcast_coalesced and tests", "time": "2019-05-07T19:25:38Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20236 Make DistributedDataParallel usable with CPU models&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15245428/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20235 Refactor core DistributedDataParallel tests&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15245429/)\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20234 Add c10d::broadcast_coalesced and tests**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15228099/)\n\nThe differences with the existing function _dist_broadcast_coalesced\nis that this one works for both CPU and CUDA tensors and that it has a\nmaximum number of in flight operations.\n\nThis should be the final change needed to have only a single version\nof DistributedDataParallel that both supports CPU and CUDA models, or\neven a mix of both.\n\nSee #17757 for more information.\n\nDifferential Revision: [D15228099](https://our.internmc.facebook.com/intern/diff/D15228099/)"}
{"number": 20235, "title": "Refactor core DistributedDataParallel tests", "time": "2019-05-07T19:25:45Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20236 Make DistributedDataParallel usable with CPU models&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15245428/)\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20235 Refactor core DistributedDataParallel tests**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15245429/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20234 Add c10d::broadcast_coalesced and tests&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15228099/)\n\nThe tests expected to only run for CUDA models. In a future commit we\nneed to update this to work for CPU models as well. Therefore, we can\nno longer rely on only integers being passed for device identifiers.\nWith this change we pass both the materialized list of devices to use\n(as `torch.Device` objects), as well as an optional list of integers.\nThe latter is specified to exercise the code in the\nDistributedDataParallel constructor that turns a list of integers into\nCUDA devices, IFF it is used to wrap a single-device CUDA module.\n\nThis commit also groups together the 'str' and non-'str' tests. These\nused to test passing the list of devices as integers or as\n`torch.Device` instances. These are now executed from the same test.\n\nDifferential Revision: [D15245429](https://our.internmc.facebook.com/intern/diff/D15245429/)"}
{"number": 20236, "title": "Make DistributedDataParallel usable with CPU models", "time": "2019-05-07T19:25:52Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20236 Make DistributedDataParallel usable with CPU models**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15245428/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20235 Refactor core DistributedDataParallel tests&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15245429/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20234 Add c10d::broadcast_coalesced and tests&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15228099/)\n\nUse the new version of broadcast_coalesced that deals with both CPU\nand CUDA models. Add tests that evaluate correctness of\nDistributedDataParallel for CPU models.\n\nCloses #17757.\n\nDifferential Revision: [D15245428](https://our.internmc.facebook.com/intern/diff/D15245428/)"}
{"number": 20237, "title": "Fix in benchmark_test_generator", "time": "2019-05-07T19:33:31Z", "body": "Summary:\r\nAdd missing import\r\n\r\n"}
{"number": 20238, "title": "Re-enable CUDA tests for C++ API", "time": "2019-05-07T19:37:28Z", "body": "CUDA tests for C++ API were not running on the CI due to a missing character in https://github.com/pytorch/pytorch/pull/11554. This PR fixes the bug."}
{"number": 20239, "title": "[jit] add support for tuple unpacking in for loops", "time": "2019-05-07T19:41:10Z", "body": "## üöÄ Feature\r\nAdd support for tuple unpacking in for loops\r\n\r\n## Motivation\r\n\r\nMakes working with tuples a better experience and the code shorter. \r\n\r\n## Pitch\r\n\r\nThis should be possible:\r\n\r\n```python\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef test():\r\n  fm = [(64, 32), (32, 16), (16, 8)]\r\n  \r\n  for width, height in fm:\r\n    pass\r\n```\r\n\r\n## Alternatives\r\n\r\nA current workaround is to use:\r\n\r\n```python\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef test():\r\n  fm = [(64, 32), (32, 16), (16, 8)]\r\n  \r\n  for f in fm:\r\n\twidth, height = f\r\n    pass\r\n```\r\n"}
{"number": 20240, "title": "Cumulative Maximum", "time": "2019-05-07T19:57:32Z", "body": "## üöÄ Feature\r\nAdd cumulative maximum for tensors, where each element along an axis is the max of it and everything before it.\r\n\r\n## Motivation\r\nThis is useful for evaluating mAP by creating precision-recall curves, where precision should be monotonically decreasing with respect to recall. More info: https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173\r\n\r\n## Pitch\r\nTorch has .cumsum and .cumprod so I suggest .cummax to be added.\r\n\r\n## Alternatives\r\nNumpy supports this with `np.maximum.accumulate()`, so we have to convert to numpy then back.\r\n"}
{"number": 20241, "title": "[FR] nn.Module.requires_grad_()", "time": "2019-05-07T20:02:07Z", "body": "Useful in finetuning, gan training, etc. :)"}
{"number": 20242, "title": "[jit] Add support for __getstate__/__setstate__ on module", "time": "2019-05-07T20:07:07Z", "body": "Adds support for `__getstate__` and `__setstate__` on modules that are called as part of export (`torch.save()`) and import (`torch.jit.load`).\r\n* `__getstate__` and `__setstate__` must be TorchScript functions with the signatures `() -> T` and `(T) -> None` respectively\r\n* The results of `__getstate__` are stored using the pickler in `states.pkl` with one for each module in definition order (`__getstate__` returns `None` by default if an imlpementation is not provided)\r\n    * This prevents sharing between `__getstate__` and attributes, but this should be fine since their use is mostly unrelated (attributes are for storing values to be used in script methods, `__getstate__` for running arbitrary computations during import)\r\n\r\nFollow up\r\n* Somehow replacing `__getstate__`/`__setstate__` with a `ScriptMethodStub` makes `MyScriptModule().__getstate__()` call `ScriptModule.__getstate__()` when used in Python. This should be fixed so semantics in Python are preserved, but it doesn't affect the typical usage.\r\n\r\nFixes #19030\r\n\r\nDifferential Revision: [D15287161](https://our.internmc.facebook.com/intern/diff/15287161/)"}
{"number": 20243, "title": "Manually set _GLIBCXX_USE_CXX11_ABI in devtoolset7 binary builds", "time": "2019-05-07T20:11:47Z", "body": "Fix for https://github.com/pytorch/pytorch/issues/17492"}
{"number": 20244, "title": "Fix memory leak in torch._dirichlet_grad()", "time": "2019-05-07T20:12:40Z", "body": "Fixes https://github.com/pyro-ppl/pyro/issues/1853\r\n\r\nThis fixes a memory leak in `torch._dirichlet_grad()`. This function is used for reparametrized gradients for the `Dirichlet` and `Beta` distributions.\r\n\r\n## Questions for reviewiers\r\n\r\n- [x] Could a reviewer please confirm that `freeCopyTo()` is being used correctly and doesn't need an additional `decref()`? The author is unfamiliar with PyTorch C++ memory utilities. Help appreciated.\r\n\r\n## Tested\r\n- ran locally and confirmed leak is fixed"}
{"number": 20245, "title": "ProcessGroupMPI tests in test_c10d.py", "time": "2019-05-07T20:24:08Z", "body": "These are not in place yet due to the non trivial work required to launch an MPI test.\r\n\r\nGo through the test cases in `test_c10d.py` and evaluate where we need to add tests that use `ProcessGroupMPI`. Also figure out where refactoring is needed to support this.\r\n\r\nThere is an opportunity to consolidate the multi process test harness in `MultiProcessTestCase` between `test_c10d.py` and `test_distributed.py` and make it work for both MPI and non-MPI tests."}
{"number": 20246, "title": "fix parsing bugs", "time": "2019-05-07T20:35:59Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* **#20246 fix parsing bugs**\r\n\r\nFixes #20229\r\n\r\nDifferential Revision: [D15247081](https://our.internmc.facebook.com/intern/diff/D15247081)"}
{"number": 20247, "title": "Problem running custom operator test, pytorch 1.1.0", "time": "2019-05-07T20:45:39Z", "body": "## üêõ Bug\r\n\r\nI cannot save and then load a custom operator, using the test code available in pytorch/test/custom_operator in Pytorch 1.1.0.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n```bash\r\ncd pytorch/test/custom_operator\r\n# Change save path of the model (like model.save in test_custom_ops.py) to current folder\r\npython3 test_custom_ops.py # generates a something.pt file\r\nmkdir build && cd build && cmake -DCMAKE_PRIFIX_PATH=/path/to/libtorch/ .. && make \r\n./test_custom_ops something.pt\r\n```\r\n## Expected behavior\r\n\r\npass AT_ASSERT(module != nullptr); in load_serialized_module_with_custom_op_and_execute\r\n\r\n## output\r\n\r\nI get the following error:\r\n```\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  isGenericList() ASSERT FAILED at /pytorch/aten/src/ATen/core/ivalue.h:385, please report a bug to PyTorch. (toGenericList at /pytorch/aten/src/ATen/core/ivalue.h:385)\r\nframe #0: std::function<std::string ()>::operator()() const + 0x11 (0x7ff8121cd441 in /home/shakiba/Downloads/libtorch/lib/libc10.so)\r\nframe #1: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x2a (0x7ff8121ccd7a in /home/shakiba/Downloads/libtorch/lib/libc10.so)\r\nframe #2: <unknown function> + 0x9728f2 (0x7ff855c088f2 in /home/shakiba/Downloads/libtorch/lib/libtorch.so.1)\r\nframe #3: torch::jit::Unpickler::parse_ivalue_list() + 0x41 (0x7ff855c05f31 in /home/shakiba/Downloads/libtorch/lib/libtorch.so.1)\r\nframe #4: <unknown function> + 0xa6f91b (0x7ff855d0591b in /home/shakiba/Downloads/libtorch/lib/libtorch.so.1)\r\nframe #5: torch::jit::load(std::unique_ptr<caffe2::serialize::ReadAdapterInterface, std::default_delete<caffe2::serialize::ReadAdapterInterface> >, c10::optional<c10::Device>, std::unordered_map<std::string, std::string, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::string> > >&) + 0x10d (0x7ff855d06f5d in /home/shakiba/Downloads/libtorch/lib/libtorch.so.1)\r\nframe #6: torch::jit::load(std::string const&, c10::optional<c10::Device>, std::unordered_map<std::string, std::string, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::string> > >&) + 0x68 (0x7ff855d07088 in /home/shakiba/Downloads/libtorch/lib/libtorch.so.1)\r\nframe #7: load_serialized_module_with_custom_op_and_execute(std::string const&) + 0x58 (0x44413f in ./test_custom_ops)\r\nframe #8: main + 0x85 (0x445787 in ./test_custom_ops)\r\nframe #9: __libc_start_main + 0xf0 (0x7ff811862830 in /lib/x86_64-linux-gnu/libc.so.6)\r\nframe #10: _start + 0x29 (0x442969 in ./test_custom_ops)\r\n```\r\nAborted (core dumped)\r\n\r\n\r\n## Environment\r\n\r\n - PyTorch Version: 1.1.0\r\n - OS: Linux 16.04\r\n - both with torch installation via pip and using pytorch dockerfile\r\n - Python version: 3.5\r\n - CUDA/cuDNN version: 10.0, 7.5\r\n - GPU models and configuration: GV100 && 2080\r\n"}
{"number": 20248, "title": "Sparse tensors can't be used in DataLoader running many workers", "time": "2019-05-07T22:33:43Z", "body": "## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nSparse tensors can't be used in DataLoader running many workers\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n```python\r\nimport torch.utils.data as D\r\nfrom scipy.sparse import csr_matrix\r\nimport numpy as np\r\nimport torch\r\n\r\nrow  = np.array([0, 3, 0, 7])\r\ncol  = np.array([0, 3, 1, 9])\r\ndata = np.array([4, 5, 7, 9])\r\n# using CSR matrix as can be row indexed\r\nsparse = csr_matrix((data, (row, col)), shape=(8, 10))\r\n\r\nclass Dataset(D.Dataset):\r\n    \r\n    def __init__(self, sparse):\r\n        self.data = sparse\r\n\r\n    def __len__(self):\r\n        return self.data.shape[0]\r\n    \r\n    def __getitem__(self, index):\r\n        # convert it to COO so to get the atributes to create a sparse tensor\r\n        data = self.data[index].tocoo()\r\n        i = torch.LongTensor(np.vstack((data.row, data.col)))\r\n        v = torch.FloatTensor(data.data)\r\n        data = torch.sparse.FloatTensor(i, v, torch.Size(data.shape))\r\n        return data\r\n\r\nd = Dataset(sparse)\r\n\r\nloader = torch.utils.data.DataLoader(d, \r\n                                     batch_size=2,\r\n                                     num_workers=2)\r\n\r\nfor i in loader:\r\n    print(i)\r\n```\r\n\r\n```\r\nRuntimeError: Traceback (most recent call last):\r\n  File \"/Users/efelix/miniconda3/envs/release3.7/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 99, in _worker_loop\r\n    samples = collate_fn([dataset[i] for i in batch_indices])\r\n  File \"/Users/efelix/miniconda3/envs/release3.7/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\", line 41, in default_collate\r\n    storage = batch[0].storage()._new_shared(numel)\r\nRuntimeError: sparse tensors do not have storage\r\n```\r\n\r\n## Expected behavior\r\n\r\nThe same one that I have when not setting n_workers parameter.\r\n\r\n```python\r\nloader = torch.utils.data.DataLoader(d, \r\n                                     batch_size=2)\r\nfor i in loader:\r\n    print(i)\r\n```\r\n\r\n```\r\ntensor(indices=tensor([[0, 0],\r\n                       [0, 0],\r\n                       [0, 1]]),\r\n       values=tensor([4., 7.]),\r\n       size=(2, 1, 10), nnz=2, layout=torch.sparse_coo)\r\ntensor(indices=tensor([[1],\r\n                       [0],\r\n                       [3]]),\r\n       values=tensor([5.]),\r\n       size=(2, 1, 10), nnz=1, layout=torch.sparse_coo)\r\ntensor(indices=tensor([], size=(3, 0)),\r\n       values=tensor([], size=(0,)),\r\n       size=(2, 1, 10), nnz=0, layout=torch.sparse_coo)\r\ntensor(indices=tensor([[1],\r\n                       [0],\r\n                       [9]]),\r\n       values=tensor([9.]),\r\n       size=(2, 1, 10), nnz=1, layout=torch.sparse_coo)\r\n```\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.14.4\r\nGCC version: Could not collect\r\nCMake version: version 3.14.3\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.3\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.1                      144  \r\n[conda] mkl-service               1.1.2            py36hfbe908c_5  \r\n[conda] mkl_fft                   1.0.10           py36h5e564d8_0  \r\n[conda] mkl_random                1.0.2            py36h27c97d8_0  \r\n[conda] pytorch                   1.1.0                   py3.6_0    pytorch\r\n[conda] torchvision               0.2.2.post3              pypi_0    pypi\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"}
{"number": 20249, "title": "Dynamic quantized nn.LSTM module", "time": "2019-05-07T22:41:20Z", "body": "Previously we only had a Python wrapper for `torch.quantized_lstm_cell`. We had the op `torch.quantized_lstm`, but it didn't have a wrapper. This makes the wrapper"}
{"number": 20250, "title": "Build error on Win for THD: Cannot open gflags/gflags.h", "time": "2019-05-07T23:16:45Z", "body": "## Bug\r\n\r\nGFlags are correctly detected by CMake (listed in CMakeCache and also found `-I` to gflags in other caffe2 compiling command).\r\n\r\nBut there're a few files from THD referencing gflags via c10 without proper include dir dependency.\r\n\r\n```\r\nFAILED: caffe2/torch/lib/THD/CMakeFiles/THD.dir/base/data_channels/DataChannelMPI.cpp.obj\r\nC:\\PROGRA~2\\MICROS~4\\2019\\ENTERP~1\\VC\\Tools\\MSVC\\1420~1.275\\bin\\Hostx64\\x64\\cl.exe   /TP -DONNX_NAMESPACE=onnx_c2 -DTH_BLAS_MKL -DUSE_MSC_ATOMICS=1 -DWITH_MPI=1 -D_CRT_SECURE_NO_DEPRECATE=1 -D_OPENMP_NOFORCE_MANIFEST -D_THD_CORE=1 -I..\\aten\\src -I. -I..\\ -I\"C:\\Program Files\\protobuf\\include\" -I..\\cmake\\..\\third_party\\benchmark\\include -Icaffe2\\contrib\\aten -I..\\third_party\\onnx -Ithird_party\\onnx -I..\\third_party\\foxi -Ithird_party\\foxi -I..\\torch\\lib\\THD\\.. -Iaten\\src -Icaffe2\\aten\\src -I\"C:\\Program Files (x86)\\IntelSWTools\\compilers_and_libraries_2019.2.190\\windows\\mpi\\intel64\\include\" -I..\\cmake\\..\\third_party\\googletest\\googlemock\\include -I..\\cmake\\..\\third_party\\googletest\\googletest\\include -I\"C:\\Program Files (x86)\\IntelSWTools\\compilers_and_libraries\\windows\\mkl\\include\" -I..\\cmake\\..\\third_party\\eigen -I\"C:\\Program Files\\Python37\\include\" -I\"C:\\Program Files\\Python37\\lib\\site-packages\\numpy\\core\\include\" -I\"C:\\Program Files\\pybind11\\include\" -I\\opt\\rocm\\hip\\include -I\\include /EHsc /FS /GL /MP /Z7 /arch:AVX2 /DGFLAGS_IS_A_DLL=1 /DPROTOBUF_USE_DLLS /DWIN32 /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /D_SILENCE_TR1_NAMESPACE_DEPRECATION_WARNING /w -openmp /MP /bigobj -DHAVE_AVX_CPU_DEFINITION -DHAVE_AVX2_CPU_DEFINITION -std=c++11 /MD /O2 /Ob2  /MP /bigobj   -DUSE_GCC_GET_CPUID -DUSE_AVX -DUSE_AVX2 -DTH_HAVE_THREAD /showIncludes /Focaffe2\\torch\\lib\\THD\\CMakeFiles\\THD.dir\\base\\data_channels\\DataChannelMPI.cpp.obj /Fdcaffe2\\torch\\lib\\THD\\CMakeFiles\\THD.dir\\THD.pdb /FS -c ..\\torch\\lib\\THD\\base\\data_channels\\DataChannelMPI.cpp\r\nMicrosoft (R) C/C++ Optimizing Compiler Version 19.20.27508.1 for x64\r\nCopyright (C) Microsoft Corporation.  All rights reserved.\r\n\r\ncl : Command line warning D9002 : ignoring unknown option '-std=c++11'\r\nc10/util/Flags.h(78): fatal error C1083: Cannot open include file: 'gflags/gflags.h': No such file or directory\r\nninja: build stopped: subcommand failed.\r\n```\r\n\r\n## Environment\r\n\r\n- PyTorch Version (e.g., 1.0): master\r\n - OS (e.g., Linux): Win10\r\n - How you installed PyTorch (`conda`, `pip`, source): source\r\n - Build command you used (if compiling from source): cmake+ninja+vs2019\r\n - Python version: 3.7\r\n"}
{"number": 20251, "title": "[fix] Rebase conflict fix for isFusableDevice", "time": "2019-05-08T00:04:46Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20251 [fix] Rebase conflict fix for isFusableDevice**\n\nDifferential Revision: [D15262850](https://our.internmc.facebook.com/intern/diff/D15262850)"}
{"number": 20252, "title": "[pt1][quant][qat] CUDA implementation of fakequant", "time": "2019-05-08T00:24:16Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20252 [pt1][quant][qat] CUDA implementation of fakequant**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15243386/)\n\nAdd CUDA implementation for fakequant op for quantization aware training.\n\nDifferential Revision: [D15243386](https://our.internmc.facebook.com/intern/diff/D15243386/)"}
{"number": 20253, "title": "Update ROCm 2.4", "time": "2019-05-08T00:24:23Z", "body": "@xw285cornell "}
{"number": 20254, "title": "RuntimeError: Not compiled with GPU support ( win10 )", "time": "2019-05-08T00:26:18Z", "body": "## ‚ùì Questions and Help\r\n\r\nHey , guys, I`m a beginner.I need a help.\r\n\r\nWhen i tried maskrcnn-benchmark, at last i encountered the issues as the title: \r\n\r\nRuntimeError: Not compiled with GPU support (nms at c:\\users\\gs63vr\\train\\maskrcnn-benchmark\\maskrcnn_benchmark\\csrc\\nms.h:22)\r\n(no backtrace available)\r\n[ WARN:0] terminating async callback\r\n\r\nThe method i used is Step-by-step installation \r\n(Anaconda3‚ÜíCreating Virtual Environment ‚Üíipython‚Üí maskrcnn_benchmark and coco api dependencies ‚Üí conda install pytorch torchvision cudatoolkit=9.0 -c pytorch ‚Üí install pycocotools ‚Üí apex ‚Üí download maskrcnn-benchmark ‚Üí revise SigmodiFocalLoss_cuda.cu ‚Üí instal  maskrcnn-benchmark , run demo test\r\n )\r\nand my enviroment is \r\nwin10.\r\nPytorch 1.1 \r\npy3.6 cuda90 cudnn7.1\r\ntorchvision 0.2.2\r\n\r\nThe whole information as below:\r\n\r\nTraceback (most recent call last):\r\n  File \"webcam.py\", line 80, in <module>\r\n    main()\r\n  File \"webcam.py\", line 71, in main\r\n    composite = coco_demo.run_on_opencv_image(img)\r\n  File \"C:\\Users\\GS63VR\\train\\maskrcnn-benchmark\\demo\\predictor.py\", line 172, in run_on_opencv_image\r\n    predictions = self.compute_prediction(image)\r\n  File \"C:\\Users\\GS63VR\\train\\maskrcnn-benchmark\\demo\\predictor.py\", line 205, in compute_prediction\r\n    predictions = self.model(image_list)\r\n  File \"C:\\Users\\GS63VR\\Anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 493, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"c:\\users\\gs63vr\\train\\maskrcnn-benchmark\\maskrcnn_benchmark\\modeling\\detector\\generalized_rcnn.py\", line 50, in forward\r\n    proposals, proposal_losses = self.rpn(images, features, targets)\r\n  File \"C:\\Users\\GS63VR\\Anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 493, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"c:\\users\\gs63vr\\train\\maskrcnn-benchmark\\maskrcnn_benchmark\\modeling\\rpn\\rpn.py\", line 161, in forward\r\n    return self._forward_test(anchors, objectness, rpn_box_regression)\r\n  File \"c:\\users\\gs63vr\\train\\maskrcnn-benchmark\\maskrcnn_benchmark\\modeling\\rpn\\rpn.py\", line 187, in _forward_test\r\n    boxes = self.box_selector_test(anchors, objectness, rpn_box_regression)\r\n  File \"C:\\Users\\GS63VR\\Anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 493, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"c:\\users\\gs63vr\\train\\maskrcnn-benchmark\\maskrcnn_benchmark\\modeling\\rpn\\inference.py\", line 140, in forward\r\n    sampled_boxes.append(self.forward_for_single_feature_map(a, o, b))\r\n  File \"c:\\users\\gs63vr\\train\\maskrcnn-benchmark\\maskrcnn_benchmark\\modeling\\rpn\\inference.py\", line 120, in forward_for_single_feature_map\r\n    score_field=\"objectness\",\r\n  File \"c:\\users\\gs63vr\\train\\maskrcnn-benchmark\\maskrcnn_benchmark\\structures\\boxlist_ops.py\", line 27, in boxlist_nms\r\n    keep = _box_nms(boxes, score, nms_thresh)\r\n  File \"C:\\Users\\GS63VR\\Anaconda3\\envs\\test\\lib\\site-packages\\apex-0.1-py3.6.egg\\apex\\amp\\amp.py\", line 22, in wrapper\r\nRuntimeError: Not compiled with GPU support (nms at c:\\users\\gs63vr\\train\\maskrcnn-benchmark\\maskrcnn_benchmark\\csrc\\nms.h:22)\r\n(no backtrace available)\r\n[ WARN:0] terminating async callback\r\n\r\n"}
{"number": 20255, "title": "[ONNX] Support exporting tensor factories from scripting", "time": "2019-05-08T00:41:09Z", "body": ""}
{"number": 20256, "title": "[ONNX] Fix bug in exporting node with multiple outputs by scripting", "time": "2019-05-08T00:51:54Z", "body": ""}
{"number": 20257, "title": "RuntimeError: ONNX export failed: Couldn't export operator aten::upsample_trilinear3d", "time": "2019-05-08T02:10:07Z", "body": "Currently I am facing the error\r\n\r\n`\r\nRuntimeError: ONNX export failed: Couldn't export operator aten::upsample_trilinear3d\r\n`\r\n\r\nAccording to the folks at ONNX, this is a bug in PyTorch. https://github.com/onnx/onnx/issues/1999\r\n\r\n\n\ncc @suo @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof"}
{"number": 20258, "title": "[WIP][RFC] Edit nn.Linear to take arbitrary in and out feature dimensions", "time": "2019-05-08T02:16:32Z", "body": "### Current Behavior\r\n\r\nCurrently, `nn.Linear` takes two integer parameters `in_features` and `out_features` and creates a weight parameter of size `[out_features, in_features]`, and bias parameter of size `[out_features]`. This API is a little cumbersome if users would like to run higher-dimensional features through a linear layer. Say I have 2D input features of size `[2, 3]`, and provide an input batch of size `[4, 5, 2, 3]`. To use the existing linear layer, I will have to first reshape the the input into size `[4, 5, 6]`, and pass `6` as `in_features` arg for `nn.Linear`. \r\n\r\n\r\n### Proposed Behavior\r\n\r\nIt would be nice if `nn.Linear` can handle arbitrary feature dimensions. For example, if I have input feature size `[2, 3]`, and output feature size `4`, I should be able to do the following:\r\n\r\n```python\r\nm = Linear(in_features=[2, 3], out_features=4)\r\ninput = torch.rand(4, 5, 2, 3)\r\noutput = m(input) # get output of size [4, 5, 4]\r\n```\r\n\r\nLet `in_features=[in_1, in_2, ..., in_x]`, `out_features=[out_1, out_2, ..., out_y]`, and `input.size() == [d1, d2, ..., dz]`. The output dimensions should be `[d_1, ..., d_{z-x}, out_1, ..., out_y]`. \r\n\r\nIt is allowed to set `in_features` and/or `out_features` to `[]` or `()`, and its behavior will stay consistent with `torch.matmul`.\r\n\r\n\r\n### Implementation Alternatives\r\n\r\n#### Option 1 [implemented in this PR]\r\n\r\nWeights and bias dimensions respect `in_features` and `out_features` provided by users, where `weight.size() == [out_1, out_2, ..., out_y, in_1, in_2, ..., in_x]`  and `bias.size() == out_features`. In every `forward()` iteration, `weight` is reshaped to a 2D tensor of size `[prod(out_features), prod(in_features)]`, and `bias` of size `[prod(in_features)]`. Then, inputs are reshaped to `[d_1, ..., d_{z-x}, prod(in_features)]`. `F.linear` function creates an intermediate output of size `[d_1, ..., d_{z-x}, prod(out_features)]`, and are reshaped to `[d_1, ..., d_z, out_1, ..., out_y]`. \r\n\r\nThe reason for not keeping weight and bias views as member var is to avoid showing them in `Linear.parameters()` \r\n\r\nPro: Dimensions shown in `Linear.parameters()` will match users' expectation.\r\nCon: Have to create multiple views in every `forward`.\r\n\r\n#### Option 2\r\n\r\nIn constructor, directly create weight of size `[prod(out_features), prod(in_features)]` and bias of size `[prod(out_features)]`. And only reshape input and output in every `forward()` iteration. \r\n\r\nPro: Avoid reshaping `weight` and `bias` in `forward()`\r\nCon: Dimensions shown in `Linear.parameters()` won't match `in_features` and `out_features`\r\n\r\n#### Option 3\r\nImplement a new `linear` native function which address the dimensions internal, and modify both Python and C++ frontend to call it.\r\n\r\nWill edit docs and add comments when we have consensus.\r\n\r\n@gchanan @vishwakftw @yf225 \r\n\r\n### Benchmark [Python API only]\r\n\r\nbefore\r\n\r\n```python\r\nIn [2]: l = torch.nn.Linear(10, 10)\r\n\r\nIn [3]: input = torch.rand(2, 10)\r\n\r\nIn [4]: %timeit l(input)\r\n26.2 ¬µs ¬± 5.17 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10000 loops each)\r\n\r\nIn [5]: %timeit l(input)\r\n25.1 ¬µs ¬± 1.57 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10000 loops each)\r\n\r\nIn [6]: %timeit l(input)\r\n25.6 ¬µs ¬± 1.93 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10000 loops each)\r\n```\r\n\r\nafter\r\n\r\n```python\r\nIn [2]: l = torch.nn.Linear(10, 10)\r\n\r\nIn [4]: input = torch.rand(2, 10)\r\n\r\nIn [5]: %timeit l(input)\r\n24.4 ¬µs ¬± 383 ns per loop (mean ¬± std. dev. of 7 runs, 10000 loops each)\r\n\r\nIn [6]: %timeit l(input)\r\n26.2 ¬µs ¬± 4.25 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10000 loops each)\r\n\r\nIn [7]: %timeit l(input)\r\n24.8 ¬µs ¬± 2.37 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10000 loops each)\r\n```"}
{"number": 20259, "title": "Disable warning line for machine checker by default in caffe2", "time": "2019-05-08T02:59:55Z", "body": "Disable warning line for machine checker by default  in caffe2\r\n\r\n"}
{"number": 20260, "title": "tensor.cuda() method errors out in multi-thread dataloader", "time": "2019-05-08T03:03:45Z", "body": "## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nI was trying to use GPU for some heavy-lifting pro-processing work so I wrote my own transform function.\r\nThe tranform function keeps throwing 'RuntimeError: CUDA error: initialization error' whenever I use num_worker > 0.\r\nThe error happens at the line when I try to send tensor to GPU using tensor.cuda()\r\n\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Write a customized transform function.\r\n2. Use .cuda() method within.\r\n3. Use num_worker >0.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\npytorch == 1.1.0 stable\r\ntorchvision == 0.2.2.post3\r\nCUDA == 10.0.130\r\ncuDNN == 7501\r\n\r\n\r\n"}
{"number": 20261, "title": "Set HCC_LAZYINIT=ON in caffe2 CI", "time": "2019-05-08T04:09:30Z", "body": "Only meant for testing at this point, will consult amd folks before proceeding."}
{"number": 20262, "title": "üëå", "time": "2019-05-08T05:16:15Z", "body": ""}
{"number": 20263, "title": "C++ Can only index with tensors that are scalars (zero-dim)", "time": "2019-05-08T05:24:29Z", "body": "I got an error ```Can only index with tensors that are scalars (zero-dim) (operator [] at C:\\w\\1\\s\\windows\\pytorch\\aten\\src\\ATen/TensorOperators.h:62)(no backtrace available)```\r\nwhile the python one ran normal:\r\n```python\r\ntorch.Size([2, 2, 1])\r\ntensor([[[[[1],\r\n           [1]]],\r\n\r\n         [[[1],\r\n           [1]]]],\r\n\r\n        [[[[1],\r\n           [1]]],\r\n\r\n         [[[1],\r\n           [1]]]]])\r\n```\r\ncode is shown below\r\n```c++\r\n#include <torch/torch.h>\r\n#include <iostream>\r\nusing namespace std;\r\nint main()\r\n{\r\n\ttry\r\n\t{\r\n\t\ttorch::Tensor data = torch::eye(3);\r\n\t\ttorch::Tensor index = torch::eye(3);\r\n\t\tdata[index];\r\n\t}\r\n\tcatch (const std::exception&e)\r\n\t{\r\n\t\tcout << e.what();\r\n\t}\r\n\tgetchar();\r\n\treturn 0;\r\n}\r\n```\r\n\r\n```python \r\nimport torch\r\nimport torch.nn as nn\r\ndata=torch.tensor([[[1],[1]],[[1],[1]]]);\r\nindex=torch.tensor([[[1],[1]],[[1],[1]]]);\r\nprint(data.shape)\r\nprint(data[index])\r\n```\r\n\r\nis there anything i didn't notice about the C++ frontend?or the interface is a little different from the python one."}
{"number": 20264, "title": "Remove static variable definitions to support libtorch dlls delayed loading", "time": "2019-05-08T06:37:34Z", "body": "## üöÄ Feature\r\n\r\nRemove static variable definitions to support delayed loading libtorch dlls.\r\n\r\n## Motivation\r\nSome dlls required by libtorch are quite big. So when embeding libtorch into a product, we want them to be optional if the user doesn't use any functionalities in libtorch. Users need to have those dlls only when they need libtorch. To achieve the goal, we usually use delayed loading feature(/delayed switch with MSBuild on Win, and there are equivalents on Linux too). \r\n\r\nHowever, source code of libtorch contains some static definations, e.g.\r\n\r\n```C++\r\n#include <torch/csrc/jit/import.h>\r\n...\r\nstatic script::ExtraFilesMap default_extra_files;\r\n...\r\n```\r\n\r\nThis will stop the delayed loading to work, even when the user doesn't call any functions in torch explicitly.\r\n\r\nIs it possible to find alternatives to remove those static/global variable definitions?\r\n"}
{"number": 20265, "title": "Fix missing files for upload jobs", "time": "2019-05-08T07:52:54Z", "body": "The earlier fix to extract scripts missed an attach_workspace which was used to make the built binaries available to the nightly build upload jobs.\r\n\r\n"}
{"number": 20266, "title": "Improve bmm performance on CPU by applying TensorAccessor", "time": "2019-05-08T08:50:43Z", "body": "Currently `bmm()` has very heavy performance overhead on CPU due to construction/deconstruction of `TensorImpl`. Applying `TensorAccessor` when indexing tensor data can greatly improve the performance.\r\n\r\nI tested this on `fairseq` Transformer model. Results on Xeon 6148 (20*2 cores @2.5GHz) indicate this PR improves Transformer training performance by approximately **10%** (seconds per iteration reduced from **3.60** to **3.21**). Considering the fact that `bmm()` takes only **14%** of the total time, 10% overall improvement indicates `bmm()` itself improves by roughly **3x**.\r\n\r\nBefore:\r\n```\r\n| epoch 001:   0%| | 43/25337 [02:34<25:17:11,  3.60s/it, loss=16.179, nll_loss=16.137, ppl=72045.59, wps=1320, ups=0, wpb=4758.767, bsz=136.558, num_updates=43, lr=6.45e-06, gnorm=6.88\r\n```\r\n\r\nAfter:\r\n```\r\n| epoch 001:   0%| | 23/25337 [01:13<22:32:48,  3.21s/it, loss=17.072, nll_loss=17.068, ppl=137419.42, wps=1478, ups=0, wpb=4746.870, bsz=128.348, num_updates=23, lr=3.45e-06, gnorm=10.\r\n```"}
{"number": 20267, "title": "Refine CosineAnnealingWarmRestarts doc for issue #20028", "time": "2019-05-08T10:10:33Z", "body": "Fixes #20028"}
{"number": 20268, "title": "[Caffe2] Convert caffe to caffe2 with solverstate", "time": "2019-05-08T11:35:21Z", "body": "Can I convert  a caffemodel and a solverstate file to caffe2 training file?\r\n\r\nI have used below code to convert caffemodel, but it only convert the weight.\r\nI want to get the momentum information which is in solverstate.\r\n\r\n`python -m caffe2.python.caffe_translator XXX.prototxt XXX.caffemodel`\r\n"}
{"number": 20269, "title": "[WIP] Optimize after generating AutoDiff code from Symbolic script", "time": "2019-05-08T12:26:20Z", "body": "Our symbolic script is large enough to deserve optimization after it is applied. In particular, obsolete if branches from constant propagation seem to be a thing.\r\n\r\nThe grad_sum_to_size elimination (#18697) will benefit from the implicit specialization.\r\n"}
{"number": 20270, "title": "Fix in file position logic: file descriptor and Python-side handle", "time": "2019-05-08T12:28:55Z", "body": "This addresses #18436 \r\n\r\nThe logic replicates the essence of closing file descriptors in numpy:\r\nhttps://github.com/numpy/numpy/blob/bf20e3034085716c4559ec4bf31b23b6016f266c/numpy/core/include/numpy/npy_3kcompat.h#L278\r\n\r\nThis stores the position of the file descriptor before resetting it to the Python handle offset, then resets to the original position before exit. The Python-side handle is then updated to reflect the new position. Also added somewhat more demanding tests to cover this."}
{"number": 20271, "title": "Official instructions for how to build libtorch don't have same structure as prebuilt binaries", "time": "2019-05-08T13:02:51Z", "body": "On Slack, Geoffrey Yu asked:\r\n\r\n> Are there instructions for building libtorch from source? I feel like I'm missing something since I've tried building with `tools/build_libtorch.py`. However the build output doesn't seem to have the same structure as the prebuilt libtorch that you can download on pytorch.org\r\n\r\n@pjh5 responded: \"If you're curious, here's exactly what builds the libtorches https://github.com/pytorch/builder/blob/master/manywheel/build_common.sh#L120 . It's mostly tools/build_libtorch.py but also copies some header files from a wheel file\"\r\n\r\nThis is not mentioned at all in the \"how to build libtorch\" documentation: https://github.com/pytorch/pytorch/blob/master/docs/libtorch.rst Normally we give build instructions in README but there are no libtorch build instructions in the README. Additionally, the C++ API docs https://pytorch.org/cppdocs/ don't explain how to build from source.\r\n\r\nSome more users being confused about the matter:\r\n* https://discuss.pytorch.org/t/building-libtorch-c-distribution-from-source/27519/2\r\n* https://github.com/pytorch/pytorch/issues/20156\r\n\r\n"}
{"number": 20272, "title": "Redundantly saving sizes of SavedVariables in autograd Function", "time": "2019-05-08T14:06:56Z", "body": "I was reading some autograd code and I noticed this:\r\n\r\n```\r\nstruct MmBackward : public TraceableFunction {\r\n  using TraceableFunction::TraceableFunction;\r\n  variable_list apply(variable_list&& grads) override;\r\n  std::string name() const override { return \"MmBackward\"; }\r\n  void release_variables() override {\r\n    self_.reset_data();\r\n    self_.reset_grad_function();\r\n    mat2_.reset_data();\r\n    mat2_.reset_grad_function();\r\n  }\r\n\r\n  SavedVariable self_;\r\n  std::vector<int64_t> mat2_sizes;\r\n  SavedVariable mat2_;\r\n\r\n};\r\n```\r\n\r\nThe `mat2_sizes` here is pointless; we can trivially reconstruct it from `mat2_`. So it's just a waste of space.\r\n\r\nMarked as low priority as I don't think is actually affecting anything."}
{"number": 20273, "title": "Linker errors when linking statically (avx perfkernels) [Caffe2]", "time": "2019-05-08T14:26:39Z", "body": "## üêõ Bug\r\n\r\nI've been trying to build the caffe2 v1.1.0 release and link it statically with our application, but I'm getting a lot of undefined symbols related to avx functions.\r\n\r\n## To Reproduce\r\n\r\nI've built caffe2 142c973f4179e768164cd578951489e89021b29c using the following flags:\r\n```bash\r\n$ cd pytorch\r\n$ mkdir build\r\n$ cd build\r\n$ cmake .. -DCMAKE_INSTALL_PREFIX=/home/giachero/runtimes/caffe2-v1.1.0-avx -DBUILD_PYTHON=OFF -DUSE_CUDA=OFF -DUSE_NATIVE_ARCH=ON -DUSE_GFLAGS=OFF -DUSE_GLOG=OFF -DUSE_GLOO=OFF -DBUILD_SHARED_LIBS=OFF -DBUILD_TEST=OFF -DBUILD_BINARY=OFF -DUSE_LMDB=OFF -DUSE_LEVELDB=OFF -DUSE_MPI=OFF -DUSE_OPENMP=OFF -DUSE_OPENCV=OFF -DBUILD_ATEN_MOBILE=ON -DUSE_NNPACK=OFF -DCAFFE2_DISABLE_NUMA=1 -DUSE_QNNPACK=OFF\r\n```\r\nMinimal application code:\r\n```c++\r\n#include <caffe2/core/init.h>\r\n\r\nint main(int argc, char *argv[])\r\n{\r\n    caffe2::GlobalInit();\r\n    return 0;\r\n}\r\n```\r\nBuild flags:\r\n```bash\r\n$ g++ main.cpp -I /home/giachero/runtimes/caffe2-v1.1.0-avx/include/ -L /home/giachero/runtimes/caffe2-v1.1.0-avx/lib/ -Wl,--whole-archive -lcaffe2 -Wl,--no-whole-archive -lcaffe2_protos -lcpuinfo -lc10 -lclog -lonnx -lprotobuf -lonnx_proto -lonnxifi_loader -lm -lpthread -ldl -o test\r\n```\r\nOutput:\r\n```\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int32_t_float_float_false(long, long, long, long, float const*, int const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x2caa): undefined reference to `caffe2::EmbeddingLookup_int32_t_float_float_false__avx2_fma(long, long, long, long, float const*, int const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int64_t_float_float_false(long, long, long, long, float const*, long const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x2e3a): undefined reference to `caffe2::EmbeddingLookup_int64_t_float_float_false__avx2_fma(long, long, long, long, float const*, long const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int32_t_half_float_false(long, long, long, long, c10::Half const*, int const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x2fca): undefined reference to `caffe2::EmbeddingLookup_int32_t_half_float_false__avx2_fma(long, long, long, long, c10::Half const*, int const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int64_t_half_float_false(long, long, long, long, c10::Half const*, long const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x315a): undefined reference to `caffe2::EmbeddingLookup_int64_t_half_float_false__avx2_fma(long, long, long, long, c10::Half const*, long const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int32_t_uint8_t_float_false(long, long, long, long, unsigned char const*, int const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x32ec): undefined reference to `caffe2::EmbeddingLookup_int32_t_uint8_t_float_false__avx2_fma(long, long, long, long, unsigned char const*, int const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int64_t_uint8_t_float_false(long, long, long, long, unsigned char const*, long const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x347c): undefined reference to `caffe2::EmbeddingLookup_int64_t_uint8_t_float_false__avx2_fma(long, long, long, long, unsigned char const*, long const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int32_t_float_float_true(long, long, long, long, float const*, int const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x360a): undefined reference to `caffe2::EmbeddingLookup_int32_t_float_float_true__avx2_fma(long, long, long, long, float const*, int const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int64_t_float_float_true(long, long, long, long, float const*, long const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x379a): undefined reference to `caffe2::EmbeddingLookup_int64_t_float_float_true__avx2_fma(long, long, long, long, float const*, long const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int32_t_half_float_true(long, long, long, long, c10::Half const*, int const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x392a): undefined reference to `caffe2::EmbeddingLookup_int32_t_half_float_true__avx2_fma(long, long, long, long, c10::Half const*, int const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int64_t_half_float_true(long, long, long, long, c10::Half const*, long const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x3aba): undefined reference to `caffe2::EmbeddingLookup_int64_t_half_float_true__avx2_fma(long, long, long, long, c10::Half const*, long const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int32_t_uint8_t_float_true(long, long, long, long, unsigned char const*, int const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x3c4c): undefined reference to `caffe2::EmbeddingLookup_int32_t_uint8_t_float_true__avx2_fma(long, long, long, long, unsigned char const*, int const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int64_t_uint8_t_float_true(long, long, long, long, unsigned char const*, long const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x3ddc): undefined reference to `caffe2::EmbeddingLookup_int64_t_uint8_t_float_true__avx2_fma(long, long, long, long, unsigned char const*, long const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(fused_8bit_rowwise_embedding_lookup.cc.o): In function `caffe2::Fused8BitRowwiseEmbeddingLookup_int64_t_uint8_t_float(long, long, long, long, unsigned char const*, long const*, int const*, float const*, bool, float*)':\r\nfused_8bit_rowwise_embedding_lookup.cc:(.text+0x858): undefined reference to `caffe2::Fused8BitRowwiseEmbeddingLookup_int64_t_uint8_t_float_false__avx2_fma(long, long, long, long, unsigned char const*, long const*, int const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(fused_8bit_rowwise_embedding_lookup.cc.o): In function `caffe2::Fused8BitRowwiseEmbeddingLookup_int32_t_uint8_t_float(long, long, long, long, unsigned char const*, int const*, int const*, float const*, bool, float*)':\r\nfused_8bit_rowwise_embedding_lookup.cc:(.text+0x958): undefined reference to `caffe2::Fused8BitRowwiseEmbeddingLookup_int32_t_uint8_t_float_false__avx2_fma(long, long, long, long, unsigned char const*, int const*, int const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(typed_axpy.cc.o): In function `void caffe2::TypedAxpy<c10::Half, float>(int, float, c10::Half const*, float*)':\r\ntyped_axpy.cc:(.text+0x10d): undefined reference to `caffe2::TypedAxpyHalffloat__avx2_fma(int, float, c10::Half const*, float*)'\r\ntyped_axpy.cc:(.text+0x131): undefined reference to `caffe2::TypedAxpyHalffloat__avx_f16c(int, float, c10::Half const*, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(typed_axpy.cc.o): In function `void caffe2::TypedAxpy<unsigned char, float>(int, float, unsigned char const*, float*)':\r\ntyped_axpy.cc:(.text+0x221): undefined reference to `caffe2::TypedAxpy_uint8_float__avx2_fma(int, float, unsigned char const*, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(adagrad.cc.o): In function `caffe2::adagrad_update(int, float const*, float const*, float const*, float*, float*, float, float, float)':\r\nadagrad.cc:(.text+0x621): undefined reference to `caffe2::adagrad_update__avx_f16c(int, float const*, float const*, float const*, float*, float*, float, float, float)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(adagrad.cc.o): In function `caffe2::adagrad_update_prefetch(int, float const*, float const*, float const*, float const*, float const*, float*, float*, float*, float*, float, float)':\r\nadagrad.cc:(.text+0x789): undefined reference to `caffe2::adagrad_update_prefetch__avx_f16c(int, float const*, float const*, float const*, float const*, float const*, float*, float*, float*, float*, float, float)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(adagrad.cc.o): In function `caffe2::adagrad_fp16_update_prefetch(int, c10::Half const*, c10::Half const*, float const*, c10::Half const*, c10::Half const*, c10::Half*, c10::Half*, c10::Half*, c10::Half*, float, float)':\r\nadagrad.cc:(.text+0xac9): undefined reference to `caffe2::adagrad_fp16_update_prefetch__avx_f16c(int, c10::Half const*, c10::Half const*, float const*, c10::Half const*, c10::Half const*, c10::Half*, c10::Half*, c10::Half*, c10::Half*, float, float)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(adagrad.cc.o): In function `caffe2::rowwise_adagrad_update(int, float*, float*, float const*, float*, float*, float, float)':\r\nadagrad.cc:(.text+0xd36): undefined reference to `caffe2::rowwise_adagrad_update__avx_f16c(int, float*, float*, float const*, float*, float*, float, float)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(adagrad.cc.o): In function `int caffe2::sparse_adagrad<int>(int, int, unsigned long, float const*, float const*, float const*, int const*, float*, float*, float, float)':\r\nadagrad.cc:(.text+0xfd6): undefined reference to `caffe2::sparse_adagrad_int32_t__avx_f16c(int, int, unsigned long, float const*, float const*, float const*, int const*, float*, float*, float, float)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(adagrad.cc.o): In function `int caffe2::sparse_adagrad<long>(int, int, unsigned long, float const*, float const*, float const*, long const*, float*, float*, float, float)':\r\nadagrad.cc:(.text+0x1276): undefined reference to `caffe2::sparse_adagrad_int64_t__avx_f16c(int, int, unsigned long, float const*, float const*, float const*, long const*, float*, float*, float, float)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(math_cpu_base.cc.o): In function `caffe2::math::quantize_and_compress(float const*, unsigned char*, unsigned long, unsigned long, bool, float const*)':\r\nmath_cpu_base.cc:(.text+0x31d): undefined reference to `caffe2::math::quantize_and_compress__avx2(float const*, unsigned char*, unsigned long, unsigned long, bool, float const*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(math_cpu_base.cc.o): In function `caffe2::math::decompress_and_dequantize(unsigned char const*, float*, unsigned long)':\r\nmath_cpu_base.cc:(.text+0x455): undefined reference to `caffe2::math::decompress_and_dequantize__avx2(unsigned char const*, float*, unsigned long)'\r\ncollect2: error: ld returned 1 exit status\r\n```\r\n\r\n## Environment\r\n\r\n - Caffe2 version: 142c973f4179e768164cd578951489e89021b29c (v1.1.0)\r\n - OS: Ubuntu 18.04 x86_64\r\n - Build command you used (if compiling from source): cmake .. -DCMAKE_INSTALL_PREFIX=/home/giachero/runtimes/caffe2-v1.1.0-avx -DBUILD_PYTHON=OFF -DUSE_CUDA=OFF -DUSE_NATIVE_ARCH=ON -DUSE_GFLAGS=OFF -DUSE_GLOG=OFF -DUSE_GLOO=OFF -DBUILD_SHARED_LIBS=OFF -DBUILD_TEST=OFF -DBUILD_BINARY=OFF -DUSE_LMDB=OFF -DUSE_LEVELDB=OFF -DUSE_MPI=OFF -DUSE_OPENMP=OFF -DUSE_OPENCV=OFF -DBUILD_ATEN_MOBILE=ON -DUSE_NNPACK=OFF -DCAFFE2_DISABLE_NUMA=1 -DUSE_QNNPACK=OFF\r\n\r\n"}
{"number": 20274, "title": "[WIP] [FR] add Sequential.append, .extend , .insert & .pop", "time": "2019-05-08T14:41:21Z", "body": "adding features as required from : #20117 \r\ncc: @SsnL @apaszke \r\n\r\n"}
{"number": 20275, "title": "Memory (CPU/sys) leak with custom batch norm layer", "time": "2019-05-08T14:42:01Z", "body": "## üêõ Bug\r\n\r\nUnreasonable memory increase (probably memory leak) while training a simple CNN with a custom mean-only batch-norm layer on GPU. This is probably related to the module buffer, since removing the buffer stops the problem and training on CPU also seems to work fine.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Evaluate script below while monitoring CPU memory usage\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nclass MeanOnlyBatchNorm(nn.Module):\r\n    def __init__(self, module, momentum=0.1):\r\n        super(MeanOnlyBatchNorm, self).__init__()\r\n        if isinstance(module, nn.Linear):\r\n            self.num_features = module.out_features\r\n        else:\r\n            self.num_features = module.out_channels\r\n        self.do_init = False\r\n        self.momentum = momentum\r\n        self.weight = nn.Parameter(torch.Tensor(self.num_features))\r\n        self.bias = nn.Parameter(torch.Tensor(self.num_features))\r\n        self.register_buffer('running_mean', torch.zeros(self.num_features))\r\n        self.reset_parameters()\r\n\r\n    def reset_running_stats(self):\r\n        self.running_mean.zero_()\r\n\r\n    def reset_parameters(self):\r\n        self.reset_running_stats()\r\n        nn.init.ones_(self.weight)\r\n        nn.init.zeros_(self.bias)\r\n        self.do_init = False\r\n\r\n    def forward(self, x):\r\n        exp_dim = (None, slice(None)) + tuple(None for _ in range(2, x.dim()))\r\n        if self.training:\r\n            red_dim = (0,) + tuple(range(2, x.dim()))\r\n            mean = x.mean(red_dim)\r\n            activation = x - mean[exp_dim]\r\n            self.running_mean = (self.momentum * mean +\r\n                                 (1 - self.momentum) * self.running_mean)\r\n        else:\r\n            activation = x - self.running_mean[exp_dim]\r\n\r\n        return activation + self.bias[exp_dim]\r\n\r\n    def extra_repr(self):\r\n        return '{num_features}, eps={eps}, momentum={momentum}, affine=True, ' \\\r\n               'track_running_stats=True'.format(**self.__dict__)\r\n\r\n\r\nn_class = 10\r\nbatch_size = 100\r\nn_batches = 2000\r\ninput_size = (3, 32, 32)\r\ndevice = torch.device('cuda')\r\nc1 = nn.Conv2d(3, 1000, kernel_size=3, padding=1, stride=4)\r\nbn1 = MeanOnlyBatchNorm(c1)\r\nact1 = nn.ReLU(True)\r\npool1 = nn.AdaptiveAvgPool2d(1)\r\nn2 = nn.Conv2d(1000, n_class, kernel_size=1)\r\nbn2 = MeanOnlyBatchNorm(n2)\r\nm = nn.Sequential(c1, bn1, act1, pool1, n2, bn2).to(device)\r\noptimizer = torch.optim.Adam(m.parameters())\r\nfor _ in range(n_batches):\r\n    t = torch.randn(*((batch_size,) + input_size), device=device)\r\n    x = m(t).view(batch_size, -1)\r\n    loss = torch.nn.functional.cross_entropy(\r\n        x, torch.randint(n_class, (batch_size,), dtype=torch.long,\r\n                         device=device))\r\n    optimizer.zero_grad()\r\n    loss.backward()\r\n    optimizer.step()\r\n```\r\n\r\nEDIT: was able to reproduce with a much smaller example, simplified the script above.\r\n\r\n## Expected behavior\r\n\r\nThere should be no memory leak, just like when training on CPU, or using the `_BatchNorm` modules.\r\n\r\n## Environment\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.148\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 410.79\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.3\r\n[pip] torch==1.1.0\r\n[pip] torchvision==0.2.2\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.3                      199  \r\n[conda] mkl_fft                   1.0.12           py36ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py36hd81dba3_0  \r\n[conda] pytorch                   1.1.0           py3.6_cuda10.0.130_cudnn7.5.1_0    pytorch\r\n[conda] torchvision               0.2.2                      py_3    pytorch\r\n\r\n## Additional context\r\n\r\nTo create the `MeanOnlyBatchNorm` layer, I followed more or less what is done here: https://github.com/ptrblck/pytorch_misc/blob/master/batch_norm_manual.py and of course the `_BatchNorm` module.\r\n"}
{"number": 20276, "title": "[ROCm] enable ROCm 2.4 for the CI docker images", "time": "2019-05-08T15:22:16Z", "body": "With ROCm 2.4, the yum package name for hip-thrust is consistent with the deb one.\r\n\r\n"}
{"number": 20277, "title": "[DO NOT MERGE] Keep .tensor_data() in VariableType unpack()", "time": "2019-05-08T15:35:01Z", "body": ""}
{"number": 20278, "title": "flake fixes", "time": "2019-05-08T15:57:18Z", "body": ""}
{"number": 20279, "title": "Eliminate some const_cast's.", "time": "2019-05-08T17:43:19Z", "body": ""}
{"number": 20280, "title": "Enable caffe2 softmax tests with ROCm 2.4", "time": "2019-05-08T17:56:54Z", "body": "cc @xw285cornell @petrex "}
{"number": 20281, "title": "[easy] profiler: improve repr for averaged events", "time": "2019-05-08T18:14:40Z", "body": "Summary:\nThis is how it looks like now:\n\n```\n<FunctionEventAvg key=mm self_cpu_time=11.404s cpu_time=2.895ms\ncuda_time=0.000us input_shapes=[[26, 4096], [4096, 1024]]>\n```\n\nBefore I forgot to update the repr for these when updated it for not\naveraged events\n\nDifferential Revision: D15262862\n\n"}
{"number": 20282, "title": "Add unit test to ensure no gradients sync when calling ddp.module(input)", "time": "2019-05-08T18:28:42Z", "body": "Summary:\nAdd unit test to ensure no gradients sync when calling ddp.module(input), e.g not invoking prepare_for_backward\n\nPyText is depending on DDP for data parallel distributed training. To support accumulate gradients locally before gradients sync, we are calling orig_model.forward instead of ddp_model.forward. Add a unit test to avoid changes break the assumption.\n\nDifferential Revision: D15263155\n\n"}
{"number": 20283, "title": "error instead of crashing on attempt to subclass typed tensors", "time": "2019-05-08T19:35:41Z", "body": "https://github.com/pytorch/pytorch/issues/20052\r\n\r\ntyped tensors (e.g. torch.FloatTensor) can't be subclassed. Was causing\r\ncrashes and other errors.\r\n\r\n"}
{"number": 20284, "title": "[jit] make linear avaiable in tracing", "time": "2019-05-08T20:00:31Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* **#20284 make linear avaiable in tracing**\r\n* #20039 [jit] dispatch and expose linear op\r\n* #19988 [jit] split canonicalize_ops, make a decompose pass\r\n\r\nSummary:\r\n\r\ntracing only record the c++ function we called (addmm/matmul), we should expose linear in tracing since we did the decomposition pass to decompose linear "}
{"number": 20285, "title": "Fix strict weak ordering failure", "time": "2019-05-08T20:42:08Z", "body": "Summary: had to add for std::sort to not fail\r\n\r\nEdit: export from internal commit\r\n\r\nDifferential Revision: D15262741\r\n\r\n"}
{"number": 20286, "title": "Only record grad_fn in C++ Scatter and Gather when required so", "time": "2019-05-08T21:03:48Z", "body": "C++ `Scatter` and `Gather` always set autograd history for input data tensors regardless whether they require grad. This hits assertion failure in `set_history(Tensor, shared_ptr<Function> grad_fn)`\r\nwhere `grad_fn` cannot be nullptr. After this PR, C++ `Scatter` and `Gather` only record `grad_fn` when required.\r\n\r\n"}
{"number": 20287, "title": "Get people to stop improperly using AT_ASSERT", "time": "2019-05-08T21:05:52Z", "body": "Somehow, we have ended up in a situation where a lot of people in the PyTorch codebase are using `AT_ASSERT` when they should be using `AT_CHECK` (just do a grep for `AT_ASSERT` in `aten/src/ATen/native`, I audited the first five files and *every* occurrence of `AT_ASSERT` was wrong). Just to review the intended semantics:\r\n\r\n* `AT_ASSERT` means \"internal error, someone on the PyTorch dev team fucked up.\" If an `AT_ASSERT` triggers, that's a bug, and you should report a bug to PyTorch. In fact, the error message on `AT_ASSERT` says exactly this.\r\n* `AT_CHECK` means \"user error, report a message to the user.\" Users might trigger `AT_CHECK` and that's fine, it just means they misused the API. Give them a nice, human-friendly message, and let them try something else in their Jupyter session, whatever,\r\n\r\nUse of `AT_ASSERT` when you mean `AT_CHECK` is bad for a few reasons:\r\n\r\n1. At some point in the future, we may legitimately want to add a compilation mode that disables asserts (since, in principle, they should never be triggered). We cannot do this if half of the asserts are legitimate user input validation checks; disabling them will lead to hard to understand crashes on this \"optimized\" build.\r\n2. When an improperly tagged `AT_ASSERT` fails, we get spurious bug reports where users report a bug to PyTorch, because the error message told them to. Examples: #13503, #13052, #17911\r\n3. I hate it!\r\n\r\nLet's do some renaming to make the distinction clearer.\r\n\r\nWe propose the final state be just two macros (I'm lying; there's also an IndexError macro; I intend to preserve it but most people won't use it):\r\n\r\n```\r\nAT_INTERNAL_ASSERT(cond, ...); // panic if cond is not true; with optional message after\r\nAT_CHECK(cond, ...); // error if cond is not true; with optional message after\r\n```\r\n\r\nTranslation guide:\r\n* `AT_ASSERT(cond)` ==> `AT_INTERNAL_ASSERT(cond)`\r\n* `AT_ASSERTM(cond, msg)` ==> `AT_INTERNAL_ASSERT(cond, msg)`\r\n* `AT_CHECK(cond, msg)` ==> `AT_CHECK(cond, msg)`\r\n* `AT_ERROR(msg)` ==> `AT_CHECK(false, msg)`\r\n\r\nI'm OK with bikeshedding names.  \r\n\r\nFor BC-reasons, we will need to retain the old macros with deprecation warnings. We'll use a similar strategy to `AT_DISPATCH_` here. The transition won't be done all in one go; at the same time we'll audit `AT_ASSERT` sites to see if they really are internal errors or not."}
{"number": 20288, "title": "[distributions] clip sigmoid to prevent transforms return inf/nan values", "time": "2019-05-08T23:08:22Z", "body": "This PR addresses some numerical issues of Sigmoid/StickBreakingTransform, where these transforms give +-inf when the unconstrained values move to +-20 areas.\r\n\r\nFor example, with\r\n```\r\nt = torch.distributions.SigmoidTransform()\r\nx = torch.tensor(20.)\r\nt.inv(t(x)), t.log_abs_det_jacobian(x, t(x))\r\n```\r\ncurrent behaviour the inverse will return `inf` and logdet return `-inf` while this PR makes it to `15.9424` and `-15.9424`.\r\n\r\nAnd for\r\n```\r\nt = torch.distributions.StickBreakingTransform()\r\nx = torch.tensor([20., 20.])\r\nt.inv(t(x)), t.log_abs_det_jacobian(x, t(x))\r\n```\r\ncurrent value is `(inf, nan)` and `-inf` for logdet, while this PR makes it `[16.6355, 71.3942]` and `-47.8272` for logdet.\r\n\r\nAlthough these finite values are wrong and seems unavoidable, it is better than returning `inf` or `nan` in my opinion. This is useful in HMC where despite that the grad will be zero when the unconstrained parameter moves to unstable area (due to clipping), velocity variable will force the parameter move to another area which by chance can move the parameter out of unstable area. But inf/nan can be useful to stop doing inference early. So the changes in this PR might be inappropriate.\r\n\r\nI also fix some small issues of `_Simplex` and `_RealVector` constraints where batch shape of the input is not respected when checking validation."}
{"number": 20289, "title": "define use_cuda in dropout backward to allow peephole optimization to‚Ä¶", "time": "2019-05-08T23:14:48Z", "body": "‚Ä¶ work\r\n\r\n"}
{"number": 20290, "title": "[ONNX] Enable ONNX constant folding in test_pytorch_onnx_caffe2.py tests.", "time": "2019-05-08T23:22:52Z", "body": "This is a step towards enabling the ONNX constant folding pass by default in the PT->ONNX export. In this change we have enabled test points in `test/onnx/test_pytorch_onnx_caffe2.py`  to run with constant folding pass enabled. "}
{"number": 20291, "title": "move DistillBatchLRLoss Layer from open source to fb", "time": "2019-05-09T00:39:06Z", "body": "Summary: as titled. The move is needed for the new design which needs to access fb internal layers\n\nDifferential Revision: D15272181\n\n"}
{"number": 20292, "title": "Move THCTensor_(uniform) to ATen", "time": "2019-05-09T00:44:26Z", "body": "# Summary\r\nAs a first step for this plan: https://github.com/pytorch/pytorch/issues/19508#issuecomment-485178192, this PR moves `THCTensor_(uniform)` to ATen. Major changes are:\r\n- `uniform_` cuda kernel now utilizes a philox generator.\r\n- the kernel also utilizes TensorIterator\r\n- the kernel uses a grid-stride loop to achieve peak effective bandwidth\r\n\r\n# BC breaking change\r\n- Since the engine has changed from `curandStateMTGP32` to `curandStatePhilox4_32_10`, the randoms generated now will be different.\r\n- Here is the diff showing codegen changes: https://gist.github.com/syed-ahmed/4af9ae0d42b6c7dbaa13b9dd0d1dd1e8 (BC breaking change if any)\r\n\r\n# Testing\r\n- Philox4_32_10 is known to pass the standard TestU01 Big Crush test (https://www.thesalmons.org/john/random123/papers/random123sc11.pdf) and hence the quality of random numbers generated isn't an issue when compared to the previously used `curandStateMTGP32`.\r\n- I have added a test case in `aten/src/ATen/test/cuda_distributions_test.cu` which verifies that philox offset is incremented properly\r\n\r\n# Benchmark\r\nThe benchmark was done on a DGX station with 4 V100s.\r\n## Runtime\r\nI modified the script from @jcjohnson 's [multinomial benchmark](https://github.com/jcjohnson/pytorch-multinomial-benchmark) to produce this notebook which shows that there is a general speedup with this PR and a regression hasn't been introduced: https://gist.github.com/syed-ahmed/9d26d4e96308aed274d0f2c7be5218ef\r\n\r\nTo reproduce the notebook:\r\n- Run https://gist.github.com/syed-ahmed/4208c22c541f1d30ad6a9b1efc1d728f in a container with the current pytorch top of tree with the command: `python uniform_benchmark.py --stats_json before.json`\r\n- Apply this diff to the current pytorch top of tree and run the same script in a container with the command: `python uniform_benchmark.py --stats_json after.json`\r\n- Run the notebook attached above with the `after.json` and `before.json` in the same directory\r\n\r\n## Effective Bandwidth\r\nThe effected bandwidth was calculated using the script (thanks to @ngimel ): https://gist.github.com/syed-ahmed/f8b7384d642f4bce484228b508b4bc68\r\nFollowing are the numbers before and after.\r\n#### Before\r\n```\r\nuniform, size, elements 65536 forward 5.168914794921875e-06 bandwidth (GB/s) 50.71548098597786\r\nuniform, size, elements 131072 forward 5.056858062744141e-06 bandwidth (GB/s) 103.67860705101367\r\nuniform, size, elements 262144 forward 7.164478302001953e-06 bandwidth (GB/s) 146.357621001797\r\nuniform, size, elements 524288 forward 1.1217594146728515e-05 bandwidth (GB/s) 186.9520302275877\r\nuniform, size, elements 1048576 forward 1.923084259033203e-05 bandwidth (GB/s) 218.10297600317384\r\nuniform, size, elements 2097152 forward 3.640890121459961e-05 bandwidth (GB/s) 230.39992200138826\r\nuniform, size, elements 4194304 forward 6.778717041015625e-05 bandwidth (GB/s) 247.49839679819922\r\nuniform, size, elements 8388608 forward 0.00012810707092285157 bandwidth (GB/s) 261.92490202361347\r\nuniform, size, elements 16777216 forward 0.00025241613388061524 bandwidth (GB/s) 265.86598474620627\r\nuniform, size, elements 33554432 forward 0.000497891902923584 bandwidth (GB/s) 269.5720239913193\r\n```\r\n#### After\r\n```\r\nuniform, size, elements 65536 forward 5.550384521484375e-06 bandwidth (GB/s) 47.22988091821306\r\nuniform, size, elements 131072 forward 5.581378936767578e-06 bandwidth (GB/s) 93.93520954942333\r\nuniform, size, elements 262144 forward 6.165504455566406e-06 bandwidth (GB/s) 170.071404141686\r\nuniform, size, elements 524288 forward 6.3276290893554685e-06 bandwidth (GB/s) 331.4277702414469\r\nuniform, size, elements 1048576 forward 8.509159088134765e-06 bandwidth (GB/s) 492.91639239047356\r\nuniform, size, elements 2097152 forward 1.2989044189453124e-05 bandwidth (GB/s) 645.8218077979443\r\nuniform, size, elements 4194304 forward 2.347707748413086e-05 bandwidth (GB/s) 714.6211452997259\r\nuniform, size, elements 8388608 forward 4.4286251068115234e-05 bandwidth (GB/s) 757.6715389250498\r\nuniform, size, elements 16777216 forward 8.672237396240235e-05 bandwidth (GB/s) 773.8356427961071\r\nuniform, size, elements 33554432 forward 0.00016920566558837892 bandwidth (GB/s) 793.2224227438523\r\n```\r\n "}
{"number": 20293, "title": "numpy like nonzero (called nonzero_tuple)", "time": "2019-05-09T01:11:17Z", "body": "No performance degradation compared to Numpy when indexing:\r\n\r\n```\r\nIn [15]: x=torch.randn((1000,1000))                                                                                                                                                         \r\n\r\nIn [16]: %timeit x[x.nonzero_tuple()]                                                                                                                                                       \r\n4.63 ms ¬± 102 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\r\n\r\nIn [17]: y=x.numpy()                                                                                                                                                                        \r\n\r\nIn [18]: %timeit y[y.nonzero()]                                                                                                                                                             \r\n14.6 ms ¬± 281 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\r\n\r\nIn [20]: x=x.t()                                                                                                                                                                            \r\n\r\nIn [22]: %timeit x[x.nonzero_tuple()]                                                                                                                                                       \r\n9.01 ms ¬± 626 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\r\n\r\nIn [24]: y=x.numpy()                                                                                                                                                                        \r\n\r\nIn [25]: %timeit y[y.nonzero()]                                                                                                                                                             \r\n16.8 ms ¬± 770 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\r\n\r\n```"}
{"number": 20294, "title": "Suo master", "time": "2019-05-09T01:59:16Z", "body": ""}
{"number": 20295, "title": "[DO NOT MERGE] Test if sparse tensor `values_` can be a requires grad variable", "time": "2019-05-09T02:40:51Z", "body": ""}
{"number": 20296, "title": "the capacity of memory when compiling pytorch from source", "time": "2019-05-09T02:57:45Z", "body": "how large the capacity of memory is needed when compiling pytorch from source ?\r\n\r\nI've got about 5GB available when compiling in ubuntu 18.04.  Each time I compile with **python setup.py install**, the compiling program will occupy all the capacity of memory. Then my screen is stuck. After a while,  the screen becomes dark and my laptop reboots.... \r\n\r\nIs there any way to setup some parameters to use less memory when compiling ? Many thanks."}
{"number": 20297, "title": "Gumbel-Softmax Arxiv Docs Link Broken", "time": "2019-05-09T03:14:20Z", "body": "## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\nDocs for `torch.nn.functional.gumbel_softmax`. \r\n\r\nFunction `gumbel_softmax` docstring entry for `Gumbel-Softmax distribution` has two Arxiv links:\r\n\r\n    .. _Gumbel-Softmax distribution:\r\n        https://arxiv.org/abs/1611.00712\r\n        https://arxiv.org/abs/1611.01144\r\n\r\nThe official [PyTorch docs](https://pytorch.org/docs/stable/nn.html#gumbel-softmax) concatenate the two links together to make [https://arxiv.org/abs/1611.00712https://arxiv.org/abs/1611.01144](https://arxiv.org/abs/1611.00712https://arxiv.org/abs/1611.01144) which is clearly an invalid link.\r\n\r\n*Note*: I am not sure how you would like this fixed.  It might be cleanest to separate it into two links. If you let me know, I can issue a pull request."}
{"number": 20298, "title": "Remove support for CUDA 8", "time": "2019-05-09T03:35:48Z", "body": "1.1.0 stopped support for CUDA 8"}
{"number": 20299, "title": "Move librosa and psutil installation from CI script to docker images build script", "time": "2019-05-09T05:22:08Z", "body": "pip install librosa randomly coredump, causes CI flakiness"}
{"number": 20300, "title": "Remove SourceLocation", "time": "2019-05-09T05:36:02Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20300 Remote SourceLocation**\n\nSummary: We only ever really used SourceRange or a string.\nWe can fold the string behavior into SourceRange and simplify the\nimplementation. This also makes Node::sourceRange() non-optional.\nIt will use the raw node print if nothing else was provided.\n\nTest Plan: test_jit.py\n\nDifferential Revision: [D15275731](https://our.internmc.facebook.com/intern/diff/D15275731)"}
{"number": 20301, "title": "Softmax Docs Example Deprecated", "time": "2019-05-09T05:44:55Z", "body": "## üìö Documentation\r\n\r\nIn the master docs for `nn.Softmax()`, the example provided is:\r\n\r\n        >>> m = nn.Softmax()\r\n        >>> input = torch.randn(2, 3)\r\n        >>> output = m(input)\r\n\r\nHowever, this implementation appears deprecated in 1.1.  You now get a warning like:\r\n\r\n`UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.`\r\n\r\nIf you agree, I can change the example to `m = nn.Softmax(dim=1)` and issue a pull request so there is no warning.\r\n\r\n**Version**: I observed the warning with Python 1.1.0 and Python 3.7.1 on OSX."}
{"number": 20302, "title": "Switch off USE_DISTRIBUTED on default for MSVC", "time": "2019-05-09T05:53:53Z", "body": "Fixes https://github.com/pytorch/pytorch/issues/20250"}
{"number": 20303, "title": "Remove const_cast's from subgraph matcher.", "time": "2019-05-09T06:13:05Z", "body": "The trick here is that creating a mapping from const values to\r\nconst values means that downstream clients that want to mutate\r\nthe output of the mapping are stuck.  However, a mapping from\r\nconst values to non-const values is just fine and doesn't put\r\nconstraints on downstream clients.\r\n\r\n"}
{"number": 20304, "title": "Fix overlay_vc_env when called by legacy python", "time": "2019-05-09T06:14:53Z", "body": "Fixes https://github.com/pytorch/pytorch/issues/20155."}
{"number": 20305, "title": "Fix bug in non_blocking copy", "time": "2019-05-09T06:47:26Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20305 Fix bug in non_blocking copy**\n\nDifferential Revision: [D15276094](https://our.internmc.facebook.com/intern/diff/D15276094)"}
{"number": 20306, "title": "fix typo in adaptive methods annotation ", "time": "2019-05-09T07:08:41Z", "body": "fixes #20215 \r\nThe confusing behavior was caused by typos in type annotation :( "}
{"number": 20307, "title": "Speed up RecordFunction with sampled callbacks", "time": "2019-05-09T07:26:39Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* **#20307 Allow lazy initialization of inputs in RecordFunction**\r\n\r\nSummary:\r\nIn production environment we typically use sampling and don't\r\nrequire expensive input copying on every function call, this PR\r\nenables lazy inputs copying, when actual RecordFunction callback\r\ncalls inputs()\r\n\r\nTest Plan:\r\nBLAS=MKL USE_MKLDNN=1 USE_OPENCV=1 USE_FFMPEG=1 python setup.py develop --cmake\r\n./build/bin/test_jit --gtest_filter=JitTest.RecordFunction\n\nDifferential Revision: [D15276308](https://our.internmc.facebook.com/intern/diff/D15276308)"}
{"number": 20308, "title": "Change view dispatch to abstract", "time": "2019-05-09T07:34:13Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* #20309 Generate TH functions outside of Type\r\n* **#20308 Change view dispatch to abstract**\r\n\r\nBecause the following PR will change native_functions that wrap TH to dispatch differently by backend, we will need to dispatch mkldnn `view` separately, which means the condition in this if statement will never be met. Removing it and dispatching `view` by backend preemptively to prepare for next PR.\n\nDifferential Revision: [D15509849](https://our.internmc.facebook.com/intern/diff/D15509849)"}
{"number": 20309, "title": "Generate TH functions outside of Type", "time": "2019-05-09T07:34:19Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* **#20309 Generate TH functions outside of Type**\r\n* #20308 Change view dispatch to abstract\r\n\r\nThis change generates TH functions into at::legacy::cpu and\r\nat::legacy::cuda rather than Type. TH functions will no longer be\r\nvisible from Functions.h, NativeFunctions.h, etc. Those interfaces will\r\nnow only show native functions.\r\n\r\nThese new functions in at::legacy are called by native function\r\nwrappers, most of which already exist. In the case of these already\r\nexisting wrappers, this saves us a Type dispatch.\r\n\r\nSome native function implementions previously called the old generated\r\nTH functions. In these cases, we add a wrapper for those functions and\r\ncall it instead. Note the number of Type dispatches stays the same.\r\n\r\nWe also need to add differentiability logic for some native wrappers\r\nbecause this was previously done as part of VariableType generation.\r\n\r\nI ran a benchmark on pow and before the patch it takes ~3.6ms and after it takes ~3.5ms. I've previously benchmarked the cost of an additional native dispatch to be around 100ns, so this seems as expected.\r\n```\r\nimport torch\r\na = torch.randn(1,1)\r\n%timeit torch.pow(a, 1)\r\n```\n\nDifferential Revision: [D15509848](https://our.internmc.facebook.com/intern/diff/D15509848)"}
{"number": 20310, "title": "Fixed Softmax doc to specify dimension to prevent warning in 1.1.0.", "time": "2019-05-09T07:36:49Z", "body": "See Issue #20301\r\n\r\nSpecifying dim in docstring example to prevent UserWarning.\r\n"}
{"number": 20311, "title": "pytorch1.1 training speed slower than pytorch1.0", "time": "2019-05-09T07:58:36Z", "body": "## üêõ Bug\r\n\r\nWith the same configuration, I trained a face recognition model with pytorch1.0 and pytorch1.1 separately and found that training speed under v1.0 is faster than v1.1. To be more exact, I got about 2.5 batchs/sec/gpu with v1.0 while 2.0 batchs/sec/gpu with v1.1. Both experiments configurations are the same(same batch size, init seed, number of processes and so on). \r\n\r\nI use the `torch.nn.parallel.DistributedDataParallel` to realize data parallel. and startup my training task with command `python -m torch.distirbuted.launch --nnodes=1 --nproc_per_node=8 train.py`. But pytorch1.1 got slower speed, I don't know why.\r\n\r\nBoth pytorch 1.0 and 1.1 was installed by `conda install pytorch torchvision cudatoolkit=10.0 -c pytorch`\r\n\r\n## Expected behavior\r\n\r\npytorch1.1 should get the same training speed with pytorch1.0.\r\n\r\n## Environment\r\n\r\npytorch1.1 container\r\n```\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: \r\nGPU 0: TITAN V\r\nGPU 1: TITAN V\r\nGPU 2: TITAN V\r\nGPU 3: TITAN V\r\nGPU 4: TITAN V\r\nGPU 5: TITAN V\r\nGPU 6: TITAN V\r\nGPU 7: TITAN V\r\n\r\nNvidia driver version: 410.78\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.2\r\n[pip] numpydoc==0.8.0\r\n[pip] torch==1.1.0\r\n[pip] torchvision==0.2.2\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.3                      199  \r\n[conda] mkl-include               2019.3                      199  \r\n[conda] mkl-service               1.1.2            py37he904b0f_5  \r\n[conda] mkl_fft                   1.0.10           py37ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] pytorch                   1.1.0           py3.7_cuda10.0.130_cudnn7.5.1_0    pytorch\r\n[conda] torchvision               0.2.2                      py_3    pytorch\r\n```\r\n\r\npytorch1.0 container\r\n```\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: \r\nGPU 0: TITAN V\r\nGPU 1: TITAN V\r\nGPU 2: TITAN V\r\nGPU 3: TITAN V\r\nGPU 4: TITAN V\r\nGPU 5: TITAN V\r\nGPU 6: TITAN V\r\nGPU 7: TITAN V\r\n\r\nNvidia driver version: 410.78\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.2\r\n[pip] numpydoc==0.8.0\r\n[pip] torch==1.0.1.post2\r\n[pip] torchvision==0.2.2\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.3                      199  \r\n[conda] mkl-service               1.1.2            py37he904b0f_5  \r\n[conda] mkl_fft                   1.0.10           py37ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] pytorch                   1.0.1           py3.7_cuda10.0.130_cudnn7.4.2_2    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch\r\n[conda] torchvision               0.2.2                      py_3    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch\r\n\r\n```\r\n"}
{"number": 20312, "title": " python setup.py install erroy", "time": "2019-05-09T08:12:19Z", "body": "launchx-500-10802@launchx50010802:~/github/pytorch$ python setup.py install\r\nBuilding wheel torch-1.1.0a0+d24c0aa\r\n-- Building version 1.1.0a0+d24c0aa\r\nmake: *** No rule to make target 'install'„ÄÇ ÂÅúÊ≠¢„ÄÇ\r\nTraceback (most recent call last):\r\n  File \"setup.py\", line 728, in <module>\r\n    build_deps()\r\n  File \"setup.py\", line 294, in build_deps\r\n    build_dir='build')\r\n  File \"/home/launchx-500-10802/github/pytorch/tools/build_pytorch_libs.py\", line 291, in build_caffe2\r\n    check_call(['make', '-j', str(max_jobs), 'install'], cwd=build_dir, env=my_env)\r\n  File \"/home/launchx-500-10802/anaconda3/lib/python3.6/subprocess.py\", line 291, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['make', '-j', '4', 'install']' returned non-zero exit status 2."}
{"number": 20313, "title": "nccl runtime error: unhandled system error", "time": "2019-05-09T08:56:06Z", "body": "## üêõ Bug\r\n\r\nGot NCCL RuntimeError when startup a distributed training task with 2 nodes. Error code is:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"train_recognition.py\", line 235, in <module>\r\n    BACKBONE = DistributedDataParallel(BACKBONE, device_ids=[args.local_rank], output_device=args.local_rank)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\", line 286, in __init__\r\n    self.broadcast_bucket_size)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\", line 410, in _dist_broadcast_coalesced\r\n    dist._dist_broadcast_coalesced(self.process_group, tensors, buffer_size, False)\r\nRuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1556653114079/work/torch/lib/c10d/ProcessGroupNCCL.cpp:272, unhandled system error\r\n```\r\n\r\nBut when I run the same code inside 1 nodes with multi-processes and each process per gpu, it works well.\r\n\r\n## Environment\r\n\r\nboth node has the same configuration.\r\n\r\n```\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: \r\nGPU 0: TITAN V\r\nGPU 1: TITAN V\r\nGPU 2: TITAN V\r\nGPU 3: TITAN V\r\nGPU 4: TITAN V\r\nGPU 5: TITAN V\r\nGPU 6: TITAN V\r\nGPU 7: TITAN V\r\n\r\nNvidia driver version: 410.78\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.2\r\n[pip] numpydoc==0.8.0\r\n[pip] torch==1.1.0\r\n[pip] torchvision==0.2.2\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.3                      199  \r\n[conda] mkl-include               2019.3                      199  \r\n[conda] mkl-service               1.1.2            py37he904b0f_5  \r\n[conda] mkl_fft                   1.0.10           py37ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] pytorch                   1.1.0           py3.7_cuda10.0.130_cudnn7.5.1_0    pytorch\r\n[conda] torchvision               0.2.2                      py_3    pytorch\r\n```\r\n"}
{"number": 20314, "title": "What‚Äôs the difference between the two hypothesis?", "time": "2019-05-09T09:12:55Z", "body": "## ‚ùì Questions and Help\r\n\r\n### Please note that this issue tracker is not a help form and this issue will be closed.\r\n\r\nWe have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:\r\n\r\n- [Discussion Forum](https://discuss.pytorch.org/)\r\n\r\nResnet has a 4 component: layer1, layer2, layer3, layer4.\r\n\r\nHypothesis oneÔºö\r\nAnd I initialize the layer4 use the same parameters, just like this:\r\n**self.layer4_1 = model_resnet.layer4\r\nself.layer4_2 = model_resnet.layer4**\r\nThat means the layer4_1 and layer4_2 pointing to the same parameters, they share the same parameter. I update them alternately.\r\n\r\nHypothesis twoÔºö\r\nI just define one layer4 just like this:\r\n**self.layer4 = model_resnet.layer4**\r\nAnd I update the layer4 two times than Hypothesis one.\r\n\r\n![image](https://user-images.githubusercontent.com/27360803/57441996-a4085a00-727d-11e9-961a-414e6c069b9c.png)\r\n\r\nIn hypothesis 1/top image, the layer4_1 and layer4_2 share the same parameters, which is the same in layer4 in hypothesis 2/down image.\r\n\r\nI update layer4_1 and layer4_2 alternatively in hypothesis 1/top image and I update layer4 in hypothesis 2/down image twice to get the the amount of updates compared to hypothesis 1.\r\n\r\nAnd I promise the data are all the same.\r\n\r\nI want to ask what‚Äôs the difference between the two hypothesisÔºü**Why are the model results different when the model convergesÔºü**\r\n\r\nMy English is poor, if you are chinese, we can talk in chinese.\r\n\r\nLooking forward to your reply!!\r\n\r\n\r\n"}
{"number": 20315, "title": "libtorch+opencv Mat result error: different from the python ones", "time": "2019-05-09T11:46:24Z", "body": "```\r\n#include <torch/script.h> // One-stop header.\r\n#include <iostream>\r\n#include <memory>\r\n#include <string>\r\n#include <opencv2/opencv.hpp>\r\nusing namespace std;\r\nusing namespace cv;\r\n\r\nint main(int argc, const char* argv[]) {\r\n\t//Âä†ËΩΩÊ®°Âûã\r\n\tstring model = \"model.pt\";\r\n\t// Deserialize the ScriptModule from a file using torch::jit::load().\r\n\tshared_ptr<torch::jit::script::Module> module = torch::jit::load(model);\r\n\tassert(module != nullptr);\r\n\tcout << \"ok\";\r\n\t//load ÂõæÂÉè\r\n\tMat img=imread(\"2.jpg\");\r\n\tresize(img, img, Size(224, 224), (0, 0), (0, 0), INTER_LINEAR);\r\n\r\n\timg.convertTo(img, CV_32F, 1.0 / 255.0);\r\n\t\r\n\ttorch::TensorOptions option(torch::kFloat32);\r\n\t//Mat to Torch\r\n\tat::Tensor img_tensor = torch::from_blob(img.data, { 1,img.channels(),img.rows,img.cols }, option);\r\n\t//prediction\r\n\tat::Tensor result = module->forward({ img_tensor }).toTensor();//result different from python \r\n\t//Torch to Mat\r\n\tMat prediction(Size(img.cols, img.rows), CV_32FC3, result.data_ptr());\r\n\t//cout << result;\r\n\t//result = result.unsqueeze(0);\r\n\t//result.accessor<float, 3>();\r\n\t//cout << result;\r\n\t\r\n\twaitKey(3000);\r\n\treturn 0;\r\n}\r\n```\r\n![image](https://user-images.githubusercontent.com/34785633/57451266-631b4000-7293-11e9-9766-07e76c330ee4.png)\r\n\r\nwhy there are lines in my results?It's so strange.\r\n I tested the model.pt in python and it  didnt have such problem.\r\nCould you help me?\r\nthanks very much !"}
{"number": 20316, "title": "What is the meaning of such a formula in some functions", "time": "2019-05-09T13:01:12Z", "body": "## üìö Documentation\r\nWhat is the meaning of such a formula in some functionsÔºü Just like the formula in the following picture\r\nmath::\r\n        v = \\frac{v}{\\max(\\lVert v \\rVert_p, \\epsilon)}.\r\n![image](https://user-images.githubusercontent.com/22348625/57455073-e8efb900-729c-11e9-895f-588ec050ed4b.png)\r\nIs my chrome lack of some Plug-ins so they can not show correctly?? "}
{"number": 20317, "title": "[tensorboard] add test coverage for make_np", "time": "2019-05-09T13:07:36Z", "body": "addresses https://github.com/pytorch/pytorch/pull/16196#discussion_r276381946\r\n\r\ncc @orionr "}
{"number": 20318, "title": "Remove the legacy code about selective build", "time": "2019-05-09T13:08:16Z", "body": ""}
{"number": 20319, "title": "Fix THD->c10 dependency to gflags.h", "time": "2019-05-09T13:45:04Z", "body": "Fixed #20250 \r\n\r\nNot sure if there's any specific design reason to `add_dependecy()` and manually add a few include dir, instead of linking the target."}
{"number": 20320, "title": "C++ tensor.print() no longer gives useful information", "time": "2019-05-09T13:51:59Z", "body": "It's currently implemented as\r\n\r\n```\r\n  if (defined()) {\r\n    std::cerr << \"[\" << dispatch_type().toString() << \" \" << sizes() << \"]\" << std::endl;\r\n  } else {\r\n    std::cerr << \"[UndefinedTensor]\" << std::endl;\r\n  }\r\n```\r\n\r\nBut `dispatch_type()` no longer is giving useful information.\r\n\r\n@li-roy, do you think you could take a look at this?"}
{"number": 20321, "title": "Rename AT_ASSERT to TORCH_INTERNAL_ASSERT; other macro updates", "time": "2019-05-09T14:27:59Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20435 Replace AT_CHECK with TORCH_CHECK [shard 9/10]&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15318877/)\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20321 Rename AT_ASSERT to TORCH_INTERNAL_ASSERT; other macro updates**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15278439/)\n\nFirst part of https://github.com/pytorch/pytorch/issues/20287\n\n- Rename `AT_ASSERT` to `TORCH_INTERNAL_ASSERT`\n- Make `TORCH_INTERNAL_ASSERT` work with variadic inputs\n- Deprecated `AT_ASSERT` and `AT_ASSERTM`\n- Rename `AT_CHECK` to `TORCH_CHECK`\n- Make `TORCH_CHECK` give a better error message when no arguments are\n  provided\n- Deprecate `AT_ERROR` in favor of `TORCH_CHECK(false, ...)`\n- Deprecate `AT_INDEX_ERROR` in favor of `TORCH_CHECK_INDEX(false, ...)`\n- Rename `AT_WARN` to `TORCH_WARN`\n\nNo use sites are changed; I'll work on that in follow up patches\n(or disable the deprecation, if necessary.)\n\nDifferential Revision: [D15278439](https://our.internmc.facebook.com/intern/diff/D15278439/)"}
{"number": 20322, "title": "Segmentation fault occur when using index_copy_", "time": "2019-05-09T14:53:58Z", "body": "## üêõ Bug\r\n\r\nWhen using the index_copy_ function, the programme will throw a segmentation fault.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\nRun the code below:\r\n```python\r\nimport torch\r\na = torch.randn(3, 5)\r\nc = torch.zeros(3)\r\na.index_copy_(dim=1, index=torch.tensor([3]), source=c)\r\n```\r\n```bash\r\nMrFive@mrfive-home:~$ python\r\nPython 3.7.2 (default, Dec 29 2018, 06:19:36) \r\n[GCC 7.3.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import torch\r\n>>> a = torch.randn(3, 5)\r\n>>> b = torch.zeros(3, 4)\r\n>>> c = torch.zeros(3)\r\n>>> a.index_copy_(dim=1, index=torch.tensor([3]), source=c)\r\nSegmentation fault (core dumped)\r\n```\r\nI know the code following will work, but it seems that the issue above is an unexpected performent.\r\n```python\r\na.index_copy_(dim=1, index=torch.tensor([3]), source=c.unsqueeze(1))\r\n```\r\n## Environment\r\n```\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.2\r\n[conda] blas                      1.0                         mkl    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\n[conda] mkl                       2019.1                      144    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\n[conda] mkl_fft                   1.0.10           py37ha843d7b_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\n[conda] mkl_random                1.0.2            py37hd81dba3_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\n[conda] pytorch-cpu               1.0.1               py3.7_cpu_2    pytorch\r\n[conda] torchvision-cpu           0.2.2                      py_3    pytorch\r\n```\r\n"}
{"number": 20323, "title": "Support size to `torch.normal`", "time": "2019-05-09T14:56:03Z", "body": "This would be more consistent with numpy.\r\n\r\n```python\r\ntorch.normal(0.0, 4.0, size=5)\r\n```\n\ncc @mruberry @rgommers @heitorschueroff"}
{"number": 20324, "title": "Make C10_NODISCARD macro more portable for nvcc+clang.", "time": "2019-05-09T15:29:51Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20324 Make C10_NODISCARD macro more portable for nvcc+clang.**\n\nFixes #13118.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nDifferential Revision: [D15359317](https://our.internmc.facebook.com/intern/diff/D15359317)"}
{"number": 20325, "title": "Adding setup job as prereq to html update jobs", "time": "2019-05-09T17:01:53Z", "body": ""}
{"number": 20326, "title": "Libtorch error in header file", "time": "2019-05-09T17:37:03Z", "body": "I'm creating a class with libtorch members. While including \"`#include <torch/script.h>`\" into the header, I'm getting the following error:\r\n\r\n`1>c:\\code\\externallibraries\\libtorch_cuda10_debug\\libtorch\\include\\aten\\core\\ivalue.h(713): error C2334: unexpected token(s) preceding '{'; skipping apparent function body (compiling source file GeneratedFiles\\Release\\moc_ImageProcessingInterface.cpp)`\r\n\r\nIf I put the include into the cpp part, no compilation error is found. Any suggestion?\r\n\r\nThank you in advance. \r\n"}
{"number": 20327, "title": "[ONNX] Add fast_neural_style to the CI", "time": "2019-05-09T17:58:54Z", "body": "Like what we did for other examples in test_pyttorch_onnx_caffe2.py."}
{"number": 20328, "title": "option to thrash cache in BenchmarkNet", "time": "2019-05-09T18:20:26Z", "body": "Summary: Allocate memory before each run for op profiling in TEST_Benchmark to invalidate cache\n\nDifferential Revision: D15090886\n\n"}
{"number": 20329, "title": "[onnx] Malformed model exported with nn.Sequential and nn.Embedding", "time": "2019-05-09T18:41:54Z", "body": "## üêõ Bug\r\n\r\nONNX exporter gets tripped by a model using `nn.Sequential` and `nn.Embedding` when the embedding layer is stored as a class property. The generated onnx file is malformed: it has an initializer for the user-defined input. The initializer used is the weight metrix of the embedding table.\r\n\r\nThe embedding table appears in initializers the second time (this time for the embedding layer).\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Define a model:\r\n\r\n```\r\n    n = 8\r\n    dim = 10\r\n\r\n    class SomeModel(nn.Module):\r\n\r\n        def __init__(self):\r\n            super(SomeModel, self).__init__()\r\n            self.embedding = nn.Embedding(n, dim)\r\n            self.seq = nn.Sequential(\r\n                self.embedding,\r\n                nn.Linear(dim, 1),\r\n                nn.Sigmoid()\r\n            )\r\n\r\n        def forward(self, indices):\r\n            return self.seq(indices)\r\n\r\n    model = SomeModel()\r\n```\r\n\r\n\r\n2. Export:\r\n\r\n```\r\ninput = torch.LongTensor([2])\r\ntorch.onnx.export(model, input, \"foo.onnx\")\r\n```\r\n\r\n3. Check `foo.onnx` and notice it's malformed.\r\n\r\n## Expected behavior\r\n\r\nNo initializer for the user-provided input of the model.\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: Could not collect\r\n\r\nPython version: 2.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] msgpack-numpy==0.4.3.1\r\n[pip] numpy==1.14.1\r\n[pip] torch==1.0.0\r\n[conda] Could not collect\r\n```\r\n\r\n## Additional context\r\n\r\n#19227 is another issue where the onnx export and `nn.Sequential` appear to be at odds.\r\n\r\nThis bug can be worked around by inlining embedding definition to `nn.Sequetial` or by getting rid of `nn.Sequential` and chaining ops manually.\r\n\r\nEither way, the fact that a malformed file is outputted is not great."}
{"number": 20330, "title": "[BC-breaking] Shallow-copy indices and values in sparse tensor ctor", "time": "2019-05-09T18:48:30Z", "body": "After the Variable/Tensor merge, there is no guarantee that `indices` and `values` passed into the sparse tensor constructor don't contain AutogradMeta. However, we want to maintain the existing invariant that `indices_` and `values_` of a sparse tensor don't contain AutogradMeta, and to achieve this we need do shallow-copy in the sparse tensor constructor.\r\n\r\nNote that this is BC-breaking for code that changes the sizes / strides of the indices or values tensor after it's used to create a sparse tensor. In current master, such changes will be reflected in the sparse tensor and break sparse tensor invariants. After this PR, those changes will not be reflected in the sparse tensor, and thus the sparse tensor invariants are always preserved. Specifically, running in-place size/stride-changing ops such as `resize_` / `resize_as_` / `as_strided_` / `set_` / `transpose_` on the original values tensor will not update the sparse tensor's `values_`. For example:\r\n```python\r\n# Calling resize_ on non-requires-grad value tensor\r\ni2 = torch.zeros([1, 1])\r\nv2 = torch.ones([1, 2, 3])\r\nt2 = torch.sparse_coo_tensor(i2, v2, torch.Size([2, 2, 3]))\r\nv2.resize_(4, 5)\r\nt2.coalesce().values().size()\r\n# On current master, this throws \"indices and values must have same nnz, but got nnz from indices: 1, nnz from values: 4\", because resizing the original value tensor affects `values_` of the sparse tensor.\r\n# After this PR, this prints \"torch.Size([1, 2, 3])\", which means resizing the original value tensor doesn't affect `values_` of the sparse tensor.\r\n```"}
{"number": 20331, "title": "Avoid unnecessary refcount bump in unary operators.", "time": "2019-05-09T18:50:19Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20331 Avoid unnecessary refcount bump in unary operators.**\n\nIf you convert a Tensor& to a Tensor, a copy-construction occurs\nand you require a refcount bump.  If you just return the Tensor\non the stack, RVO applies and no refcount bump is necessary.\n\nSigned-off-by: Edward Z. Yang <ezyang@mit.edu>\n\nDifferential Revision: [D15295193](https://our.internmc.facebook.com/intern/diff/D15295193)"}
{"number": 20332, "title": "[DO NOT MERGE] Test if we can shallow-copy indices and values in sparse tensor ctor, along with VariableImpl removal", "time": "2019-05-09T18:52:11Z", "body": ""}
{"number": 20333, "title": "Remove SourceLocation (respin)", "time": "2019-05-09T20:24:53Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20333 Remove SourceLocation (respin)**\n\nDifferential Revision: [D15284081](https://our.internmc.facebook.com/intern/diff/D15284081)"}
{"number": 20334, "title": "Publish c10::RegisterOperators as torch::RegisterOperators", "time": "2019-05-09T20:44:45Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20334 Publish c10::RegisterOperators as torch::RegisterOperators**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15284557/)\n\n-\n\nDifferential Revision: [D15284557](https://our.internmc.facebook.com/intern/diff/D15284557/)"}
{"number": 20335, "title": "cat()/stack() calls in JIT do not accept lists of tensors", "time": "2019-05-09T21:02:05Z", "body": "## üêõ Bug\r\n\r\nWhen including a call to `th.cat(tensors)` in a ScriptModule, if `tensors` is a tuple of tensors the ScriptModule can be constructed, but if `tensors` is a list of tensors, the construction fails.\r\n\r\n## To Reproduce\r\n\r\nRun this short repro example:\r\n```python\r\nimport torch as th\r\n\r\ndef f():\r\n    return th.ones(3), th.ones(3)\r\n\r\nclass Mod(th.jit.ScriptModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.fn = th.jit.trace(f, tuple())\r\n\r\n    @th.jit.script_method\r\n    def forward(self):\r\n        x1, x2 = self.fn()\r\n        return th.cat([x1, x2], dim=1) # <---- Fails\r\n        #return th.cat((x1, x2), dim=1) # <---- Succeeds\r\n\r\nmod = Mod()\r\n```\r\n\r\nThe module will fail to construct unless the first cat() call is replaced w/the second.\r\n\r\n## Expected behavior\r\n\r\ncat() / stack() accepts lists or tuples of  #tensors\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.1\r\n - OS (e.g., Linux): linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Python version: 3.7\r\n\r\n## Additional Info\r\n\r\nStack trace of failed construction:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 17, in <module>\r\n    mod = Mod()\r\n  File \".../lib/python3.7/site-packages/torch/jit/__init__.py\", line 1049, in init_then_register                                    \r\n    _create_methods_from_stubs(self, methods)\r\n  File \".../lib/python3.7/site-packages/torch/jit/__init__.py\", line 1014, in _create_methods_from_stubs                            \r\n    self._c._create_methods(self, defs, rcbs, defaults)\r\nRuntimeError:\r\narguments for call are not valid:\r\n\r\n  for operator aten::cat(Tensor[] tensors, int dim=<default>) -> Tensor:\r\n  expected a value of type Tensor[] for argument 'tensors' but found Tensor[]\r\n  Empty lists default to List[Tensor]. Use torch.jit.annotate(List[my_type], []) to create an empty list of another type                                                                 \r\n  @th.jit.script_method\r\n  def forward(self):\r\n      x1, x2 = self.fn()\r\n      return th.cat([x1, x2], dim=1) # <---- Fails\r\n                    ~~~~~~~ <--- HERE\r\n\r\n  for operator aten::cat(Tensor[] tensors, int dim=<default>, *, Tensor out) -> Tensor:\r\n  expected a value of type Tensor[] for argument 'tensors' but found Tensor[]\r\n  Empty lists default to List[Tensor]. Use torch.jit.annotate(List[my_type], []) to create an empty list of another type                                                                 \r\n  @th.jit.script_method\r\n  def forward(self):\r\n      x1, x2 = self.fn()\r\n      return th.cat([x1, x2], dim=1) # <---- Fails\r\n                    ~~~~~~~ <--- HERE\r\nfor call at:\r\n@th.jit.script_method\r\ndef forward(self):\r\n    x1, x2 = self.fn()\r\n    return th.cat([x1, x2], dim=1) # <---- Fails\r\n           ~~~~~~ <--- HERE\r\n```"}
{"number": 20336, "title": "make torch.zeros slightly faster", "time": "2019-05-09T21:07:19Z", "body": "Try skipping some omp code when it isn't used and using bool instead of int to make a torch.zero's operation slightly faster.\r\n\r\nOn master\r\n```\r\n[bvaughan@devgpu005.ash6 ~/repos] python perf_comp.py\r\ntorch: 25.932421684265137\r\nnumpy: 10.50987696647644\r\ntorch slower by:146.743342162637 %\r\n\r\n[bvaughan@devgpu005.ash6 ~/repos] python perf_comp.py\r\ntorch: 22.03349804878235\r\nnumpy: 10.413864135742188\r\ntorch slower by:111.57850497741335 %\r\n\r\n[bvaughan@devgpu005.ash6 ~/repos] python perf_comp.py\r\ntorch: 24.49870467185974\r\nnumpy: 10.26451826095581\r\ntorch slower by:138.67369173132994 %\r\n```\r\n\r\nOn this branch:\r\n```\r\n(base) [bvaughan@devgpu005.ash6 ~/repos] python perf_comp.py\r\ntorch: 19.191195249557495\r\nnumpy: 10.524352073669434\r\ntorch slower by:82.35037288016409 %\r\n\r\n(base) [bvaughan@devgpu005.ash6 ~/repos] python perf_comp.py\r\ntorch: 20.151944875717163\r\nnumpy: 12.652233839035034\r\ntorch slower by:59.275785858018246 %\r\n\r\n(base) [bvaughan@devgpu005.ash6 ~/repos] python perf_comp.py\r\ntorch: 22.14459490776062\r\nnumpy: 11.31650972366333\r\ntorch slower by:95.68396483109345 %\r\n```\r\n\r\n\r\n\r\n```\r\ncat perf_comp.py\r\nimport torch\r\nimport time\r\nimport numpy as np\r\n\r\ndef timeit(package, dtype, iters=100000):\r\n    start = time.time()\r\n    for x in range(0, iters):\r\n        f = package.zeros(0, dtype=dtype)\r\n    duration = time.time() - start\r\n    return duration\r\n\r\n# ensure dtypes match, default differs.\r\ntorch_type = torch.float64\r\nnp_type = np.float64\r\n\r\ndef run_once(iters):\r\n    td = timeit(torch, torch_type, iters)\r\n    npd = timeit(np, np_type, iters)\r\n    print(\"torch: \" + str(td))\r\n    print(\"numpy: \" + str(npd))\r\n    print(\"torch slower by:\" + str(100 * (td / npd - 1.0)) + \" %\")\r\n    print()\r\n\r\n#warmup\r\ntimeit(np, np_type)\r\ntimeit(torch, torch_type)\r\n\r\nrun_once(10000000)\r\n```"}
{"number": 20337, "title": "[jit] move batchnorm and layernorm fusion to decompose", "time": "2019-05-09T21:15:05Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* **#20337 move batchnorm and layernorm to decompose**\r\n\r\nSummary:\r\nThe term `isFusableNorm` is confusing, since we partially fusing batchnorm and layernorm, the `isFusable` check should actually be false for these two, otherwise we will try to merge/move these two ops into FusionGroup when there's no fusion group already (createSingletonFusionGroup will try to move these two ops into a newly created FusionGroup), which is wrong in the first place. This causes some error when we put the `batchnorm` as the last op of the graph because we want to move `batchnorm` to fusion group but its inputs are not valid to move to. \r\n\r\nThis PR move these two from fuser to decomposition pass, and only decompose when the input is on a GPU device, which still enables the faster CPU inference batchnorm op by not decomposing them. It will also not changing the previous semantics, fusion will still happen after the decomposition.\r\n\r\nThis fixes #20099\n\nDifferential Revision: [D15448596](https://our.internmc.facebook.com/intern/diff/D15448596)"}
{"number": 20338, "title": "CUDA too slow", "time": "2019-05-09T21:46:17Z", "body": "Hi all,\r\n\r\nI'm working with libtorch (pytorch build 1.1) in Visual Studio 2017, Windows 10, NVIDIA Quadro P2000. Hi started by creating an initial model in python. My forward stage required 50 ms (in python with GPU). When moving to C++ (with GPU), I registered a similar result (approximately 50 ms). Then, I moved for a simple network (the original originated a .pt file of approximately 60 mb, and the last one created a file with 3 mb). In python, the forward stage required 10 ms, but in C++ it took 40 ms. I noticed that the model load was much faster (which is logic since the file is small now). I was not waiting for this result. Do you any suggestion? I also tested with the recent nightly version of pytorch, but the forward took 80 ms in C++ (really strange).  \r\n\r\nI'm using CUDA 10 and CUDNN7.1. \r\n\r\nThank you in advance. \r\n"}
{"number": 20339, "title": "Add documentation to Dispatch.h", "time": "2019-05-09T22:17:34Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20339 Add documentation to Dispatch.h**\n\nI also reordered the macros so the deprecated ones are all\nat the end.\n\nSigned-off-by: Edward Z. Yang <ezyang@mit.edu>\n\nDifferential Revision: [D15295652](https://our.internmc.facebook.com/intern/diff/D15295652)"}
{"number": 20340, "title": "[wip] Allow nested lists/dicts in legacy operator API", "time": "2019-05-09T22:54:22Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20340 [wip] Allow nested lists/dicts in legacy operator API**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15287693/)\n\n\n\nDifferential Revision: [D15287693](https://our.internmc.facebook.com/intern/diff/D15287693/)"}
{"number": 20341, "title": "Use registry for BoundShapeInferencer", "time": "2019-05-09T23:17:40Z", "body": "Summary: Att\n\nDifferential Revision: D15288131\n\n"}
{"number": 20342, "title": "TensorIterator resizes output to a scalar if there are no inputs", "time": "2019-05-10T00:10:32Z", "body": "## üêõ Bug\r\n\r\nIn master e47b210 -- May 9, 2019\r\n\r\nWhen TensorIterator is built with only a single output (and no inputs) it resizes the output to a scalar (0-dim).  The problem lies in `compute_shape`. If there are no inputs (and `resize_outputs_` is the default value of `true`) then `shape_` remains empty.\r\n\r\nThis bug is not user visible. There's no code currently that triggers it, but it makes it harder to write operators using TensorIterator.\r\n\r\nhttps://github.com/pytorch/pytorch/blob/e47b21007511e3e427ffc25ac6fca339bd7953a6/aten/src/ATen/native/TensorIterator.cpp#L516-L552\r\n\r\nDiscovered by @syed-ahmed in https://github.com/pytorch/pytorch/pull/20292#discussion_r282686933"}
{"number": 20343, "title": "torch.distributions.Binomial.sample() uses a massive amount of memory", "time": "2019-05-10T00:28:15Z", "body": "## üêõ Bug\r\n\r\nI'd like to use a random sampling process as part of my training (related to work in [this paper](https://arxiv.org/abs/1901.11365)). In my case this entails taking a binomial sample every epoch from the raw data, and using that sample to train. The dataset itself is small enough to fit into GPU memory (~8000 samples by 7000 features). Unfortunately, the binomial sample method contains [this](https://github.com/pytorch/pytorch/blob/master/torch/distributions/binomial.py#L97-L99):\r\n\r\n```python\r\nmax_count = max(int(self.total_count.max()), 1)\r\nshape = self._extended_shape(sample_shape) + (max_count,)\r\nbernoullis = torch.bernoulli(self.probs.unsqueeze(-1).expand(shape))\r\n```\r\n\r\nWhich, for my data, means that it tries to allocate a >600GB matrix to take a sample from a 240MB matrix. This is not ideal, to say the least. I guess no one has tried to sample a binomial distribution of this size before?\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Create a binomial distribution with a big max count, e.g. `b = torch.distributions.binomial.Binomial(max_count=2000, probs=0.5)`\r\n1. Try to take a big sample: `b.sample(sample_shape=torch.Size(10000, 10000)`\r\n1. Laugh when pytorch tells you to buy 745GB of RAM just so you can perform this operation\r\n\r\n## Expected behavior\r\n\r\nThe numpy version, on the same machine:\r\n\r\n```\r\n%timeit b = np.random.binomial(2000, 0.5, size=(10000, 10000))\r\n11.4 s ¬± 60.3 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\r\n```\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.0.1.post2\r\n - OS (e.g., Linux): Ubuntu16 via Docker container\r\n - How you installed PyTorch (`conda`, `pip`, source): `conda install pytorch -c torch`\r\n - Build command you used (if compiling from source): N/A\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 9.0\r\n - GPU models and configuration: running on an NVIDIA Titan XP with 12GB of RAM\r\n - Any other relevant information: N/A?\r\n"}
{"number": 20344, "title": "Problem using Pytorch-GPU", "time": "2019-05-10T00:55:15Z", "body": "Hi all,\r\n\r\nI am having trouble to run my code using GPU but it works totally fine on CPU. On GPU I get the following error:\r\n\r\n```\r\n/var/spool/slurmd/job1126990/slurm_script: line 32: 26153 Killed \r\npython $SLURM_SUBMIT_DIR/$my_file.py slurmstepd: error: \r\nDetected 3 oom-kill event(s) in step 1126990.batch cgroup. \r\nSome of your processes may have been killed by the cgroup out-of-memory handler.\r\n```\r\n\r\nIn order to avoid the out of memory error I converted my mat file to h5 and I do not load all of my dataset into memory at the same time. My code runs perfectly fine on CPU. But this time I get the following error:\r\n\r\n```\r\nTraceback (most recent call last):                                              \r\n  File \"/scratch/mkhoshle/dl_final_project/gpu/Training.py\", line 8, in <module>\r\n    from torchvision import datasets                                            \r\n  File \"/home/mkhoshle/miniconda3/envs/ml-gpu/lib/python3.6/site-packages/torchvision/__init__.py\", line 1, in <module>\r\n    from torchvision import models                                              \r\n  File \"/home/mkhoshle/miniconda3/envs/ml-gpu/lib/python3.6/site-packages/torchvision/models/__init__.py\", line 62, in <module>\r\n    from .alexnet import *                                                      \r\n  File \"/home/mkhoshle/miniconda3/envs/ml-gpu/lib/python3.6/site-packages/torchvision/models/alexnet.py\", line 1, in <module>\r\n    import torch.nn as nn                                                       \r\n  File \"/home/mkhoshle/miniconda3/envs/ml-gpu/lib/python3.6/site-packages/torch/__init__.py\", line 53, in <module>\r\n    from torch._C import *                                                      \r\nImportError: libiomp5.so: cannot open shared object file: No such file or directory\r\n```\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\n\r\nOS: CentOS Linux release 7.4.1708 (Core) \r\nGCC version: (GCC) 8.2.0\r\nCMake version: version 2.8.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: N/A\r\nCUDA runtime version: 8.0.61\r\nGPU models and configuration: GPU 0: GeForce GTX 1080 Ti\r\nNvidia driver version: 396.26\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] cuda80                    1.0                           0    soumith\r\n[conda] cuda90                    1.0                  h6433d27_0    pytorch\r\n[conda] cuda92                    1.0                           0    pytorch\r\n[conda] libtorch-gpu              0.1.12                  nomkl_0  \r\n[conda] mkl                       2019.3                      199  \r\n[conda] pytorch                   1.0.0           py3.6_cuda9.0.176_cudnn7.4.1_1    soumith\r\n[conda] torch-nightly             1.0.0.dev20181206          pypi_0    pypi\r\n[conda] torchfile                 0.1.0                      py_0    conda-forge\r\n[conda] torchvision               0.2.2.post3              pypi_0    pypi\r\n\r\n```\r\n\r\nAnd here is how I installed pytorch:\r\n\r\n```\r\n# Private conda environment                                                     \r\nsource activate ml-gpu\r\n                                                                 \r\nmodule load cuda/8.0.61                                                         \r\nmodule load gcc/8.2.0                                                                                                   \r\nmodule load cudnn/7.0                                                           \r\nmodule list:\r\nCurrently Loaded Modulefiles:\r\n  1) cuda/8.0.61       2) binutils/latest   3) gcc/8.2.0         4) cudnn/7.0\r\n\r\nconda install pytorch cuda80 -c soumith\r\nconda install torchvision cuda80 -c soumith\r\n```\r\nDoes anyone knows what is the problem with my installation?"}
{"number": 20345, "title": "Optimize pytorch layer_norm forward", "time": "2019-05-10T01:06:25Z", "body": "Summary:\nSeperate from D15194600\nOptimize pytorch layer_norm op part 1:\n  optimize layer_norm_forward_cpu\n  import Eigen Maps for the performance of reduction\n\nDifferential Revision: D15290608\n\n"}
{"number": 20346, "title": "[wip] Generate TH Functions into LegacyTHFunctions", "time": "2019-05-10T01:14:12Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20346 Generate TH Functions into LegacyTHFunctions**\n* #20309 [wip] Wrap TH functions with native functions\n* #20308 [wip] Change view dispatch to abstract\n\nDifferential Revision: [D15290831](https://our.internmc.facebook.com/intern/diff/D15290831)"}
{"number": 20347, "title": "[RFC] Make sure sparse tensor smm works after making VariableType unpack() call tensor_data()", "time": "2019-05-10T03:01:24Z", "body": "As part of the Variable/Tensor merge, we will make VariableType `unpack()` call `var.tensor_data()` to get the equivalent Tensor of a Variable. However, `var.tensor_data()` makes a shallow copy of `var`, and size changes to the shallow copy will not be reflected back to the original tensor. This causes problem in the `smm()` path. Consider the following example (`TestSparse.test_saddmm`):\r\n\r\n```cpp\r\n// aten/src/ATen/native/sparse/SparseTensorMath.cpp:827\r\nTensor smm(const Tensor& self, const Tensor& mat2) {\r\n  auto result = at::empty({0}, self.options());\r\n  // [MY COMMENT] Here, we expect the first arg and the second arg share the same tensor\r\n  at::sspaddmm_out(result, result, self, mat2, 0.0, 1.0);\r\n  return result;\r\n}\r\n```\r\n\r\n```cpp\r\n// torch/csrc/autograd/generated/VariableType_1.cpp:17480\r\nTensor & VariableType::sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {\r\n  ...\r\n  // [MY COMMENT] Since unpack() makes shallow copy, `out_` and `self_` are not pointing to the same tensor anymore\r\n  auto out_ = unpack(out, \"out\", 0);\r\n  auto self_ = unpack(self, \"self\", 1);\r\n  ...\r\n  {\r\n    ...\r\n    baseType->sspaddmm_out(out_, self_, mat1_, mat2_, beta, alpha);\r\n  }\r\n}\r\n```\r\n\r\n```cpp\r\n// aten/src/ATen/native/sparse/SparseTensorMath.cpp:741\r\nSparseTensor& _sspaddmm_out_cpu(\r\n    SparseTensor& r,\r\n    const SparseTensor& t,\r\n    const SparseTensor& sparse_,\r\n    const Tensor& dense,\r\n    Scalar beta,\r\n    Scalar alpha\r\n) {\r\n  ...\r\n  // [original comment] NB: This has to occur before the checks, because r may alias t.\r\n  // [original comment] See test_saddmm\r\n  // [MY COMMENT] : Since r doesn't alias t anymore, resizing r will not resize t, which causes the size check to fail in subsequent lines.\r\n  get_sparse_impl(r)->raw_resize_(2, 0, {dim_i, dim_k});\r\n  ...\r\n  // [MY COMMENT] This fails, because t is not resized along with r.\r\n  AT_CHECK(t.size(0) == dim_i,\r\n      \"sspaddmm: Argument #1: Expected dim 0 size \", dim_i, \", got \", t.size(0));\r\n  ...\r\n}\r\n```\r\n\r\nOne solution is to resize the `result` tensor in `smm()` properly before going into `at::sspaddmm_out()`, so that we don't need to rely on the tensor aliasing to have both `r` and `t` resized properly."}
{"number": 20348, "title": "[RFC] Make SparseTensorRef the same as Tensor", "time": "2019-05-10T03:26:28Z", "body": "As part of the Variable/Tensor merge, we will make VariableType `unpack()` call `var.tensor_data()` to get the equivalent Tensor of a Variable. Under the hood, `var.tensor_data()` return a temporary object which is a shallow copy of `var`. This causes problem with the sparse tensor version of `unpack()`:\r\n```cpp\r\n// torch/csrc/autograd/VariableTypeManual.cpp\r\nSparseTensorRef VariableType::unpack(SparseTensorRef t, const char * name, int pos) {\r\n  return SparseTensorRef(checked_cast_variable(t.tref, name, pos).tensor_data());\r\n}\r\n```\r\n```cpp\r\n// aten/src/ATen/core/SparseTensorRef.h\r\nclass Tensor;\r\nstruct SparseTensorRef {\r\n  explicit SparseTensorRef(const Tensor& t): tref(t) {}\r\n  const Tensor& tref;\r\n};\r\n```\r\nIn the first code block for `unpack()`, `.tensor_data()` returns a temporary object, but since SparseTensorRef only stores a const reference of Tensor `tref` (which doesn't extend the lifetime of a temporary according to https://stackoverflow.com/questions/2784262/does-a-const-reference-class-member-prolong-the-life-of-a-temporary), after `unpack()` is done, the tensor referenced by SparseTensorRef `tref` is gone, and accessing the returned value of `unpack()` will result in error. One example of such failure is in `VariableType::sparse_mask`:\r\n\r\n```cpp\r\nTensor VariableType::sparse_mask(const Tensor & self, SparseTensorRef mask) const {\r\n  ...\r\n  auto mask_ = unpack(mask, \"mask\", 1);\r\n  std::cout << \"mask_: \" << mask_.tref << std::endl;  // This segfaults! Because `tref` can't hold a temporary object\r\n  ...\r\n}\r\n```\r\n\r\nOne solution to this is to make SparseTensorRef the same as Tensor, and allow sparse tensors to be passed around using the Tensor class. The reasoning is that since Tensor is just wrapper of TensorImpl, and copying a Tensor doesn't deep copy the underlying TensorImpl, there should be no need to have another SparseTensorRef wrapper class that wraps the Tensor, which will also solve the problem we encounter here."}
{"number": 20349, "title": "Make sure advanced pooling is not used when fp16 training is enabled", "time": "2019-05-10T05:24:56Z", "body": "\r\n\r\nDifferential Revision: D15291545\r\n\r\n"}
{"number": 20350, "title": "libtorch problem on windows: fatBinaryCtl_CreateHandle", "time": "2019-05-10T05:35:22Z", "body": "\r\n## Environment\r\n- libtorch version win-cu90-1.1.0\r\n- cuda-driver 10.1\r\n- gpu devices GTX1080 and RTX2070 (on one computer)\r\n\r\n## Description\r\non gtx1080, the trace-model can be loaded through jit, but an error occurs on rtx2070,it says  fatBinaryCtl_CreateHandle can not be located on C:\\WINDOWS\\system32\\nvcuda.dll. I have tested successfully on notebooks with GTX1060,GTX965M,GTX920M,GTX960M,GTX940MX. The issue looks like has something to do with the cuda-driver  or the new RTX card ?\r\n\r\nany device or help ? thank you.\r\n\r\n"}
{"number": 20351, "title": "Fix DistributedDataParallelTest.test_accumulate_gradients", "time": "2019-05-10T05:55:31Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20351 Fix DistributedDataParallelTest.test_accumulate_gradients**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15292786/)\n\nThis was broken because of a merge race between #20282 and the stack in #20236.\n\nCleaned up the test and comments a bit as well.\n\nDifferential Revision: [D15292786](https://our.internmc.facebook.com/intern/diff/D15292786/)"}
{"number": 20352, "title": "[jit] Convenience APIs for script objects", "time": "2019-05-10T06:13:58Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20352 [jit] Convenience APIs for script objects**\n\nThe compiler should be emitting static lookups to object slots, but\npeople using the C++ API should have some sugar for doing attribute\nlookup."}
{"number": 20353, "title": "Some essential changes needed before updating the Windows AMI", "time": "2019-05-10T07:10:01Z", "body": "1. Add cuda 10.1 build\r\n2. Turn on openmp loop support for VS 2019\r\n3. Remove legacy code about selective builds\r\n\r\nTested through CI."}
{"number": 20354, "title": "Update installing.rst", "time": "2019-05-10T08:43:04Z", "body": "Delete useless `cd`\r\n\r\n"}
{"number": 20355, "title": "Backwards hangs", "time": "2019-05-10T09:15:12Z", "body": "Training is hanging during a call to backwards - I might be wrong but it looks like it hangs while one of the threads tries to acquire a lock.\r\n\r\nThe setup is the following:\r\n - pytorch version is 1.0.0post2\r\n - start 4 python processes on same machine (using multiprocessing spawn, but same happens with Popen)\r\n - each process uses a different gpu and different data (all data is loaded into memory prior to start training)\r\n - use torch.distributed (nccl) to synchronise training; all communication between different processes happens via nccl\r\n\r\nWhat I observe is that at random times, one of the processes hangs - and the others then wait for it in the call to all_reduce.\r\n\r\nUnfortunately, it is hard to reproduce, and it can take an arbitrary amount of time to hang - sometimes it hangs after 10min, sometimes it hangs after 10h+.\r\n\r\nAny help is appreciated - thanks.\r\n\r\nAt the moment where it hangs, the threads of the frozen process are in this state:\r\n> (gdb) info threads\r\n  Id   Target Id         Frame\r\n  16   Thread 0x7f22711b9700 (LWP 23578) \"python\" 0x00007f22d64fff0d in poll () from /lib64/libc.so.6\r\n  15   Thread 0x7f2270943700 (LWP 23614) \"python\" 0x00007f22d650c03f in accept4 () from /lib64/libc.so.6\r\n  14   Thread 0x7f2265fff700 (LWP 23644) \"python\" 0x00007f22d64fff0d in poll () from /lib64/libc.so.6\r\n  13   Thread 0x7f22617fe700 (LWP 30293) \"python\" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n  12   Thread 0x7f22609ed800 (LWP 30294) \"python\" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n  11   Thread 0x7f22605eb880 (LWP 30295) \"python\" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n  10   Thread 0x7f224dffe900 (LWP 30296) \"python\" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n  9    Thread 0x7f224dbfc980 (LWP 30297) \"python\" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n  8    Thread 0x7f224d7faa00 (LWP 30298) \"python\" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n  7    Thread 0x7f224d3f8a80 (LWP 30299) \"python\" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n  6    Thread 0x7f221aee5700 (LWP 68788) \"python\" 0x00007f22d70f851d in __lll_lock_wait () from /lib64/libpthread.so.0\r\n  5    Thread 0x7f221a6e4700 (LWP 68789) \"python\" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n  4    Thread 0x7f2219ee3700 (LWP 68790) \"python\" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n  3    Thread 0x7f22196e2700 (LWP 68791) \"python\" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n  2    Thread 0x7f2218ee1700 (LWP 68792) \"python\" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n  1    Thread 0x7f22d750f740 (LWP 23478) \"python\" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n\r\nThe stack trace per thread is below.\r\n\r\nThread 1:\r\n>#0  0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n#1  0x00007f22bfb9656f in __gthread_cond_wait (__mutex=<optimized out>, __cond=<optimized out>)\r\n    at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/build/build-cc-gcc-final/x86_64-conda_cos6-linux-gnu/libstdc++-v3/include/x86_64-conda_cos6-linux-gnu/bits/gthr-default.h:877\r\n#2  std::condition_variable::wait (this=<optimized out>, __lock=...) at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/src/c++11/condition_variable.cc:53\r\n#3  0x00007f2278eb08e3 in torch::autograd::Engine::execute(std::vector<torch::autograd::Edge, std::allocator<torch::autograd::Edge> > const&, std::vector<torch::autograd::Variable, std::allocator<torch::autograd::Variable> > const&, bool, bool, std::vector<torch::autograd::Edge, std::allocator<torch::autograd::Edge> > const&) () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch.so.1\r\n#4  0x00007f22bc696a0c in torch::autograd::python::PythonEngine::execute(std::vector<torch::autograd::Edge, std::allocator<torch::autograd::Edge> > const&, std::vector<torch::autograd::Variable, std::allocator<torch::autograd::Variable> > const&, bool, bool, std::vector<torch::autograd::Edge, std::allocator<torch::autograd::Edge> > const&) () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so\r\n#5  0x00007f22bc69722c in THPEngine_run_backward(THPEngine*, _object*, _object*) () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so\r\n...\r\n\r\nThreads 2-5:\r\n>#0  0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n#1  0x00007f22bfb9656f in __gthread_cond_wait (__mutex=<optimized out>, __cond=<optimized out>)\r\n    at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/build/build-cc-gcc-final/x86_64-conda_cos6-linux-gnu/libstdc++-v3/include/x86_64-conda_cos6-linux-gnu/bits/gthr-default.h:877\r\n#2  std::condition_variable::wait (this=<optimized out>, __lock=...) at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/src/c++11/condition_variable.cc:53\r\n#3  0x00007f2278eac92b in torch::autograd::ReadyQueue::pop() () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch.so.1\r\n#4  0x00007f2278eaf083 in torch::autograd::Engine::thread_main(torch::autograd::GraphTask*) () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch.so.1\r\n#5  0x00007f2278eaba77 in torch::autograd::Engine::thread_init(int) () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch.so.1\r\n#6  0x00007f22bc6968aa in torch::autograd::python::PythonEngine::thread_init(int) () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so\r\n#7  0x00007f22bfb9a678 in std::execute_native_thread_routine_compat (__p=<optimized out>) at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/src/c++11/thread.cc:94\r\n#8  0x00007f22d70f1e25 in start_thread () from /lib64/libpthread.so.0\r\n#9  0x00007f22d650abad in clone () from /lib64/libc.so.6\r\n\r\nThread 6:\r\n>#0  0x00007f22d70f851d in __lll_lock_wait () from /lib64/libpthread.so.0\r\n#1  0x00007f22d70f61a0 in pthread_cond_broadcast@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n#2  0x00007f22bfb96594 in __gthread_cond_broadcast (__cond=<optimized out>)\r\n    at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/build/build-cc-gcc-final/x86_64-conda_cos6-linux-gnu/libstdc++-v3/include/x86_64-conda_cos6-linux-gnu/bits/gthr-default.h:865\r\n#3  std::condition_variable::notify_all (this=<optimized out>) at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/src/c++11/condition_variable.cc:73\r\n#4  0x00007f2278eaf167 in torch::autograd::Engine::thread_main(torch::autograd::GraphTask*) () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch.so.1\r\n#5  0x00007f2278eaba77 in torch::autograd::Engine::thread_init(int) () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch.so.1\r\n#6  0x00007f22bc6968aa in torch::autograd::python::PythonEngine::thread_init(int) () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so\r\n#7  0x00007f22bfb9a678 in std::execute_native_thread_routine_compat (__p=<optimized out>) at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/src/c++11/thread.cc:94\r\n#8  0x00007f22d70f1e25 in start_thread () from /lib64/libpthread.so.0\r\n#9  0x00007f22d650abad in clone () from /lib64/libc.so.6\r\n\r\nThread 7-13:\r\n>#0  0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n#1  0x00007f22cb364725 in __kmp_suspend_64 () from /opt/anaconda3/lib/python3.7/site-packages/numpy/../../../libiomp5.so\r\n#2  0x00007f22cb2d5e4e in bool _INTERNAL_25_______src_kmp_barrier_cpp_3dc39ea5::__kmp_wait_template<kmp_flag_64, 1, false, true>(kmp_info*, kmp_flag_64*, void*) ()\r\n   from /opt/anaconda3/lib/python3.7/site-packages/numpy/../../../libiomp5.so\r\n#3  0x00007f22cb2d9398 in _INTERNAL_25_______src_kmp_barrier_cpp_3dc39ea5::__kmp_hyper_barrier_release(barrier_type, kmp_info*, int, int, int, void*) () from /opt/anaconda3/lib/python3.7/site-packages/numpy/../../../libiomp5.so\r\n#4  0x00007f22cb2df4d2 in __kmp_fork_barrier(int, int) () from /opt/anaconda3/lib/python3.7/site-packages/numpy/../../../libiomp5.so\r\n#5  0x00007f22cb320d8e in __kmp_launch_thread () from /opt/anaconda3/lib/python3.7/site-packages/numpy/../../../libiomp5.so\r\n#6  0x00007f22cb360571 in _INTERNAL_26_______src_z_Linux_util_cpp_51eec780::__kmp_launch_worker(void*) () from /opt/anaconda3/lib/python3.7/site-packages/numpy/../../../libiomp5.so\r\n#7  0x00007f22d70f1e25 in start_thread () from /lib64/libpthread.so.0\r\n#8  0x00007f22d650abad in clone () from /lib64/libc.so.6\r\n\r\nThread 14:\r\n>#0  0x00007f22d64fff0d in poll () from /lib64/libc.so.6\r\n#1  0x00007f22beb8a323 in ?? () from /lib64/libcuda.so.1\r\n#2  0x00007f22bebecacd in ?? () from /lib64/libcuda.so.1\r\n#3  0x00007f22beb8c988 in ?? () from /lib64/libcuda.so.1\r\n#4  0x00007f22d70f1e25 in start_thread () from /lib64/libpthread.so.0\r\n#5  0x00007f22d650abad in clone () from /lib64/libc.so.6\r\n\r\nThread 15:\r\n>#0  0x00007f22d650c03f in accept4 () from /lib64/libc.so.6\r\n#1  0x00007f22beb8b2ca in ?? () from /lib64/libcuda.so.1\r\n#2  0x00007f22beb7d8dd in ?? () from /lib64/libcuda.so.1\r\n#3  0x00007f22beb8c988 in ?? () from /lib64/libcuda.so.1\r\n#4  0x00007f22d70f1e25 in start_thread () from /lib64/libpthread.so.0\r\n#5  0x00007f22d650abad in clone () from /lib64/libc.so.6\r\n\r\nThread 16:\r\n>#0  0x00007f22d64fff0d in poll () from /lib64/libc.so.6\r\n#1  0x00007f22bca13879 in c10d::TCPStoreDaemon::run() () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so\r\n#2  0x00007f22bfb9a678 in std::execute_native_thread_routine_compat (__p=<optimized out>) at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/src/c++11/thread.cc:94\r\n#3  0x00007f22d70f1e25 in start_thread () from /lib64/libpthread.so.0\r\n#4  0x00007f22d650abad in clone () from /lib64/libc.so.6"}
{"number": 20356, "title": "Load tensor from file in C++ fails ", "time": "2019-05-10T09:27:51Z", "body": "## üêõ Bug\r\n\r\nI don't manage to import a tensor saved in PyTorch from C++. Any help on this is welcome üòÑ \r\n\r\n## To Reproduce\r\n\r\nSave a tensor to a file in python.\r\n\r\n```python\r\n>>> import torch\r\n>>> torch.save(torch.tensor([1., 2., 3.]), \"tensor.pt\")\r\n>>> torch.load(\"tensor.pt\")\r\ntensor([1., 2., 3.])\r\n```\r\n\r\nBuild this small C++ program\r\n\r\n```cpp\r\n#include <torch/torch.h>\r\n\r\nint main() {\r\n  torch::Tensor tensor;\r\n  torch::load(tensor, \"tensor.pt\");\r\n  std::cout << tensor << std::endl;\r\n}\r\n```\r\n\r\nAt runtime I get:\r\n\r\n```\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  [enforce fail at inline_container.cc:137] . PytorchStreamReader failed reading zip archive: failed finding central directory\r\nframe #0: std::function<std::string ()>::operator()() const + 0x11 (0x7f1da515e691 in /___/libtorch/lib/libc10.so)\r\nframe #1: c10::ThrowEnforceNotMet(char const*, int, char const*, std::string const&, void const*) + 0x49 (0x7f1da515e4a9 in /__/libtorch/lib/libc10.so)\r\nframe #2: caffe2::serialize::PyTorchStreamReader::valid(char const*) + 0x6b (0x7f1d9c69735b in /__/libtorch/lib/libcaffe2.so)\r\nframe #3: caffe2::serialize::PyTorchStreamReader::init() + 0x9d (0x7f1d9c69912d in /___/libtorch/lib/libcaffe2.so)\r\nframe #4: caffe2::serialize::PyTorchStreamReader::PyTorchStreamReader(std::unique_ptr<caffe2::serialize::ReadAdapterInterface, std::default_delete<caffe2::serialize::ReadAdapterInterface> >) + 0x3b (0x7f1d9c69ab8b in /__/libtorch/lib/libcaffe2.so)\r\nframe #5: <unknown function> + 0xa9b5bf (0x7f1da5e125bf in /__/libtorch/lib/libtorch.so.1)\r\nframe #6: torch::jit::load(std::unique_ptr<caffe2::serialize::ReadAdapterInterface, std::default_delete<caffe2::serialize::ReadAdapterInterface> >, c10::optional<c10::Device>, std::unordered_map<std::string, std::string, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::string> > >&) + 0x9a (0x7f1da5e15d2a in /__/libtorch/lib/libtorch.so.1)\r\nframe #7: torch::jit::load(std::string const&, c10::optional<c10::Device>, std::unordered_map<std::string, std::string, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::string> > >&) + 0x68 (0x7f1da5e15ec8 in /___/libtorch/lib/libtorch.so.1)\r\nframe #8: torch::serialize::InputArchive::load_from(std::string const&, c10::optional<c10::Device>) + 0x38 (0x7f1da604c058 in /___/libtorch/lib/libtorch.so.1)\r\nframe #9: void torch::load<at::Tensor, char const (&) [10]>(at::Tensor&, char const (&) [10]) + 0x86 (0x55a502f5fe97 in ./load)\r\nframe #10: main + 0x37 (0x55a502f5fb51 in ./load)\r\nframe #11: __libc_start_main + 0xe7 (0x7f1d9a338b97 in /lib/x86_64-linux-gnu/libc.so.6)\r\nframe #12: _start + 0x2a (0x55a502f5f96a in ./load)\r\n\r\n[1]    30788 abort      ./load\r\n```\r\n## Expected behavior\r\n\r\nI would expect to be able to load a tensor from the PyTorch dump in the C++ program.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.105\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX TITAN X\r\nGPU 1: GeForce GTX TITAN X\r\nGPU 2: GeForce GTX TITAN X\r\nGPU 3: GeForce GTX TITAN X\r\nGPU 4: GeForce GTX TITAN X\r\nGPU 5: GeForce GTX TITAN X\r\nGPU 6: GeForce GTX TITAN X\r\nGPU 7: GeForce GTX TITAN X\r\n\r\nNvidia driver version: 418.56\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] msgpack-numpy==0.4.3.2\r\n[pip] numpy==1.15.4\r\n[pip] numpydoc==0.8.0\r\n[pip] pytorch-nlp==0.4.1\r\n[pip] torch==1.1.0\r\n[pip] torchvision==0.2.2\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2018.0.3                      1\r\n[conda] mkl-service               1.1.2            py37h90e4bf4_5\r\n[conda] mkl_fft                   1.0.6            py37h7dd41cf_0\r\n[conda] mkl_random                1.0.1            py37h4414c95_1\r\n[conda] pytorch                   1.1.0           py3.7_cuda9.0.176_cudnn7.5.1_0    pytorch\r\n[conda] pytorch-nlp               0.4.1                    pypi_0    pypi\r\n[conda] torchvision               0.2.2                      py_3    pytorch\r\n\r\n## Additional context\r\n\r\nI downloaded the C++ frontend from [https://download.pytorch.org/libtorch/nightly/cpu/libtorch-shared-with-deps-latest.zip](https://download.pytorch.org/libtorch/nightly/cpu/libtorch-shared-with-deps-latest.zip) yesterday.\r\n"}
{"number": 20357, "title": "Process finished with exit code 139 (interrupted by signal 11: SIGSEGV) ctc gpu ", "time": "2019-05-10T10:04:23Z", "body": ""}
{"number": 20358, "title": "Build Libtorch source in Windows", "time": "2019-05-10T11:41:29Z", "body": "Hi,\r\n\r\nI'm trying to create the libtorch project using CMAKE 3.13.1. I'm using Windows 10, CUDA 10, pytorch 1.1, CUDNN 7.5. \r\n\r\nHowever, while configuring the project I'm getting the following error: \r\n\r\n```\r\nCMake Error at cmake/ProtoBuf.cmake:53 (add_subdirectory):\r\n  add_subdirectory given source\r\n  \"C:/Code/ExternalLibraries/pytorch-master/cmake/../third_party/protobuf/cmake\"\r\n  which is not an existing directory.\r\n```\r\n```\r\nCMake Error at cmake/ProtoBuf.cmake:53 (add_subdirectory):\r\n  add_subdirectory given source\r\n  \"C:/Code/ExternalLibraries/pytorch-master/cmake/../third_party/protobuf/cmake\"\r\n  which is not an existing directory.\r\n```\r\n```\r\nCMake Error at cmake/Dependencies.cmake:1039 (add_subdirectory):\r\n  The source directory\r\n\r\n    C:/Code/ExternalLibraries/pytorch-master/third_party/onnx\r\n\r\n  does not contain a CMakeLists.txt file.\r\n```\r\n\r\n```\r\nCMake Error at caffe2/CMakeLists.txt:210 (INSTALL):\r\n  INSTALL TARGETS given no ARCHIVE DESTINATION for static library target\r\n  \"caffe2_protos\".\r\n```\r\n\r\nDo you have any suggestion? Thank you in advance. "}
{"number": 20359, "title": "[feature request] Run examples from docs as tests", "time": "2019-05-10T12:16:33Z", "body": "Quite a few times (e.g. https://github.com/pytorch/pytorch/issues/20301) some examples from docs became obsolete and ran unnoticed.\r\n\r\nProposal: ability to mark some examples in docs as tests and run them on CI and check execution for errors, warnings etc."}
{"number": 20360, "title": "Make check-doxygen.sh output more interpretable.", "time": "2019-05-10T13:12:27Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20360 Make check-doxygen.sh output more interpretable.**\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>"}
{"number": 20361, "title": "[TESTING]", "time": "2019-05-10T13:22:21Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20361 [TESTING]**\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>"}
{"number": 20362, "title": "Make check-doxygen.sh output more interpretable.", "time": "2019-05-10T13:37:23Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20362 Make check-doxygen.sh output more interpretable.**\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nDifferential Revision: [D15375139](https://our.internmc.facebook.com/intern/diff/D15375139)"}
{"number": 20363, "title": "Define portable M_PI replacement, use it instead of non-standard M_PI in math.h", "time": "2019-05-10T13:57:33Z", "body": "A common mistake is to use `M_PI` from `<math.h>` which is not portable on Windows. Standard workaround is to `#define _USE_MATH_DEFINES` before including the header. We should have a shim header that handles this for you, and then add a lint rule to forbid direct inclusion of `math.h`. Alternately, if only uses of `M_PI` are in cpp files, we can just add `-D_USE_MATH_DEFINES` to our Windows compiler flags (this approach is not permissible if they occur in `M_PI`\r\n\r\nRecent occurrence: https://github.com/pytorch/pytorch/pull/19316"}
{"number": 20364, "title": "RoiAlignTest.CheckCPUGPUEqual hangs on ROCm", "time": "2019-05-10T14:06:11Z", "body": "Sample log: https://ci.pytorch.org/jenkins/job/caffe2-builds/job/py2-clang7-rocmdeb-ubuntu16.04-test/20240/console\r\n\r\n```\r\n07:33:47 [==========] Running 1 test from 1 test case.\r\n07:33:47 [----------] Global test environment set-up.\r\n07:33:47 [----------] 1 test from RoiAlignTest\r\n07:33:47 [ RUN      ] RoiAlignTest.CheckCPUGPUEqual\r\n07:33:47 WARNING: Logging before InitGoogleLogging() is written to STDERR\r\n07:33:47 W0510 07:33:47.448523 10009 init.h:115] Caffe2 GlobalInit should be run before any other API calls.\r\n08:51:37 Build timed out (after 90 minutes). Marking the build as failed.\r\n08:51:37 /tmp/jenkins5007338337181727856.sh: line 208: 24450 Done                    ( echo \"cd $docker_workspace\"; if [ \r\n```\r\n\r\ncc @bddppq @iotamudelta "}
{"number": 20365, "title": "quantize_rnn_modules in ensemble_export", "time": "2019-05-10T15:12:57Z", "body": "Summary: Enable quantization of `torch.nn.LSTM` module in the decoder of PyTorch natively exported beam search models.\n\nDifferential Revision: D15260631\n\n"}
{"number": 20366, "title": "Avoid dynamic dispatch inside the omp loop in AdaptiveAvgPool2d", "time": "2019-05-10T15:37:17Z", "body": "This PR changes CPU implementation of `AdaptiveAveragePool2D` by\r\n- move dispatch to outside the OpenMP loop\r\n- support fp16"}
{"number": 20367, "title": "Overhead performance regression over time umbrella issue.", "time": "2019-05-10T16:12:20Z", "body": "This issue is meant to collect various performance-regression-over-time bug reports that aren't specific op regressions, that almost certainly overlap, but which we should track separately to make sure we cover all the cases.\r\n\r\nTo start:\r\nhttps://github.com/pytorch/pytorch/issues/5388\r\nhttps://github.com/pytorch/pytorch/issues/16717\r\nhttps://github.com/pytorch/pytorch/issues/2560\n\ncc @ezyang @gchanan @zou3519 @VitalyFedyunin @ngimel @mruberry"}
{"number": 20368, "title": "make trace's errors more helpful in terms of what it can and can't do when tracing module's methods", "time": "2019-05-10T16:56:00Z", "body": ""}
{"number": 20369, "title": "Fixed histc return type for CUDA", "time": "2019-05-10T17:21:45Z", "body": "Fixing reported [issue](https://github.com/pytorch/pytorch/issues/20208)."}
{"number": 20370, "title": "Functions: conv2d, sum_pool2d for LongTensor", "time": "2019-05-10T18:00:04Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20370 conv2d implmentation for LongTensor**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15192353/)\n\n- enable generation of conv2d for LongTensor\n\nDifferential Revision: [D15192353](https://our.internmc.facebook.com/intern/diff/D15192353/)"}
{"number": 20371, "title": "Eliminate a const_cast.", "time": "2019-05-10T18:11:20Z", "body": ""}
{"number": 20372, "title": "Dict", "time": "2019-05-10T18:21:15Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20383 [wip] torch::jit::RegisterOperators forwards to c10::RegisterOperators&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15300937/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20514 Options based registration API&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15346348/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20513 Infer schema for experimental ops&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15346349/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20379 Allow nested lists/dicts in legacy operator API&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15287693/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20374 Extend testAvailableArgTypes&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15298233/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20373 Allow Dict type in c10 operators&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15298235/)\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20372 Dict**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15298234/)\n\nImplement a Dict type that allows us to abstract away from the concrete implementation used.\nThe API is similar to std::unordered_map, but behind the scenes we can switch to any map implementation we like. ska::flat_hash_map, google dense map, or any future map implementation with better performance.\nSwitching such an implementation choice does not have to break backwards compatibility of kernel code using the Dict type.\n\nDifferential Revision: [D15298234](https://our.internmc.facebook.com/intern/diff/D15298234/)"}
{"number": 20373, "title": "Allow Dict type in c10 operators", "time": "2019-05-10T18:21:25Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20383 [wip] torch::jit::RegisterOperators forwards to c10::RegisterOperators&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15300937/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20514 Options based registration API&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15346348/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20513 Infer schema for experimental ops&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15346349/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20379 Allow nested lists/dicts in legacy operator API&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15287693/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20374 Extend testAvailableArgTypes&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15298233/)\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20373 Allow Dict type in c10 operators**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15298235/)\n\n- Add support for Dict<Key, Value> arguments and returns to c10 operators\n- Add support for std::unordered_map<Key, Value> to the legacy API (but not to c10 kernels)\n\nDifferential Revision: [D15298235](https://our.internmc.facebook.com/intern/diff/D15298235/)"}
{"number": 20374, "title": "Extend testAvailableArgTypes", "time": "2019-05-10T18:21:34Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20383 [wip] torch::jit::RegisterOperators forwards to c10::RegisterOperators&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15300937/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20514 Options based registration API&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15346348/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20513 Infer schema for experimental ops&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15346349/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20379 Allow nested lists/dicts in legacy operator API&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15287693/)\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20374 Extend testAvailableArgTypes**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15298233/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20373 Allow Dict type in c10 operators&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15298235/)\n\nThis test case now also tests that the argument type works correctly in kernels that\n- don't return outputs\n- return multiple outputs\n\nDifferential Revision: [D15298233](https://our.internmc.facebook.com/intern/diff/D15298233/)"}
{"number": 20375, "title": "Better documentation / molly-guards around use of multiprocessing with spawn in Jupyter/ipython notebooks", "time": "2019-05-10T18:23:26Z", "body": "Apparently, it's really popular for users to use multiprocessing + spawn in ipython/Jupyter notebooks. For example, #17680; also, https://fb.workplace.com/groups/1405155842844877/permalink/2766399306720517/ (FB-only)\r\n\r\nThere is a loaded footgun that occurs when you combine these three ingredients: the spawn method will **run everything in your notebook top-level** (and you probably have live statements on your notebook top-level; that's why you're running in a notebook) before actually running the requested function in question. This is unexpected for users, who just expected to be directly dropped into the function in question, and leads to all sorts of fun (`mp.set_start_method` reports that it's already been called; deadlock when the child process starts trying to run the same thing again).\r\n\r\nIt would be *really good* to document this somewhere people are actually going to read it, and maybe add some runtime checks to detect if this situation is happening and help users do the right thing.\r\n\r\ncc @colesbury @chandlerzuo "}
{"number": 20376, "title": "assertEqual in tests ignores dtype when comparing tensors.", "time": "2019-05-10T18:55:28Z", "body": "In unit tests, a check like this: \r\n`self.assertEqual(torch.tensor([1], dtype=torch.int), torch.tensor([1], dtype=torch.double))` should fail but it doesnt.\n\ncc @ezyang @gchanan @zou3519"}
{"number": 20377, "title": "Remove dependencies from Caffe2Go on PyTorch JIT", "time": "2019-05-10T19:31:33Z", "body": "Summary:\nSource file changes mostly involve ifdef'ing-out references to JIT code\nfrom files that are part of Caffe2Go.  Update Internal build scripts to\nremove those files from our globs.\n\nAfter this, changes to most of the JIT files should not trigger mobile CI.\n\nDifferential Revision: D15283908\n\n"}
{"number": 20378, "title": "Not for commit: trigger Caffe2 and downstream tests [v6]", "time": "2019-05-10T19:34:07Z", "body": "Summary: Just trying to find bad tests :)\n\nDifferential Revision: D14716554\n\n"}
{"number": 20379, "title": "Allow nested lists/dicts in legacy operator API", "time": "2019-05-10T19:38:20Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20383 [wip] torch::jit::RegisterOperators forwards to c10::RegisterOperators&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15300937/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20561 De-deprecate parts of the legacy API&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15364271/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20514 Options based registration API&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15346348/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20513 Infer schema for experimental ops&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15346349/)\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20379 Allow nested lists/dicts in legacy operator API**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15287693/)\n\nThe legacy custom op API allowed nesting of std::unordered_map and std::vector. While we haven't figured out yet how to do that with the new API,\nwe at least have to keep backwards compatibility. This diff adds the feature so we can switch to the new API without breaking third party code.\n\nDifferential Revision: [D15287693](https://our.internmc.facebook.com/intern/diff/D15287693/)"}
{"number": 20380, "title": "torch.distributed support on MacOS is missing", "time": "2019-05-10T20:00:53Z", "body": "Currently trying to use distributed on MacOS crashes because torch.distributed namespace is empty\r\nI vaguely recall it working a year ago.\r\n\r\nThis is useful for quick sanity checks on my MacBook before deploying to cluster.\r\n\r\n```\r\nPython 3.7.3 (default, Mar 27 2019, 16:54:48) \r\n[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import torch\r\n>>> print(torch.version.__version__)\r\n1.1.0\r\n>>> import torch.distributed as dist\r\n>>> dist.init_process_group\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'torch.distributed' has no attribute 'init_process_group'\r\n```"}
{"number": 20381, "title": "[WIP][TEST] Minor edit", "time": "2019-05-10T20:22:36Z", "body": ""}
{"number": 20382, "title": "[tensorboard] Preserve log_dir arg and member for SummaryWriter", "time": "2019-05-10T20:35:02Z", "body": "Given that  tensorboardX and our PyTorch 1.1 release had `log_dir` as the argument for SummaryWriter initialization and member variable (which some users access), we need to  preserve this name. However, we might deprecate this in the future and I've added a `get_logdir` method that can be used in the future.\r\n\r\ncc @natalialunova, @lanpa "}
{"number": 20383, "title": "torch::jit::RegisterOperators forwards to c10::RegisterOperators", "time": "2019-05-10T20:36:04Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20383 torch::jit::RegisterOperators forwards to c10::RegisterOperators**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15300937/)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/20383\n\nDifferential Revision: [D15300937](https://our.internmc.facebook.com/intern/diff/D15300937/)"}
{"number": 20384, "title": "Extract feature length information from SigridTransforms op (#20171)", "time": "2019-05-10T21:08:10Z", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/20171\n\nExtract feature length information from SigridTransforms op\n\nDifferential Revision: D15219408\n\n"}
{"number": 20385, "title": "Needed fixes for binaries", "time": "2019-05-10T21:25:57Z", "body": ""}
{"number": 20386, "title": "[jit] Add `save()` to `torch._C.Function`", "time": "2019-05-10T22:06:06Z", "body": "Fixes #20017\r\n\r\nThis wraps the `torch._C.Function` currently returned from `torch.jit.script` and `torch.jit.trace` in a `ScriptFunction` and `TracedFunction` respectively, both of which are just wrappers to hold the function.\n\nDifferential Revision: [D15403161](https://our.internmc.facebook.com/intern/diff/15403161/)"}
{"number": 20387, "title": "use feenableexcept when glibc is available", "time": "2019-05-10T23:00:46Z", "body": "Summary: glibc has a non-standard function, feenableexcept, that triggers floating-point exception handler . Compared to feclearexcept + fetestexcept , this approach allows us to see precisely where the exception is raised from the stack trace.\n\nDifferential Revision: D15301095\n\n"}
{"number": 20388, "title": "[JIT] Support Forward Reference Type Annotations in UDT", "time": "2019-05-11T00:38:51Z", "body": "## üêõ Bug\r\n\r\nCannot type a class with an annotation that contains itself using python 3 style comments\r\n\r\n## To Reproduce\r\n```\r\n@torch.jit.script\r\nClass UDT(object):\r\n\tdef __init__(x):\r\n\t\tself.x = x\r\n\r\n\tdef __add__(other: 'UDT'): -> 'UDT'\r\n\t\treturn UDT(self.x + other.x)\r\n```\r\n## Expected behavior\r\n\r\nShould work based on the description of forward references in PEP 484\r\nhttps://www.python.org/dev/peps/pep-0484/#forward-references\r\n\n\ncc @suo"}
{"number": 20389, "title": "Fix bug in non_blocking copy", "time": "2019-05-11T01:10:54Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20389 Fix bug in non_blocking copy**\n\n"}
{"number": 20390, "title": "eliminate FE_INVALID exceptions related to fp16 conversion", "time": "2019-05-11T02:34:30Z", "body": "Summary: duc0 Ngo implemented observing floating point exceptions but there were a couple of places where we have \"benign\" floating point exceptions leading to false positives. This diff eliminates one source of such false positives, namely using _mm256_cvtph_ps and _mm256_cvtps_ph for partially uninitialized array for the remainder loop.\n\nDifferential Revision: D15307358\n\n"}
{"number": 20391, "title": "[BC-breaking] Fix version counter sharing in set_data()", "time": "2019-05-11T03:39:37Z", "body": "In https://github.com/pytorch/pytorch/pull/18223/files#diff-77a6f3462f2233b921d3042412fed6d3R178, we used `auto saved_version_ = data_.unsafeGetTensorImpl()->version_counter().current_version()` and then `new_data_impl_copy->set_version_counter(saved_version_)`, which actually doesn't preserve the original semantics that `var.set_data(tensor)` should keep `var`'s version counter object intact. This PR fixes the bug and adds test to make sure it doesn't happen again.\r\n\r\nThis PR is BC-breaking for v1.1.0, because v1.1.0 contains https://github.com/pytorch/pytorch/pull/18223 which introduces the bug. As an example, the following test passes on v1.0.1 and will pass on master after this PR is merged, but doesn't pass on v1.1.0:\r\n```python\r\nx = torch.randn(2, 3)\r\nxz = x[:]\r\nassert x._version == xz._version\r\nx.add_(1)\r\nassert x._version == xz._version  # We can confirm that `x` and `xz` have the same version counter\r\n\r\nx.data = torch.randn(3, 4)\r\nx.add_(1)\r\nassert x._version == xz._version  # We can confirm that `x` and `xz` still have the same version counter\r\n```"}
{"number": 20392, "title": "Move at::NonVariableTypeMode to TensorImpl, and check it in is_variable()", "time": "2019-05-11T04:49:00Z", "body": "As part of the Variable/Tensor merge, we allow passing Tensor with AutogradMeta into ATen ops, but we want to make sure they are not treated as Variables (i.e. their `is_variable()` is false). This PR makes the necessary change to make this work."}
{"number": 20393, "title": "NameError: name 'List' is not defined in custom_rnns LSTMCell jit fastrnns", "time": "2019-05-11T08:45:53Z", "body": "## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nseq_len, batch, input_size = 5, 12, 20\r\nhidden_size = 32\r\ninp = torch.randn(seq_len, batch, input_size)\r\nstate = LSTMState(torch.randn(batch, hidden_size),\r\n                      torch.randn(batch, hidden_size))\r\n\r\n\r\nlstm = LSTMLayer(LSTMCell, input_size, hidden_size)\r\nout, out_state = lstm(inp, state)\r\nprint(out.shape)\r\n\r\nrlstm = ReverseLSTMLayer(LSTMCell, input_size, hidden_size)\r\nrlstm.forward(inp, state)\r\n\r\nFile \"<string>\", line 1, in <module>\r\nNameError: name 'List' is not defined\r\n\r\nThe error happens in ReverseLSTMLayer. I debugged the codes. It happens since the ReverseLSTMLayer is initialized. There shouldn't be such problems cos the ReverseLSTMLayer and LSTMLayer are almost the same. Can you help me to fix it? Thanks. \r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/.pycharm_helpers/pydev/pydevd.py\", line 1664, in <module>\r\n    main()\r\n  File \"/root/.pycharm_helpers/pydev/pydevd.py\", line 1658, in main\r\n    globals = debugger.run(setup['file'], None, None, is_module)\r\n  File \"/root/.pycharm_helpers/pydev/pydevd.py\", line 1068, in run\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"/root/.pycharm_helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"/home/PycharmProjects/Hawkes/gnn/custom_lstms.py\", line 443, in <module>\r\n    rlstm = ReverseLSTMLayer(LSTMCell, input_size, hidden_size)\r\n  File \"/usr/local/lib/python3.6/site-packages/torch/jit/__init__.py\", line 952, in init_then_register\r\n    _create_methods_from_stubs(self, methods)\r\n  File \"/usr/local/lib/python3.6/site-packages/torch/jit/__init__.py\", line 913, in _create_methods_from_stubs\r\n    self._create_methods(defs, rcbs, defaults)\r\n  File \"/usr/local/lib/python3.6/site-packages/torch/jit/annotations.py\", line 52, in get_signature\r\n    return parse_type_line(type_line)\r\n  File \"/usr/local/lib/python3.6/site-packages/torch/jit/annotations.py\", line 90, in parse_type_line\r\n    arg_ann = eval(arg_ann_str, _eval_env)\r\n  File \"<string>\", line 1, in <module>\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.1):\r\n - OS (e.g., Linux):\r\n - How you installed PyTorch (`conda`):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version: 9.2\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"}
{"number": 20394, "title": "[tensorboard] Remove duplicated _optimize_trace", "time": "2019-05-11T11:25:21Z", "body": "The duplicated code of `_optimize_trace` in _pytorch_graph.py is used to bypass some optimization step which causes missing scope.\r\n\r\nIt seems that most of the problematic steps have been fixed recently. Standard models implemented in torchvision are visually inspected before the commit. However, the `+=` in https://github.com/pytorch/vision/blob/50d54a82d1479ffb6dd7469ed05fccdf290a1d84/torchvision/models/resnet.py#L63 will let https://github.com/pytorch/pytorch/blob/f4d9bfaa4dd983288518a310bb900756ee3c6046/torch/onnx/utils.py#L159 produce a bad result. It can be fixed by replacing it with `out += identity`. This also implies that `+=` has non-intuitive behavior.\r\n\r\ncc @orionr @ezyang "}
{"number": 20395, "title": "added CUDA tensor to numpy compability", "time": "2019-05-11T13:32:57Z", "body": "Edit on line 458 and 460.\r\nCopying tensor to host memory before performing numpy operation.\r\n\r\n"}
{"number": 20396, "title": "add ram limit to net for production", "time": "2019-05-11T15:50:33Z", "body": "## üöÄ Feature\r\nAllow a ram limitation that would slow down the inference in favor of limiting ram usage. \r\n\r\n## Motivation\r\nIn many production settings, we want to use very cheap servers that have specific limitations on the ram. However, we get all sorts of samples ranging in sizes (audio in my case, but this could be true for text or other variable length samples ). The default way to deal with this is to split the samples into overlaping parts. run the net and stitch the results. I have no doubt that there is a better way to process rnns and cnns ( and maybe transformers ) in such a way where the ram usage is limited to the required ram of one ( or \"n\") timestep per layer.\r\n\r\n## key features \r\n1) allow to set a max ram - throw an error if its lower than possible.\r\n2) optimize any sequence of operations such as stacked rnns and cnns to limit ram at the cost of runtime. \r\n\r\nI think this will help many users in serving models for production.  \r\n\r\n\r\n"}
{"number": 20397, "title": "Break reference cycle in load_state_dict", "time": "2019-05-11T21:12:09Z", "body": "load_state_dict includes a recursive inner function `load` that captures\r\nTensors through the close-over variable `state_dict`. Because it's\r\nrecursive, it also captures itself leading to a reference cycle.\r\n\r\nThis breaks the reference cycle so that any Tensors in state_dict can be\r\ncollected immediately instead of waiting until the next GC cycle.\r\n\r\nAlternatively, we could have passed `state_dict` and `metadata` as\r\narguments to load to prevent capture of Tensors. (That would still\r\nresult in cyclic garbage, but not any cyclic garbage of Tensors).\r\n\r\nSee:\r\nhttps://github.com/pytorch/pytorch/issues/20199#issuecomment-491089004\r\n\r\n"}
{"number": 20398, "title": "Print registry warning only when DEBUG is set", "time": "2019-05-11T21:27:58Z", "body": "Summary: Reduce logging volume from the Registry\n\nDifferential Revision: D15312262\n\n"}
{"number": 20399, "title": "[jit] can't use script classes as torch.jit.Attribute", "time": "2019-05-11T23:08:36Z", "body": "```\r\n@torch.jit.script\r\nclass BoundingBoxList:\r\n    def __init__(self, bbox, image_size: Tuple[int, int], mode: str):\r\n        self.size = image_size\r\n        self.mode = mode\r\n        self.bbox = bbox\r\n\r\nclass Foo(torch.jit.ScriptModule):\r\n    def __init__(self, bbox):\r\n        super(Foo, self).__init__(False)\r\n        self.words = torch.jit.Attribute(bbox, BoundingBoxList)\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, input):\r\n        # type: (str) -> int\r\n        return self.words.convert(\"xyxy\")\r\n\r\nf = Foo(BoundingBoxList(torch.rand(3, 4), (2, 3), \"xyxy\"))\r\n```\r\nresults in:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/suo/scratch/test.py\", line 64, in <module>\r\n    f = Foo(BoundingBoxList(torch.rand(3, 4), (2, 3), \"xyxy\"))\r\n  File \"/Users/suo/pytorch/torch/jit/__init__.py\", line 1178, in init_then_register\r\n    original_init(self, *args, **kwargs)\r\n  File \"/Users/suo/scratch/test.py\", line 57, in __init__\r\n    self.words = torch.jit.Attribute(bbox, BoundingBoxList)\r\n  File \"/Users/suo/pytorch/torch/jit/__init__.py\", line 1377, in __setattr__\r\n    the_type = torch.jit.annotations.ann_to_type(value.type)\r\n  File \"/Users/suo/pytorch/torch/jit/annotations.py\", line 184, in ann_to_type\r\n    raise ValueError(\"Unknown type annotation: '{}'\".format(ann.__name__))\r\nValueError: Unknown type annotation: 'BoundingBoxList'\r\n```"}
{"number": 20400, "title": "Improper handling of momentum in CyclicLR", "time": "2019-05-11T23:44:19Z", "body": "## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nThe CyclicLR scheduler tries to access the optimizer's momentum parameter even if `cycle_momentum=False`.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Setup a model named `net`\r\n2. Run the following:\r\n```\r\nimport torch.optim as optim\r\noptimizer = optim.Adam(net.parameters(), lr=0.001)\r\nscheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0001, max_lr=0.01, cycle_momentum=False)\r\n```\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\nResulting error:\r\n```\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-113-a9df59301cc9> in <module>\r\n      1 optimizer = optim.Adam(net.parameters(), lr=0.001)\r\n----> 2 scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0001, max_lr=0.01, cycle_momentum=False)\r\n\r\n~/.pyenv/versions/3.6.6/lib/python3.6/site-packages/torch/optim/lr_scheduler.py in __init__(self, optimizer, base_lr, max_lr, step_size_up, step_size_down, mode, gamma, scale_fn, scale_mode, cycle_momentum, base_momentum, max_momentum, last_epoch)\r\n    591                 for momentum, group in zip(base_momentums, optimizer.param_groups):\r\n    592                     group['momentum'] = momentum\r\n--> 593         self.base_momentums = list(map(lambda group: group['momentum'], optimizer.param_groups))\r\n    594         self.max_momentums = self._format_param('max_momentum', optimizer, max_momentum)\r\n    595 \r\n\r\n~/.pyenv/versions/3.6.6/lib/python3.6/site-packages/torch/optim/lr_scheduler.py in <lambda>(group)\r\n    591                 for momentum, group in zip(base_momentums, optimizer.param_groups):\r\n    592                     group['momentum'] = momentum\r\n--> 593         self.base_momentums = list(map(lambda group: group['momentum'], optimizer.param_groups))\r\n    594         self.max_momentums = self._format_param('max_momentum', optimizer, max_momentum)\r\n    595 \r\n\r\nKeyError: 'momentum'\r\n```\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nI expect the scheduler not try to access the `momentum` key if I've told it not to cycle the momentum parameter. The error seems to be due to a minor indentation mistake here: \r\n\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/optim/lr_scheduler.py#L594-L595\r\n\r\nThe instance variables `base_momentums` and `max_momentums` are only ever accessed if `cycle_momentum` is true. Thus the two lines where they're being should be indented.\r\n\r\n## Environment\r\nN/A\r\n\r\n## Additional comments\r\n\r\nI'll push a PR to fix this in a minute, though may need a little help with PyTorch's specific contribution process."}
{"number": 20401, "title": "Fix momentum bug in CyclicLR", "time": "2019-05-12T00:07:56Z", "body": "Resolves issue #19003\r\n\r\nThe author of this issue also asked that `cycle_momentum` default to `False` if the optimizer does not have a momentum parameter, but I'm not sure what the best way to do this would be. Silently changing the value based on the optimizer may confuse the user in some cases (say the user explicitly set `cycle_momentum=True` but doesn't know that the Adam optimizer doesn't use momentum). \r\n\r\nMaybe printing a warning when switching this argument's value would suffice?"}
{"number": 20402, "title": "Tensor and nn.Module Pruning", "time": "2019-05-12T01:37:55Z", "body": "## üöÄ Tensor and nn.Module Pruning\r\nTensor method and/or `nn` util to sparsify tensors and/or model, according to various pruning techniques in the literature. \r\n\r\n## Motivation\r\nState-of-the-art deep learning techniques rely on over-parametrized models that are hard to deploy. On the contrary, biological neural networks are known to use efficient sparse connectivity. It's important to identify best techniques to compress models by reducing the number of parameters in them, in order to reduce memory, battery, and hardware consumption without sacrificing accuracy, deploy lightweight models on device, and guarantee privacy with private on-device computation. On the research front, pruning is used to investigate the differences in learning dynamics of over-parametrized and under-parametrized networks, to study the role of lucky sparse subnetworks and initializations (\"lottery tickets\" [[1]](https://arxiv.org/abs/1803.03635)), as a destructive neural architecture search technique, and others.\r\nGoal of this feature: harmonizing pruning practices by providing a standard interface in PyTorch.\r\nTarget audience: researchers, engineering and product teams.\r\n\r\n## Pitch\r\nMinimalist API, with deeper flexibility for power-users. \r\n\r\nAt the tensor level, this could look as follows:\r\n```python\r\nt = torch.randn(3, 2, 4)\r\n# e.g.: randomly mask 6 entries\r\npruned_t = t.prune(method='random', amount=6)\r\n# e.g.: prune bottom 80% of entries by absolute value\r\npruned_t = t.prune(method='L1', amount=0.8)\r\n# e.g.: prune 50% of channels along the last dimension by L1 norm\r\npruned_t = t.prune(method='L1Structured', amount=0.5)\r\n# e.g.: prune 2 channels along the 0th dimension by L2 norm\r\npruned_t = t.prune(method='L2Structured', amount=2, axis=0)\r\n# e.g.: prune 1 channel along the last dimension by L0 norm\r\npruned_t = t.prune(method='LnStructured', n=0, amount=1)\r\n```\r\nA not-in-place `.prune` method will return a `torch.Tensor` of the same type and size as the one it acts on.\r\nIn-place pruning supported via `t.prune_(...)`.\r\n\r\nAt the model level, this will require a bit of thinking but should follow similar API patterns. This is important because not all pruning methods make sense on all parameters in a model (pruning conv kernels != pruning biases != pruning RNNs != pruning in the presence of batch norm, etc.). \r\nFirst, we should have a sensible, well-documented default behavior for the average-user's API, where a call to `net.prune(method='L1', amount=0.8)` defaults to pruning PyTorch \"prepackaged\" modules (such as linear, conv, and recurrent layers) in some sensible, expected way. \r\nMost power users though would probably want to prune custom layers, or prune different layer types or layers at different depths using different pruning methods or pruning method parameters. This could be specified via a dictionary, which maps parameter names (contained in `net.state_dict().keys()`) to a pruning method and its parameters: \r\n```python\r\n{\r\n    'features.0.weight' : L1PruningMethod(amount=0.8),\r\n    ...\r\n}\r\n```\r\n\r\nSimilar to the tensor operations, model-level pruning could return a copy of the model, or act on the model in place.\r\n\r\nPruning methods can be used during or post- training; this implementation will be training-loop-agnostic: the user will have to take care of writing their own training loop to decide when to prune and what to do with the pruned object (re-initialize and retrain, finetune, etc.).\r\n\r\n## Alternatives\r\nDepending on where this will live within the codebase, `t.prune(method=method, **kwargs)` could also look like: `torch.nn.utils.pruning_function(t, **kwargs)` or `torch.nn.utils.pruning_function(module=module, name=param_name, **kwargs)`. I personally would prefer the first option because the kwargs are parameters of the pruning method itself, while `t` is the tensor it acts on (some pruning methods will also have to take in some data `X` or `(X, y)` when they're applied), but the last option is more in line with how, say, `weight_norm` is implemented. Perhaps, for Module-level application, following the [example of the `weight_norm` implementation](https://pytorch.org/docs/stable/_modules/torch/nn/utils/weight_norm.html) using hooks will make this more PyTorch-y, but I don't know if we want to sacrifice the ability to act directly on a tensor that is not part of a Module. Would that go into `torch.nn.functional`? Open to suggestions here.\r\n\n\ncc @albanD @mruberry @jbschlosser"}
{"number": 20403, "title": "Installing PyTorch 1.1 into cloned conda env that contained PyTorch 1.0 gives \"Getting \"module 'torch._C' has no attribute 'BoolStorageBase'\" with PyTorch 1.1\"", "time": "2019-05-12T02:04:44Z", "body": "Trying to install and use PyTorch 1.1 on AWS image derived from DLAMI 22 fails with following on `import torch`\r\n```\r\nTraceback (most recent call last):\r\n  File \"check_versions.py\", line 12, in <module>\r\n    import torch.optim as optim\r\n  File \"/home/ubuntu/anaconda3/envs/pytorch_11/lib/python3.6/site-packages/torch/__init__.py\", line 222, in <module>\r\n    class BoolStorage(_C.BoolStorageBase, _StorageBase):\r\nAttributeError: module 'torch._C' has no attribute 'BoolStorageBase'\r\n```\r\nThings tried\r\n\r\n```\r\nconda create --name=pytorch_11 --clone pytorch_p36\r\nsource activate pytorch_11\r\npip install torch==1.1.0\r\npython -c \"import torch\"\r\n```\r\n\r\nOn a different machine\r\n```\r\nconda create --name=pytorch_11 --clone pytorch_p36\r\nsource activate pytorch_11\r\nconda install pytorch torchvision -c pytorch\r\npython -c \"import torch\"\r\n```\r\n\r\nUninstalling torch and reinstalling version 1.0 with pip fixes the issue\r\nAnother solution that worked was to install torch 1.1 directly into pytorch_p36 environment instead of cloning it"}
{"number": 20404, "title": "Sleef build failure: Usage : ../../bin/mkalias <vector width> <vector FP type> <vector int type> <mangled ISA> <extension>", "time": "2019-05-12T02:32:30Z", "body": "```\r\nninja: build stopped: subcommand failed.\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"setup.py\", line 728, in <module>\r\n    build_deps()\r\n  File \"setup.py\", line 294, in build_deps\r\n    build_dir='build')\r\n  File \"/usr/ports/science/py-pytorch/work-py36/pytorch-1.0rc0-4217-g163f0e182/tools/build_pytorch_libs.py\", line 293, in build_caffe2\r\n    check_call(ninja_cmd, cwd=build_dir, env=my_env)\r\n  File \"/usr/local/lib/python3.6/subprocess.py\", line 311, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['ninja', 'install', '-v']' returned non-zero exit status 1.\r\n*** Error code 1\r\n\r\nStop.\r\nmake: stopped in /usr/ports/science/py-pytorch\r\n```\r\n\r\nOR the same with GNU make\r\n\r\n```\r\n[ 41%] Built target libprotoc\r\nmake[2]: Leaving directory '/usr/ports/science/py-pytorch/work-py36/pytorch-1.0rc0-4217-g163f0e182/build'\r\nmake[1]: *** [Makefile:144: all] Error 2\r\nmake[1]: Leaving directory '/usr/ports/science/py-pytorch/work-py36/pytorch-1.0rc0-4217-g163f0e182/build'\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"setup.py\", line 728, in <module>\r\n    build_deps()\r\n  File \"setup.py\", line 294, in build_deps\r\n    build_dir='build')\r\n  File \"/usr/ports/science/py-pytorch/work-py36/pytorch-1.0rc0-4217-g163f0e182/tools/build_pytorch_libs.py\", line 296, in build_caffe2\r\n    check_call(['make', '-j', str(max_jobs), 'install'], cwd=build_dir, env=my_env)\r\n  File \"/usr/local/lib/python3.6/subprocess.py\", line 311, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['make', '-j', '8', 'install']' returned non-zero exit status 2.\r\n-- Building version 1.1.0a0\r\n```\r\n\r\nLog: https://people.freebsd.org/~yuri/py-pytorch-failure.log\r\n\r\n1.0rc0-4217-g163f0e182\r\nFreeBSD 11.2"}
{"number": 20405, "title": "CosineAnnealingLR has unexpected behavior with large step", "time": "2019-05-12T02:55:07Z", "body": "## üêõ Bug\r\n\r\nCosineAnnealingLR has unexpected behavior with large step. We use token count as the step for language modeling and this is often in the 1e9 range.\r\n\r\n## To Reproduce\r\n1. Construct torch.optim.lr_scheduler.CosineAnnealingLR with max_step=1e9\r\n2. scheduler.step(x) for large x\r\n\r\nRepro in this collab notebook\r\nhttps://colab.research.google.com/drive/11htMBtisAHq7fQViW0inUMPAiA2J93t3\r\n\r\n## Expected behavior\r\n\r\nBehaves nicely with large numbers for step size\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.1\r\n - OS (e.g., Linux): colab\r\n - Python version: 3.x\r\n - GPU models and configuration: None\r\n"}
{"number": 20406, "title": "torch.nn.functional.nll_loss return unexpected nan", "time": "2019-05-12T03:05:41Z", "body": "## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n`torch.nn.functional.nll_loss` returns unexpected `nan` loss on some `ignore_index`  value\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nimport torch\r\nimport torch.nn.functional as F\r\n```\r\nThe inputs and targets are following\r\n```python\r\ninput = torch.tensor([[[-0.7104, -0.6762],\r\n  [-0.7114, -0.6753],\r\n  [-0.6960, -0.6903],\r\n  [-0.6883, -0.6981],\r\n  [-0.6932, -0.6930],\r\n  [-0.6973, -0.6890],\r\n  [-0.6973, -0.6890],\r\n  [-0.6973, -0.6890],\r\n  [-0.6973, -0.6890],\r\n  [-0.6973, -0.6890],\r\n  [-0.6973, -0.6890],\r\n  [-0.6973, -0.6890]],\r\n[[-0.6651, -0.7220],\r\n  [-0.6567, -0.7310],\r\n  [-0.6614, -0.7260],\r\n  [-0.6499, -0.7384],\r\n  [-0.6586, -0.7289],\r\n  [-0.6550, -0.7328],\r\n  [-0.6656, -0.7215],\r\n  [-0.6512, -0.7370],\r\n  [-0.6405, -0.7487],\r\n  [-0.6484, -0.7400],\r\n  [-0.6527, -0.7353],\r\n  [-0.6593, -0.7282]]])\r\ntarget=torch.tensor([[1,0,0,0,0,-1,-1,-1,-1,-1,-1,-1],\r\n[0,0,0,0,0, 0, 1, 0, 0, 0, 0, 0]])\r\n\r\n\r\noutput = F.nll_loss(input.view(-1, input.size(-1)), target.contiguous().view(-1), ignore_index=-1, reduction='none')\r\n```\r\nthis is the output result\r\n```\r\noutput\r\ntensor([0.6762, 0.7114, 0.6960, 0.6883, 0.6932, 0.0000, 0.0000, 0.0000, 0.0000,\r\n           nan, 0.0000, 0.0000, 0.6651, 0.6567, 0.6614, 0.6499, 0.6586, 0.6550,\r\n        0.7215, 0.6512, 0.6405, 0.6484, 0.6527, 0.6593])\r\n\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nI think the output result on `ignore_index` value (i.e. `-1` in this case) should be zero (or other values except `nan`), but I get `nan` on 10'th output result.\r\n## Environment\r\n\r\n```bash\r\nCollecting environment information...\r\nPyTorch version: 0.4.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: Could not collect\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: GPU 0: GeForce GTX 1080 Ti\r\nNvidia driver version: 384.130\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.1\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.4.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] msgpack-numpy==0.4.4.2\r\n[pip3] numpy==1.14.5\r\n[pip3] torch==0.4.1\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] Could not collect\r\n```\r\n\r\nBTW, the same bug occur at PyTorch version: 1.1.0\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\nwhile on Win10 pytorch, the above example produces expected result:\r\n```python\r\nIn[93]: output\r\nOut[93]: \r\ntensor([6.7620e-01, 7.1140e-01, 6.9600e-01, 6.8830e-01, 6.9320e-01, 6.7210e-01,\r\n        6.8620e-01, 6.8170e-01, 1.0842e-19, 1.9067e+00, 2.0692e+00, 1.6831e+00,\r\n        6.6510e-01, 6.5670e-01, 6.6140e-01, 6.4990e-01, 6.5860e-01, 6.5500e-01,\r\n        7.2150e-01, 6.5120e-01, 6.4050e-01, 6.4840e-01, 6.5270e-01, 6.5930e-01])\r\n```"}
{"number": 20407, "title": "[DO NOT MERGE] test CI", "time": "2019-05-12T03:42:47Z", "body": ""}
{"number": 20408, "title": "DLL error on Windows 10", "time": "2019-05-12T08:56:42Z", "body": "## üêõ Bug\r\n\r\nI was trying to build a documentation for a python package that uses pytorch and I got the following error:\r\n\r\n```\r\nC:\\Users\\Jonas\\Documents\\GitHub\\kymatio\\doc>sphinx-build -b html source build\r\nRunning Sphinx v2.0.1\r\n\r\nConfiguration error:\r\nThere is a programmable error in your configuration file:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\python36\\lib\\site-packages\\sphinx\\config.py\", line 361, in eval_config_file\r\n    execfile_(filename, namespace)\r\n  File \"c:\\python36\\lib\\site-packages\\sphinx\\util\\pycompat.py\", line 86, in execfile_\r\n    exec(code, _globals)\r\n  File \"C:\\Users\\Jonas\\Documents\\GitHub\\kymatio\\doc\\source\\conf.py\", line 17, in <module>\r\n    import kymatio\r\n  File \"c:\\users\\jonas\\documents\\github\\kymatio\\kymatio\\__init__.py\", line 15, in <module>\r\n    from .scattering1d.scattering1d import Scattering1D\r\n  File \"c:\\users\\jonas\\documents\\github\\kymatio\\kymatio\\scattering1d\\__init__.py\", line 4, in <module>\r\n    from .scattering1d import Scattering1D\r\n  File \"c:\\users\\jonas\\documents\\github\\kymatio\\kymatio\\scattering1d\\scattering1d.py\", line 7, in <module>\r\n    import torch\r\n  File \"c:\\python36\\lib\\site-packages\\torch\\__init__.py\", line 79, in <module>\r\n    from torch._C import *\r\nImportError: DLL load failed: Une routine d\\u2019initialisation d\\u2019une biblioth\\xe8que de liens dynamiques (DLL) a \\xe9chou\\xe9.\r\n```\r\n\r\nIt's pretty hard to reproduce because when I run the same commands manually it works:\r\n```\r\nC:\\Users\\Jonas\\Desktop>python36\r\nPython 3.6.7 (v3.6.7:6ec5cf24b7, Oct 20 2018, 13:35:33) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import kymatio\r\n>>> import torch\r\n>>> from torch._C import *\r\n>>> torch.cuda.is_available()\r\nTrue\r\n>>>\r\n```\r\n\r\nI also switched python version to 3.6.8 and same error.\r\n\r\n## Environment\r\n\r\n```\r\nC:\\Users\\Jonas\\Desktop>py36 collect_env.py\r\nCollecting environment information...\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0\r\n\r\nOS: Microsoft Windows 10 Famille\r\nGCC version: Could not collect\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: GeForce 940MX\r\nNvidia driver version: 430.64\r\ncuDNN version: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin\\cudnn64_7.dll\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.3+mkl\r\n[pip3] torch==1.1.0\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] Could not collect\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.1.0\r\n - OS (e.g., Linux): windows 10 17763\r\n - How you installed PyTorch (`conda`, `pip`, source): pip (https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-win_amd64.whl)\r\n - Python version: 3.6.7\r\n - CUDA/cuDNN version: 10.0\r\n - GPU models and configuration: 940mx\r\n- NVIDIA drivers: 430.64\r\n\r\n## Path\r\n![Capture](https://user-images.githubusercontent.com/15847892/57579985-78b38400-74a4-11e9-9fb4-22ead1f71a12.PNG)\r\n"}
{"number": 20409, "title": "[DataLoader] RuntimeError: unable to resize file </torch_22438_3818345986> to the right size", "time": "2019-05-12T10:10:37Z", "body": "## üêõ Bug\r\n\r\nWhen I run program on server by SSH, DataLoader may cause this RuntimeError. \r\n\r\nIf this error happens, in the same SSH session it can reproduce again (but file number varys).\r\n\r\nBut after I reconnecting SSH, everything is OK.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nCannot always reproduce.\r\n\r\nTraceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 9, in <module>\r\n    manager.train_img()\r\n  File \"/home/jimmy/graduation/video/manager.py\", line 40, in train_img\r\n    self._train_img_epoch()\r\n  File \"/home/jimmy/graduation/video/manager.py\", line 181, in _train_img_epoch\r\n    for i, (images, labels) in enumerate(loader):\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 637, in __next__\r\n    return self._process_next_batch(batch)\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 658, in _process_next_batch\r\n    raise batch.exc_type(batch.exc_msg)\r\nRuntimeError: Traceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 138, in _worker_loop\r\n    samples = collate_fn([dataset[i] for i in batch_indices])\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 232, in default_collate\r\n    return [default_collate(samples) for samples in transposed]\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 232, in <listcomp>\r\n    return [default_collate(samples) for samples in transposed]\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 232, in default_collate\r\n    return [default_collate(samples) for samples in transposed]\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 232, in <listcomp>\r\n    return [default_collate(samples) for samples in transposed]\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 207, in default_collate\r\n    storage = batch[0].storage()._new_shared(numel)\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/storage.py\", line 120, in _new_shared\r\n    return cls._new_using_filename(size)\r\nRuntimeError: unable to resize file </torch_22438_3818345986> to the right size\r\n```\r\n\r\n## Environment\r\n\r\n(I use the official docker image)\r\n\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 418.56\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.4\r\n[pip3] torch==1.0.1.post2\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.1                      144  \r\n[conda] mkl_fft                   1.0.10           py36ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py36hd81dba3_0  \r\n[conda] torch                     1.0.1.post2              pypi_0    pypi\r\n[conda] torchvision               0.2.2.post3              pypi_0    pypi\r\n\r\n\r\n## Additional context\r\n\r\nSomething I think may cause this Error:\r\n1. pytorch flags, I set \r\n- `torch.backends.cudnn.benchmark = True`\r\n- `torch.multiprocessing.set_sharing_strategy('file_system')`\r\n2. Long running SSH session. The session was running for more than 1 day before the error happens \r\n"}
{"number": 20410, "title": "pytorch 1.0.1.post2 crash at forward on Mac Os after fork ", "time": "2019-05-12T12:21:58Z", "body": "hello,\r\ni created the following scripts for testing,\r\nnot sure what is the source of the problem but it's seems that in some scenario it's happened on linux as well at different but similar scenario(crash on forward after fork)\r\nthe following script crash on mac OS (see next comment)"}
{"number": 20411, "title": "Non blocking tensor copy to GPU not working from torch 1.0", "time": "2019-05-12T13:43:48Z", "body": "## üêõ Bug\r\n\r\nThe non_blocking option of tensor.cuda()/tensor.to() does not seem to have any effect as of torch 1.0, or possibly something additional needs to be configured for it to work?\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\n```\r\nimport time\r\nimport torch\r\n\r\nshape = (128,128)\r\ncount = 640\r\n\r\ntensors = [torch.randint(255,shape).byte().pin_memory() for i in range(count)]\r\n\r\nfor i in range(3):\r\n    start = time.time()\r\n    tensors_cuda = [item.cuda(non_blocking = True) for item in tensors]\r\n    torch.cuda.synchronize()\r\n    print(\"took\", (time.time()-start)*1000.)\r\n\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nIn torch 0.4 the time it took is ~5ms (Aside from the 1st iteration), and ~25ms when setting non_blocking=False.\r\nAs of torch 1.0 up to nightly the time is ~25ms regardless of the non_blocking being True\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0\r\n\r\nOS: Microsoft Windows 10 Pro\r\nGCC version: Could not collect\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: GPU 0: GeForce GTX 1080 Ti\r\nNvidia driver version: 398.11\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.3\r\n[pip3] torch==1.1.0\r\n[conda] blas                      1.0                         mkl\r\n[conda] cuda90                    1.0                           0    pytorch\r\n[conda] mkl                       2018.0.2                      1\r\n[conda] mkl-service               1.1.2            py36h57e144c_4\r\n[conda] mkl_fft                   1.0.1            py36h452e1ab_0\r\n[conda] mkl_random                1.0.1            py36h9258bd6_0\r\n[conda] torch                     0.4.0                    pypi_0    pypi\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"}
{"number": 20412, "title": "String in tensor", "time": "2019-05-12T16:21:20Z", "body": "## üêõ Bug\r\n\r\n\r\n`\"whateverstring\" in torch.tensor(1.)` returns True\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/8230801/57584985-5d805d00-7514-11e9-9303-d0e9f6d72257.png)\r\n\r\nThis bug happens in torch.1.1.0, I'm running pytorch with python 3.6.5, on windows linux subsystem. \r\n\r\nBut it won't happen in 1.0.1post2, with Ubuntu 16.04 and python 3.6.8\r\n\r\n![image](https://user-images.githubusercontent.com/8230801/57585000-9e787180-7514-11e9-8e03-c19320af7fcb.png)\r\n\r\n"}
{"number": 20413, "title": "[tensorboard] Support 3D mesh/point cloud", "time": "2019-05-12T19:29:34Z", "body": "I started adding support for the new **[mesh/point cloud](https://github.com/tensorflow/graphics/blob/master/tensorflow_graphics/g3doc/tensorboard.md)** data type introduced to TensorBoard recently.\r\n\r\nI created the functions to add the data, created the appropriate summaries.\r\nThis new data type however requires a **Merged** summary containing the data for the vertices, colors and faces.\r\n\r\nI got stuck at this stage. Maybe someone can help. @lanpa?\r\n\r\nI converted the example code by Google to PyTorch:\r\n```python\r\nimport numpy as np\r\nimport trimesh\r\n\r\nimport torch\r\nfrom torch.utils.tensorboard import SummaryWriter\r\n\r\nsample_mesh = 'https://storage.googleapis.com/tensorflow-graphics/tensorboard/test_data/ShortDance07_a175_00001.ply'\r\nlog_dir = 'runs/torch'\r\nbatch_size = 1\r\n\r\n# Camera and scene configuration.\r\nconfig_dict = {\r\n    'camera': {'cls': 'PerspectiveCamera', 'fov': 75},\r\n    'lights': [\r\n        {\r\n            'cls': 'AmbientLight',\r\n            'color': '#ffffff',\r\n            'intensity': 0.75,\r\n        }, {\r\n            'cls': 'DirectionalLight',\r\n            'color': '#ffffff',\r\n            'intensity': 0.75,\r\n            'position': [0, -1, 2],\r\n        }],\r\n    'material': {\r\n        'cls': 'MeshStandardMaterial',\r\n        'roughness': 1,\r\n        'metalness': 0\r\n    }\r\n}\r\n\r\n# Read all sample PLY files.\r\nmesh = trimesh.load_remote(sample_mesh)\r\nvertices = np.array(mesh.vertices)\r\n# Currently only supports RGB colors.\r\ncolors = np.array(mesh.visual.vertex_colors[:, :3])\r\nfaces = np.array(mesh.faces)\r\n\r\n# Add batch dimension, so our data will be of shape BxNxC.\r\nvertices = np.expand_dims(vertices, 0)\r\ncolors = np.expand_dims(colors, 0)\r\nfaces = np.expand_dims(faces, 0)\r\n\r\n# Create data placeholders of the same shape as data itself.\r\nvertices_tensor = torch.as_tensor(vertices)\r\nfaces_tensor = torch.as_tensor(faces)\r\ncolors_tensor = torch.as_tensor(colors)\r\n\r\nwriter = SummaryWriter(log_dir)\r\n\r\nwriter.add_mesh('mesh_color_tensor', vertices=vertices_tensor, faces=faces_tensor,\r\n                colors=colors_tensor, config_dict=config_dict)\r\n\r\nwriter.close()\r\n```\r\n\r\nI tried adding only the vertex summary, hence the others are supposed to be optional.\r\nI got the following error from TensorBoard and it also didn't display the points:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/dawars/workspace/pytorch/venv/lib/python3.6/site-packages/werkzeug/serving.py\", line 302, in run_wsgi\r\n    execute(self.server.app)\r\n  File \"/home/dawars/workspace/pytorch/venv/lib/python3.6/site-packages/werkzeug/serving.py\", line 290, in execute\r\n    application_iter = app(environ, start_response)\r\n  File \"/home/dawars/workspace/pytorch/venv/lib/python3.6/site-packages/tensorboard/backend/application.py\", line 309, in __call__\r\n    return self.data_applications[clean_path](environ, start_response)\r\n  File \"/home/dawars/workspace/pytorch/venv/lib/python3.6/site-packages/werkzeug/wrappers/base_request.py\", line 235, in application\r\n    resp = f(*args[:-2] + (request,))\r\n  File \"/home/dawars/workspace/pytorch/venv/lib/python3.6/site-packages/tensorboard/plugins/mesh/mesh_plugin.py\", line 252, in _serve_mesh_metadata\r\n    tensor_events = self._collect_tensor_events(request)\r\n  File \"/home/dawars/workspace/pytorch/venv/lib/python3.6/site-packages/tensorboard/plugins/mesh/mesh_plugin.py\", line 188, in _collect_tensor_events\r\n    tensors = self._multiplexer.Tensors(run, instance_tag)\r\n  File \"/home/dawars/workspace/pytorch/venv/lib/python3.6/site-packages/tensorboard/backend/event_processing/plugin_event_multiplexer.py\", line 400, in Tensors\r\n    return accumulator.Tensors(tag)\r\n  File \"/home/dawars/workspace/pytorch/venv/lib/python3.6/site-packages/tensorboard/backend/event_processing/plugin_event_accumulator.py\", line 437, in Tensors\r\n    return self.tensors_by_tag[tag].Items(_TENSOR_RESERVOIR_KEY)\r\nKeyError: 'mesh_color_tensor_COLOR'\r\n```"}
{"number": 20414, "title": "torch.max, the first or the last occurrence?", "time": "2019-05-12T22:34:55Z", "body": "This is not truly a bug, but a change in PyTorch 1.1.0 which I found it by chance. This change was very important to my project [SpykeTorch](https://github.com/miladmozafari/SpykeTorch).\r\nIn PyTorch 0.4.1, using max (or min) function results in finding the first occurrence of the max-value, however in PyTorch 1.1.0 (not sure about other versions > 1.0.0) the result is the last occurrence. I search the change-log page and I found that it is not reported.\r\n\r\nHere is the execution of a same function on a same data with two different versions of PyTorch:\r\n![change](https://user-images.githubusercontent.com/24203184/57588540-07acb480-7516-11e9-96cf-bc314244323d.png)\r\n\r\n"}
{"number": 20415, "title": "Split nn.MultiHeadAttention into Module + functional", "time": "2019-05-12T22:44:48Z", "body": "Moving functions from torch/nn/modules/activation.py to torch/nn/functional.py. For functions not implemented (_get_input_buffer and _set_input_buffer), a TODO is added."}
{"number": 20416, "title": "Class based Sampler for Class Incremental/Continual Learning research", "time": "2019-05-13T00:43:02Z", "body": "## üöÄ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nA class based dataset sampler for class incremental and continual learning research.\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\nFor research in class incremental learning domain, we need datasets to be split up on the basis of classes so that after training on one subset of classes, others can be introduced and the model can be made to generalize on that too in an architectural fashion or using generalization techniques like Elastic Weight Consolidation. Pytorch is strong enough with dynamic graphs but currently it lacks a sampler which can return examples from specific classes.\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\nI would like to add a sampler similar to others but which takes in the class labels as argument and returns examples from those classes. A random flag can be added to the arguments to randomize the order of samples in the subset. The implementation would be similar to that of SubsetRandomSampler in torch.utils.data but instead of indices, it would take in the class labels as argument.\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\nPlease feel free to suggest any alternative implementations. :)"}
{"number": 20417, "title": "Hotfix for caffe2 windows build", "time": "2019-05-13T00:53:16Z", "body": "We don't need to overlay vc env when not using ninja. CMake will deal with it automatically. Overlaying is a no-op when the env is the same with the generator specified but will generate the error \"Cannot find CMAKE_CXX_COMPILER\" when they are different."}
{"number": 20418, "title": "PyTorch operator observer", "time": "2019-05-13T01:38:36Z", "body": "Differential Revision: D15314820\n\n"}
{"number": 20419, "title": "Add the missing scale in the forward of SpatialSoftmaxWithLossOp", "time": "2019-05-13T01:51:16Z", "body": "Add the missing `scale_` for computing the averaged loss in the final part of `SpatialSoftmaxWithLossOp`. Docs and some variables' names are also refined to be consistent with `SoftmaxWithLossOp`."}
{"number": 20420, "title": "subprocess.CalledProcessError: Command '['cmake', '--build', '.', '--target', 'install', '--config', 'Release', '--', '-j', '7']' returned non-zero exit status 1", "time": "2019-05-13T05:38:53Z", "body": "When compiling the pytorch from github, after I excute \r\n> python setup.py\r\n\r\nafter a few minuters, the following problem occurs:\r\n\r\n> [364/2489] C:\\PROGRA~2\\MICROS~3.0\\VC\\bin\\amd64\\cl.exe   /TP -DONNX_NAMESPACE=onnx_torch -DUSE_MSC_ATOMICS=1 -D_CRT_SECURE_NO_DEPRECATE=1 -I..\\aten\\src -I. -I..\\ -I..\\third_party\\protobuf\\src -I..\\cmake\\..\\third_party\\benchmark\\include -Icaffe2\\contrib\\aten -I..\\third_party\\onnx -Ithird_party\\onnx -I..\\third_party\\foxi -Ithird_party\\foxi -I..\\c10\\.. -I..\\cmake\\..\\third_party\\googletest\\googlemock\\include -I..\\cmake\\..\\third_party\\googletest\\googletest\\include -I..\\cmake\\..\\third_party\\eigen -IF:\\Program\\Codes\\Anaconda\\envs\\python37forPytorch\\include -IF:\\Program\\Codes\\Anaconda\\envs\\python37forPytorch\\lib\\site-packages\\numpy\\core\\include -I..\\cmake\\..\\third_party\\pybind11\\include -I..\\cmake\\..\\third_party\\cub -I..\\third_party\\googletest\\googlemock\\include -I..\\third_party\\googletest\\googlemock -I..\\third_party\\googletest\\googletest\\include -I..\\third_party\\googletest\\googletest /EHa -openmp /MP /bigobj /MD /O2 /Ob2  /MP /bigobj   -DCUDA_HAS_FP16=1 -DUSE_GCC_GET_CPUID -DUSE_AVX -DTH_HAVE_THREAD /showIncludes /Foc10\\test\\CMakeFiles\\c10_either_test.dir\\util\\either_test.cpp.obj /Fdc10\\test\\CMakeFiles\\c10_either_test.dir\\ /FS -c ..\\c10\\test\\util\\either_test.cpp\r\nÁî®‰∫é x64 ÁöÑ Microsoft (R) C/C++ ‰ºòÂåñÁºñËØëÂô® 19.00.24215.1 Áâà\r\nÁâàÊùÉÊâÄÊúâ(C) Microsoft Corporation„ÄÇ‰øùÁïôÊâÄÊúâÊùÉÂà©„ÄÇ\r\n\r\n> ninja: build stopped: subcommand failed.\r\nTraceback (most recent call last):\r\n  File \"setup.py\", line 728, in <module>\r\n    build_deps()\r\n  File \"setup.py\", line 294, in build_deps\r\n    build_dir='build')\r\n  File \"J:\\Projects\\caffe2\\pytorch\\tools\\build_pytorch_libs.py\", line 283, in build_caffe2\r\n    check_call(build_cmd, cwd=build_dir, env=my_env)\r\n  File \"F:\\Program\\Codes\\Anaconda\\envs\\python37forPytorch\\lib\\subprocess.py\", line 347, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['cmake', '--build', '.', '--target', 'install', '--config', 'Release', '--', '-j', '7']' returned non-zero exit status 1.\r\n\r\n\r\n\r\nI find some similar problems, but none solved the one I  encounters.\r\n\r\nDoes anyone have any idea about it? It takes so long to complie pytorch, so I want to know the  reason why this happend, so it will not happen next time I excute a comlilation....\r\n\r\ncomments:\r\n\r\n> The cmake package havebeen updated to the latest version, so does ninja.\r\n\r\n> cuda cudnn and python env  is correct.\r\n"}
{"number": 20421, "title": "The result of  gloo all_gather error", "time": "2019-05-13T08:52:54Z", "body": "I use gloo for Model Parallelism, when I use all_gather, the result is error.\r\n\r\nThere are two process, I expect the all_gather result is [Tensor1, Tensor2], but the result is actually [Tensor1, Tensor1].\r\nThe Tensor2 is like this\r\n![image](https://user-images.githubusercontent.com/50440190/57607871-949b5080-759e-11e9-9f9e-d39b4faaf1a0.png)\r\nThe result  is like this, The senod Tensor in the result should be equal as the Tensor2\r\n![image](https://user-images.githubusercontent.com/50440190/57608339-581c2480-759f-11e9-9c08-75d657551d9c.png)\r\nBut before all_gather, I use torch.reshape(torch.tensor(range(xxxx), dtype=torch.float32), [16, 16, 16]) to create two Tensor and all_gather. The result is correctly.\r\nThe code is:\r\n\r\n```python\r\nfor _ in range(stage.get_devices_num()):\r\n        gather_tensor.append(torch.zeros_like(in_slice))\r\ndist.all_gather(gather_tensor, in_slice.contiguous(), group=group)\r\n```\r\n\r\nEnvironment:\r\n     macos\r\n     pytorch                   1.0.1\r\n     pytorch-cpu               1.1.0\r\n     numpy                     1.16.2\r\n\r\n\r\nPS: We use torch.chunk to split the Tensor and the dim is 0, and all_gather the chunked tensor by gloo, the all_gather  result is error.I think  although I use contiguous() to make memory contiguous, but the it is not effective after I chunk the tensor at dim 0.\r\n\r\n\r\n"}
{"number": 20422, "title": "Update documentation for CTCLoss", "time": "2019-05-13T09:39:55Z", "body": "Change `Inputs` to `Shape` to unify the format of CTCLoss `class`, and add the type of `Output` in `Shape`."}
{"number": 20423, "title": "Error downloading MNIST dataset", "time": "2019-05-13T10:23:47Z", "body": "hi all,\r\n\r\nhaving problems downloading the MNIST dataset, also tried with FashionMNIST which works fine.\r\n\r\n```\r\n\r\nfrom torchvision.datasets import MNIST\r\nfrom torchvision import transforms\r\n\r\ntrain_dataset = MNIST('data/', train=True, download=True,\r\n                             transform=transforms.Compose([\r\n                                 transforms.ToTensor(),\r\n                                 transforms.Normalize((mean,), (std,))\r\n                             ]))\r\n```\r\nerror message\r\n\r\n> \r\n> Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\r\n> Extracting data/MNIST/raw/train-images-idx3-ubyte.gz\r\n> \r\n> \r\n> ---------------------------------------------------------------------------\r\n> FileNotFoundError                         Traceback (most recent call last)\r\n> <ipython-input-30-1dca197de290> in <module>()\r\n>       7                              transform=transforms.Compose([\r\n>       8                                  transforms.ToTensor(),\r\n> ----> 9                                  transforms.Normalize((mean,), (std,))\r\n>      10                              ]))\r\n>      11 test_dataset = MNIST('data/', train=False, download=True,\r\n> \r\n> /anaconda/envs/py36/lib/python3.6/site-packages/torchvision/datasets/mnist.py in __init__(self, root, train, transform, target_transform, download)\r\n>      66 \r\n>      67         if download:\r\n> ---> 68             self.download()\r\n>      69 \r\n>      70         if not self._check_exists():\r\n> \r\n> /anaconda/envs/py36/lib/python3.6/site-packages/torchvision/datasets/mnist.py in download(self)\r\n>     142             file_path = os.path.join(self.raw_folder, filename)\r\n>     143             download_url(url, root=self.raw_folder, filename=filename, md5=None)\r\n> --> 144             self.extract_gzip(gzip_path=file_path, remove_finished=True)\r\n>     145 \r\n>     146         # process and save as torch files\r\n> \r\n> /anaconda/envs/py36/lib/python3.6/site-packages/torchvision/datasets/mnist.py in extract_gzip(gzip_path, remove_finished)\r\n>     123         print('Extracting {}'.format(gzip_path))\r\n>     124         with open(gzip_path.replace('.gz', ''), 'wb') as out_f, \\\r\n> --> 125                 gzip.GzipFile(gzip_path) as zip_f:\r\n>     126             out_f.write(zip_f.read())\r\n>     127         if remove_finished:\r\n> \r\n> /anaconda/envs/py36/lib/python3.6/gzip.py in __init__(self, filename, mode, compresslevel, fileobj, mtime)\r\n>     161             mode += 'b'\r\n>     162         if fileobj is None:\r\n> --> 163             fileobj = self.myfileobj = builtins.open(filename, mode or 'rb')\r\n>     164         if filename is None:\r\n>     165             filename = getattr(fileobj, 'name', '')\r\n> \r\n> FileNotFoundError: [Errno 2] No such file or directory: 'data/MNIST/raw/train-images-idx3-ubyte.gz'\r\n> \r\n> \r\n\r\nusing:\r\n\r\ntorch==1.0.1.post2\r\ntorchvision==0.2.2\r\n"}
{"number": 20424, "title": "pos_weight argument in torch.nn.BCELoss", "time": "2019-05-13T12:55:38Z", "body": "## üöÄ Feature requested\r\nI'd like to have a `pos_weight` arguments in `torch.nn.BCELoss`\r\n\r\n## Motivation\r\nto have consistency with `torch.nn.BCEWithLogitsLoss` and to be able to address a multi-label task with a strong class imbalance\r\n\r\n\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n"}
{"number": 20425, "title": "Tiny spelling mistake fix.", "time": "2019-05-13T14:45:42Z", "body": "\"then the output would also has k tensors\" -> \"then the output would also have k tensors\"\r\n\r\n"}
{"number": 20426, "title": "Replace AT_CHECK with TORCH_CHECK [shard 1/10]", "time": "2019-05-13T14:50:14Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20426 Replace AT_CHECK with TORCH_CHECK [shard 1/10]**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15318160/)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/20426\n\nDifferential Revision: [D15318160](https://our.internmc.facebook.com/intern/diff/D15318160/)"}
{"number": 20427, "title": "Replace AT_CHECK with TORCH_CHECK [shard 2/10]", "time": "2019-05-13T14:52:47Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20427 Replace AT_CHECK with TORCH_CHECK [shard 2/10]**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15318190/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20321 Rename AT_ASSERT to TORCH_INTERNAL_ASSERT; other macro updates&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15278439/)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/20427\n\nDifferential Revision: [D15318190](https://our.internmc.facebook.com/intern/diff/D15318190/)"}
{"number": 20428, "title": "Replace AT_CHECK with TORCH_CHECK [shard 3/10]", "time": "2019-05-13T14:53:59Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20428 Replace AT_CHECK with TORCH_CHECK [shard 3/10]**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15318209/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20321 Rename AT_ASSERT to TORCH_INTERNAL_ASSERT; other macro updates&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15278439/)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/20428\n\nDifferential Revision: [D15318209](https://our.internmc.facebook.com/intern/diff/D15318209/)"}
{"number": 20429, "title": "Replace AT_CHECK with TORCH_CHECK [shard 4/10]", "time": "2019-05-13T14:55:25Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20429 Replace AT_CHECK with TORCH_CHECK [shard 4/10]**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15318222/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20321 Rename AT_ASSERT to TORCH_INTERNAL_ASSERT; other macro updates&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15278439/)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/20429\n\nDifferential Revision: [D15318222](https://our.internmc.facebook.com/intern/diff/D15318222/)"}
{"number": 20430, "title": "Replace AT_CHECK with TORCH_CHECK [shard 6/10]", "time": "2019-05-13T14:58:14Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20430 Replace AT_CHECK with TORCH_CHECK [shard 6/10]**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15318250/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20321 Rename AT_ASSERT to TORCH_INTERNAL_ASSERT; other macro updates&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15278439/)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/20430\n\nDifferential Revision: [D15318250](https://our.internmc.facebook.com/intern/diff/D15318250/)"}
{"number": 20431, "title": "Replace AT_CHECK with TORCH_CHECK [shard 5/10]", "time": "2019-05-13T14:59:34Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20431 Replace AT_CHECK with TORCH_CHECK [shard 5/10]**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15318266/)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/20431\n\nDifferential Revision: [D15318266](https://our.internmc.facebook.com/intern/diff/D15318266/)"}
{"number": 20432, "title": "Replace AT_CHECK with TORCH_CHECK [shard 7/10]", "time": "2019-05-13T15:01:43Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20432 Replace AT_CHECK with TORCH_CHECK [shard 7/10]**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15318289/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20321 Rename AT_ASSERT to TORCH_INTERNAL_ASSERT; other macro updates&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15278439/)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/20432\n\nDifferential Revision: [D15318289](https://our.internmc.facebook.com/intern/diff/D15318289/)"}
{"number": 20433, "title": "Dataloader's memory usage keeps increasing during one single epoch.", "time": "2019-05-13T15:09:37Z", "body": "(All codes were tested on Pytorch 1.0.0 and Pytorch 1.0.1. Memory capacity of my machine is `256Gb`. )\r\n\r\n## Description and Reproduction\r\n\r\nHi,\r\n\r\nI create a dataloader to load features from local files by their file paths but find this results in OOM problem even though the code is simple.\r\n\r\nThe dataloader can be simplfied as:\r\n\r\n```python\r\nimport numpy as np\r\nimport torch\r\nimport torch.utils.data as data\r\nimport time\r\n\r\nclass MyDataSet(data.Dataset):\r\n\r\n  def __init__(self):\r\n    super(MyDataSet, self).__init__()\r\n\r\n    # Assume that the self.infoset here contains the description information about the dataset,\r\n    # such as a list of file names or paths.\r\n    # Here it is a list of strings. I set it aoubt 8Gb in memory.\r\n    # In my real project, this infoset is 40Gb in memory.\r\n    self.infoset = [str(i).zfill(1024) for i in range(len(self))]\r\n\r\n\r\n  def __getitem__(self, index):\r\n    info = self.infoset[index]  # problem is here\r\n    items = {}\r\n    items['features'] = self.load_feature(info)\r\n    return items\r\n\r\n  def load_feature(self, info):\r\n    '''\r\n    Load feature from files\r\n    '''\r\n    feature = torch.Tensor(np.ones([8, 4, 2], dtype=np.float32))\r\n    return feature\r\n\r\n  def __len__(self):\r\n    return 8000000\r\n\r\n\r\ndataset = MyDataSet()\r\n\r\ndataloader = data.DataLoader(dataset, batch_size=1024, shuffle=True, num_workers=16, pin_memory=True)\r\n\r\nwhile True:\r\n  for i, sample in enumerate(dataloader):\r\n    print(i, len(dataloader))\r\n    time.sleep(0.05) # slow down the process to see the mem-usage increasing during one epoch\r\n```\r\n\r\nDuring each epoch, the memory usage is about `13GB` at the very beginning and keeps inscreasing and finally up to about `46Gb`, like this:\r\n\r\n![Annotation 2019-05-13 191331](https://user-images.githubusercontent.com/21999660/57630625-89f9af00-75d0-11e9-830b-c8e6ede5b322.png)\r\n\r\n\r\nAlthough it will decrease to `13GB` at the beginning of next epoch, this problem is serious to me because in my real project the `infoset` is about `40Gb` due to the large number of samples and finally leads to `Out of Memory` (OOM) at the end of the first epoch.\r\n\r\n## Expected behavior\r\n\r\nI have found that the problem is caused by the first line of `MyDataset.__getitem__()` : `info = self.infoset[index]`, in the following code, if I remove this line, then memory usage is normal, which is also my expected behavior.\r\n\r\n```python\r\nclass MyDataSet(data.Dataset):\r\n\r\n  def __init__(self):\r\n\r\n    super(MyDataSet, self).__init__()\r\n\r\n    # Assume that the self.infoset here contains the description information about the dataset.\r\n    # Here it is a list of strings. I set it aoubt 8Gb in memory.\r\n    # In my real project, this infoset may be 40Gb in memory.\r\n    self.infoset = [str(i).zfill(1024) for i in range(len(self))]\r\n\r\n\r\n  def __getitem__(self, index):\r\n\r\n    # info = self.infoset[index]  # problem is here\r\n    info = 'fake info'\r\n\r\n    items = {}\r\n    items['features'] = self.load_feature(info)\r\n\r\n    return items\r\n\r\n  def load_feature(self, info):\r\n    '''\r\n    Load feature from files\r\n    '''\r\n    feature = torch.Tensor(np.ones([8, 4, 2], dtype=np.float32))\r\n\r\n    return feature\r\n\r\n  def __len__(self):\r\n\r\n    return 8000000\r\n\r\ndataset = MyDataSet()\r\n\r\ndataloader = data.DataLoader(dataset, batch_size=1024, shuffle=True, num_workers=16, pin_memory=True)\r\n\r\nwhile True:\r\n\r\n  for i, sample in enumerate(dataloader):\r\n\r\n    print(i, len(dataloader))\r\n\r\n    time.sleep(0.05) # slow down the process to see the mem-usage increasing during one epoch\r\n```\r\n\r\nAnd the corresponding mem usage is stable at `13GB`:\r\n\r\n![Annotation 2019-05-13 191509](https://user-images.githubusercontent.com/21999660/57631324-dd203180-75d1-11e9-8386-ea3be4609543.png)\r\n\r\n## More test\r\n\r\nIn the following code, I don't even load features in `__getitem__()` but just read a string of `infoset`, but get the same problem:\r\n\r\n```python\r\nclass MyDataSet(data.Dataset):\r\n\r\n  def __init__(self):\r\n\r\n    super(MyDataSet, self).__init__()\r\n\r\n    # Assume that the self.infoset here contains the description information about the dataset.\r\n    # Here it is a list of strings. I set it aoubt 8Gb in memory.\r\n    # In my real project, this infoset may be 40Gb in memory.\r\n    self.infoset = [str(i).zfill(1024) for i in range(len(self))]\r\n\r\n\r\n  def __getitem__(self, index):\r\n\r\n    info = self.infoset[index]  # problem is here\r\n\r\n    items = {}\r\n    # items['features'] = self.load_feature(info)\r\n\r\n    return items\r\n\r\n  def load_feature(self, info):\r\n    '''\r\n    Load feature from files\r\n    '''\r\n    feature = torch.Tensor(np.ones([8, 4, 2], dtype=np.float32))\r\n\r\n    return feature\r\n\r\n  def __len__(self):\r\n\r\n    return 8000000\r\n\r\ndataset = MyDataSet()\r\n\r\ndataloader = data.DataLoader(dataset, batch_size=1024, shuffle=True, num_workers=16, pin_memory=True)\r\n\r\nwhile True:\r\n\r\n  for i, sample in enumerate(dataloader):\r\n\r\n    print(i, len(dataloader))\r\n\r\n    time.sleep(0.05) # slow down the process to see the mem-usage increasing during one epoch\r\n\r\n```\r\n\r\nMem usage:\r\n\r\n\r\n![Annotation 2019-05-13 191418](https://user-images.githubusercontent.com/21999660/57631896-f37abd00-75d2-11e9-90f5-9139eb4215b8.png)\r\n\r\nAny suggestions or reasons about this problem?\r\n\r\nThanks."}
{"number": 20434, "title": "Replace AT_CHECK with TORCH_CHECK [shard 8/10]", "time": "2019-05-13T15:14:18Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20434 Replace AT_CHECK with TORCH_CHECK [shard 8/10]**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15318396/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20321 Rename AT_ASSERT to TORCH_INTERNAL_ASSERT; other macro updates&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15278439/)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/20434\n\nDifferential Revision: [D15318396](https://our.internmc.facebook.com/intern/diff/D15318396/)"}
{"number": 20435, "title": "Replace AT_CHECK with TORCH_CHECK [shard 9/10]", "time": "2019-05-13T16:02:36Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20435 Replace AT_CHECK with TORCH_CHECK [shard 9/10]**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15318877/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20321 Rename AT_ASSERT to TORCH_INTERNAL_ASSERT; other macro updates&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15278439/)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/20435\n\nDifferential Revision: [D15318877](https://our.internmc.facebook.com/intern/diff/D15318877/)"}
{"number": 20436, "title": "Replace AT_CHECK with TORCH_CHECK [shard 10/10]", "time": "2019-05-13T16:07:28Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20436 Replace AT_CHECK with TORCH_CHECK [shard 10/10]**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15318926/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20321 Rename AT_ASSERT to TORCH_INTERNAL_ASSERT; other macro updates&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15278439/)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/20436\n\nDifferential Revision: [D15318926](https://our.internmc.facebook.com/intern/diff/D15318926/)"}
{"number": 20437, "title": "fix two typos: \"a the\" => \"the\"", "time": "2019-05-13T16:33:41Z", "body": ""}
{"number": 20438, "title": "InsertGuards pass", "time": "2019-05-13T16:59:09Z", "body": ""}
{"number": 20439, "title": "Change onnxifi workflow to support multi-group quantized & Add multi quantization info to caffe2.proto", "time": "2019-05-13T17:26:27Z", "body": "Summary:\nThis is the QTensorProto workflow for multi group quantization in C2 side.\nNo DNNLOWP Tensor related thing is included in this pr, so once we finished glow side, we should be able to test this pr using resnet50.\n\nDifferential Revision: D15096919\n\n"}
{"number": 20440, "title": "[jit] Fix lint", "time": "2019-05-13T17:44:27Z", "body": "\n\nDifferential Revision: [D15320614](https://our.internmc.facebook.com/intern/diff/15320614/)"}
{"number": 20441, "title": "Clang-format ImageInputOp", "time": "2019-05-13T17:56:17Z", "body": "Summary:\nThis op is fairly complex and the fact that it isn't formatted\ncorrectly makes things that much harder to reason about. Clean it up.\n\nDifferential Revision: D15220006\n\n"}
{"number": 20442, "title": "[jit] temporarily disbale layernorm AD", "time": "2019-05-13T17:57:04Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* **#20442 temporarily disbale layernorm AD**\r\n\r\nTemporarily disable the AD formula for layernorm because of a weird bug in our autodiff infrastructure. refer to #19769\n\nDifferential Revision: [D15321524](https://our.internmc.facebook.com/intern/diff/D15321524)"}
{"number": 20443, "title": "Automatic update of fbcode/onnx to e08efaa35ed54362dfa283240506c003175889b7", "time": "2019-05-13T18:15:45Z", "body": "Summary:\nPrevious import was 5bde6371620b76302864bce90f521d72eda95d0e\n\nIncluded changes:\n- **[e08efaa3](https://github.com/onnx/onnx/commit/e08efaa3)**: Fix shape inference logic for TopK operator (#2005) <Hariharan Seshadri>\n- **[d80ea947](https://github.com/onnx/onnx/commit/d80ea947)**: Nullary variadic (#1889) <G. Ramalingam>\n- **[50dc186b](https://github.com/onnx/onnx/commit/50dc186b)**: Removed setting MD/MDd flags manually through cmake. The MTd/MT part is still necessary. Looks like CI fails without it. (#1995) <Alexander Yermolovich>\n- **[e7f81c5e](https://github.com/onnx/onnx/commit/e7f81c5e)**: Move NonMaxSupression to object_detection folder (#2001) <Hector Li>\n- **[86ab4517](https://github.com/onnx/onnx/commit/86ab4517)**: Prevent using invalid iterator, fix arithmetics. (#2004) <Dmitri Smirnov>\n\nDifferential Revision: D15302141\n\n"}
{"number": 20444, "title": "[jit] Bump torch proto version", "time": "2019-05-13T18:16:17Z", "body": "Tagging along to changes in #20191 which added more support for types in the pickler\n\nDifferential Revision: [D15321463](https://our.internmc.facebook.com/intern/diff/15321463/)"}
{"number": 20445, "title": "[ONNX] Improve ONNX Loop export", "time": "2019-05-13T18:45:38Z", "body": "~~This is work in progress due to its dependency on multiple pending PRs.~~\r\n\r\n- [x] ONNX: Relax constraint on subgraph input/output type & shape check. https://github.com/onnx/onnx/pull/2009\r\n- [x] PyTorch: Add infra to test_pytorch_onnx_caffe2.py to test ScriptModule models. https://github.com/pytorch/pytorch/pull/20256\r\n\r\nThis PR should partially resolve https://github.com/pytorch/pytorch/issues/17531. However, ideally we shouldn't need to put cast(and reshape) node to help the conversion for loop condition. \r\n\r\n- Added cast node for condition values before entering loop node. The ONNX spec only accepts Bool type, while in PyTorch if the condition value is an output from other node it could potentially have any integral type.\r\n- Tidying up the exported ONNX loop subgraph input type & shape. According to ONNX spec, input \"M\" is exported as 0-d scalar tensor with type int64. input \"Cond\" is exported as incomplete tensor of type Bool without shape information. This is because through out the iteration, the rank of condition value is dynamic, either 0-d or 1-d, as long as it holds a single value. "}
{"number": 20446, "title": "Output Effective_lr and effective_update for sparse adagrad, adam, and sparse adam", "time": "2019-05-13T19:07:59Z", "body": "Summary: as title\n\nDifferential Revision: D15322297\n\n"}
{"number": 20447, "title": "[DO NOT MERGE] Re-running html jobs", "time": "2019-05-13T19:40:27Z", "body": ""}
{"number": 20448, "title": "Add matmul optimization for the case A.ndim <= 2 && B.ndim >= 3", "time": "2019-05-13T20:15:01Z", "body": "This addresses #18862."}
{"number": 20449, "title": "\"derivative for _thnn_fused_lstm_cell_backward is not implemented\" while using GPU", "time": "2019-05-13T20:28:14Z", "body": "## üêõ Bug\r\nI got RuntimeError : \"derivative for _thnn_fused_lstm_cell_backward is not implemented\" while computing second order derivative of LSTMcell. \r\nThis works fine while using cpu, but got this error while using GPU. \r\nVery confusing about this issue. Is this mean second order derivative of LSTMcell is not available on CUDA?\r\n\r\n\r\n"}
{"number": 20450, "title": "Update Sleef to include fix for FMA4 detection", "time": "2019-05-13T20:40:03Z", "body": "FMA4 support is in bit 16 of register ECX, not EDX of the \"extended\r\nprocessor info\" (0x80000001).\r\n\r\nOnce we verify that this change fixes the issue https://github.com/pytorch/pytorch/issues/12112, I'll make a PR for upstream Sleef.\r\n\r\nThe mapping of registers to reg is:\r\n\r\n```\r\n  reg[0] = eax\r\n  reg[1] = ebx\r\n  reg[2] = ecx <---\r\n  reg[3] = edx\r\n```\r\n\r\nBit 16 of EDX is PAT (Page Attribute Table) on AMD CPUs, which is widely\r\nsupported. Intel CPUs do not set this bit. This causes \"Illegal\r\ninstruction\"\r\nerrors on AMD CPUs that do not support FMA4.\r\n\r\nSee https://github.com/pytorch/pytorch/issues/12112\r\nSee https://github.com/shibatch/sleef/issues/261\r\n\r\nhttp://developer.amd.com/wordpress/media/2012/10/254811.pdf (Page 20)\r\n\r\n"}
{"number": 20451, "title": "Provide a few default args for numpy translation", "time": "2019-05-13T21:00:15Z", "body": "Add automatic translations for a few argument names that commonly differ between PyTorch and NumPy.\r\n\r\nFor now, they are as follows:\r\n\r\n* `keepdim` -> `keepdims`\r\n* `dim` -> `axis`\r\n* `input` -> (any of `a`, `x`, `x1`)\r\n* `other` -> `x2`\r\n\r\nBasic examples:\r\n```python\r\n>>> t=torch.randn(10,10)\r\n>>> torch.sum(x=t, axis=1)\r\ntensor([ 0.5199, -0.3768,  4.3619, -0.9105,  1.1804,  1.0837, -0.9036,  0.2365,\r\n         1.1171, -0.0999])\r\n```\r\n```python\r\n>>> torch.add(x1=5, x2=6)\r\ntensor(11)\r\n```\r\n\r\nThe additional overhead is zero when using traditional PyTorch argument names, and a few (usually 1) extra PyDict lookups when using NumPy argument names."}
{"number": 20452, "title": "Corner case in matmul optimization", "time": "2019-05-13T21:02:50Z", "body": "If the shape contains multiple zeros (which can happen as a result of slicing), the `matmul` optimization for `A.ndim >= 3 && B.ndim <= 2` fails.\r\n\r\nFirst an example *without* optimization:\r\n\r\n```\r\n>>> import torch\r\n>>> x = torch.arange(0).reshape(0, 0, 0)\r\n>>> y = torch.arange(0).reshape(0, 0, 0)\r\n>>> x @ y\r\ntensor([], size=(0, 0, 0), dtype=torch.int64)\r\n```\r\n\r\nThe optimization gives a `Runtime` error:\r\n\r\n```\r\n>>> x = torch.arange(0).reshape(0, 0)\r\n>>> y = torch.arange(0).reshape(0, 0, 0)\r\n>>> x @ y\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nRuntimeError: cannot reshape tensor of 0 elements into shape [-1, 0] because the unspecified dimension size -1 can be any value and is ambiguous\r\n```\r\n\r\nThis is the error location:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/824d4f9957b1e2b5d812b7b7b0379bf0f7e270a9/aten/src/ATen/native/LinearAlgebra.cpp#L427"}
{"number": 20453, "title": "resnet50 export to ONNX messes up Flatten layer", "time": "2019-05-13T21:33:11Z", "body": "Converting resnet50, (similar issue for vgg16, densenet201, squeezenet as well) from examples directory into ONNX.  What's supposed to be simple flattening at the end of the model turns out to be a complicated network:\r\n\r\nModel prints this as last two layers:\r\n```\r\n(avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\r\n(fc): Linear(in_features=2048, out_features=1000, bias=True)\r\n```\r\nONNX produced model final layers: \r\n```\r\nGlobalAveragerPool->Shape->Gather->Unsqueeze->Concat->Reshape->Gemm\r\n                                  Unsqueeze -^\r\n```\r\n\r\nIsn't the converter just supposed to insert a Flatten layer ?\r\n\r\nSome frameworks/compilers don't support non-constant reshape caused by this network (e.g. Facebook Glow) and adding such support by folding the nodes (e.g. starting from Shape) seems too patchy. \r\n\r\nDetails:\r\nresnet50 obtained  using `examples/imagenet/main.py` . Converted using:\r\n```\r\nmodel.train(False)\r\ninput = torch.randn(1, 3, 224, 224, requires_grad=True)\r\noutput = model(input)\r\ntorch_out = torch.onnx._export(model, input, \"model.onnx\", export_params=True)\r\n```"}
{"number": 20454, "title": "Restore TBB module", "time": "2019-05-13T21:46:02Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* **#20454 Restore TBB module**\r\n* #20157 Support for Eigen thread pool\r\n* #20087 Native ATen/Parallel backend\r\n* #20057 Split ATen/Parallel into interface and backend\r\n* #20050 Move inter-op settings into ATen/Parallel\r\n\r\nSummary:\r\nThis partially reverts https://github.com/pytorch/pytorch/pull/8255 and adds ability to build with MKL and MKL-DNN using TBB\r\n\r\nTest Plan:\r\nMKL_TBB=1 MKLDNN_THREADING=TBB USE_TBB=1 USE_OPENMP=0  \r\nUSE_EIGEN_THREADPOOL=1 PARALLEL_BACKEND=NATIVE BLAS=MKL\r\nUSE_CUDA=0 BUILD_BINARY=1\r\nUSE_MKLDNN=1 USE_OPENCV=1 USE_FFMPEG=1 python setup.py develop --cmake\r\n\r\n./build/bin/parallel_info\r\n\r\n\r\n\r\nDifferential Revision: [D15326062](https://our.internmc.facebook.com/intern/diff/D15326062)"}
{"number": 20455, "title": "Memory format support for contiguous and is_contiguous", "time": "2019-05-13T22:16:53Z", "body": "#19975 was separated by 2 PRs. \r\n\r\nThis one:\r\n\r\nIntroduce MemoryFormat argument to the `x.is_contiguous(memory_format=torch.channels_last)` and to the `y = x.contiguous(memory_format=torch.channels_last)` functions. \r\n\r\nAt this moment both functions just operate with strides and doesn't store any tensor state. \r\n\r\n(Original RFC #19092)\r\n\r\n-----\r\n\r\nExpands functionality of two tensor functions `.is_contiguous` and `.contiguous` (both python and c++ api).\r\n\r\nNote: We had several complaints about `.to(memory_format)` function, and decided not to support it.\r\n\r\n1.  `.contiguous` now support optional keyword-only argument - `memory_format`, which can be either `torch.contiguous_format` or `torch.channels_last`.\r\n\r\n    - Using `torch.contiguous_format` will preserve existing `.contiguous()` behavior. \r\n\r\n    - Calling `x.contiguous(memory_format=torch.channels_last)` returns new tensor which maintain same semantical layout (NCHW), but have different memory allocation pattern.\r\n\r\n        `x.contiguous(memory_format=torch.channels_last)` expects input tensor to be 3d, 4d or 5d; and fails otherwise. \r\n\r\n2. `.is_contiguous` now support optional keyword-only argument - `memory_format`, which can be either `torch.contiguous_format` or `torch.channels_last`.\r\n\r\n\r\n    - `x.is_contiguous(memory_format=torch.contiguous_format)` preserves same functionality as `x.is_contiguous()` and remains unchanged.\r\n\r\n    - `x.is_contiguous(memory_format=torch.channels_last)` returns true if A) input tensor is contiguous in memory AND B) allocated in the memory in NWHC (or similar for 3d,5d) format.\r\n\r\nNote: By the end of the phase one `x.is_contiguous(memory_format=torch.channels_last)` will calculate state of the Tensor on every call. This functionality going to be updated later.\r\n\r\n"}
{"number": 20456, "title": "[WIP] Guard Elimination pass depends on 20438", "time": "2019-05-13T22:32:00Z", "body": ""}
{"number": 20457, "title": "index_put_ no longer accepts indices with non-matching backend", "time": "2019-05-13T23:11:20Z", "body": "## üêõ Bug\r\nin index_put_ operation indices backend now has to match source backend. It used to be not necessary, so that e.g. cuda tensor could be indexed by cpu tensor. Blame points to #17991  https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Indexing.cpp#L204\r\n\r\n## To Reproduce\r\n```\r\nimport torch\r\n\r\na=torch.arange(30, dtype=torch.float).view(5,6).cuda()\r\nind0 = torch.arange(0,a.size(0), step=2)\r\ngO = torch.randn(a[ind0].size()).cuda()\r\na.index_put_((ind0,), gO, accumulate=True)\r\ntorch.cuda.synchronize()\r\n```\r\nused to work, now it does not. Note that forward operation with ind0 on the cpu still works (`a[ind0]`), but index_put_ breaks\r\n## Expected behavior\r\nEither clarification in the docs and in the code that indices tensor has to have the same backend as source tensor, or fixing the RuntimeError. In case we want to disable indexing with a different backend, it would make sense to disable it for the forward indexing operation too, right now forward would be fine, but backward will through a runtime error. \r\n\r\n## Environment\r\nPytorch 1.1 binary and recent source builds\r\n\r\ncc @colesbury \r\n\r\n"}
{"number": 20458, "title": "Add autograd for to_sparse.", "time": "2019-05-13T23:17:02Z", "body": "https://github.com/pytorch/pytorch/issues/18111"}
{"number": 20459, "title": "Use expect to fill in pytorchbot token", "time": "2019-05-13T23:38:37Z", "body": "In this PR, we use `expect` to fill in the token for pytorchbot when doing `git push`, so that we don't need to save the token in the git remote URL."}
{"number": 20460, "title": "Remove dependencies from Caffe2Go on PyTorch JIT", "time": "2019-05-14T00:02:26Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20460 Remove dependencies from Caffe2Go on PyTorch JIT**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15283908/)\n\nSource file changes mostly involve ifdef'ing-out references to JIT code\nfrom files that are part of Caffe2Go.  Update Internal build scripts to\nremove those files from our globs.\n\nAfter this, changes to most of the JIT files should not trigger mobile CI.\n\nDifferential Revision: [D15283908](https://our.internmc.facebook.com/intern/diff/D15283908/)"}
{"number": 20461, "title": "restore hidden visibility by default for Linux builds", "time": "2019-05-14T00:02:37Z", "body": "Symbols are given hidden visibility by default on Linux to emulate the behavior on Windows.  This helps developers catch visibility issues in their streamlined Linux dev environment before being surprised, late in the process, by Windows errors."}
{"number": 20462, "title": "Remove dependencies from Caffe2Go on PyTorch JIT", "time": "2019-05-14T00:07:32Z", "body": null}
{"number": 20463, "title": "Remove dependencies from Caffe2Go on PyTorch JIT", "time": "2019-05-14T00:08:55Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20463 Remove dependencies from Caffe2Go on PyTorch JIT**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15329407/)\n\nSource file changes mostly involve ifdef'ing-out references to JIT code\nfrom files that are part of Caffe2Go.  Update Internal build scripts to\nremove those files from our globs.\n\nAfter this, changes to most of the JIT files should not trigger mobile CI.\n\nDifferential Revision: [D15329407](https://our.internmc.facebook.com/intern/diff/D15329407/)"}
{"number": 20464, "title": "Implementing GELU activation", "time": "2019-05-14T00:32:04Z", "body": "Gaussian Error Linear Unit (GELU) has become really mainstream with models such as BERT using it. But nn.modules.activation doesn't have a GELU implementation. Feature request to expand the arsenal of activation functions available right off the bat in Pytorch :)\r\n\r\nI could work on it, to get it merged upstream if assigned.\r\n\r\n"}
{"number": 20465, "title": "Second order gradient cuda error", "time": "2019-05-14T00:45:24Z", "body": "## üêõ Bug\r\nConvnet training on GPU: when penalizing gradient growth (backpropagating a gradient of a gradient) the following error happens:\r\n\r\n```\r\n/opt/conda/conda-bld/pytorch_1556653099582/work/aten/src/THC/THCTensorScatterGather.cu:100: void THCudaTensor_gatherKernel(TensorInfo<Real, IndexType>, TensorInfo<Real, IndexType>, TensorInfo<long, IndexType>, int, IndexType) [with IndexType = unsigned int, Real = float, Dims = 3]: block: [40,0,0], thread: [31,0,0] Assertion `indexValue >= 0 && indexValue < src.sizes[dim]` failed.\r\nTHCudaCheck FAIL file=/opt/conda/conda-bld/pytorch_1556653099582/work/aten/src/THC/generic/THCTensorScatterGather.cu line=71 error=59 : device-side assert triggered\r\nTraceback (most recent call last):\r\n  File \"cudafail.py\", line 47, in <module>\r\n    grad_sum.backward(retain_graph=False)\r\n  File \"/home/michael/miniconda2/envs/pt/lib/python3.6/site-packages/torch/tensor.py\", line 107, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File \"/home/michael/miniconda2/envs/pt/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 93, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\nRuntimeError: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1556653099582/work/aten/src/THC/generic/THCTensorScatterGather.cu:71\r\n```\r\n\r\n## To Reproduce\r\n\r\n```\r\nimport torch\r\nfrom torch import nn\r\nfrom torchvision import datasets, transforms\r\n\r\nclass Net(nn.Module):\r\n\tdef __init__(self):\r\n\t\tsuper(Net, self).__init__()\r\n\t\tself.conv1 = nn.Conv2d(3, 20, kernel_size=5, bias=False)\r\n\t\tself.conv2 = nn.Conv2d(20, 40, kernel_size=5, bias=False)\r\n\t\tself.linear1 = nn.Linear(40 * 5 * 5, 300, bias=False)\r\n\t\tself.linear2 = nn.Linear(300, 10, bias=False)\r\n\t\tself.pool = nn.MaxPool2d(2, 2)\r\n\t\tself.relu = nn.ReLU()\r\n\r\n\tdef forward(self, input):\r\n\t\tx = self.relu(self.pool(self.conv1(input)))\r\n\t\tx = self.relu(self.pool(self.conv2(x)))\r\n\t\tx = x.view(x.size(0), -1)\r\n\t\tx = self.relu(self.linear1(x))\r\n\t\treturn self.linear2(x)\r\n\r\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\r\ntrainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\r\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=8)\r\n\r\nmodel = Net().cuda()\r\noptimizer = torch.optim.SGD(model.parameters(), lr=1, momentum=0, nesterov=False)\r\n\r\nfor epoch in range(100):\r\n        print(epoch)\r\n\tmodel.train()\r\n\tfor i, (images, labels) in enumerate(trainloader, 0):\r\n\t\timages = images.cuda()\r\n\t\tlabels = labels.cuda()\r\n\t\toutputs = model(images)\r\n\t\tloss = nn.CrossEntropyLoss()(outputs, labels)\r\n\t\toptimizer.zero_grad()\r\n\t\tloss.backward(retain_graph=True)\r\n\r\n\t\tgrads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\r\n\t\tgrad_sum = 0\r\n\t\tfor grad in grads:\r\n\t\t\tgrad_sum += 0.1 * grad.pow(2).sum()\r\n\t\tgrad_sum.backward(retain_graph=False)\r\n\r\n\t\toptimizer.step()\r\n```\r\n\r\n## Environment\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.6.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.1.85\r\nGPU models and configuration:\r\nGPU 0: TITAN X (Pascal)\r\nGPU 1: TITAN X (Pascal)\r\nGPU 2: TITAN X (Pascal)\r\nGPU 3: TITAN X (Pascal)\r\n\r\nNvidia driver version: 418.56\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.3\r\n/usr/local/cuda-9.0/lib64/libcudnn.so.7.2.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.0\r\n[pip3] torch==1.1.0\r\n[pip3] torchfile==0.1.0\r\n[pip3] torchvision==0.2.2\r\n[conda] blas                      1.0                         mkl\r\n[conda] cuda92                    1.0                           0    pytorch\r\n[conda] mkl                       2018.0.3                      1\r\n[conda] mkl_fft                   1.0.4            py36h4414c95_1\r\n[conda] mkl_random                1.0.1            py36h4414c95_1\r\n[conda] pytorch                   1.1.0           py3.6_cuda10.0.130_cudnn7.5.1_0    pytorch\r\n[conda] torchfile                 0.1.0                      py_0    conda-forge\r\n[conda] torchvision               0.2.2                      py_3    pytorch\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\nIf I change the coefficient in `grad_sum += 0.1 * grad.pow(2).sum()` from 0.1 to 10 the error does NOT happen (at least not before ~40 epochs), same if I reduce the learning rate from 1 to 0.001. \r\n"}
{"number": 20466, "title": "Lint rule to prevent direct use of #pragma omp", "time": "2019-05-14T00:52:28Z", "body": "@ilia-cher, I noticed, quite by accident, that you had expunged all occurrences of `#pragma omp` in favor of `at::parallel_for`. That's cool, should we add a lint rule to prevent people from reintroducing direct use of `#pragma omp`? There's still one occurrence of it in aten/src/THNN/generic/VolumetricConvolutionMM.c and I doubt most reviewers will know to reject reviews if they contain fresh occurrences of `#pragma omp`"}
{"number": 20467, "title": "[pt1] refactor registerStoragePyTypeObject", "time": "2019-05-14T01:18:07Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20467 [pt1] refactor registerStoragePyTypeObject**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15330865/)\n\nfor upcoming changes in Storage for QInt8\n\nDifferential Revision: [D15330865](https://our.internmc.facebook.com/intern/diff/D15330865/)"}
{"number": 20468, "title": "Add mandatory ScalarType nodes as input to the quant-dequant nodes.", "time": "2019-05-14T02:29:02Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20468 Add mandatory ScalarType nodes as input to the quant-dequant nodes.**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15331600/)\n\nScalarType node is mandatory for activations and parameters now.\nThis change inserts ScalarType node for all the quant-dequant nodes. For the activations, currently the default value is at::ScalarType::Undefined. Remove this and explicitly pass the at::ScalarType::QUint8 dtype\n\nDifferential Revision: [D15331600](https://our.internmc.facebook.com/intern/diff/D15331600/)"}
{"number": 20469, "title": "ubuntu Build Libtorch source error", "time": "2019-05-14T02:44:01Z", "body": "[ 25%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/python/python_generator.cc.o\r\n[ 25%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/ruby/ruby_generator.cc.o\r\n[ 25%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/subprocess.cc.o\r\n[ 25%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/zip_writer.cc.o\r\n[ 25%] Linking CXX static library ../../../lib/libprotoc.a\r\n[ 25%] Built target libprotoc\r\nMakefile:138: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\nTraceback (most recent call last):\r\n  File \"../tools/build_libtorch.py\", line 30, in <module>\r\n    subprocess.check_call(command, universal_newlines=True, env=my_env)\r\n  File \"/usr/lib/python2.7/subprocess.py\", line 541, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['/home/xiaoran/pytorch/tools/build_pytorch_libs.sh', '--use-cuda', '--use-nnpack', '--use-mkldnn', '--use-qnnpack', 'caffe2']' returned non-zero exit status 2\r\n\r\n"}
{"number": 20470, "title": "Export sign onnx operator", "time": "2019-05-14T03:13:48Z", "body": "A trivial commit that supports exporting sign operator"}
{"number": 20471, "title": "[wip] Replace Type dispatch with ATenDispatch", "time": "2019-05-14T03:37:04Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20471 [wip] Replace Type dispatch with ATenDispatch**\n\nDifferential Revision: [D15549150](https://our.internmc.facebook.com/intern/diff/D15549150)"}
{"number": 20472, "title": "Some small performance fixes for c10 dispatcher", "time": "2019-05-14T04:05:54Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* **#20472 Some small performance fixes for c10 dispatcher**\r\n\r\nDifferential Revision: [D15332284](https://our.internmc.facebook.com/intern/diff/D15332284)\r\n\r\n- Add path to call operator directly without creating kernel cache\r\n- Save a refcount when getting dispatch key"}
{"number": 20473, "title": "Fixes the tests after introduction of `qint8`", "time": "2019-05-14T05:07:32Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20474 [RFC] Quantized Max Pool op&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15327923/)\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20473 Fixes the tests after introduction of `qint8`**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15332106/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #19984 [pt1][quant] Add qint8 type (int8_t)&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15150715/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #19932 [pt1][quant] Rename qint8 data type&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15137838/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #19816 [pt1][quant] Add QInt32 ScalarType and qint32 data type&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15094174/)\n\nThe initial assumption was that `qint8` would be unsigned. After introduction of `quint8` and `qint8`, some tests break.\n\nDifferential Revision: [D15332106](https://our.internmc.facebook.com/intern/diff/D15332106/)"}
{"number": 20474, "title": "Quantized Max Pool op", "time": "2019-05-14T05:09:12Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20474 Quantized Max Pool op**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15327923/)\n\nparallel implementaiton of the MaxPool (no ReLU).\n\nDifferential Revision: [D15327923](https://our.internmc.facebook.com/intern/diff/D15327923/)"}
{"number": 20475, "title": "Add linear learning rate schedule", "time": "2019-05-14T05:10:37Z", "body": "The linear learning rate schedule is: base_lr * (1 - t/T), where t is the current iteration and T is the total training iterations. Although this simple schedule is literally parameter-free, it has been show [here](https://arxiv.org/abs/1905.04753) that linear learning rate schedule can often surpass existing schedules on various computer vision benchmarks, especially in the case with a small training budget (measured in the number of iterations)."}
{"number": 20476, "title": "separate option for FE_OVERFLOW", "time": "2019-05-14T06:24:46Z", "body": "Summary:\nThere're overflow exceptions happening for legitimate computation like\nfor big x, sigmoid(x) = 1 / (1 + exp(-x)) = 1 / (1 + inf) = 1\nThis diff separates the option for FE_OVERFLOW to make caffe2_operator_throw_if_fp_exceptions=1 option less noisy.\n\nDifferential Revision: D15332947\n\n"}
{"number": 20477, "title": "tensorboard not updating", "time": "2019-05-14T06:39:36Z", "body": "i wrote a loss summary every 10 iters, while tensorboard did not show the latest loss.\r\neg. training iter now is 1000, while tensorboard only shows 100 \r\nfollowing is my code:\r\ninitialization:\r\n        self.writer = SummaryWriter(log_dir=cfg.LOG_DIR, flush_secs=10)\r\nloss:\r\n        self.writer.add_scalar('Train/iter_loss', losses.val.item(), i + (epoch - 1) * epoch_size)\r\n"}
{"number": 20478, "title": "time", "time": "2019-05-14T06:49:05Z", "body": "## ‚ùì Questions and Help\r\n\r\n### Please note that this issue tracker is not a help form and this issue will be closed.\r\n\r\nWe have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:\r\n\r\n- [Discussion Forum](https://discuss.pytorch.org/)\r\n"}
{"number": 20479, "title": "Customize loss function with caffe2.python", "time": "2019-05-14T07:16:30Z", "body": "Hi I just wondering if there is a way for us to customize loss function by using caffe2 python interface?\r\n\r\nIf the answer is yes, how could I implement the customized loss function? \r\n\r\nThank you very much!\r\n"}
{"number": 20480, "title": "Native TBB parallel backend", "time": "2019-05-14T08:27:20Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* **#20480 Native TBB parallel backend**\r\n* #20454 Restore TBB module\r\n* #20157 Support for Eigen thread pool\r\n* #20087 Native ATen/Parallel backend\r\n* #20057 Split ATen/Parallel into interface and backend\r\n* #20050 Move inter-op settings into ATen/Parallel\r\n\r\nSummary:\r\nUsing TBB for explicit parallelization within ops\r\n\r\nTest Plan:\r\nPARALLEL_BACKEND=NATIVE_TBB USE_CUDA=0 USE_OPENMP=0 USE_TBB=1 MKL_TBB=1\r\nBLAS=MKL USE_MKLDNN=1 MKLDNN_THREADING=TBB BUILD_BINARY=1\r\npython setup.py develop --cmake\r\n\r\n./build/bin/parallel_info\r\nOMP_NUM_THREADS=5 ./build/bin/parallel_info\r\nOMP_NUM_THREADS=5 MKL_NUM_THREADS=6 ./build/bin/parallel_info\r\n./build/bin/thread_init_test\r\n./build/bin/intra_inter_benchmark\r\n./build/bin/intra_inter_benchmark --intra_op_threads 1\r\npython  test/test_jit.py\r\npython  test/test_torch.py\r\n\r\nDifferential Revision: [D15333692](https://our.internmc.facebook.com/intern/diff/D15333692)"}
{"number": 20481, "title": "[docs] Automatically detect docs missing in rst", "time": "2019-05-14T08:50:32Z", "body": "I propose something like this to find automatically members missing from rst (could be put on some doc page as well). This could also be improved to make a list of public members missing docs altogether.\r\n```python\r\nrst_dir = 'docs/source'\r\n\r\nrst = '\\n'.join(open(os.path.join(rst_dir, rst_file)).read() for rst_file in os.listdir(rst_dir))\r\n\r\nmembers = lambda obj : [(member, getattr(getattr(obj, member), '__doc__') is not None) for member in dir(obj) if not member.startswith('_') and member not in rst]\r\n\r\ndoctodo = members(torch) + members(torch.nn) + members(torch.nn.functional)\r\n```"}
{"number": 20482, "title": "Remove checks for CUDA 8 in LU-based tests", "time": "2019-05-14T09:36:39Z", "body": "CUDA 8 is no longer supported and removed from CI, so these checks are irrelevant\r\n\r\n"}
{"number": 20483, "title": "[BC-BREAKING] Add scalar type info to tensor print", "time": "2019-05-14T10:09:43Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* **#20483 Add scalar type info to tensor print**\r\n\r\nThis got taken out when Type became scalar type agnostic. Adding it back. closes #20320\r\n\r\nDifferential Revision: [D15334010](https://our.internmc.facebook.com/intern/diff/D15334010)"}
{"number": 20484, "title": "Portable way of the warning clause", "time": "2019-05-14T10:26:48Z", "body": ""}
{"number": 20485, "title": "Wrap TH functions with native functions", "time": "2019-05-14T10:40:52Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* (to be filled)\n\n"}
{"number": 20486, "title": "MultiStepLR is broken", "time": "2019-05-14T10:48:40Z", "body": "## üêõ Bug\r\n\r\nMultiStepLR drops learning rate by gamma**2 times instead of gamma on each milestone. After one epoch it restores the lr to normal value.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nUse the following code:\r\n```python\r\nimport torch\r\nparams = [torch.nn.Parameter(torch.randn(1, 1))]\r\noptimizer = torch.optim.SGD(params, lr=0.2)\r\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[2,8], gamma=0.1)\r\nfor epoch in range(10):\r\n    print(scheduler.get_lr())\r\n    scheduler.step()\r\n```\r\nThe output is:\r\n[0.2]\r\n[0.2]\r\n[0.0020000000000000005] here should be 0.02\r\n[0.020000000000000004]\r\n[0.020000000000000004]\r\n[0.020000000000000004]\r\n[0.020000000000000004]\r\n[0.020000000000000004]\r\n[0.00020000000000000006] here should be 0.002\r\n[0.0020000000000000005]\r\n\r\n## Expected behavior\r\n\r\nIn the provided sample the learning rate should be adjusted only twice.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.5.0-12ubuntu1~16.04) 5.5.0 20171010\r\nCMake version: version 3.9.1\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: \r\nGPU 0: GeForce RTX 2080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 418.56\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.3\r\n[pip3] torch==1.1.0\r\n[pip3] torchstat==0.0.7\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] Could not collect\r\n\r\n"}
{"number": 20487, "title": "Don't concatenate string literals to reduce mobile binary size.", "time": "2019-05-14T13:22:03Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20487 Don't concatenate string literals to reduce mobile binary size.**\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nDifferential Revision: [D15334785](https://our.internmc.facebook.com/intern/diff/D15334785)"}
{"number": 20488, "title": "RoiAlignTest.CheckCPUGPUEqual is still flaky", "time": "2019-05-14T15:05:25Z", "body": "https://circle.pytorch.org/pattern-details.html?pattern_id=86\r\n\r\nPreviously #8084\r\n\r\nCC @yinghai @xkszltl\r\n\r\n```\r\nMay 14 00:21:35 [ RUN      ] RoiAlignTest.CheckCPUGPUEqual\r\nMay 14 00:21:37 WARNING: Logging before InitGoogleLogging() is written to STDERR\r\nMay 14 00:21:37 W0514 00:21:37.976260   425 init.h:115] Caffe2 GlobalInit should be run before any other API calls.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:221: Failure\r\nMay 14 00:21:39 Expected equality of these values:\r\nMay 14 00:21:39   y_cpu_vec[max_diff_idx]\r\nMay 14 00:21:39     Which is: 101.09059\r\nMay 14 00:21:39   y_gpu_vec[max_diff_idx]\r\nMay 14 00:21:39     Which is: 0\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 721.8125, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 721.8125,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 2275.1875, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 2275.1875,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 2096.53125, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 2096.53125,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 4172.3125, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 4172.3125,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 6203.2705078125, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 6203.2705078125,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 591.08331298828125, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 591.08331298828125,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 1683.916748046875, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 1683.916748046875,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 2437.04150390625, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 2437.04150390625,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 1097.0833740234375, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 1097.0833740234375,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 2124.6875, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 2124.6875,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 6226, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 6226,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 3807.91650390625, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 3807.91650390625,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 8495.6669921875, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 8495.6669921875,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 139, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 139,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 1715.8333740234375, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 1715.8333740234375,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 1678.8748779296875, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 1678.8748779296875,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 12598.4580078125, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 12598.4580078125,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 100.43750762939453, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 100.43750762939453,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 15708.25, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 15708.25,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 71.333335876464844, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 71.333335876464844,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 510.2083740234375, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 510.2083740234375,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 1136.6875, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 1136.6875,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 30, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 30,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 1689.541748046875, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 1689.541748046875,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 233.9375, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 233.9375,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 16.703125, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 16.703125,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 22944.833984375, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 22944.833984375,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 209.625, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 209.625,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 1629, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 1629,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 1963.3125, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 1963.3125,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 11.375000953674316, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 11.375000953674316,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 10961.9990234375, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 10961.9990234375,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 5428.7919921875, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 5428.7919921875,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 1781.9375, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 1781.9375,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 276.0625, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 276.0625,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 70.416664123535156, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 70.416664123535156,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 375.0625, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 375.0625,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 517.33331298828125, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 517.33331298828125,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 3557.33349609375, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 3557.33349609375,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 320.75, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 320.75,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:39 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:39 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 7591.5, which exceeds 1e-1, where\r\nMay 14 00:21:39 y_cpu_vec[max_diff_idx] evaluates to 7591.5,\r\nMay 14 00:21:39 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:39 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 1364.604248046875, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 1364.604248046875,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 6256.75, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 6256.75,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 9761.25, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 9761.25,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 1609.75, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 1609.75,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 1734.979248046875, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 1734.979248046875,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 1634.91650390625, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 1634.91650390625,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 1115.1875, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 1115.1875,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 82.500007629394531, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 82.500007629394531,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 23338.75, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 23338.75,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 142.125, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 142.125,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 624.0833740234375, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 624.0833740234375,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 2212.5625, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 2212.5625,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 3544.0625, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 3544.0625,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 5507.75, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 5507.75,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 1064.5, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 1064.5,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 1466.3123779296875, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 1466.3123779296875,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 1587.90625, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 1587.90625,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 146.72915649414062, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 146.72915649414062,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 21.375, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 21.375,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 27.125, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 27.125,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 1973.229248046875, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 1973.229248046875,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 147.25, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 147.25,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 7530.1875, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 7530.1875,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 563.22918701171875, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 563.22918701171875,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 916.75, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 916.75,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 5647.5, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 5647.5,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 405.8333740234375, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 405.8333740234375,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 287.87496948242188, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 287.87496948242188,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 471.4583740234375, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 471.4583740234375,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 14.625, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 14.625,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 297.9375, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 297.9375,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 23657.85546875, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 23657.85546875,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 1374.5625, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 1374.5625,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 3182, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 3182,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 1133.645751953125, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 1133.645751953125,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 5989.6669921875, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 5989.6669921875,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 1842.895751953125, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 1842.895751953125,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 102.02084350585938, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 102.02084350585938,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 4801.5625, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 4801.5625,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 7784.5, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 7784.5,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 1116.3125, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 1116.3125,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 1967.833251953125, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 1967.833251953125,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 2699.416748046875, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 2699.416748046875,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 12939.45703125, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 12939.45703125,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 /var/lib/jenkins/workspace/caffe2/operators/roi_align_op_gpu_test.cc:262: Failure\r\nMay 14 00:21:40 The difference between y_cpu_vec[max_diff_idx] and y_gpu_vec[max_diff_idx] is 292.50003051757812, which exceeds 1e-1, where\r\nMay 14 00:21:40 y_cpu_vec[max_diff_idx] evaluates to 292.50003051757812,\r\nMay 14 00:21:40 y_gpu_vec[max_diff_idx] evaluates to 0, and\r\nMay 14 00:21:40 1e-1 evaluates to 0.10000000000000001.\r\nMay 14 00:21:40 [  FAILED  ] RoiAlignTest.CheckCPUGPUEqual (5376 ms)\r\nMay 14 00:21:40 [----------] 1 test from RoiAlignTest (5376 ms total)\r\nMay 14 00:21:40 \r\nMay 14 00:21:40 [----------] Global test environment tear-down\r\nMay 14 00:21:40 [==========] 1 test from 1 test case ran. (5376 ms total)\r\nMay 14 00:21:40 [  PASSED  ] 0 tests.\r\nMay 14 00:21:40 [  FAILED  ] 1 test, listed below:\r\nMay 14 00:21:40 [  FAILED  ] RoiAlignTest.CheckCPUGPUEqual\r\n```\r\n\r\nhttps://circleci.com/gh/pytorch/pytorch/1680769?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link\r\n\r\nRNG should be deterministic here, I don't know why it is flaking."}
{"number": 20489, "title": "RuntimeError: Creating MTGP constants failed", "time": "2019-05-14T15:08:08Z", "body": "The MTGP constants error came back.\r\n\r\n```\r\nMay 14 00:04:46 + python main.py --epochs 1 --no-log\r\nMay 14 00:04:52 Traceback (most recent call last):\r\nMay 14 00:04:52   File \"main.py\", line 116, in <module>\r\nMay 14 00:04:52     train(epoch)\r\nMay 14 00:04:52   File \"main.py\", line 84, in train\r\nMay 14 00:04:52     output = model(data)\r\nMay 14 00:04:52   File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 493, in __call__\r\nMay 14 00:04:52     result = self.forward(*input, **kwargs)\r\nMay 14 00:04:52   File \"main.py\", line 64, in forward\r\nMay 14 00:04:52     x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\r\nMay 14 00:04:52   File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 493, in __call__\r\nMay 14 00:04:52     result = self.forward(*input, **kwargs)\r\nMay 14 00:04:52   File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/dropout.py\", line 102, in forward\r\nMay 14 00:04:52     return F.dropout2d(input, self.p, self.training, self.inplace)\r\nMay 14 00:04:52   File \"/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py\", line 870, in dropout2d\r\nMay 14 00:04:52     else _VF.feature_dropout(input, p, training))\r\nMay 14 00:04:52 RuntimeError: Creating MTGP constants failed. at /var/lib/jenkins/workspace/aten/src/THC/THCTensorRandom.cu:33\r\n```\r\n\r\nhttps://circleci.com/gh/pytorch/pytorch/1680491?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link"}
{"number": 20490, "title": "Fix strtod for MSVC", "time": "2019-05-14T15:17:01Z", "body": "Fixes https://github.com/pytorch/pytorch/issues/20408. Tested locally by @Jonas1312."}
{"number": 20491, "title": "[WIP] Another hotfix for Caffe2 Windows build", "time": "2019-05-14T16:02:52Z", "body": "Avoid the in source build that may cause the timeout issue."}
{"number": 20492, "title": "Add an AssertError check back to MultiheadAttention module", "time": "2019-05-14T16:21:02Z", "body": "Fix a typo in the doc.\r\nAdd an AssertError check back to MultiheadAttention module"}
{"number": 20493, "title": "Added engine information to the profiling result.", "time": "2019-05-14T17:51:31Z", "body": "Summary: This helps distinguish if the op was a quantized op or not.\n\nDifferential Revision: D15337854\n\n"}
{"number": 20494, "title": "Install only a specific version via pip", "time": "2019-05-14T18:21:14Z", "body": "Including `torch==1.1.0` as a requirement ends up installing ~500mb package which is not desirable in a production environment (we build a lot of containers and this makes them way larger than necessary).\r\n\r\nI know you can install a wheel directly like this: `pip install https://download.pytorch.org/whl/cpu/torch-1.0.1-cp36-cp36m-linux_x86_64.whl`, however, when you run `pip freeze` you end up with `torch==1.0.1` again, which means that this package doesn't fit into our normal workflow (`pip freeze > requirements.txt`).\r\n\r\nIs there a way to just install the CPU version of PyTorch that we need in prod? Something like `pip install torch-cpu`, for example."}
{"number": 20495, "title": "Improve attribute missing error message.", "time": "2019-05-14T18:39:35Z", "body": "When accessing a non-existent attribute on a script module, it gives the error below. It should instead say the attribute does not exist (which is different from it being None).\r\n```\r\n        class A(torch.jit.ScriptModule):\r\n            def __init__(self):\r\n                super(A, self).__init__()\r\n\r\n            @torch.jit.script_method\r\n            def forward(self, x):\r\n                return x + self.whatisgoingon\r\n\r\n        class B(A):\r\n            def __init__(self):\r\n                super(B, self).__init__()\r\n            @torch.jit.script_method\r\n            def bar(self, x):\r\n                return x * x \r\n        \r\n        A()\r\n```\r\n\r\n```\r\nattribute 'whatisgoingon' of type 'NoneType' is not usable in a script method (did you forget to add it __constants__?):\r\n@torch.jit.script_method\r\ndef forward(self, x):\r\n    return x + self.whatisgoingon\r\n               ~~~~~~~~~~~~~~~~~~ <--- HERE\r\n```"}
{"number": 20496, "title": "Require passing version_counter and allow_tensor_metadata_change to shallow_copy_and_detach()", "time": "2019-05-14T18:46:18Z", "body": "Previously, the caller of `shallow_copy_and_detach()` is responsible for deciding whether the shallow-copy should share the source TensorImpl's version counter, or have its own new version counter. However, since this decision is crucial for ensuring the correctness of the shallow-copy's version counter, we want to enforce users of `shallow_copy_and_detach()` to pass a version counter to the function call, so that they are required to make the decision at the time of API usage, not as an afterthought.\r\n\r\nFor similar reasons, we want to enforce users of `shallow_copy_and_detach()` to pass `allow_tensor_metadata_change` to the function call, so that they are required to decide \"whether the TensorImpl shallow-copy should allow tensor metadata change\" at the time of API usage, not as an afterthought."}
{"number": 20497, "title": "Statically make `__setstate__` set all attributes/parameters ", "time": "2019-05-14T19:07:32Z", "body": "#20242 added support for `__getstate__` and `__setstate__` on a module which is an alternative to automatically saving all parameters/attributes. This means the user must be initializing all parameters/attributes themselves. Like in TorchScript classes' `__init__()`, we should be statically verifying that this is actually happening.\n\ncc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @mruberry"}
{"number": 20498, "title": "identify important circleci builds", "time": "2019-05-14T19:25:56Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20498 identify important circleci builds**\n\nWe want to move toward running only dev-blocking builds on PRs and the\nrest on continuous. This saves tons of capacity and reduces noise for\ndevelopers.\n\nThe first step is trying to identify the minimal set of builds that can\nbe considered \"important\". The hope is that if all important builds\npass, the likelihood that the change is good is super high.\n\nThis PR also gives important builds more juice so that we get signal\nfaster.\n\nDifferential Revision: [D15342506](https://our.internmc.facebook.com/intern/diff/D15342506)"}
{"number": 20499, "title": "No guard against empty tensors for nn.functional.dropout()", "time": "2019-05-14T19:54:54Z", "body": "## üêõ Bug\r\n\r\npython crashes (with SIGFPE) when dropout is called on an empty cuda tensor.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nimport torch\r\ntorch.nn.functional.dropout(torch.Tensor([]).to('cuda'))\r\nFloating point exception (core dumped)\r\n\r\n## Expected behavior\r\n\r\nEither a python exception should be raised or it should be a no-op.\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): master\r\n - OS (e.g., Linux): centos\r\n - How you installed PyTorch (`conda`, `pip`, source): source\r\n - Build command you used (if compiling from source): \r\n - Python version: 3.6\r\n - CUDA/cuDNN version: cuda 9.2\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n"}
{"number": 20500, "title": "[jit] RuntimeError: v->type()->isSubtypeOf(elem_type)", "time": "2019-05-14T20:02:38Z", "body": "## üêõ Bug\r\n\r\n## To Reproduce\r\n\r\nThis code:\r\n```python\r\nclass MyMod2(torch.jit.ScriptModule):\r\n    __constants__ = ['mean', 'std']\r\n    def __init__(self):\r\n        super(MyMod2, self).__init__()\r\n\r\n        self.mean = [0.485, 0.456, 0.406]\r\n        self.std = [0.229, 0.224, 0.225]\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, input):\r\n        mean = torch.tensor(self.mean)\r\n        std = torch.tensor(self.std)\r\n        return input.sub(mean[:, None, None]).div_(std[:, None, None])\r\n```\r\n\r\nyields the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:/Development/_PytorchDev/pytorch-box2pix/jit.py\", line 75, in <module>\r\n    test = MyMod2()\r\n  File \"C:\\Python36\\lib\\site-packages\\torch\\jit\\__init__.py\", line 1047, in init_then_register\r\n    _create_methods_from_stubs(self, methods)\r\n  File \"C:\\Python36\\lib\\site-packages\\torch\\jit\\__init__.py\", line 1012, in _create_methods_from_stubs\r\n    self._c._create_methods(self, defs, rcbs, defaults)\r\nRuntimeError: v->type()->isSubtypeOf(elem_type) ASSERT FAILED at ..\\torch\\csrc\\jit\\ir.cpp:1272, please report a bug to PyTorch. (createList at ..\\torch\\csrc\\jit\\ir.cpp:1272)\r\n(no backtrace available)\r\n```\r\n\r\n## Expected behavior\r\n\r\nShould work without a problem.\r\n\r\n## Environment\r\n\r\n - PyTorch Version: 1.1.0\r\n\n\ncc @ezyang @gchanan @zou3519 @suo"}
{"number": 20501, "title": "eliminate FE_INVALID in optimizer related operators and tests", "time": "2019-05-14T20:05:25Z", "body": "Summary: Fixing unit tests related to optimizer related operators and tests\n\nDifferential Revision: D15307410\n\n"}
{"number": 20502, "title": "eliminate FE_INVALID in unit test", "time": "2019-05-14T20:09:37Z", "body": "Summary: Following D15307410 removing more floating point exceptions in unit tests\n\nDifferential Revision: D15340930\n\n"}
{"number": 20503, "title": "Allow static inheritence for ScriptModules", "time": "2019-05-14T20:25:10Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20503 Allow static inheritence for ScriptModules**\n\nSummary: Previously, we registered script modules of parent classes during their init methods, making it impossible\nto correctly 'override' a method or leave it blank in a subclass. This diff adds the functionality by waiting\nuntil the child init is finished before registering all the methods.\n\nTest Plan: python test/test_jit.py\n\nDifferential Revision: [D15341555](https://our.internmc.facebook.com/intern/diff/D15341555)"}
{"number": 20504, "title": "Merge pull request #1 from pytorch/master", "time": "2019-05-14T20:46:02Z", "body": "Update repo\r\n\r\n"}
{"number": 20505, "title": "Fix upsample kernel launch / reorder arguments", "time": "2019-05-14T20:53:48Z", "body": "this is a follow up for https://github.com/pytorch/pytorch/pull/19630"}
{"number": 20506, "title": "Allow recency weight pooling for fp16", "time": "2019-05-14T21:18:25Z", "body": "Summary: as titled\n\nDifferential Revision: D15342758\n\n"}
{"number": 20507, "title": "[DO NOT MERGE] Testing if builder changes are compatible with master", "time": "2019-05-14T21:23:38Z", "body": ""}
{"number": 20508, "title": "stop build spew on development", "time": "2019-05-14T21:28:49Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* **#20508 stop build spew on development**\r\n\r\nIt is important when developing that trivial rebuilds do not blow away your terminal state. This reverts the addition of -v to ninja, which causes the build to spew.\n\nDifferential Revision: [D15343207](https://our.internmc.facebook.com/intern/diff/D15343207)"}
{"number": 20509, "title": "split and register CollectAndDistributeFpnRpnProposals with C10", "time": "2019-05-14T21:30:19Z", "body": "Differential Revision: D15302181\n\n"}
{"number": 20510, "title": "[wip][jit] Don't lower graphs by default", "time": "2019-05-14T22:16:31Z", "body": ""}
{"number": 20511, "title": "Removing cyclic dependency", "time": "2019-05-14T22:16:49Z", "body": "Summary: Removed cyclic dependency of caffe2/core/net.h and workspace.h\n\nDifferential Revision: D15303412\n\n"}
{"number": 20512, "title": "Fixing typos in schema description for BatchMatMul", "time": "2019-05-14T22:31:21Z", "body": "Summary: Fixing typos in the description of schema for one of the inputs for BatchMatMul operator.\n\nDifferential Revision: D15343879\n\n"}
{"number": 20513, "title": "Infer schema for experimental ops", "time": "2019-05-14T23:27:32Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20383 [wip] torch::jit::RegisterOperators forwards to c10::RegisterOperators&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15300937/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20561 De-deprecate parts of the legacy API&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15364271/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20514 Options based registration API&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15346348/)\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20513 Infer schema for experimental ops**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15346349/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20379 Allow nested lists/dicts in legacy operator API&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15287693/)\n\nThey've been using an old API, switch them to the new one instead.\n\nDifferential Revision: [D15346349](https://our.internmc.facebook.com/intern/diff/D15346349/)"}
{"number": 20514, "title": "Options based registration API", "time": "2019-05-14T23:27:40Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20383 torch::jit::RegisterOperators forwards to c10::RegisterOperators&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15300937/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20561 De-deprecate parts of the legacy API&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15364271/)\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20514 Options based registration API**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15346348/)\n\nChange API from\n\n    static auto registry = c10::RegisterOperators()\n      .op(\"my::op\",\n        c10::kernel(...),\n        c10::dispatchKey(...)\n      );\n\nto\n\n    static auto registry = c10::RegisterOperators()\n      .op(\"my::op\", c10::RegisterOperators::options()\n        .kernel(...)\n        .dispatchKey(...)\n      );\n\nbecause this allows better discoverability. People looking for which options are available will easier find it and IDE autocompletion will work better.\n\nDifferential Revision: [D15346348](https://our.internmc.facebook.com/intern/diff/D15346348/)"}
{"number": 20515, "title": "Add GivenTensorInt16Fill", "time": "2019-05-14T23:48:07Z", "body": "Summary: Needed by the upcoming quantized version of GenerateProposals\n\nDifferential Revision: D14430952\n\n"}
{"number": 20516, "title": "MobileNetV2 export to ONNX fails", "time": "2019-05-15T00:52:33Z", "body": "## üêõ Bug\r\nConverting mobilenet_v2  from examples directory into ONNX fails with:\r\n```\r\n ...\r\nFile \"...\\torch\\onnx\\utils.py\", line 189, in _optimize_graph\r\n    graph = torch._C._jit_pass_onnx(graph, operator_export_type)\r\nFile \"...\\torch\\onnx\\__init__.py\", line 57, in _run_symbolic_function\r\n    return utils._run_symbolic_function(*args, **kwargs)\r\nFile \"...\\torch\\onnx\\utils.py\", line 593, in _run_symbolic_function\r\n    return op_fn(g, *inputs, **attrs)\r\nFile \"...\\torch\\onnx\\symbolic_opset9.py\", line 208, in symbolic\r\n    dim, keepdim = sym_help._get_const(dim, 'i', 'dim'), sym_help._get_const(keepdim, 'i', 'keepdim')\r\nFile \"...torch\\onnx\\symbolic_helper.py\", line 114, in _get_const\r\n    return _parse_arg(value, desc)\r\nFile \"...\\torch\\onnx\\symbolic_helper.py\", line 73, in _parse_arg\r\n    return int(tval)\r\nValueError: only one element tensors can be converted to Python scalars\r\n```\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nmobilenet_v2 obtained using `examples/imagenet/main.py` . Converted using:\r\n```\r\nmodel.train(False)\r\ninput = torch.randn(1, 3, 224, 224, requires_grad=True)\r\noutput = model(input)\r\ntorch_out = torch.onnx._export(model, input, \"model.onnx\", export_params=True, do_constant_folding=True)\r\n```\r\n## Environment\r\nPyTorch version: 1.1.0a0+6e82b1c\r\nIs debug build: No\r\nCUDA used to build PyTorch: Could not collect\r\n\r\nOS: Microsoft Windows 10 Enterprise\r\nGCC version: Could not collect\r\nCMake version: version 3.13.3\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: NVS 310\r\nNvidia driver version: 353.62\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.3\r\n[pip3] torch==1.1.0\r\n[pip3] torch-nightly==1.1.0.dev20190513\r\n[pip3] torchvision==0.2.2.post3\r\n[pip3] torchvision-nightly==0.2.3\r\n\r\n\r\n"}
{"number": 20517, "title": "[pt1][tensor] Tensor codemod for instance_norm", "time": "2019-05-15T01:31:11Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20517 [pt1][tensor] Tensor codemod for instance_norm**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15349006/)\n\nfixing a bug in instance_norm\n\nDifferential Revision: [D15349006](https://our.internmc.facebook.com/intern/diff/D15349006/)"}
{"number": 20518, "title": "[pypi] Unavailability of Source Distribution for torch and torchvision", "time": "2019-05-15T02:34:05Z", "body": "## üöÄ Feature\r\nI need to install `pytorch` through `pypi` (https://pypi.org/project/pytorch/) but it says:\r\n\r\n> You tried to install ‚Äúpytorch‚Äù. The package named for PyTorch is ‚Äútorch\"\r\n\r\nWhile trying to install `torch` and `torchvision` through `pypi`, I found that there is no source distribution available for both, there are only wheels. In order to include any library, we require only source distribution available on `pypi` and due to unavailability of the same, it is not part of officially supported libraries within our company due to which we are not able to use `pytorch`.\r\n\r\n\r\n## Motivation\r\nNot able to build any `pytorch` model. Reason: Unavailability of source distribution of `torch` and `torchvision` on `pypi` and unsuccessful installation of `pytorch`\r\n\r\nQuestion: What is the purpose of having `pytorch` source distribution if it can't be installed via `pip`? (shows above error)\r\n\r\n## Pitch\r\n1. Availability of source distributions of `torch` and `torchvision` on `pypi`\r\n2. Alternatively successful installation of `pytorch` (since it's source distribution is available) on `pypi`.\r\n\r\n## Alternatives\r\nNone\r\n\r\n## Additional context\r\nNone\r\n"}
{"number": 20519, "title": "Import things on torch.utils", "time": "2019-05-15T03:56:23Z", "body": "So that users can use it directly"}
{"number": 20520, "title": "enable CopyVector for type of int on CUDA", "time": "2019-05-15T06:22:50Z", "body": "Summary: as title\n\nDifferential Revision: D15351010\n\n"}
{"number": 20521, "title": "[jit] add 'all' builtin", "time": "2019-05-15T06:33:32Z", "body": "[jit] add 'all' builtin"}
{"number": 20522, "title": "nn.CTCLoss RuntimeError on GPU", "time": "2019-05-15T06:47:43Z", "body": "## üêõ Bug\r\n\r\nnn.CTCLoss Run the official documentation sample code works fine on the CPU but  RuntimeError on the GPU.\r\n\r\n## To Reproduce\r\n\r\n```py\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\n>>> T = 255      # Input sequence length\r\n>>> C = 20      # Number of classes (excluding blank)\r\n>>> N = 16      # Batch size\r\n>>> S = 30      # Target sequence length of longest target in batch\r\n>>> S_min = 10  # Minimum target length, for demonstration purposes\r\n>>>\r\n>>> # Initialize random batch of input vectors, for *size = (T,N,C)\r\n>>> input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()\r\n>>>\r\n>>> # Initialize random batch of targets (0 = blank, 1:C+1 = classes)\r\n>>> target = torch.randint(low=1, high=C+1, size=(N, S), dtype=torch.long)\r\n>>>\r\n>>> input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\r\n>>> target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)\r\n\r\ninput = input.cuda()\r\ntarget = target.cuda()\r\ninput_lengths = input_lengths.cuda()\r\ntarget_lengths = target_lengths.cuda()\r\n\r\n>>> ctc_loss = nn.CTCLoss()\r\n>>> loss = ctc_loss(input, target, input_lengths, target_lengths)\r\n>>> loss.backward()\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-10-24c5c245c677> in <module>\r\n      1 ctc_loss = nn.CTCLoss()\r\n      2 loss = ctc_loss(input, target, input_lengths, target_lengths)\r\n----> 3 loss.backward()\r\n\r\n~/anaconda3/lib/python3.7/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)\r\n    105                 products. Defaults to ``False``.\r\n    106         \"\"\"\r\n--> 107         torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n    108 \r\n    109     def register_hook(self, hook):\r\n\r\n~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\r\n     91     Variable._execution_engine.run_backward(\r\n     92         tensors, grad_tensors, retain_graph, create_graph,\r\n---> 93         allow_unreachable=True)  # allow_unreachable flag\r\n     94 \r\n     95 \r\n\r\nRuntimeError: setStorage: sizes [16, 255, 31], strides [15045, 59, 2], and storage offset 0 requiring a storage size of 240722 are out of bounds for storage with numel 240720\r\n```\r\n\r\nSee https://gist.github.com/ypw-rich/faafe22d108c4fe24f128ecb4aaabda3\r\n\r\n## Expected behavior\r\n\r\nExpect same grad on CPU.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: GeForce GTX 1080 Ti\r\nNvidia driver version: 418.56\r\ncuDNN version: /data/ocr/bin/libcudnn.so.7\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.2\r\n[pip3] numpydoc==0.8.0\r\n[pip3] torch==1.1.0\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.3                      199\r\n[conda] mkl-service               1.1.2            py37he904b0f_5\r\n[conda] mkl_fft                   1.0.10           py37ha843d7b_0\r\n[conda] mkl_random                1.0.2            py37hd81dba3_0\r\n[conda] torch                     1.1.0                    pypi_0    pypi\r\n[conda] torchvision               0.2.2.post3              pypi_0    pypi\r\n"}
{"number": 20523, "title": "nn.Upsample scale_factor does not support tuple inputs", "time": "2019-05-15T08:01:07Z", "body": "## üêõ Bug\r\n\r\nWhen adding an nn.Upsample module with different scale factors per axis, i.e. a tuple of floats matching input size, a TypeError is raised.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\nimport torch.nn as nn\r\nupsample_layer = nn.Upsample(scale_factor=(2,2))\r\n\r\nFails and raises: \r\nTypeError: float() argument must be a string or a number, not 'tuple'\r\n\r\nAccording to the docs, it should handle, and in previous versions (0.4.0) it did. Broke after upgrading to 1.1.0\r\n\r\nLikely to be caused by the added type check not supporting tuples in nn/modules/upsampling.py line 125:\r\n        self.scale_factor = float(scale_factor) if scale_factor else None\r\nWhich cannot handle tuple inputs.\r\n\r\n## Environment\r\nPyTorch 1.1.0"}
{"number": 20524, "title": "[MacOS] datasets.ImageFolder() silently fails conversion for 16-bit PNG", "time": "2019-05-15T08:05:51Z", "body": "The following code falsely converts my 16-bit PNG (0-65535, mean ~40000) into weird (0-1, mean 0.99) distributions. This wasted me ~3 hours to debug !!! Please at least show error message instead of silently fail !!!!\r\nI attached the pngs, when I converted my PNG to 8-bit, it works.\r\n\r\nNote that standalone `transforms.ToTensor()(PIL Image)` always works, see below.\r\n\r\nMy code following https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html:\r\n```\r\nimport torch,PIL\r\nfrom PIL import Image\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nfrom torch.optim import lr_scheduler\r\nimport numpy as np\r\nimport cv2\r\nimport torchvision\r\nfrom torchvision import datasets, models, transforms\r\nimport matplotlib.pyplot as plt\r\nimport time\r\nimport os\r\nfrom collections import Counter\r\nimport copy\r\nprint(PIL.__version__)\r\nprint(torch.__version__)\r\nprint(torchvision.__version__)\r\n\r\nplt.ion()   # interactive mode\r\n\r\n# Data augmentation and normalization for training\r\n# Just normalization for validation\r\ndata_transforms = {\r\n    'train': transforms.Compose([\r\n#         transforms.RandomResizedCrop(224),\r\n        transforms.Resize((256,256)),\r\n#         transforms.RandomHorizontalFlip(),\r\n        transforms.ToTensor(),\r\n#         transforms.Normalize((0,0,0), (0.1,0.1,0.1))\r\n    ]),\r\n    'val': transforms.Compose([\r\n#         transforms.RandomResizedCrop(224),\r\n        transforms.Resize((256,256)),\r\n#         transforms.CenterCrop(224),\r\n        transforms.ToTensor(),\r\n#         transforms.Normalize((0.5,0.5,0.5), (1,1,1))\r\n#         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\r\n#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\r\n    ]),\r\n}\r\n\r\ndata_dir = 'data/hymenoptera_data'\r\ndata_dir = 'data/m_data'\r\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\r\n                                          data_transforms[x])\r\n                  for x in ['train', 'val']}\r\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\r\n                                             shuffle=0, num_workers=4)\r\n              for x in ['train', 'val']}\r\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\r\nclass_names = image_datasets['train'].classes\r\n\r\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\nprint(device)\r\n\r\ndef imshow(inp, title=None):\r\n    \"\"\"Imshow for Tensor.\"\"\"\r\n    print(inp.shape) # m_data = torch.Size([3, 260, 1034])\r\n    s(inp.numpy())\r\n    # ants = torch.Size([3, 260, 1034])\r\n    inp = inp.numpy().transpose((1, 2, 0))\r\n#     mean = 0#np.array([0.485, 0.456, 0.406])\r\n#     std = 0.1#np.array([0.229, 0.224, 0.225])\r\n#     inp = std * inp + mean\r\n#     inp = np.clip(inp, 0, 1)\r\n#     s(inp)\r\n    plt.imshow(inp)\r\n    if title is not None:\r\n        plt.title(title)\r\n    plt.pause(0.001)  # pause a bit so that plots are updated\r\n\r\n\r\n# Get a batch of training data\r\ninputs, classes = next(iter(dataloaders['train']))\r\na=inputs.numpy()[0,:,:,:].transpose((1, 2, 0))\r\ns(a)\r\nplt.imshow(a), plt.pause(0.001)\r\n\r\n# Make a grid from batch\r\nout = torchvision.utils.make_grid(inputs)\r\n\r\nd1 = 'data/hymenoptera_data'\r\nd2 = 'data/m_data'\r\nim = transforms.ToTensor()(Image.open(d2+'/train/benign/0.png')).numpy().transpose((1,2,0))\r\ns(im)\r\ndef s(a):\r\n    print(a.shape, a.max(), a.min(), a.mean(), a.std(), Counter(a.ravel()).most_common()[:10])\r\n```\r\n\r\n//print(PIL.__version__)\r\n//print(torch.__version__)\r\n//print(torchvision.__version__)\r\n5.4.1\r\n1.1.0\r\n0.2.2\r\n\r\n#Code Output (WRONG output when using 16-bit PNG): \r\ncpu\r\n(256, 256, 3) 1.0 0.0 **0.96930856 0.16557895** [(1.0, 188979), (0.0, 4488), (0.3137255, 144), (0.56078434, 135), (0.8117647, 117), (0.9372549, 114), (0.6862745, 114), (0.0627451, 108), (0.4392157, 93), (0.1882353, 69)]\r\n\r\n(224, 224, 1) 65535 0 **42820.1917450574 14888.386293001755** [(0, 1511), (54708, 117), (54611, 116), (54489, 114), (54513, 113), (55001, 111), (54002, 109), (54172, 108), (53660, 107), (53391, 107)]\r\n\r\n\r\n#Code Output (correct when using 8-bit PNG): \r\ncpu\r\n(256, 256, 3) 0.9607843 0.0 **0.65344715 0.22703604** [(0.0, 5157), (0.827451, 4164), (0.83137256, 4098), (0.8235294, 4041), (0.8352941, 3846), (0.81960785, 3714), (0.8392157, 3639), (0.7764706, 3432), (0.8156863, 3375), (0.77254903, 3303)]\r\n\r\n(224, 224, 1) 1.0 0.0 **0.65339714 0.22718629** [(0.0, 1539), (0.83137256, 1087), (0.8235294, 970), (0.8392157, 966), (0.827451, 941), (0.81960785, 937), (0.8156863, 896), (0.78039217, 895), (0.8352941, 855), (0.7647059, 832)]\r\n\r\n\r\n - OS (e.g., Linux): Mac\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n\r\n\r\n![0_8bit](https://user-images.githubusercontent.com/4177403/57758574-d1556d80-76ac-11e9-946e-35e9cc13784f.png)\r\n![0_16bit](https://user-images.githubusercontent.com/4177403/57758587-d6b2b800-76ac-11e9-828f-78e800d44666.png)\r\n"}
{"number": 20525, "title": "Wrong CMake version in libtorch C++ binary packages", "time": "2019-05-15T09:05:54Z", "body": "## üêõ Bug\r\n\r\nCMake version (in `TorchConfigVersion.cmake`) seems stuck to 1.0.0 in the latest libtorch C++ windows packages (CUDA or not).\r\nThis problem may also have affected the previous 1.0.1 packages.\r\n\r\nAnother big problem for me is that I can't find any more link to download libtorch C++ 1.0.1 packages, \r\nupon which I based all my current developments.\r\n\r\n## To Reproduce\r\n\r\nDownload the latest package here:\r\nhttps://download.pytorch.org/libtorch/cu90/libtorch-win-shared-with-deps-latest.zip\r\n\r\n## Expected behavior\r\n\r\n`TorchConfigVersion.cmake` with correct version.\r\n\r\n## Environment\r\n\r\n - PyTorch Version: 1.1\r\n - OS: Windows\r\n - How you installed PyTorch: binary package\r\n - CUDA/cuDNN version: 9\r\n\r\nThanks for your help,\r\n\r\n Albert"}
{"number": 20526, "title": "Fix GetLastError in THAllocator for Windows", "time": "2019-05-15T09:13:13Z", "body": ""}
{"number": 20527, "title": "StepLR, MultiStepLR, ExponentialLR and CosineAnnealingLR scheduler wrong lr value", "time": "2019-05-15T09:16:34Z", "body": "When the StepLR, MultiStepLR, ExponentialLR or CosineAnnealingLR scheduler is called with the same epoch parameter the optimizer value is further reduced even though it's the same epoch\r\n\r\na sample code\r\n```\r\nimport torch.optim as optim\r\nfrom torch import nn\r\n\r\nconv = nn.Conv2d(3,3,3)\r\noptimizer = optim.Adam(conv.parameters()) \r\nlr_scheduler = optim.lr_scheduler.StepLR(optimizer, 2)\r\n```\r\nnow if we call `lr_scheduler.step(epoch=2)` multiple times \r\n```\r\nfor _ in range(10):\r\n    lr_scheduler.step(2)\r\n    print(optimizer.param_groups[0]['lr'])\r\n```\r\noutput:\r\n```\r\n>>> 0.0001\r\n>>> 1e-05\r\n>>> 1.0000000000000002e-06\r\n>>> 1.0000000000000002e-07\r\n>>> 1.0000000000000004e-08\r\n>>> 1.0000000000000005e-09\r\n>>> 1.0000000000000006e-10\r\n>>> 1.0000000000000006e-11\r\n>>> 1.0000000000000006e-12\r\n>>> 1.0000000000000007e-13\r\n```\r\n\r\nEven if such use-case is bizarre, this is extremely unexpected.\r\nThis is happens using PyTorch version 1.1.0 but not 1.0.1. Because pull(#14010) redefined StepLR, MultiStepLR, ExponentialLR and CosineAnnealingLR to directly use the learning rate variable of the optimizer rather than using `base_lr` defined by `_LRScheduler`. This was in order to support multiple simultaneous schedulers (#13022)!\r\n"}
{"number": 20528, "title": "An outer torch.no_grad() is ignored inside a thread", "time": "2019-05-15T09:32:48Z", "body": "## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nAn outer torch.no_grad() is ignored inside a thread (e.g. `ThreadPoolExecutor.map`).\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n``` python\r\nimport torch\r\nfrom concurrent.futures import ThreadPoolExecutor\r\n\r\n# dummy data and network\r\ninput_data = torch.arange(10, dtype=torch.float32)\r\nnet = torch.nn.Sequential(\r\n    torch.nn.Linear(10, 1)\r\n)\r\n\r\nwith ThreadPoolExecutor(2) as ex:\r\n    with torch.no_grad():\r\n        # no_grad is working\r\n        assert net(input_data).grad_fn is None\r\n        \r\n        # Should fail because of the \"not\" <-------------------------------\r\n        assert list(ex.map(net, [input_data]))[0].grad_fn is not None\r\n        \r\n        # no_grad is working\r\n        assert list(ex.map(torch.no_grad()(net), [input_data]))[0].grad_fn is None\r\n        \r\n        # no_grad is working\r\n        assert net(input_data).grad_fn is None\r\n        \r\n    # Can calculate the gradient\r\n    assert net(input_data).grad_fn is not None\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\nThe result of `list(ex.map(net, [input_data]))` should not have a `grad_fn`.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: \r\nGPU 0: GeForce RTX 2070\r\nGPU 1: GeForce RTX 2070\r\n\r\nNvidia driver version: 415.25\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.5.0\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.3\r\n[pip] numpy-ringbuffer==0.2.1\r\n[pip] numpydoc==0.8.0\r\n[pip] pytorch-sconce==1.3.4\r\n[pip] pytorchviz==0.0.1\r\n[pip] torch==1.1.0\r\n[pip] torch-nightly==1.0.0.dev20181114\r\n[pip] torchvision==0.2.1\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.3                      199  \r\n[conda] mkl-service               1.1.2            py36he904b0f_5  \r\n[conda] mkl_fft                   1.0.10           py36ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py36hd81dba3_0  \r\n[conda] torch                     1.1.0                    pypi_0    pypi\r\n[conda] torchvision               0.2.2.post3              pypi_0    pypi\r\n\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"}
{"number": 20529, "title": "Segmentation fault (core dumped) when passing empty Tensors to pack_padded_sequence", "time": "2019-05-15T11:17:41Z", "body": "Passing an empty tensor (so with shape (0,n,m)) to pack_padded_sequence causes a \"segmentation fault (core dumped)\"\r\n\r\nProduce the error:\r\n```\r\nimport torch\r\nfrom torch.nn.utils.rnn import pack_padded_sequence\r\n#bs, max_sequence_len, emb_dim\r\nx = torch.zeros([16,10,20])\r\nlength = torch.zeros([16])\r\nmask = [length != 0]\r\nmasked_x = x[mask]\r\nmasked_length = length[mask]\r\nprint(masked_x.shape, masked_length.shape)\r\npack_padded_sequence(masked_x, masked_length, batch_first = True)\r\n```\r\n\r\nExpected behavior:\r\nAn exception raised stating that the input tensor must not be empty.\r\n\r\nEnvironments created on:\r\n-PyTorch 1.0, Windows (installed via conda), CUDA (9.0)\r\n-PyTorch 1.0, Linux (installed via pip), CUDA ('9.0.176')\r\n\r\nThanks,\r\nJamie"}
{"number": 20530, "title": "Enable simd and loop vectorizer with MSVC", "time": "2019-05-15T13:12:37Z", "body": ""}
{"number": 20531, "title": "c++ torch::nn::Sequential increments count on name errors", "time": "2019-05-15T14:43:41Z", "body": "## üêõ Bug\r\n\r\nSequential size() returns incremented count when _push_back_ fails on name errors:\r\n\r\n```\r\n torch::nn::Sequential s; size_t n=0;\r\n std::cout << \"initial size: \" << s->size() << \"\\n\";\r\n ASSERT_THROWS_WITH(\r\n      s->push_back(\"name.with.dot\", torch::nn::Linear(3, 4)),\r\n      \"Submodule name must not contain a dot (got 'name.with.dot')\");\r\n  ASSERT_THROWS_WITH(\r\n      s->push_back(\"\", torch::nn::Linear(3, 4)),\r\n      \"Submodule name must not be empty\");\r\n  std::cout << \"size after name errors: \" << s->size() << \"\\n\";\r\n  for(auto&c:s->named_children()) n++;\r\n  std::cout << \"size of named children: \" << n << \"\\n\";\r\n```\r\nprints:\r\n```\r\ninitial size: 0\r\nsize after name errors: 2\r\nsize of named children: 0\r\n```"}
{"number": 20532, "title": "Couple hundred MB are taken just by initializing cuda", "time": "2019-05-15T15:06:09Z", "body": "hello,\r\nit's seems that any process that use cuda consume at initializing step around 300MB to 400MB!.\r\nThe problem that this memory overhead happened on the GPU RAM, what cause cuda OUT OF MEMORY ERROR.  \r\n\r\nThis is very problematic in my scenario of multiple process that use the cuda resource.\r\nEnvironments:\r\nCloud: AWS\r\nMachine Type: single p3.8xlarge machine with only 4 gpus\r\nMachine Image: \"Deep Learning AMI (Amazon Linux) Version 11.0\" with pytorch 1.1(installed locally)\r\nnumber of workers that use CUDA: 2 - 100 workers\r\n\r\nto use cuda at all process i implemented smart GPU shared policy to allow all the worker to use the GPU(with acquiring available GPU and release at the end of the training)\r\n and accelerate the training time of distributed system.\r\n\r\nat multiprocessing environment this initialization overhead in memory at the GPU is is not feasible and cause cuda OOM error, because each worker \"contribute\" 400MB for each GPU! what cause the exception (overthead of 400*num_workers MB for each GPU) \r\n\r\ni  test the init by creating a=torch.cuda.FloatTensor(1).\r\n\r\n\r\nwhy it's need so many memory on the GPU?\r\ncan i release it?\r\n  "}
{"number": 20533, "title": "ONNX Export Slice and Flip ops for opset 10", "time": "2019-05-15T15:47:12Z", "body": ""}
{"number": 20534, "title": "Remove TH/THC link for single matrix inverse", "time": "2019-05-15T15:48:07Z", "body": "- Earlier, we had to use the legacy implementation of `getri` for single matrix inverse from TH and THC\r\n- Now, this has been moved to ATen\r\n\r\nChangelog:\r\n- Move single matrix inverse implementation to ATen\r\n- Remove unused code in TH and THC resulting from the change\r\n- Minor modifications made to single matrix CPU function implementations in ATen to avoid redundancy\r\n\r\nTest Plan:\r\n- Existing tests should pass to verify if the change is valid"}
{"number": 20535, "title": "Unfork sleef", "time": "2019-05-15T15:54:43Z", "body": "We are currently tracking a fork of sleef that lives in https://github.com/zdevito/sleef This is bad because it prevents us from easily taking upstream fixes; e.g., the fix for https://github.com/pytorch/pytorch/issues/20404 We should unfork.\r\n\r\nWe are running two patches:\r\n- https://github.com/zdevito/sleef/commit/191f655caa25526ae226cf88dd2529265176014a it is fixed upstream by 79df29e3a2f9a4ef9c4220a70c1b40cf120e7e17\r\n- https://github.com/zdevito/sleef/commit/9b249c53a80343cc1a394ca961d7d5696ea76409 reported upstream at https://github.com/shibatch/sleef/issues/261 but we haven't submitted the PR upstream yet\r\n\r\ncc @zdevito @colesbury "}
{"number": 20536, "title": "Update sleef.", "time": "2019-05-15T16:02:40Z", "body": "This is still forked, but we merge in changes from upstream.\r\n\r\nAmong other changes, this should fix #20404.\r\n\r\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\r\n\r\n"}
{"number": 20537, "title": "Recursively checkout submodules for Pytorch", "time": "2019-05-15T16:41:44Z", "body": ""}
{"number": 20538, "title": "Expose SparseToDenseMask in torch", "time": "2019-05-15T16:42:52Z", "body": "Differential Revision: D15354753\n\n"}
{"number": 20539, "title": "Update submodules recursively in all circleci jobs", "time": "2019-05-15T16:57:15Z", "body": "similar for jenkins jobs done here\r\nhttps://github.com/pytorch/ossci-job-dsl/commit/f59ee7b827de7418bee38036035c0cb50e26c282\r\nhttps://github.com/pytorch/ossci-job-dsl/commit/b6bdbdeece843811c4159a054d5cedcf8cbf83eb\r\n"}
{"number": 20540, "title": "Forcing gcc ABI and safer bash scripts, v2", "time": "2019-05-15T16:58:27Z", "body": "First time this was merged it broke master and was reverted. This time I do not add ```set -u``` to the .circleci/scripts/setup* scripts. There's still a chance that ```set -u``` breaks the binary builds on master, but at least those can be fixed in parallel and don't completely eliminate signal from all merges.\r\n\r\nFor  https://github.com/pytorch/pytorch/issues/17492"}
{"number": 20541, "title": "fix empty dropout", "time": "2019-05-15T17:16:21Z", "body": "Fix for #20499 "}
{"number": 20542, "title": "Automatic update of fbcode/onnx to ead449a30d026a7a0a59e2ba0a42ca8e52ec2359", "time": "2019-05-15T17:38:13Z", "body": "Summary:\nPrevious import was e08efaa35ed54362dfa283240506c003175889b7\n\nIncluded changes:\n- **[ead449a3](https://github.com/onnx/onnx/commit/ead449a3)**: fix array range bug (#2015) <one-hello>\n- **[0442d426](https://github.com/onnx/onnx/commit/0442d426)**: Relax constraint on subgraph input/output type and shape (#2009) <Bowen Bao>\n\nDifferential Revision: D15350320\n\n"}
{"number": 20543, "title": "Use slimmer exception raising code when on mobile.", "time": "2019-05-15T19:13:51Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20543 Use slimmer exception raising code when on mobile.**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15353447/)\n\nAll of that code for concatenating strings together adds up. Just discard it all for mobile builds.\n\nDifferential Revision: [D15353447](https://our.internmc.facebook.com/intern/diff/D15353447/)"}
{"number": 20544, "title": "[WIP] Memory_format part 2", "time": "2019-05-15T19:21:45Z", "body": ""}
{"number": 20545, "title": "provide \"size\" parameter in torch.normal when called with two floats", "time": "2019-05-15T19:28:01Z", "body": "This has been requested in #20323\r\n\r\n(It is still not exactly the same as NumPy, which allows you to pass tensors at mean/std and broadcast them with size, but the present PR is extremely simple and does the main thing people are asking for)"}
{"number": 20546, "title": "Saving state_dicts should capture shared state", "time": "2019-05-15T19:41:02Z", "body": "## üöÄ Feature\r\nAdd metadata to the state_dict to support round trip saving and loading. Specifically, if the same object appears more than once in the object hierarchy, then that fact should be captured in the state_dict representation so that an object produced via `load_state_dict` is equivalent to the original object.\r\n\r\n## Motivation\r\n\r\nIn a hierarchy of nn.Modules wherein one object can appear in multiple places, the state_dict shows the state in all places, but when the object is mutated via one of its attribute aliases, the state will update in all places. This is good and correct, but when you save this state_dict this information is lost; If you load the state_dict from a file, what was once a single object is now multiple objects that are completely separate in memory. This means that saving and loading in PyTorch is not round-trip equivalent.\r\n\r\nIdeally, saving an object x, and then loading x's state into a new object y should produce two objects that are completely equivalent, in other words x should equal y (besides the memory address)\r\n\r\nHere is an example in code:\r\n```python\r\nclass A(nn.Module):\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.x = nn.Linear(2, 2)\r\nclass B(nn.Module):\r\n    def __init__(self, *args, a=None, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.a = A() if a is None else a\r\nclass C(nn.Module):\r\n    def __init__(self, *args, a=None, b=None, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.a = A() if a is None else a\r\n        self.b = B() if b is None else b\r\n```\r\nAnd the actual behavior is:\r\n```\r\n>>> a = A()\r\n>>> b = B(a=a)\r\n>>> c = C(a=a, b=b)\r\n>>> c.b.a is c.a\r\nTrue\r\n>>> c.b.a.x[0][0] = 0.1111\r\n>>> c.a.x[0][0]\r\n0.1111\r\n>>> s = c.state_dict()\r\n>>> c2 = C()\r\n>>> c2.load_state_dict(s)\r\n>>> c2.b.a is c2.a\r\nFalse\r\n```\r\nIdeally, the final check would produce:\r\n```\r\n>>> c2.b.a is c2.a\r\nTrue\r\n>>> c2.b.a.x[0][0] = 0.2222\r\n>>> c2.b.a.x[0][0] == c2.a.x[0][0]\r\nTrue\r\n```\r\n\r\n## Pitch\r\n\r\nUsing the `state_dict._metadata`, we could add information about children modules being equivalent. This information can be duplicated across all common ancestors. Then, either before or after loading state in `load_state_dict` or its helper methods, you could check\r\n`if self.b.a is not self.a: self.b.a = self.a`\r\n\r\n## Alternatives\r\n\r\n[TODO?]\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n"}
{"number": 20547, "title": "Fix dll linkage for tensor type ids", "time": "2019-05-15T20:12:56Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20547 Fix dll linkage for tensor type ids**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15359988/)\n\n-\n\nDifferential Revision: [D15359988](https://our.internmc.facebook.com/intern/diff/D15359988/)"}
{"number": 20548, "title": "optimizer.pyi incorrectly describes the params type, leading to mypy linting errors", "time": "2019-05-15T20:39:37Z", "body": "## üêõ Bug\r\n\r\ntorch.optim.optimizer.pyi currently contains:\r\n\r\n```\r\nfrom typing import Iterable, Union, Callable, Optional\r\nfrom .. import Tensor\r\n\r\n_params_t = Union[Iterable[Tensor], dict]\r\n\r\nclass Optimizer:\r\n    def __init__(self, params: _params_t) -> None: ...\r\n    def state_dict(self) -> dict: ...\r\n    def load_state_dict(self, state_dict: dict) -> None: ...\r\n    def zero_grad(self) -> None: ...\r\n    def step(self, closure: Optional[Callable[[], float]]=...) -> None: ...\r\n    def add_param_group(self, param_group: dict) -> None: ...\r\n```\r\n\r\nHowever, the doc states that the `params` in the constructor is `iterable of parameters to optimize or dicts defining parameter groups` (i.e. - 'dicts' plural).  The doc description is correct (matches the code), which makes the `.pyi` incorrect.\r\n\r\nUnless I'm missing something it should be:\r\n\r\n```\r\n_params_t = Union[Iterable[torch.Tensor], Iterable[Dict]]\r\n```\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. In user code import `Optimizer` and define a function that takes parameters in the same form (with [correct] type annotation) and just wraps some Optimizer construction.\r\n2. Check it with mypy\r\n\r\nFor example:\r\n```\r\nfrom typing import Union, Iterable, Dict\r\nimport torch\r\nfrom torch.optim import Adam, Optimizer\r\n\r\ndef _default_optimizer(params: Union[Iterable[torch.Tensor], Iterable[Dict]]) -> Optimizer:\r\n    return Adam(params)\r\n```\r\n\r\n## Expected behavior\r\n\r\nShould lint without error\r\n\r\nCurrently generates:\r\n\r\n```\r\nerror: Argument 1 to \"Adam\" has incompatible type \"Union[Iterable[Tensor], Iterable[Dict[Any, Any]]]\"; expected \"Union[Iterable[Tensor], Dict[Any, Any]]\"\r\n```\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.1.0\r\n - OS (e.g., Linux): MacOS\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source): N/A\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: N/A\r\n - GPU models and configuration: None\r\n - Any other relevant information: None\r\n\r\n"}
{"number": 20549, "title": "[jit] Refactor schema_matching.cpp", "time": "2019-05-15T20:51:43Z", "body": "It was kind of hard to read through this code so this adds a bunch of comments, no behavior should be changed\r\n\n\nDifferential Revision: [D15499974](https://our.internmc.facebook.com/intern/diff/15499974/)"}
{"number": 20550, "title": "make box plus one a legacy argument in detection ops", "time": "2019-05-15T21:28:25Z", "body": "Differential Revision: D15348610\n\n"}
{"number": 20551, "title": "torch.norm produces incorrect results", "time": "2019-05-15T21:28:51Z", "body": "## üêõ Bug\r\n\r\ntorch.norm gives incorrect results on CPU in the latest nightly build as well as in 1.1.0 stable.\r\n\r\n## To Reproduce\r\n```\r\n\r\n>>>  import torch\r\n>>> a=torch.rand(2000,2000,64)\r\n>>> b=torch.norm(a)\r\n>>> c=torch.norm(a.cuda())\r\n>>> b,c\r\n(tensor(5792.6187), tensor(9237.8311, device='cuda:0'))\r\n\r\n```\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nBoth b and c should have the same values.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0.dev20190514\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Red Hat Enterprise Linux Server release 7.4 (Maipo)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)\r\nCMake version: version 2.8.12.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: \r\nGPU 0: Tesla K40m\r\nGPU 1: Tesla K40m\r\n\r\nNvidia driver version: 387.26\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] msgpack-numpy==0.4.1\r\n[pip3] numpy==1.14.3\r\n[pip3] torch==0.4.0\r\n[pip3] torchtext==0.2.3\r\n[pip3] torchvision==0.2.1\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2018.0.2                      1  \r\n[conda] mkl_fft                   1.0.1            py36h3010b51_0  \r\n[conda] mkl_random                1.0.1            py36h629b387_0  \r\n[conda] pytorch-nightly           1.1.0.dev20190514 py3.6_cuda9.0.176_cudnn7.5.1_0    pytorch\r\n[conda] torchtext                 0.2.3                     <pip>\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"}
{"number": 20552, "title": "Remove cpu_half, cpu_bool, cuda_bool from native_functions.yaml", "time": "2019-05-15T21:34:50Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* **#20552 Remove cpu_half, cpu_bool, cuda_bool from native_functions.yaml**\r\n\r\nWhen scalar type specializations for Type were removed, we left a switch statement to guard native functions from being called with these types unless they were implemented. However, it's easier and gives better error messages if we just let these propagate until they reach an AT_DISPATCH macro or a TH wrapper, which will then throw error.\r\n\r\nThis behavior already happens currently in some cases. Before and after the patch, this hits a TH wrapper that errors out.\r\n```\r\n>>> import torch\r\n>>> torch.randn(5,5, dtype=torch.bool)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nRuntimeError: _th_normal_ not supported on CPUType for Bool\r\n```\r\n\r\nBut now this behavior extends to all cases:\r\nBefore, it hits our case statement generated by cpu_half. Now, it hits the AT_DISPATCH error.\r\n```\r\n>>> import torch\r\n>>> a = torch.zeros(5,5,dtype=torch.half)\r\n>>> a + a\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nRuntimeError: add not supported on CPUType for Half\r\n```\r\n\r\nAfter\r\n```\r\n>>> import torch\r\n>>> a = torch.zeros(5,5,dtype=torch.half)\r\n>>> a + a\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nRuntimeError: \"add_cpu\" not implemented for 'Half'\r\n```\r\n\r\nDifferential Revision: [D15362154](https://our.internmc.facebook.com/intern/diff/D15362154)"}
{"number": 20553, "title": "handle box plus one for gpu generate_proposals", "time": "2019-05-15T21:39:29Z", "body": "Differential Revision: D15362108\n\n"}
{"number": 20554, "title": "update legacy plus one for mpscnn", "time": "2019-05-15T21:55:16Z", "body": "Differential Revision: D15362378\n\n"}
{"number": 20555, "title": "[jit] Use AT_INTERNAL_ASSERT in test_base", "time": "2019-05-15T22:04:17Z", "body": "as title. We were using AT_ASSERT, which is newly deprecated. In this case, we do in fact want an internal assertion since this is used in testing code to describe expected behavior."}
{"number": 20556, "title": "[jit] Mark values entering containers as wildcards", "time": "2019-05-15T22:21:46Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #20670 [jit] make wildcards alias only each other\n* **#20556 [jit] Mark values entering containers as wildcards**\n\nThis is the first step to improving wildcard analysis.\n\nThe end-state is that the wildcard set now represents everything we're\nnot sure about. Inputs and values that enter/exit containers will be\npart of the wildcard set. There will be one wildcard set per type.\n\nThe key difference is that today, a wildcard value may alias *any other\nvalue* in the graph. After this change, the wildcard value may alias\n*any other wildcard*, but will not alias other stuff.\n\nSo if we have a lot of purely functional code in an inner block, we can\noptimize it even in the presence of wildcards.\n\nThis first PR will ruin optimization because we haven't updated what\n\"wildcard\" means but we've labeled more stuff as wildcards. That's the\nonly safe way to do it, but I'll stack something on top to fix it.\n\nDifferential Revision: [D15447568](https://our.internmc.facebook.com/intern/diff/D15447568)"}
{"number": 20557, "title": "Improve performance of advanced indexing backward", "time": "2019-05-15T22:25:53Z", "body": "This PR improves performance of advanced indexing backward, partially solving #15245 (performance is still worse than gather, but not by such outrageous margins). Before, using benchmarking harness from #15245, cuda 10/V100:\r\n```\r\nIndexing is faster by at most -270.61607820767887 us on N: 16 D: 256 K: 1\r\nIndexing is slower by at most 11127.466280784833 us on N: 16 D: 4096 K: 4096\r\n```\r\nafter:\r\n```\r\nIndexing is faster by at most 23.524456737696028 us on N: 512 D: 4096 K: 4096\r\nIndexing is slower by at most 186.24056029472553 us on N: 16 D: 1024 K: 4096\r\n```\r\nStrategy is to reuse embedding backward kernel, adapting it to handle unindexed dimensions in the beginning by launching additional threadblocks, and also allowing it to handle slices that are bigger than `65K*128`, that is hardly ever a problem for embedding. Still, integer indexing is baked in the kernel, and is important for performance, so for now bigger than 2G element tensors are not supported. \r\nThe main savings come from not having to expand index to all unindexed dimensions, and not sorting expanded index with incoming gradient values, but rather only sorting unexpanded index. \r\nThere are ways to make sorting overhead smaller (thanks @mcarilli for suggestions) but I'll get to it when it becomes a real problem, or rather, when cuda graphs will force us to get rid of thrust::sort calls. \r\nI've also added tests for indexing backward, before tests for index_put_ and indexing backward were non-existent. \r\nThis PR also fixes #20457 by casting indices to `self` backend."}
{"number": 20558, "title": "Adding memory_format to  empty and empty_like operators", "time": "2019-05-15T22:28:41Z", "body": "Original RFC #19092\r\n\r\nTo ensure that we are not introducing BC breaking change, empty_like returns contiguous tensor by default.\r\n\r\n```python\r\nnCwh = torch.randn(N, C, H, W)\r\nnhwC = nCwh.contiguous(memory_format=torch.channels_last)\r\n\r\nnew_nCwh = torch.empty_like(nhwC) \r\nnew_nCwh.is_contiguous(memory_format=torch.channels_last) == False\r\n``` \r\n\r\nNow we need a way to preserve memory format in `empty_like`\r\n\r\n```python\r\nnCwh = torch.randn(N, C, H, W)\r\nnhwC = nCwh.contiguous(memory_format=torch.channels_last)\r\n\r\nnew_nhwC = torch.empty_like(nhwC, memory_format=torch.preserve_format)\r\nnew_nhwC.is_contiguous(memory_format=torch.channels_last) == True\r\n\r\nlike_nCwh = torch.empty_like(nCwh, memory_format=torch.preserve_format)\r\nlike_nCwh.is_contiguous(memory_format=torch.channels_last) == False\r\n```\r\n\r\nUsage of `torch.preserve_format` allows us to avoid `if` constructs.\r\n\r\nWe can also generate different memory format outputs\r\n\r\n```python\r\nnCwh = torch.randn(N, C, H, W)\r\nnhwC = nCwh.contiguous(memory_format=torch.channels_last)\r\n\r\nnew_nhwC = torch.empty_like(nCwh, memory_format=torch.channels_last)\r\nnew_nhwC.is_contiguous(memory_format=torch.channels_last) == True\r\n\r\nnew_nCwh = torch.empty_like(nhwC, memory_format=torch.contiguous_format)\r\nnew_nCwh.is_contiguous(memory_format=torch.channels_last) == False\r\n```"}
{"number": 20559, "title": "[ONNX] Add ONNX export support for torch.rand.", "time": "2019-05-15T22:30:02Z", "body": "This PR adds support for torch.rand export in the PyTorch ONNX exporter. There are other generator ops that need to be supported for export and they will added in subsequent PRs. This op is needed with priority for a model on our end. "}
{"number": 20560, "title": "[WIP] Migrate distribution transform computations into functional.py + add unit tests (test_distributions.py) for functions", "time": "2019-05-15T22:36:36Z", "body": "T44448080 Refactor transforms.py into functional.py. \r\nUnit tests are added for the functions in test_distributions.py"}
{"number": 20561, "title": "De-deprecate parts of the legacy API", "time": "2019-05-15T22:54:27Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20383 torch::jit::RegisterOperators forwards to c10::RegisterOperators&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15300937/)\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20561 De-deprecate parts of the legacy API**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15364271/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20514 Options based registration API&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15346348/)\n\nWe previously planned to deprecate the direct passing of a kernel function or lambda to the op() call, e.g.\n\n    static auto registry = RegisterOperators().op(\"my::op\", &func);\n\nand push users towards the options based API:\n\n    static auto registry = RegisterOperators().op(\"my::op\", RegisterOperators::options().kernel<decltype(func), &func>());\n\nbecause that has a slightly lower performance overhead when calling the kernel.\n\nHowever, that overhead is negligible for all but exotic use cases, so there's no reason to push users towards a more verbose API.\nThis diff removes the deprecation warning from that API.\n\nHowever, if you use the API together with deprecated types like std::unordered_map, you will now get a deprecation warning there.\n\nDifferential Revision: [D15364271](https://our.internmc.facebook.com/intern/diff/D15364271/)"}
{"number": 20562, "title": "Performance issue when accessing an extremely large (10GB) longtensor", "time": "2019-05-15T22:54:57Z", "body": "## üêõ Bug\r\n\r\nI'm encountering a very strange performance issue when accessing an extremely large tensor.  I first encountered it when loading a whole dataset into gpu memory on a Tesla v100, which may be required for reproducibility because smaller tensors don't demonstrate the effect.\r\n\r\nIf the tensor is above a certain size, then once you index past a specific value, access to the tensor (or at least the performance of the dnn) begins to take longer and longer.  The linked repo includes an example notebook that demonstrates the performance dropoff.\r\n\r\nThere doesn't seem to be an issue with tensors in cpu memory.\r\n\r\nHere are the things I've tested so far to help isolate:\r\n\r\n - Different tensor shapes (2x wider tensors results in degredation at ~6M samples)\r\n - Changed the tensor size to be within the limit (no slowdown occurs)\r\n - CPU Memory (No issues)\r\n - Different GPUs, although all on the same DGX-1 (hopefully validating hardware is functioning)\r\n - Artifically starting the dataloader at the point where it slows down. (Slowdown is immediate and performance is much worse (25K samples/s)\r\n - Breaking up the tensor into multiple blocks to see if being contiguous matters (multiple tensors don't have the same effect. Only one larger tensor demonstrates this issue)\r\n - Started with multiple blocks and concated together on the GPU. (issue shows up again)\r\n - Tested just the indexing of the single large random tensor to see if it was impacted (no slowdown)\r\n\r\nThis is possibly due to a int (or long) variable in the memory addressing of the tensor; If I calculate the point in the tensor where the slowdown occurs (45x4bytesx~12M = 2.16B) that's suspiciously close to the int limit of 2147483647.\r\n\r\nWhat's strange is that the slowdown only occurs if there is significant access beyond that range. I tested a tensor that was 4 bytes larger (and one that was 100K larger) and neither of those displayed significant problems. It's only when it's much larger that it seems to cause the issue.\r\n\r\nAs mentioned above, for a larger tensor if I start accessing in that region the slowdown is immediate and much more pronounced.\r\n\r\nIt's worth noting I encountered a similar and more nefarious issue when shuffling by index a tensor loaded from dl_pack of this size where the region of the data beyond the int limit was all 0s.\r\n\r\nI understand tensors of this size are unusual (at least for now) but it would be great to figure out the limitations so that others don't run into this and so that we can plan for future GPUs with much more RAM and in GPU memory datasets.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nRun the notebook available in the repo: https://github.com/EvenOldridge/HugeTensor\r\nThe notebook uses ignite version 0.1.2 and contains a batch dataloader that I'm prototyping and hope to integrate into pytorch once its kinks are worked out.\r\n\r\n## Expected behavior\r\n\r\nPerformance of the nn should be consistent when indexing the same tensor.  As soon as the data is split into 3 tensors the performance is fine as expected.  Performance on the CPU for a single large tensor is also fine.  No performance dropoff should occur when accessing a single large tensor in GPU memory. \r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.01\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: 10.0\r\n - GPU models and configuration: TeslaV100 32 Gig (on a DGX-1)\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n"}
{"number": 20563, "title": "Remove weak_script in MultiheadAttention function.", "time": "2019-05-15T22:56:24Z", "body": "Remove weak_script. After recently splitting the forward() function in MultiheadAttention module, we notice a memory leak on GPU. Fix the problem by removing those \"weak_script\" decorator."}
{"number": 20564, "title": "Add onnxifi support for Int8FCDNNLowPPackedWeightBlob", "time": "2019-05-16T00:07:16Z", "body": "Differential Revision: D15106712\n\n"}
{"number": 20565, "title": "Add \"ndim\" property to tensor", "time": "2019-05-16T00:11:40Z", "body": "For compatibility with numpy."}
{"number": 20566, "title": " What is a C ++ torch api similar to the registor_hook function in Python?", "time": "2019-05-16T02:27:37Z", "body": "I want to know the backward grdient value of a particular layer.\r\nIn Python, there is a function called registor_hook. C ++ does not have the same function. \r\nIs there a similar method?"}
{"number": 20567, "title": "Add aten mkldnn conv2d backward operator", "time": "2019-05-16T02:28:59Z", "body": "### mkldnn backward ops list:\r\n - [ ] \\(https://github.com/pytorch/pytorch/pull/20567) Add aten mkldnn conv2d backward operator :yellow_heart:\r\n - [ ] \\(https://github.com/pytorch/pytorch/pull/20570) Add aten mkldnn backward ops: relu, linear and reshape :yellow_heart:\r\n - [ ] \\(https://github.com/pytorch/pytorch/pull/20571) Add aten mkldnn backward ops: max_pool2d, avg_pool2d and adaptive_avg_poo2d :yellow_heart:\r\n - [ ] \\(https://github.com/pytorch/pytorch/pull/20572) Add aten mkldnn batchnorm backward operator :yellow_heart:\r\n - [ ] \\(https://github.com/pytorch/pytorch/pull/20573) Add aten mkldnn zero_ operatorüíö \r\n- [ ] \\(https://github.com/pytorch/pytorch/pull/20575) Add mkldnn mul operator üíö \r\n\r\nEnable mkldnn backward which can improve the traning performance about 2x for mode resnext101. "}
{"number": 20568, "title": "Can't build caffe2 on windows", "time": "2019-05-16T03:04:34Z", "body": "I update my CUDA from 8.0 to 10.0 but get the same problem.\r\nI excute \r\n\r\n> scripts\\build_windows.bat`\r\n\r\nAnd got this error:\r\n\r\n> [1149/1232]` Linking CXX executable bin\\kernel_functor_test.exe\r\nFAILED: bin/kernel_functor_test.exe\r\nCmd.exe /C \"cd . && \"C:\\Program Files\\CMake\\bin\\cmake.exe\" -E vs_link_exe --intdir=caffe2\\CMakeFiles\\kernel_functor_test.dir --rc=C:\\PROGRA~2\\WI3CF2 ~1\\10\\bin\\100177~1.0\\x64\\rc.exe --mt=C:\\PROGRA~2\\WI3CF2~1\\10\\bin\\100177~1.0\\x64\\mt.exe --manifests -- C :\\PROGRA~2\\MIB055~1\\2017\\ENTERP~1\\VC\\Tools\\MSVC\\1416~1.270\\bin\\Hostx64\\x64\\link.exe /nologo caffe2\\CMakeFiles\\kernel_functor_test.dir\\__\\aten\\src \\ATen\\core\\op_registration\\kernel_functor_test.cpp.obj /out:bin\\kernel_functor_test.exe /implib:lib\\kernel_functor_test.lib /pdb:bin\\kernel_functor_test.pdb /version:0.0 /INCREMENTAL:NO /subsystem:console lib\\ Gtest_main.lib -WHOLEARCHIVE:E:/opencv_build/pytorch-zip/pytorch/lib/caffe2.lib -WHOLEARCHIVE:E:/opencv_build/pytorch-zip/pytorch/lib/caffe2_protos.lib lib\\cpuinfo.lib lib\\clog. Lib lib\\foxi_loader.lib -WHOLEARCHIVE:E:/opencv_build/pytorch-zip/pytorch/lib/onnx.lib lib\\onnx_proto.lib lib\\libprotobuf.lib -WHOLEARCHIVE:E:/opencv_build/pytorch-zip/pytorch/lib /Caffe2_perfkernels_avx.lib -WHOLEA RCHIVE: E:/opencv_build/pytorch-zip/pytorch/lib/Caffe2_perfkernels_avx2.lib lib\\c10.lib -WHOLEARCHIVE:E:/opencv_build/pytorch-zip/pytorch/lib/Caffe2_perfkernels_avx512.lib lib\\gtest.lib kernel32.lib User32.lib gdi32.lib winspool.lib shell32.lib ole32.lib oleaut32.lib uuid.lib comdlg32.lib advapi32.lib && cd .\"\r\nLINK: command \"C:\\PROGRA~2\\MIB055~1\\2017\\ENTERP~1\\VC\\Tools\\MSVC\\1416~1.270\\bin\\Hostx64\\x64\\link.exe /nologo caffe2\\CMakeFiles\\kernel_functor_test.dir\\ __\\aten\\src\\ATen\\core\\op_registration\\kernel_functor_test.cpp.obj /out:bin\\kernel_functor_test.exe /implib:lib\\kernel_functor_test.lib /pdb:bin\\kernel_functor_test.pdb /version:0.0 /INCREMENTAL:NO / Subsystem:console lib\\gtest_main.lib -WHOLEARCHIVE:E:/opencv_build/pytorch-zip/pytorch/lib/caffe2.lib -WHOLEARCHIVE:E:/opencv_build/pytorch-zip/pytorch/lib/caffe2_protos.lib lib\\cpuinfo. Lib lib\\clog.lib lib\\foxi_loader.lib -WHOLEARCHIVE:E:/opencv_build/pytorch-zip/pytorch/lib/onnx.lib lib\\onnx_proto.lib lib\\libprotobuf.lib -WHOLEARCHIVE:E:/opencv_build/pytorch- Zip/pytorch/lib/Caffe2_perfkernels_avx.lib -WHOLEARCHIVE:E:/opencv_build/pytorch-zip/pytorch/lib/Caffe2_perfkernels_avx2.lib lib\\c10.lib -WHOLEARCHIVE:E:/opencv_build/pytorch-zip/pytorch/lib/Caffe2_perfkernels_avx512 .lib lib\\gtest.lib kernel32.lib user32.lib gdi32.lib winspool.lib shell32 .lib ole32.lib oleaut32.lib uuid.lib comdlg32.lib advapi32.lib /MANIFEST /MANIFESTFILE:bin\\kernel_functor_test.exe.manifest\" failed (exit code 1120) with the following output:\r\nLINK : warning LNK4098: The default library \"MSVCRT\" conflicts with the use of other libraries; please use /NODEFAULTLIB:library\r\nCaffe2.lib(miniz.c.obj) : warning LNK4217: Locally defined symbol _localtime64_s is imported in function localtime_s\r\nCaffe2.lib(miniz.c.obj) : warning LNK4217: Locally defined symbol _time64 is imported in function mz_zip_writer_add_mem_ex_v2\r\nCaffe2.lib(miniz.c.obj) : warning LNK4217: Locally defined symbol _wassert is imported in function mz_zip_array_ensure_capacity\r\nCaffe2.lib(miniz.c.obj) : warning LNK4217: Locally defined symbol free Imported in function miniz_def_free_func\r\nCaffe2.lib(miniz.c.obj) : warning LNK4217: The locally defined symbol malloc is imported in the function miniz_def_alloc_func\r\nCaffe2.lib(miniz.c.obj) : warning LNK4217: Locally defined symbol realloc is imported in function miniz_def_realloc_func\r\nCaffe2.lib(miniz.c.obj) : error LNK2019: Unresolved external symbol __imp__mktime64, this symbol is referenced in function mktime\r\nBin\\kernel_functor_test.exe : fatal error LNK1120: 1 external command that cannot be resolved\r\nNinja: build stopped: subcommand failed.\r\nTraceback (most recent call last):\r\n  File \"tools\\build_libtorch.py\", line 22, in <module>\r\n    build_python=False, rerun_cmake=True, build_dir='.')\r\n  File \"E:\\opencv_build\\pytorch-zip\\pytorch\\tools\\build_pytorch_libs.py\", line 268, in build_caffe2\r\n    check_call(build_cmd, cwd=build_dir, env=my_env)\r\n  File \"C:\\Users\\84206\\Anaconda3\\lib\\subprocess.py\", line 311, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['cmake', '--build', '.', '--target', 'install', '--config', 'Release', '--', '-j', '7']' returned non-zero exit status 1.\r\n\"Caffe2 building failed\"\r\n\r\nThis ,https://github.com/pytorch/pytorch/issues/20420#issue-443213714, can't solve my problem.\r\n\r\nI transforms my project from pytroch version to caffe2 version in python.And then,I want to use caffe2 in C++ project.How am I supposed to do?\r\n\r\n\r\nMy Environment:\r\nWin10\r\nVS2017 \r\nCUDA10 whith cudnn7.5\r\nAnaconda3 , python 3.6.8\r\nCmake 3.14.4\r\n"}
{"number": 20569, "title": "upgrade mkldnn-bridge", "time": "2019-05-16T05:26:10Z", "body": "1. reduce the overhead of mkldnn-bridge itself\r\n2. remove redundant code and useless APIs\r\n3. provide new operators, including int8 inner_product,  ND permute/transpose, elem_add/mul, and etc.\r\n4. improve inner_product to support io format weights without implicit reorder\r\n5. add SoftMax support\r\n\r\n"}
{"number": 20570, "title": "Add aten mkldnn backward ops: relu, linear and reshape", "time": "2019-05-16T05:54:29Z", "body": "### mkldnn backward ops list:\r\n - [ ] \\(https://github.com/pytorch/pytorch/pull/20567) Add aten mkldnn conv2d backward operator :yellow_heart:\r\n - [ ] \\(https://github.com/pytorch/pytorch/pull/20570) Add aten mkldnn backward ops: relu, linear and reshape :yellow_heart:\r\n - [ ] \\(https://github.com/pytorch/pytorch/pull/20571) Add aten mkldnn backward ops: max_pool2d, avg_pool2d and adaptive_avg_poo2d :yellow_heart:\r\n - [ ] \\(https://github.com/pytorch/pytorch/pull/20572) Add aten mkldnn batchnorm backward operator :yellow_heart:\r\n - [ ] \\(https://github.com/pytorch/pytorch/pull/20573) Add aten mkldnn zero_ operatorüíö \r\n- [ ] \\(https://github.com/pytorch/pytorch/pull/20575) Add mkldnn mul operator üíö "}
{"number": 20571, "title": " Add aten mkldnn backward ops: max_pool2d, avg_pool2d and adaptive_av‚Ä¶", "time": "2019-05-16T05:55:37Z", "body": "### mkldnn backward ops list:\r\n - [ ] \\(https://github.com/pytorch/pytorch/pull/20567) Add aten mkldnn conv2d backward operator :yellow_heart:\r\n - [ ] \\(https://github.com/pytorch/pytorch/pull/20570) Add aten mkldnn backward ops: relu, linear and reshape :yellow_heart:\r\n - [ ] \\(https://github.com/pytorch/pytorch/pull/20571) Add aten mkldnn backward ops: max_pool2d, avg_pool2d and adaptive_avg_poo2d :yellow_heart:\r\n - [ ] \\(https://github.com/pytorch/pytorch/pull/20572) Add aten mkldnn batchnorm backward operator :yellow_heart:\r\n - [ ] \\(https://github.com/pytorch/pytorch/pull/20573) Add aten mkldnn zero_ operatorüíö \r\n- [ ] \\(https://github.com/pytorch/pytorch/pull/20575) Add mkldnn mul operator üíö "}
{"number": 20572, "title": "Add aten mkldnn batchnorm backward operator", "time": "2019-05-16T05:56:51Z", "body": "### mkldnn backward ops list:\r\n - [ ] \\(https://github.com/pytorch/pytorch/pull/20567) Add aten mkldnn conv2d backward operator :yellow_heart:\r\n - [ ] \\(https://github.com/pytorch/pytorch/pull/20570) Add aten mkldnn backward ops: relu, linear and reshape :yellow_heart:\r\n - [ ] \\(https://github.com/pytorch/pytorch/pull/20571) Add aten mkldnn backward ops: max_pool2d, avg_pool2d and adaptive_avg_poo2d :yellow_heart:\r\n - [ ] \\(https://github.com/pytorch/pytorch/pull/20572) Add aten mkldnn batchnorm backward operator :yellow_heart:\r\n - [x] \\(https://github.com/pytorch/pytorch/pull/20573) Add aten mkldnn zero_ operatorüíö \r\n- [x] \\(https://github.com/pytorch/pytorch/pull/20575) Add mkldnn mul operator üíö "}
{"number": 20573, "title": "Add aten mkldnn zero_ operator", "time": "2019-05-16T05:58:09Z", "body": "### mkldnn backward ops list:\r\n - [ ] \\(https://github.com/pytorch/pytorch/pull/20567) Add aten mkldnn conv2d backward operator :yellow_heart:\r\n - [ ] \\(https://github.com/pytorch/pytorch/pull/20570) Add aten mkldnn backward ops: relu, linear and reshape :yellow_heart:\r\n - [ ] \\(https://github.com/pytorch/pytorch/pull/20571) Add aten mkldnn backward ops: max_pool2d, avg_pool2d and adaptive_avg_poo2d :yellow_heart:\r\n - [ ] \\(https://github.com/pytorch/pytorch/pull/20572) Add aten mkldnn batchnorm backward operator :yellow_heart:\r\n - [x] \\(https://github.com/pytorch/pytorch/pull/20573) Add aten mkldnn zero_ operatorüíö \r\n - [ ] \\(https://github.com/pytorch/pytorch/pull/20575) Add mkldnn mul operator üíö "}
{"number": 20574, "title": "Fix caffe2 build failure on Windows", "time": "2019-05-16T06:10:19Z", "body": "Fixes #20568.\r\nLooks like CMake is passing `/MD` when we call `add_library`. We need to fix these with C source files too."}
{"number": 20575, "title": "Add mkldnn mul operator", "time": "2019-05-16T06:46:40Z", "body": "### mkldnn backward ops list:\r\n - [ ] \\(https://github.com/pytorch/pytorch/pull/20567) Add aten mkldnn conv2d backward operator :yellow_heart:\r\n - [ ] \\(https://github.com/pytorch/pytorch/pull/20570) Add aten mkldnn backward ops: relu, linear and reshape :yellow_heart:\r\n - [ ] \\(https://github.com/pytorch/pytorch/pull/20571) Add aten mkldnn backward ops: max_pool2d, avg_pool2d and adaptive_avg_poo2d :yellow_heart:\r\n - [ ] \\(https://github.com/pytorch/pytorch/pull/20572) Add aten mkldnn batchnorm backward operator :yellow_heart:\r\n - [ ] \\(https://github.com/pytorch/pytorch/pull/20573) Add aten mkldnn zero_ operator:yellow_heart:\r\n - [ ] \\(https://github.com/pytorch/pytorch/pull/20575) Add mkldnn mul operator :üíö"}
{"number": 20576, "title": "NVRTC_ERROR unknown when using self-built libtorch", "time": "2019-05-16T06:57:47Z", "body": "Hi, I am doing inference with my own GPU model. When i use the released nightly version of libtorch from official website, there is no issue. Then I build libtorch from the same version of pytorch source and use the self-built version of libtorch to do inference, it shows the following error:\r\n\r\npytorch/torch/csrc/jit/fuser/cuda/fused_kernel.cpp:196: NVRTC_ERROR unknown:\r\noperation failed in interpreter:\r\n:\r\noperation failed in interpreter:\r\n\r\nAborted (core dumped)\r\n\r\nFor some reasons I have to build libtorch from source, but seems the flags i use to build libtorch are not the same as the official ones used to build nightly version. Anyone know how the nightly version of libtorch is built and what are the flags used to build?  Thanks a lot!. \r\n\r\nThe following is my local environment:\r\nLinux16.04;\r\nCUDA 9;\r\nlibtorch nightly verison: May 13, 2019\r\n"}
{"number": 20577, "title": "Is there a way to utilize ATen full lib by calling caffe2.python.core.CreateOperators?", "time": "2019-05-16T07:10:11Z", "body": "Hi I just wondering if there is a way to utilize ATen full lib by creating like\r\n```\r\nop = core.CreateOperator(\r\n    \"ATen\",\r\n    [\"X2\", \"X1\"],\r\n    [\"Y\"],\r\n    operator=\"grid_sampler\"\r\n)\r\n```\r\nI wanted to use grid_sampler function in ATen lib and I have checked the `native_functions.yaml` in `aten/src/ATen/native` which has function `grid_sampler`.\r\n\r\nHowever when I run operators above, it shows an error claimed that\r\n\r\n> Attempting to run unknown ATen operator configuration: grid_sampler\r\n\r\nIt's really appreciated that if someone can answer my question!"}
{"number": 20578, "title": "why we don't need to write a backward() method when applying a new loss we personally define?", "time": "2019-05-16T08:06:01Z", "body": "## ‚ùì Questions and Help\r\n\r\n### Please note that this issue tracker is not a help form and this issue will be closed.\r\n\r\nWe have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:\r\n\r\n- [Discussion Forum](https://discuss.pytorch.org/)\r\n"}
{"number": 20579, "title": "Exception when adding custom scalars in tensorboard", "time": "2019-05-16T09:08:18Z", "body": "## üêõ Exception when adding custom scalars in tensorboard\r\n\r\nAn exception is raised when adding custom scalars using \r\n\r\n`TypeError: Parameter to MergeFrom() must be instance of same class: expected tensorboard.SummaryMetadata.PluginData got list.`\r\n\r\n## To Reproduce\r\nMinimal code to reproduce:\r\n```python\r\nfrom torch.utils.tensorboard import SummaryWriter\r\n\r\nwith SummaryWriter() as writer:\r\n    writer.add_custom_scalars({'Stuff': {\r\n        'Losses': ['MultiLine', ['loss/(one|two)']],\r\n        'Metrics': ['MultiLine', ['metric/(three|four)']],\r\n    }})\r\n```\r\n\r\nRunning this results in:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 3, in <module>\r\n  File \"/some/path/python3.7/site-packages/torch/utils/tensorboard/writer.py\", line 733, in add_custom_scalars\r\n    self._get_file_writer().add_summary(custom_scalars(layout))\r\n  File \"/some/path/python3.7/site-packages/torch/utils/tensorboard/summary.py\", line 383, in custom_scalars\r\n    smd = SummaryMetadata(plugin_data=PluginData)\r\nTypeError: Parameter to MergeFrom() must be instance of same class: expected tensorboard.SummaryMetadata.PluginData got list.\r\n```\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n```\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04) 7.4.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 1050 Ti with Max-Q Design\r\nNvidia driver version: 418.43\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.3\r\n[pip] torch==1.1.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.3                      199  \r\n[conda] mkl_fft                   1.0.12           py37ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] pytorch                   1.1.0           py3.7_cuda10.0.130_cudnn7.5.1_0    pytorch\r\n```\r\n\r\n## Additional context\r\n\r\nThe exception message is quite clear, the required type is `tensorboard.SummaryMetadata.PluginData` but we have a `list`, this happens [here](https://github.com/pytorch/pytorch/blob/09f22d10a695bfba8ffb3327b9920fd3358c00ee/torch/utils/tensorboard/summary.py#L381):\r\n```python\r\n    PluginData = [SummaryMetadata.PluginData(plugin_name='custom_scalars')]  # <--\r\n    smd = SummaryMetadata(plugin_data=PluginData)\r\n```\r\n\r\nShould be enough to remove the surrounding square brackets, PR is coming."}
{"number": 20580, "title": "Fixes error with custom scalars, fixes #20579", "time": "2019-05-16T09:11:46Z", "body": "When adding custom scalars like this\r\n```python\r\nfrom torch.utils.tensorboard import SummaryWriter\r\n\r\nwith SummaryWriter() as writer:\r\n    writer.add_custom_scalars({'Stuff': {\r\n        'Losses': ['MultiLine', ['loss/(one|two)']],\r\n        'Metrics': ['MultiLine', ['metric/(three|four)']],\r\n    }})\r\n```\r\nThis error is raised:\r\n```\r\nTypeError: Parameter to MergeFrom() must be instance of same class: expected tensorboard.SummaryMetadata.PluginData got list.\r\n```\r\n\r\nRemoving the square brackets around `SummaryMetadata.PluginData(plugin_name='custom_scalars')` should be enough to fix it.\r\n\r\n"}
{"number": 20581, "title": "Allow tuples for scale_factor argument in nn.Upsample", "time": "2019-05-16T09:28:29Z", "body": "Fixes #20523 .\r\n\r\nnn.Upsample was unable to accept tuple inputs for the scale_factor argument due to direct casting to float, which was done in #17732."}
{"number": 20582, "title": "[DO NOT MERGE] Try to set allow_tensor_metadata_change to false in sparse ctor shallow copy", "time": "2019-05-16T12:43:53Z", "body": ""}
{"number": 20583, "title": "Conv2d triggers assertion in mkl-dnn when padding=(n, 3)", "time": "2019-05-16T13:36:13Z", "body": "## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nAn assertion is triggered:\r\n```\r\nAssertion failed: (jcp.ur_w * (jcp.nb_oc_blocking + 1) <= num_avail_regs), function init_conf, file ../third_party/ideep/mkl-dnn/src/cpu/jit_avx2_conv_kernel_f32.cpp, line 567.\r\nAbort trap: 6\r\n```\r\nWhen doing:\r\n```\r\nconv = torch.nn.Conv2d(nChans, nChans, kernel_size=kernelSize, padding=(n, 3))\r\nx = conv(inputData)\r\n```\r\n\r\nError occurs when `padding[1] == 3`. The value of `padding[0]` does not seem to matter.\r\n\r\nError occurs on this processor `Intel(R) Xeon(R) CPU E5-2697 v2 @ 2.70GHz`\r\nBut not this processor `Intel(R) Core(TM) i7-8750H CPU @ 2.20GHz`\r\n\r\nError occurs using torch version `1.0.1` and `1.1.0`\r\nBut not using torch version `1.0.0`\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Save and run the script below.\r\nScript:\r\n```\r\nimport torch\r\n\r\nnChans = 4\r\nkernelSize = 3\r\nsamplesPerBatch = 6\r\ninputShape = [samplesPerBatch, nChans, 32, 32]\r\n\r\ninputData = torch.ones(*inputShape, dtype=torch.float32)\r\n\r\n\r\ndef run_conv(padding):\r\n    conv = torch.nn.Conv2d(\r\n        nChans, nChans, kernel_size=kernelSize, padding=padding)\r\n    x = conv(inputData)\r\n\r\n\r\n# These are all ok\r\nprint('Trying conv with padding (1, n)'\r\n      ' where n is all values from 0 to 99 excluding 3')\r\nfor i in range(100):\r\n    if i != 3:\r\n        run_conv((1, i))\r\n\r\n# This is ok\r\nprint('Trying conv with padding (3, 1)')\r\nrun_conv((3, 1))\r\n\r\n# This one causes an assertion\r\nprint('Trying conv with padding (1, 3)')\r\nrun_conv((1, 3))\r\n```\r\nOutput:\r\n```\r\n$ python ../poponnx/torch_test/model.py\r\nTrying conv with padding (1, n) where n is all values from 0 to 99 excluding 3\r\nTrying conv with padding (3, 1)\r\nTrying conv with padding (1, 3)\r\nAssertion failed: (jcp.ur_w * (jcp.nb_oc_blocking + 1) <= num_avail_regs), function init_conf, file ../third_party/ideep/mkl-dnn/src/cpu/jit_avx2_conv_kernel_f32.cpp, line 567.\r\nAbort trap: 6\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.13.6\r\nGCC version: Could not collect\r\nCMake version: version 3.12.4\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.3\r\n[pip3] torch==1.1.0\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] Could not collect\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\nMore info on machine where error occurs:\r\n- Model Name: Mac Pro\r\n- Model Identifier: MacPro6,1\r\n- Processor Name: 12-Core Intel Xeon E5\r\n- Processor Speed: 2.7 GHz\r\n- Number of Processors: 1\r\n- Total Number of Cores: 12"}
{"number": 20584, "title": "Auto-convert GPU arrays that support the __cuda_array_interface__ protocol", "time": "2019-05-16T14:01:04Z", "body": "This PR implements auto-conversion of GPU arrays that support the `__cuda_array_interface__` protocol (fixes #15601).\r\n\r\nIf an object exposes the `__cuda_array_interface__` attribute, `touch.as_tensor()` and `touch.tensor()` will use the exposed device memory.\r\n\r\n#### Zero-copy \r\nWhen using `touch.as_tensor(...,device=D)` where `D` is the same device as the one used in `__cuda_array_interface__`.\r\n\r\n#### Implicit copy \r\nWhen using `touch.as_tensor(...,device=D)` where `D` is the CPU or another non-CUDA device.\r\n\r\n#### Explicit copy \r\nWhen using `torch.tensor()`.\r\n\r\n#### Exception\r\nWhen using `touch.as_tensor(...,device=D)` where `D` is a CUDA device not used in `__cuda_array_interface__`.\r\n\r\n#### Lifetime \r\n`torch.as_tensor(obj)` tensor grabs a reference to `obj` so that the lifetime of `obj` exceeds the tensor\r\n\r\n"}
{"number": 20585, "title": "\"pip install --find-links\" installs package from PyPI instead of pytorch.org", "time": "2019-05-16T14:49:27Z", "body": "## üêõ Bug\r\n\r\n`pip install --find-links https://download.pytorch.org/whl/CUDA_VERSION/stable torch` installs package from PyPI and not from provided index.\r\n\r\n## To Reproduce\r\n\r\n```bash\r\n$ pip install --find-links \"https://download.pytorch.org/whl/cu80/stable\" torch --no-cache\r\nLooking in links: https://download.pytorch.org/whl/cu80/stable\r\nCollecting torch\r\n  Downloading https://files.pythonhosted.org/packages/69/60/f685fb2cfb3088736bafbc9bdbb455327bdc8906b606da9c9a81bae1c81e/torch-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (676.9MB)\r\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 676.9MB 4.1MB/s\r\nCollecting numpy (from torch)\r\n  Downloading https://files.pythonhosted.org/packages/c1/e2/4db8df8f6cddc98e7d7c537245ef2f4e41a1ed17bf0c3177ab3cc6beac7f/numpy-1.16.3-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\r\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17.3MB 2.5MB/s\r\nInstalling collected packages: numpy, torch\r\nSuccessfully installed numpy-1.16.3 torch-1.1.0\r\n\r\n```\r\nNote the `https://files.pythonhosted.org` URL.\r\nAdding `--verbose` gives more specific information:\r\n```bash\r\n$ pip install --find-links \"https://download.pytorch.org/whl/cu80/stable\" torch --no-cache --verbose --no-index\r\nIgnoring indexes: https://pypi.org/simple\r\nCreated temporary directory: /tmp/pip-ephem-wheel-cache-siy4ij3u\r\nCreated temporary directory: /tmp/pip-req-tracker-xaiowyuq\r\nCreated requirements tracker '/tmp/pip-req-tracker-xaiowyuq'\r\nCreated temporary directory: /tmp/pip-install-gvux7es0\r\nLooking in links: https://download.pytorch.org/whl/cu80/stable\r\nCollecting torch\r\n  1 location(s) to search for versions of torch:\r\n  * https://download.pytorch.org/whl/cu80/stable\r\n  Skipping link https://download.pytorch.org/whl/cu80/stable (from -f); not a file\r\n  Getting page https://download.pytorch.org/whl/cu80/stable\r\n  Starting new HTTPS connection (1): download.pytorch.org:443\r\n  https://download.pytorch.org:443 \"GET /whl/cu80/stable HTTP/1.1\" 200 6050\r\n  Skipping page https://download.pytorch.org/whl/cu80/stable because the GET request got Content-Type: binary/octet-stream\r\n  ERROR: Could not find a version that satisfies the requirement torch (from versions: none)\r\nCleaning up...\r\nRemoved build tracker '/tmp/pip-req-tracker-xaiowyuq'\r\nERROR: No matching distribution found for torch\r\nException information:\r\nTraceback (most recent call last):\r\n  File \"/home/roee/.pyenv/versions/3.6.6/envs/v3.6.6/lib/python3.6/site-packages/pip/_internal/cli/base_command.py\", line 178, in main\r\n    status = self.run(options, args)\r\n  File \"/home/roee/.pyenv/versions/3.6.6/envs/v3.6.6/lib/python3.6/site-packages/pip/_internal/commands/install.py\", line 352, in run\r\n    resolver.resolve(requirement_set)\r\n  File \"/home/roee/.pyenv/versions/3.6.6/envs/v3.6.6/lib/python3.6/site-packages/pip/_internal/resolve.py\", line 131, in resolve\r\n    self._resolve_one(requirement_set, req)\r\n  File \"/home/roee/.pyenv/versions/3.6.6/envs/v3.6.6/lib/python3.6/site-packages/pip/_internal/resolve.py\", line 294, in _resolve_one\r\n    abstract_dist = self._get_abstract_dist_for(req_to_install)\r\n  File \"/home/roee/.pyenv/versions/3.6.6/envs/v3.6.6/lib/python3.6/site-packages/pip/_internal/resolve.py\", line 242, in _get_abstract_dist_for\r\n    self.require_hashes\r\n  File \"/home/roee/.pyenv/versions/3.6.6/envs/v3.6.6/lib/python3.6/site-packages/pip/_internal/operations/prepare.py\", line 282, in prepare_linked_requirement\r\n    req.populate_link(finder, upgrade_allowed, require_hashes)\r\n  File \"/home/roee/.pyenv/versions/3.6.6/envs/v3.6.6/lib/python3.6/site-packages/pip/_internal/req/req_install.py\", line 198, in populate_link\r\n    self.link = finder.find_requirement(self, upgrade)\r\n  File \"/home/roee/.pyenv/versions/3.6.6/envs/v3.6.6/lib/python3.6/site-packages/pip/_internal/index.py\", line 792, in find_requirement\r\n    'No matching distribution found for %s' % req\r\npip._internal.exceptions.DistributionNotFound: No matching distribution found for torch\r\n```\r\nWith this line indicating the problem:\r\n```bash\r\nSkipping page https://download.pytorch.org/whl/cu80/stable because the GET request got Content-Type: binary/octet-stream\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\npip should be able to download the package matching the URL's cuda version from pytorch.org.\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\n\r\nOS: Linux Mint 19 Tara\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: N/A\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 1060 6GB\r\nNvidia driver version: 390.116\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.3\r\n[conda] Could not collect\r\n```"}
{"number": 20586, "title": "Emphasize all DDP forward() outputs must participate in computing loss", "time": "2019-05-16T14:51:03Z", "body": "\r\n\r\nCC @borguz @chenyangyu1988 "}
{"number": 20587, "title": "Fix -Wattributes warning on older versions of gcc", "time": "2019-05-16T15:09:52Z", "body": "building with cuda and gcc 4.8.5-28, we see many warnings like:\r\n\r\n[893/1645] Building NVCC (Device) object caffe2/CMakeFiles/caffe2_gpu.dir/__/aten/src/THCUNN/caffe2_gpu_generated_ELU.cu.o\r\n/home/bvaughan/repos/pytorch/c10/util/ArrayRef.h:277:48: warning: ‚Äòdeprecated‚Äô attribute directive ignored [-Wattributes]\r\n using IntList C10_DEPRECATED_USING = ArrayRef<int64_t>;\r\n\r\nThis change prevents those warnings on the older compiler.\r\n\r\n"}
{"number": 20588, "title": "Out-of-memory on GPU due to the \"weak_script\" decorators", "time": "2019-05-16T15:26:14Z", "body": "## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nThe issue has been resolved with a recently merged PR (https://github.com/pytorch/pytorch/pull/20563). This issue report is here for the record and future benchmark. The issue is related to the local scope of a weak-scripted function, which cause a memory leak.\r\n\r\nWe have the out-of-memory issue when running nn.MultiheadAttention module on CUDA. This happened since we split the forward function of nn.MultiheadAttention module and move major calculation to torch.nn.functional.py.\r\n\r\nTo fix the issue in the merged PR, we had to remove the \"weak_script\" decorators in multi_head_attention_forward() function.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Make sure you are on commit \"6e82b1c77d36386ba738af3287693105b4bbafe2\"\r\n2. Use the following script on GPU to reproduce the OOM error message.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\nimport torch                                                 \r\nimport torch.nn as nn                                        \r\n                                                                                                  \r\nd_model = 512                                                \r\nnhead = 16                                                   \r\nbptt = 10                                                    \r\nbatch_size = 15                                              \r\ndevice = torch.device(\"cuda\")                                \r\n                                                             \r\nnorm = nn.LayerNorm(d_model).to(device)                      \r\nself_attn = nn.MultiheadAttention(d_model, nhead).to(device) \r\nsrc_seq = torch.rand((bptt, batch_size, d_model)).to(device) \r\n                                                             \r\nfor _ in range(200000):                                      \r\n    src = norm(src_seq)                                      \r\n    output = self_attn(src, src, src)\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\nRuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 15.90 GiB total capacity; 13.49 GiB already allocated; 1.56 MiB free; 1.87 GiB cached)\r\n\r\n## Environment\r\n\r\nCollecting environment information...                                              \r\nPyTorch version: 1.1.0a0+1d33ab8                                                   \r\nIs debug build: No                                                                 \r\nCUDA used to build PyTorch: 9.2.88                                                 \r\n                                                                                   \r\nOS: Ubuntu 18.04.1 LTS                                                             \r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0                                  \r\nCMake version: version 3.12.2                                                      \r\n                                                                                   \r\nPython version: 3.7                                                                \r\nIs CUDA available: Yes                                                             \r\nCUDA runtime version: 9.2.88                                                       \r\nGPU models and configuration:                                                      \r\nGPU 0: Quadro GP100                                                                \r\nGPU 1: Quadro GP100                                                                \r\n                                                                                   \r\nNvidia driver version: 410.79                                                      \r\ncuDNN version: Could not collect                                                   \r\n                                                                                   \r\nVersions of relevant libraries:                                                    \r\n[pip] numpy==1.15.4                                                                \r\n[pip] numpydoc==0.8.0                                                              \r\n[pip] torch==1.1.0a0+1d33ab8                                                       \r\n[conda] blas                      1.0                         mkl                  \r\n[conda] magma-cuda90              2.5.0                         1    pytorch       \r\n[conda] mkl                       2019.1                      144                  \r\n[conda] mkl-include               2019.3                      199                  \r\n[conda] mkl-service               1.1.2            py37he904b0f_5                  \r\n[conda] mkl_fft                   1.0.6            py37hd81dba3_0                  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0                  \r\n[conda] torch                     1.1.0a0+1d33ab8           dev_0    <develop>\r\n## Additional context\r\n\r\n"}
{"number": 20589, "title": "Removing unnecessary comments (+fix flake8)", "time": "2019-05-16T15:42:29Z", "body": ""}
{"number": 20590, "title": "Change function that truncate to scalar_t on each atomicAdd", "time": "2019-05-16T15:43:13Z", "body": "## üöÄ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nat https://github.com/pytorch/pytorch/pull/19630/files#diff-5092da792c30694ee4adf0d0ae2a37c6R206 it is truncating to scalar_t on each atomicAdd call. It would be good to change that. See comments by @ngimel https://github.com/pytorch/pytorch/pull/19630#discussion_r281426912\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nPerformance improving\r\n\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n"}
{"number": 20591, "title": "@ignore annotation for user defined type", "time": "2019-05-16T16:55:45Z", "body": "## üöÄ Feature\r\nIn user defined type, it would be nice to have a @ignore annotation similar to https://github.com/pytorch/pytorch/pull/16055\r\n\r\nThis allows the user to write python only methods that are only used in training / debugging (like `def  #visualize`)\n\ncc @suo"}
{"number": 20592, "title": "ir.cpp, module.cpp: clang-format.", "time": "2019-05-16T17:08:23Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #20597 [DRAFT, DO NOT COMMIT] Add a demo pass operating differently on different functions depending on an externally provided map.\n* #20596 [DRAFT, DO NOT COMMIT] ir.*, module.cpp: add callstack to Node and Graph and use it.\n* #20595 [DRAFT, DO NOT COMMIT] scope.{cpp,h}: add CallStack.\n* #20594 Add a pybind for Module::get_functions.\n* **#20592 ir.cpp, module.cpp: clang-format.**\n\nDifferential Revision: [D15375131](https://our.internmc.facebook.com/intern/diff/D15375131)"}
{"number": 20593, "title": "Scope: Move implementations from .h to .cpp file.", "time": "2019-05-16T17:08:28Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #20597 [DRAFT, DO NOT COMMIT] Add a demo pass operating differently on different functions depending on an externally provided map.\n* #20596 [DRAFT, DO NOT COMMIT] ir.*, module.cpp: add callstack to Node and Graph and use it.\n* #20595 [DRAFT, DO NOT COMMIT] scope.{cpp,h}: add CallStack.\n* #20594 Add a pybind for Module::get_functions.\n* **#20593 Scope: Move implementations from .h to .cpp file.**\n* #20592 ir.cpp, module.cpp: clang-format.\n\nDifferential Revision: [D15375134](https://our.internmc.facebook.com/intern/diff/D15375134)"}
{"number": 20594, "title": "Add a pybind for Module::get_functions.", "time": "2019-05-16T17:08:34Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #20597 [DRAFT, DO NOT COMMIT] Add a demo pass operating differently on different functions depending on an externally provided map.\n* #20596 [DRAFT, DO NOT COMMIT] ir.*, module.cpp: add callstack to Node and Graph and use it.\n* #20595 [DRAFT, DO NOT COMMIT] scope.{cpp,h}: add CallStack.\n* **#20594 Add a pybind for Module::get_functions.**\n* #20592 ir.cpp, module.cpp: clang-format.\n\nDifferential Revision: [D15375132](https://our.internmc.facebook.com/intern/diff/D15375132)"}
{"number": 20595, "title": "[DRAFT, DO NOT COMMIT] scope.{cpp,h}: add CallStack.", "time": "2019-05-16T17:08:40Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #20596 [DRAFT, DO NOT COMMIT] ir.*, module.cpp: add callstack to Node and Graph and use it.\n* **#20595 [DRAFT, DO NOT COMMIT] scope.{cpp,h}: add CallStack.**\n\nDifferential Revision: [D15375133](https://our.internmc.facebook.com/intern/diff/D15375133)"}
{"number": 20596, "title": "[DRAFT, DO NOT COMMIT] ir.*, module.cpp: add callstack to Node and Graph and use it.", "time": "2019-05-16T17:08:45Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20596 [DRAFT, DO NOT COMMIT] ir.*, module.cpp: add callstack to Node and Graph and use it.**\n* #20595 [DRAFT, DO NOT COMMIT] scope.{cpp,h}: add CallStack.\n\nDifferential Revision: [D15375130](https://our.internmc.facebook.com/intern/diff/D15375130)"}
{"number": 20597, "title": "[DRAFT, DO NOT COMMIT] Add a demo pass operating differently on different functions depending on an externally provided map.", "time": "2019-05-16T17:08:50Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20597 [DRAFT, DO NOT COMMIT] Add a demo pass operating differently on different functions depending on an externally provided map.**\n* #20596 [DRAFT, DO NOT COMMIT] ir.*, module.cpp: add callstack to Node and Graph and use it.\n* #20595 [DRAFT, DO NOT COMMIT] scope.{cpp,h}: add CallStack.\n* #20594 Add a pybind for Module::get_functions.\n* #20592 ir.cpp, module.cpp: clang-format.\n\nDifferential Revision: [D15375129](https://our.internmc.facebook.com/intern/diff/D15375129)"}
{"number": 20598, "title": "Add `Tensor.T` attribute to reverse dimensions", "time": "2019-05-16T17:15:25Z", "body": "For compatibility with numpy"}
{"number": 20599, "title": "Delete defunct GitHub to fbcode sync script", "time": "2019-05-16T17:29:45Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20600 Finish removal of AT_CHECK, officially deprecate the macro.&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15375397/)\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20599 Delete defunct GitHub to fbcode sync script**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15373971/)\n\n\n\nDifferential Revision: [D15373971](https://our.internmc.facebook.com/intern/diff/D15373971/)"}
{"number": 20600, "title": "Finish removal of AT_CHECK, officially deprecate the macro.", "time": "2019-05-16T17:29:50Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20600 Finish removal of AT_CHECK, officially deprecate the macro.**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15375397/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20599 Delete defunct GitHub to fbcode sync script&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15373971/)\n\nAll future uses of AT_CHECK will fail our CI.\n\nDifferential Revision: [D15375397](https://our.internmc.facebook.com/intern/diff/D15375397/)"}
{"number": 20601, "title": "[DO NOT MERGE] Triggering devtoolset7 binary builds", "time": "2019-05-16T17:52:44Z", "body": ""}
{"number": 20602, "title": "[JIT] SubgraphMatcher: add attributes support.", "time": "2019-05-16T18:26:44Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20602 [JIT] SubgraphMatcher: add attributes support.**\n\nDifferential Revision: [D15377635](https://our.internmc.facebook.com/intern/diff/D15377635)"}
{"number": 20603, "title": "tracing for intra_op_parallel", "time": "2019-05-16T19:33:31Z", "body": "Summary:\nWhen we use intra_op_parallel operators, Caffe2 tracing was generating trace only for the master task giving a false impression that a lot of threads are underutilized.\nThis diff also traces child tasks.\n\nDifferential Revision: D14820008\n\n"}
{"number": 20604, "title": "[DO NOT MERGE] Running html update jobs", "time": "2019-05-16T19:47:30Z", "body": ""}
{"number": 20605, "title": "Improve torch.cdist performance", "time": "2019-05-16T20:01:06Z", "body": "Fix based on https://github.com/pytorch/pytorch/issues/15253"}
{"number": 20606, "title": "PyTorch is not using the GPU specified by CUDA_VISIBLE_DEVICES", "time": "2019-05-16T20:13:38Z", "body": "## üêõ Bug\r\n\r\nPyTorch is not using the GPU specified by `CUDA_VISIBLE_DEVICES`\r\n\r\n## To Reproduce\r\n\r\nRun the following script using command `CUDA_VISIBLE_DEVICES=3 python test.py`\r\n\r\n```python\r\n# test.py\r\nimport os\r\nimport torch\r\nimport time\r\nimport sys\r\n\r\nprint(os.environ)\r\nprint(torch.cuda.device_count())\r\nprint(torch.cuda.current_device())\r\nprint(os.getpid())\r\nsys.stdout.flush()\r\n\r\ndevice = torch.device('cuda')\r\na = torch.randn(10, 10, device=device)\r\n\r\nos.system('nvidia-smi')\r\n```\r\n\r\nOn my server, the output is:\r\n\r\n```\r\nenviron({'CUDA_VISIBLE_DEVICES': '3', 'LD_LIBRARY_PATH': '/apps/NeuroChem/build/lib:/home/jsmith48/scratch/Gits/RCDBuilder/build/lib:/apps/amber-volta/lib64:/apps/g09/bsd:/apps/g09/local:/apps/g09/extras:/apps/g09:/apps/gv/lib:/usr/local/cuda-9.2/lib64', 'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:', 'GAUSS_BSDDIR': '/apps/g09/bsd', 'SSH_CONNECTION': '10.30.162.220 55324 10.241.124.148 22', 'LESSCLOSE': '/usr/bin/lesspipe %s %s', '_DSM_BARRIER': 'SHM', 'OLDPWD': '/data/gaoxiang', 'XONSHRC': '/home/gaoxiang/.config/xonsh/config.json:/etc/xonshrc:/home/gaoxiang/.xonshrc', 'LOADED_RC_FILES': 'False,False,False', 'GV_DIR': '/apps/gv', 'GAUSS_LEXEDIR': '/apps/g09/linda-exe', 'USER': 'gaoxiang', 'XONSH_INTERACTIVE': 'True', 'GAUSS_ARCHDIR': '/apps/g09/arch', 'G09BASIS': '/apps/g09/basis', 'PWD': '/data/gaoxiang/torchani-neurochem-trainer', 'HOME': '/home/gaoxiang', 'SSH_CLIENT': '10.30.162.220 55324 22', 'XONSH_LOGIN': '1', 'GAUSS_EXEDIR': '/apps/g09/bsd:/apps/g09/local:/apps/g09/extras:/apps/g09', 'GAUSS_SCRDIR': '/home/jsmith48/scratch/g09_scratch', 'SSH_TTY': '/dev/pts/7', 'g09root': '/apps', 'MAIL': '/var/mail/gaoxiang', 'RCDB_BUILD': '/home/jsmith48/scratch/Gits/RCDBuilder/build', 'TERM': 'xterm-256color', 'SHELL': '/usr/bin/xonsh', 'SHELL_TYPE': 'prompt_toolkit', 'SHLVL': '1', 'PYTHONPATH': '/apps/ase:/apps/NeuroChem/build/lib:/home/jsmith48/Gits/ANI-Tools/lib:', 'XONSH_VERSION': '0.6.0', 'NC_ROOT': '/apps/NeuroChem', 'LOGNAME': 'gaoxiang', 'BASH_COMPLETIONS': '/usr/share/bash-completion/bash_completion', 'PATH': '/home/gaoxiang/anaconda3/bin:/data/gaoxiang/anaconda3/bin:/apps/ase/tools:/apps/NeuroChem/build/bin:/home/jsmith48/scratch/Gits/RCDBuilder/build/bin:/home/gaoxiang/bin:/apps/amber-volta/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/apps/g09/bsd:/apps/g09/local:/apps/g09/extras:/apps/g09:/usr/local/cuda-9.2/bin:/opt/local/bin:/opt/local/sbin', 'PGI_TERM': 'trace,abort', 'LESSOPEN': '| /usr/bin/lesspipe %s', '_': '/home/gaoxiang/anaconda3/bin/python'})\r\n1\r\n0\r\n18286\r\nThu May 16 16:11:51 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 410.93       Driver Version: 410.93       CUDA Version: 10.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  On   | 00000000:04:00.0 Off |                  N/A |\r\n| 27%   47C    P2   116W / 250W |   2233MiB / 11178MiB |     47%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 108...  On   | 00000000:05:00.0 Off |                  N/A |\r\n| 37%   62C    P2   116W / 250W |   1853MiB / 11178MiB |     53%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  TITAN V             On   | 00000000:08:00.0 Off |                  N/A |\r\n| 48%   66C    P2   127W / 250W |   1484MiB / 12036MiB |     81%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  TITAN V             On   | 00000000:09:00.0 Off |                  N/A |\r\n| 30%   39C    P8    27W / 250W |      0MiB / 12036MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   4  TITAN V             On   | 00000000:84:00.0 Off |                  N/A |\r\n| 28%   28C    P8    24W / 250W |      0MiB / 12036MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   5  Tesla V100-PCIE...  On   | 00000000:85:00.0 Off |                    0 |\r\n| N/A   28C    P0    38W / 250W |   1066MiB / 16130MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   6  Quadro GP100        On   | 00000000:88:00.0 Off |                  Off |\r\n| 37%   54C    P0   122W / 235W |   2371MiB / 16278MiB |     51%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   7  Quadro GP100        On   | 00000000:89:00.0 Off |                  Off |\r\n| 26%   30C    P0    28W / 235W |      0MiB / 16278MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      8682      C   /apps/amber18/bin/pmemd.cuda                2223MiB |\r\n|    1     21388      C   /apps/amber18/bin/pmemd.cuda                1843MiB |\r\n|    2      8430      C   /apps/amber18/bin/pmemd.cuda                1473MiB |\r\n|    5     18286      C   python                                      1055MiB |\r\n|    6     31207      C   /apps/amber18/bin/pmemd.cuda                2361MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nFrom the output, we can see that, the process id is `18286`, and `nvidia-smi` shows that this process is using GPU 5, which is not the 3 as specified by `CUDA_VISIBLE_DEVICES`.\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.1.0.dev20190516\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.11.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.148\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: TITAN V\r\nGPU 3: TITAN V\r\nGPU 4: TITAN V\r\nGPU 5: Tesla V100-PCIE-16GB\r\nGPU 6: Quadro GP100\r\nGPU 7: Quadro GP100\r\n\r\nNvidia driver version: 410.93\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.2\r\n[pip3] numpydoc==0.8.0\r\n[pip3] pytorch-ignite-nightly==20190514\r\n[pip3] torch-nightly==1.1.0.dev20190516\r\n[pip3] torchani==0.7.dev31+ga0451e68\r\n[conda] blas                      1.0                         mkl  \r\n[conda] cuda100                   1.0                           0    pytorch\r\n[conda] mkl                       2019.3                      199  \r\n[conda] mkl-service               1.1.2            py36he904b0f_5  \r\n[conda] mkl_fft                   1.0.10           py36ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py36hd81dba3_0  \r\n[conda] pytorch-ignite-nightly    20190514                 pypi_0    pypi\r\n[conda] torch-nightly             1.1.0.dev20190516          pypi_0    pypi\r\n[conda] torchani                  0.7.dev31+ga0451e68           dev_0    <develop>\r\n```"}
{"number": 20607, "title": "Add a new method SummaryWriter.flush()", "time": "2019-05-16T20:15:42Z", "body": "Summary: Add a new method SummaryWriter.flush()  that iterates through all of the FileWriters and flushes them\n\nDifferential Revision: D15380124\n\n"}
{"number": 20608, "title": "[pt1][quant] Add qscheme() method", "time": "2019-05-16T20:51:30Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20608 [pt1][quant] Add qscheme() method**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15364354/)\n\nExposing QScheme in python as Python objects like `torch.qscheme.per_tensor_affine` etc.\n\nDifferential Revision: [D15364354](https://our.internmc.facebook.com/intern/diff/D15364354/)"}
{"number": 20609, "title": "Don't split 256-bit AVX2 load/store intrinsics", "time": "2019-05-16T21:16:25Z", "body": "Recent versions of GCC split unaligned load and store intrinsics into\r\ntwo 128-bit instructions. On old processors (Sandy Bridge) this was a\r\nbit faster for unaligned data, but bit slower for aligned data. On new\r\nprocessors (Intel Haswell+, recent AMD) splitting loads is slower on\r\nboth aligned and unaligned data.\r\n\r\nClang, MSVC, and ICC do not split unaligned load and store intrinsics.\r\n\r\nThere's a good explanation here:\r\nhttps://stackoverflow.com/questions/52626726/why-doesnt-gcc-resolve-mm256-loadu-pd-as-single-vmovupd#tab-top\r\n\r\nSplitting load and store intrinsics makes no sense in our AVX2\r\nconfiguration because the CPUs that support AVX2 instructions are the\r\nsame CPUs where splitting is disadvantageous on all data alignemnt.\r\n\r\nNote that this doesn't change the AVX configuration (used by CPUs that\r\nsupport AVX but not AVX2). It's possible this would be benficial for\r\nthat configuration too (our data is usually 32-byte aligned), but I'd\r\nprefer the conservative change for now.\r\n\r\ntorch.add generated assembly (hot loop) (GCC 7.3.0)\r\nbefore:\r\nhttps://gist.github.com/colesbury/066376537bccd514daf8fe4ab54d8295\r\n\r\nafter:\r\nhttps://gist.github.com/colesbury/8b4b948145001d44b225c51d2428bb91\r\n\r\nTiming of `torch.add(x, y, out=z)` for size 10240 (1 thread, Broadwell,\r\nno turbo):\r\nbefore: 7.35 us after: 6.39 us\r\n\r\n(Take the torch.add timings with a grain of salt. The difference in timings\r\nis much larger than I would expect.)"}
{"number": 20610, "title": "Change InferLengthsRangeFill & InferGatherRanges, add more tests", "time": "2019-05-16T21:49:01Z", "body": "Summary:\nChange InferLengthsRangeFill\nAdd InferGatherRanges\nadd tests from ClipRangesGatherSigridHash all the way to SparseLengthsWeightedSum\nadd tests from SigridTransforms all the way to SparseLengthsWeightedSum\nmove the infer stage (for values, ids, scores, multiply inferred seq length by max_batch_size) from ClipRangesGatherSigridHash/SparseLengthsWeightedSum to GatherRanges, i.e. do the multiplication in InferGatherRanges\n\ne2e test will be added in the following diff\n\nDifferential Revision: D15382730\n\n"}
{"number": 20611, "title": "Freeze during validation with distributed training and model with batch normalization layers", "time": "2019-05-16T21:53:49Z", "body": "## ‚ùì Questions and Help\r\nHi,\r\nI got unexpected behavior during training with torch.distributed.DistributedDataParallel model on multiple GPUs.\r\n\r\n- I train my model with DistributedSampler and DataLoader from torch lib. During training everything is fine, but when I validate my model after the first epoch of training, everything gets stuck  - no crash, but GPUs are showing 100 % utilization as well as CPUs, but nothing happens.\r\n\r\n- If I remove BatchNorm2d layers from the model, everything runs just fine.\r\n\r\n- If I use model with BatchNorm2d layers with world size 1 (only one GPU but setup through DistributedDataParallel), everything is also fine.\r\n\r\nI assume that different batchnorm values are not correctly synced across gpus or any idea what am I doing wrong?\r\n\r\nThanks.\r\n\r\n## version / os\r\ndist backend -\"nccl\", Pytorch 1.1, Cuda 10, ubuntu 16.04\r\n"}
{"number": 20612, "title": "ensure version_counter gets incremented for non-differentiable outputs", "time": "2019-05-16T22:29:43Z", "body": "issue:\r\nhttps://github.com/pytorch/pytorch/issues/14571\r\n\r\nTo reproduce I:\r\n1) added these lines to derivatives.yaml:\r\n```\r\n- name: add_(Tensor self, Scalar other, Scalar alpha)\r\n  output_differentiability: [False, False, False]\r\n- name: add_(Tensor self, Tensor other, Scalar alpha)\r\n  output_differentiability: [False, False, False]\r\n```\r\n\r\n2) Ran this code:\r\n```\r\nimport torch\r\n\r\nscalar = torch.tensor(5)\r\nvar1 = torch.randn(4,2,requires_grad=True)\r\nvar2 = var1.detach().requires_grad_()\r\noutput1 = var1 * scalar\r\noutput2 = var2 * scalar\r\noutput1.sum().backward()\r\nscalar.add_(5, 1)\r\noutput2.sum().backward()\r\nprint(var1.grad)\r\nprint(var2.grad)\r\n```\r\nObserved modified var2.grad in the output:\r\n```\r\ntensor([[5., 5.],\r\n        [5., 5.],\r\n        [5., 5.],\r\n        [5., 5.]])\r\ntensor([[10., 10.],\r\n        [10., 10.],\r\n        [10., 10.],\r\n        [10., 10.]])\r\n```\r\n\r\nAfter making this change, re-running the above code produces the expected error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 18, in <module>\r\n    output2.sum().backward()\r\n  File \"/home/bvaughan/anaconda3/lib/python3.7/site-packages/torch/tensor.py\", line 107, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File \"/home/bvaughan/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 93, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\nRuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.LongTensor []] is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\r\n```\r\n\r\n\r\n"}
{"number": 20613, "title": "groupby function", "time": "2019-05-16T22:34:16Z", "body": "## üöÄ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\n## Motivation\r\n\r\nSay for example you have a batch of data but want to take a different action (e.g. run a different model) based on the label index, or based on the length of the input, etc. You want to subdivide the batch according to that value, and in some cases you need to reconstruct the outputs back into the shape of the original tensor. What is needed is a function that groups the rows of the tensor based on the value of the groupby field.\r\n\r\nI've ended up needing to implement this multiple times across projects. \r\n\r\n## Pitch\r\n\r\n```\r\ngroups, values, inverse_map = T.groupby(F[, dim=0])\r\n```\r\n\r\nBasically T is an `Nx...` tensor and `F` is a size `N` tensor (assuming dim=0). This function returns a list `groups` of length equal to the number of unique values in F. Each group contains all the rows of `T` for that unique element. `values` is just a tensor containing the unique values corresponding to each group, and `inverse_map` is a list of tensors telling you where all the rows came from.\r\n\r\n## Alternatives\r\n\r\nThis can be implemented in about 10 lines in pytorch with something like:\r\n```\r\nsorted_vals, order = F.sort(0)\r\ndelta = F[1:] - F[:-1]\r\ncutpoints = delta.nonzero()[0].tolist()\r\nres, inverse_map = [], [], torch.zeros(len(cutpoints))\r\nfor start, end, i in zip([0] + cutpoints, cutpoints + [len(F)], range(len(cutpoints) + 1)):\r\n    res.append(T[order[start:end]])\r\n    inverse_map.append(order[start:end])\r\n    values[i] = F[start]\r\n\r\nreturn res, values, inverse_map\r\n```\r\n\r\nThis could be done faster and in a more standard way as part of Pytorch.\r\n\r\n## Additional context\r\n\r\ncc @colesbury @lerks "}
{"number": 20614, "title": "[Second Try] [BC-breaking] Shallow-copy indices and values in sparse tensor ctor", "time": "2019-05-16T22:59:16Z", "body": "(Reopens https://github.com/pytorch/pytorch/pull/20330 and fixes test error.)\r\n\r\nAfter the Variable/Tensor merge, there is no guarantee that `indices` and `values` passed into the sparse tensor constructor don't contain AutogradMeta. However, we want to maintain the existing invariant that `indices_` and `values_` of a sparse tensor don't contain AutogradMeta, and to achieve this we need do shallow-copy in the sparse tensor constructor.\r\n\r\nNote that this is BC-breaking for code that changes the sizes / strides of the indices or values tensor after it's used to create a sparse tensor. In current master, such changes will be reflected in the sparse tensor and break sparse tensor invariants. After this PR, those changes will not be reflected in the sparse tensor, and thus the sparse tensor invariants are always preserved. Specifically, running in-place size/stride-changing ops such as `resize_` / `resize_as_` / `as_strided_` / `set_` / `transpose_` on the original values tensor will not update the sparse tensor's `values_`. For example:\r\n```python\r\n# Calling resize_ on non-requires-grad value tensor\r\ni2 = torch.zeros([1, 1])\r\nv2 = torch.ones([1, 2, 3])\r\nt2 = torch.sparse_coo_tensor(i2, v2, torch.Size([2, 2, 3]))\r\nv2.resize_(4, 5)\r\nt2.coalesce().values().size()\r\n# On current master, this throws \"indices and values must have same nnz, but got nnz from indices: 1, nnz from values: 4\", because resizing the original value tensor affects `values_` of the sparse tensor.\r\n# After this PR, this prints \"torch.Size([1, 2, 3])\", which means resizing the original value tensor doesn't affect `values_` of the sparse tensor.\r\n```"}
{"number": 20615, "title": "[jit] del statements aren't supported", "time": "2019-05-16T23:09:13Z", "body": "`del` isn't currently supported for dicts or lists:\r\n\r\n```python\r\n@torch.jit.script\r\ndef fn(x):\r\n    # type: (Dict[str, int])\r\n    x.pop(\"hi\")\t\t# ok\r\n    del x[\"hello\"]\t# torch.jit.frontend.UnsupportedNodeError: del statements aren't supported\r\n```\r\n\r\n`del` can also be used on lists:\r\n```python\r\n@torch.jit.script\r\ndef fn(x):\r\n    # type: (List[str])\r\n    del x[0]\t\t# torch.jit.frontend.UnsupportedNodeError: del statements aren't supported:\r\n```\r\n\r\ncc @suo"}
{"number": 20616, "title": "[jit] `list(string)` does not work", "time": "2019-05-16T23:27:54Z", "body": "This doesn't work, the error message is also an internal assert\r\n\r\n```python\r\nclass M(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n        super(M, self).__init__()\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, token: str) -> List[str]:\r\n        return list(token)\r\n\r\nb = M()\r\n```"}
{"number": 20617, "title": "[jit] Add list(string)", "time": "2019-05-17T00:02:10Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20617 [jit] Add list(string)**\n\n\n\nDifferential Revision: [D15397739](https://our.internmc.facebook.com/intern/diff/15397739/)"}
{"number": 20618, "title": "Improve CPUAllocator OOM message", "time": "2019-05-17T03:13:17Z", "body": "Spotted while debugging some problem\r\n\r\nBefore\r\n```\r\n>>> torch.empty(10**15)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nRuntimeError: [enforce fail at CPUAllocator.cpp:56] posix_memalign(&data, gAlignment, nbytes) == 0. 12 vs 0\r\n```\r\n\r\nAfter\r\n```\r\n>>> torch.empty(10**15)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nRuntimeError: [enforce fail at CPUAllocator.cpp:65] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 4000000000000000 bytes. Error code 12 (Cannot allocate memory)\r\n```"}
{"number": 20619, "title": "Allow parallel sending to device in DataLoader", "time": "2019-05-17T03:51:38Z", "body": "## üöÄ Feature\r\nI wish to request a feature that allows DataLoader to send data to devices instead of only pin memory, or create a new DataLoader which can do this.\r\n\r\n## Motivation\r\n\r\nI am currently working on a dataset with very large image sizes. Also, these images need to be processed with computationally intensive processes which are best done on GPU. \r\n\r\nDue to the large size of my data, the training process is dominated by the time necessary for reading data from disk and sending it to GPU. \r\n\r\nHowever, DataLoader does not natively support sending data to devices, only to pin memory.\r\n\r\nMoreover, when I tried to implement sending to GPU inside the transform for the Dataset, I got \"RuntimeError: CUDA error: initialization error\".\r\nI found in this [post](https://discuss.pytorch.org/t/pin-memory-vs-sending-direct-to-gpu-from-dataset/33891) that sending to GPU could only be done if only 1 process was being run. \r\n\r\nHowever, I believe that many training processes which are bottlenecked by data I/O would benefit from being able to prefetch data to GPU in parallel, even at the cost of taking up more GPU memory.\r\n\r\nI have found in the [documentation](https://pytorch.org/docs/stable/multiprocessing.html) that sharing tensors between processes is possible, at least in Python 3. I would like to request this feature to maximize GPU utilization.\r\n\r\n## Pitch\r\n\r\nI would therefore like to propose a new DataLoader that can take in \"device\" arguments as suggested here https://github.com/pytorch/pytorch/issues/11372, and allow loading to GPUs in parallel.\r\n\r\n## Alternatives\r\n\r\nAn alternative solution would be to create a new DataLoader class specifically designed to allow parallel data loading to GPU devices.\r\n\r\n"}
{"number": 20620, "title": "Move THCTensor_{random, clampedRandom, cappedRandom} to ATen", "time": "2019-05-17T04:06:30Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* #20626 Speedup bernoulli_scalar_cuda_kernel with grid-stride loop\r\n* #20625 Move THCTensor_(geometric) to ATen\r\n* #20624 Move THCTensor_(lognormal) to ATen\r\n* #20623 Move THCTensor_(exponential) to ATen\r\n* #20622 Move THCTensor_(cauchy) to ATen\r\n* #20621 Move THCTensor_{normal, normal_means, normal_stddevs, normal_means_stddevs} to ATen\r\n* **#20620 Move THCTensor_{random, clampedRandom, cappedRandom} to ATen**\r\n\r\nDifferential Revision: [D15454050](https://our.internmc.facebook.com/intern/diff/D15454050)\r\n\r\n## Effective Bandwidth Benchmark\r\n- using https://gist.github.com/syed-ahmed/f8b7384d642f4bce484228b508b4bc68\r\n- on V100\r\n### Float Type\r\n#### Before:\r\n```\r\nrandom, size, elements 65536 forward 5.106925964355469e-06 bandwidth (GB/s) 51.331075059570495\r\nrandom, size, elements 131072 forward 5.497932434082031e-06 bandwidth (GB/s) 95.36093909592368\r\nrandom, size, elements 262144 forward 7.791519165039062e-06 bandwidth (GB/s) 134.57914660660956\r\nrandom, size, elements 524288 forward 1.2221336364746093e-05 bandwidth (GB/s) 171.59760090144363\r\nrandom, size, elements 1048576 forward 2.0668506622314453e-05 bandwidth (GB/s) 202.93212647844044\r\nrandom, size, elements 2097152 forward 3.9124488830566405e-05 bandwidth (GB/s) 214.40811754315664\r\nrandom, size, elements 4194304 forward 7.290840148925782e-05 bandwidth (GB/s) 230.1136173239503\r\nrandom, size, elements 8388608 forward 0.00013821840286254883 bandwidth (GB/s) 242.76385275098409\r\nrandom, size, elements 16777216 forward 0.0002722597122192383 bandwidth (GB/s) 246.48841157211064\r\nrandom, size, elements 33554432 forward 0.0005396437644958496 bandwidth (GB/s) 248.71542456418447\r\n```\r\n#### After:\r\n```\r\nrandom, size, elements 65536 forward 5.841255187988281e-06 bandwidth (GB/s) 44.878025623510204\r\nrandom, size, elements 131072 forward 5.857944488525391e-06 bandwidth (GB/s) 89.5003360013024\r\nrandom, size, elements 262144 forward 6.563663482666016e-06 bandwidth (GB/s) 159.75468620065382\r\nrandom, size, elements 524288 forward 7.276535034179687e-06 bandwidth (GB/s) 288.207504004194\r\nrandom, size, elements 1048576 forward 1.0349750518798827e-05 bandwidth (GB/s) 405.2565317764571\r\nrandom, size, elements 2097152 forward 1.6405582427978516e-05 bandwidth (GB/s) 511.3264364021509\r\nrandom, size, elements 4194304 forward 2.7208328247070314e-05 bandwidth (GB/s) 616.6206114411497\r\nrandom, size, elements 8388608 forward 4.884481430053711e-05 bandwidth (GB/s) 686.9599665901694\r\nrandom, size, elements 16777216 forward 9.639024734497071e-05 bandwidth (GB/s) 696.2204771591086\r\nrandom, size, elements 33554432 forward 0.00017502307891845704 bandwidth (GB/s) 766.8573129291814\r\n```\r\n### Double Type\r\n#### Before:\r\n```\r\nrandom, size, elements 65536 forward 6.1082839965820315e-06 bandwidth (GB/s) 42.916144721935986\r\nrandom, size, elements 131072 forward 8.215904235839844e-06 bandwidth (GB/s) 63.81379151340685\r\nrandom, size, elements 262144 forward 1.3575553894042968e-05 bandwidth (GB/s) 77.240016001124\r\nrandom, size, elements 524288 forward 2.3760795593261718e-05 bandwidth (GB/s) 88.26101768219948\r\nrandom, size, elements 1048576 forward 4.4798851013183595e-05 bandwidth (GB/s) 93.62525835240021\r\nrandom, size, elements 2097152 forward 8.335113525390626e-05 bandwidth (GB/s) 100.64179659276888\r\nrandom, size, elements 4194304 forward 0.00015572309494018554 bandwidth (GB/s) 107.7374939564633\r\nrandom, size, elements 8388608 forward 0.0003071308135986328 bandwidth (GB/s) 109.25127181751903\r\nrandom, size, elements 16777216 forward 0.0006092119216918945 bandwidth (GB/s) 110.15684626398355\r\nrandom, size, elements 33554432 forward 0.0011054635047912597 bandwidth (GB/s) 121.41307914578674\r\n```\r\n#### After:\r\n```\r\nrandom, size, elements 65536 forward 5.834102630615234e-06 bandwidth (GB/s) 44.93304567944422\r\nrandom, size, elements 131072 forward 6.258487701416016e-06 bandwidth (GB/s) 83.77231449721906\r\nrandom, size, elements 262144 forward 7.848739624023438e-06 bandwidth (GB/s) 133.5980106653706\r\nrandom, size, elements 524288 forward 1.185894012451172e-05 bandwidth (GB/s) 176.84143591089668\r\nrandom, size, elements 1048576 forward 2.0167827606201173e-05 bandwidth (GB/s) 207.97004426546874\r\nrandom, size, elements 2097152 forward 3.463029861450195e-05 bandwidth (GB/s) 242.23319854617557\r\nrandom, size, elements 4194304 forward 6.528139114379883e-05 bandwidth (GB/s) 256.9984448254775\r\nrandom, size, elements 8388608 forward 0.00012089729309082031 bandwidth (GB/s) 277.544940355226\r\nrandom, size, elements 16777216 forward 0.00023464202880859374 bandwidth (GB/s) 286.0053006733214\r\nrandom, size, elements 33554432 forward 0.00044272661209106447 bandwidth (GB/s) 303.1616449846316\r\n```\r\n"}
{"number": 20621, "title": "Move THCTensor_{normal, normal_means, normal_stddevs, normal_means_stddevs} to ATen", "time": "2019-05-17T04:06:43Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* #20626 Speedup bernoulli_scalar_cuda_kernel with grid-stride loop\r\n* #20625 Move THCTensor_(geometric) to ATen\r\n* #20624 Move THCTensor_(lognormal) to ATen\r\n* #20623 Move THCTensor_(exponential) to ATen\r\n* #20622 Move THCTensor_(cauchy) to ATen\r\n* **#20621 Move THCTensor_{normal, normal_means, normal_stddevs, normal_means_stddevs} to ATen**\r\n* #20620 Move THCTensor_{random, clampedRandom, cappedRandom} to ATen\r\n\r\n## Effective Bandwidth Benchmark\r\n- using https://gist.github.com/syed-ahmed/f8b7384d642f4bce484228b508b4bc68\r\n- on V100\r\n### Float Type\r\n#### Before:\r\n```\r\nnormal, size, elements 65536 forward 4.956722259521484e-06 bandwidth (GB/s) 52.88656218258779\r\nnormal, size, elements 131072 forward 5.285739898681641e-06 bandwidth (GB/s) 99.18914098114568\r\nnormal, size, elements 262144 forward 7.548332214355469e-06 bandwidth (GB/s) 138.91492454529376\r\nnormal, size, elements 524288 forward 1.1980533599853516e-05 bandwidth (GB/s) 175.0466273076219\r\nnormal, size, elements 1048576 forward 2.091646194458008e-05 bandwidth (GB/s) 200.52645667862762\r\nnormal, size, elements 2097152 forward 3.9961338043212894e-05 bandwidth (GB/s) 209.91809610901498\r\nnormal, size, elements 4194304 forward 7.39765167236328e-05 bandwidth (GB/s) 226.79110538115253\r\nnormal, size, elements 8388608 forward 0.0001377725601196289 bandwidth (GB/s) 243.5494555001696\r\nnormal, size, elements 16777216 forward 0.0002710080146789551 bandwidth (GB/s) 247.62686107087774\r\nnormal, size, elements 33554432 forward 0.0005375170707702637 bandwidth (GB/s) 249.69947058177252\r\n```\r\n#### After:\r\n```\r\nnormal, size, elements 65536 forward 6.198883056640625e-06 bandwidth (GB/s) 42.288908760615385\r\nnormal, size, elements 131072 forward 6.756782531738281e-06 bandwidth (GB/s) 77.59432800112916\r\nnormal, size, elements 262144 forward 7.560253143310547e-06 bandwidth (GB/s) 138.6958849291706\r\nnormal, size, elements 524288 forward 7.550716400146485e-06 bandwidth (GB/s) 277.7421225831386\r\nnormal, size, elements 1048576 forward 1.1034011840820313e-05 bandwidth (GB/s) 380.1250225673293\r\nnormal, size, elements 2097152 forward 1.802682876586914e-05 bandwidth (GB/s) 465.34019427102237\r\nnormal, size, elements 4194304 forward 2.8417110443115234e-05 bandwidth (GB/s) 590.3913430460946\r\nnormal, size, elements 8388608 forward 4.8711299896240235e-05 bandwidth (GB/s) 688.8428777608927\r\nnormal, size, elements 16777216 forward 9.685993194580078e-05 bandwidth (GB/s) 692.8444265018856\r\nnormal, size, elements 33554432 forward 0.00018213510513305663 bandwidth (GB/s) 736.9130069787966\r\n```\r\n### Double Type\r\n#### Before:\r\n```\r\nnormal, size, elements 65536 forward 5.8841705322265624e-06 bandwidth (GB/s) 44.55071425348461\r\nnormal, size, elements 131072 forward 8.018016815185547e-06 bandwidth (GB/s) 65.38873789925661\r\nnormal, size, elements 262144 forward 1.2989044189453124e-05 bandwidth (GB/s) 80.72772597474304\r\nnormal, size, elements 524288 forward 2.2075176239013673e-05 bandwidth (GB/s) 95.00046465285668\r\nnormal, size, elements 1048576 forward 4.1041374206542965e-05 bandwidth (GB/s) 102.19696784254678\r\nnormal, size, elements 2097152 forward 7.57598876953125e-05 bandwidth (GB/s) 110.72624650312186\r\nnormal, size, elements 4194304 forward 0.00013725996017456056 bandwidth (GB/s) 122.22949779865557\r\nnormal, size, elements 8388608 forward 0.0002614736557006836 bandwidth (GB/s) 128.32815569921402\r\nnormal, size, elements 16777216 forward 0.0005080199241638184 bandwidth (GB/s) 132.0988819689674\r\nnormal, size, elements 33554432 forward 0.0009479570388793945 bandwidth (GB/s) 141.58629821311564\r\n```\r\n#### After:\r\n```\r\nnormal, size, elements 65536 forward 5.991458892822265e-06 bandwidth (GB/s) 43.75294977222444\r\nnormal, size, elements 131072 forward 7.293224334716797e-06 bandwidth (GB/s) 71.88699756626349\r\nnormal, size, elements 262144 forward 8.094310760498048e-06 bandwidth (GB/s) 129.54481623281296\r\nnormal, size, elements 524288 forward 1.2805461883544922e-05 bandwidth (GB/s) 163.7701177100726\r\nnormal, size, elements 1048576 forward 2.2592544555664064e-05 bandwidth (GB/s) 185.64991604491345\r\nnormal, size, elements 2097152 forward 3.801822662353516e-05 bandwidth (GB/s) 220.6470092112881\r\nnormal, size, elements 4194304 forward 6.761550903320313e-05 bandwidth (GB/s) 248.1267425164457\r\nnormal, size, elements 8388608 forward 0.00013209104537963867 bandwidth (GB/s) 254.02503177684966\r\nnormal, size, elements 16777216 forward 0.0002667689323425293 bandwidth (GB/s) 251.56176699703818\r\nnormal, size, elements 33554432 forward 0.0004705166816711426 bandwidth (GB/s) 285.25604559501795\r\n```"}
{"number": 20622, "title": "Move THCTensor_(cauchy) to ATen", "time": "2019-05-17T04:06:55Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* #20886 Remove curandStateMTGP32 usage\r\n* #20626 Speedup bernoulli_scalar_cuda_kernel with grid-stride loop\r\n* #20625 Move THCTensor_(geometric) to ATen\r\n* #20624 Move THCTensor_(lognormal) to ATen\r\n* #20623 Move THCTensor_(exponential) to ATen\r\n* **#20622 Move THCTensor_(cauchy) to ATen**\r\n* #20621 Move THCTensor_{normal, normal_means, normal_stddevs, normal_means_stddevs} to ATen\r\n* #20620 Move THCTensor_{random, clampedRandom, cappedRandom} to ATen\r\n\r\nDifferential Revision: [D15454052](https://our.internmc.facebook.com/intern/diff/D15454052)\r\n\r\n## Effective Bandwidth Benchmark\r\n- using https://gist.github.com/syed-ahmed/f8b7384d642f4bce484228b508b4bc68\r\n- on V100\r\n### Float Type\r\n#### Before:\r\n```\r\ncauchy, size, elements 65536 forward 4.980564117431641e-06 bandwidth (GB/s) 52.63339529803734\r\ncauchy, size, elements 131072 forward 6.232261657714844e-06 bandwidth (GB/s) 84.12483762631982\r\ncauchy, size, elements 262144 forward 9.548664093017577e-06 bandwidth (GB/s) 109.81389540833959\r\ncauchy, size, elements 524288 forward 1.59454345703125e-05 bandwidth (GB/s) 131.52052963827754\r\ncauchy, size, elements 1048576 forward 2.86865234375e-05 bandwidth (GB/s) 146.21165262978724\r\ncauchy, size, elements 2097152 forward 5.4748058319091796e-05 bandwidth (GB/s) 153.2220184158516\r\ncauchy, size, elements 4194304 forward 0.00010075807571411133 bandwidth (GB/s) 166.50988897012377\r\ncauchy, size, elements 8388608 forward 0.0001935744285583496 bandwidth (GB/s) 173.34124269355965\r\ncauchy, size, elements 16777216 forward 0.00038077831268310545 bandwidth (GB/s) 176.24129779641603\r\ncauchy, size, elements 33554432 forward 0.0006851387023925781 bandwidth (GB/s) 195.8986224705994\r\n```\r\n#### After:\r\n```\r\ncauchy, size, elements 65536 forward 6.077289581298828e-06 bandwidth (GB/s) 43.13501874366419\r\ncauchy, size, elements 131072 forward 6.2131881713867184e-06 bandwidth (GB/s) 84.38308731972373\r\ncauchy, size, elements 262144 forward 6.46829605102539e-06 bandwidth (GB/s) 162.11008150033175\r\ncauchy, size, elements 524288 forward 6.8783760070800785e-06 bandwidth (GB/s) 304.8905726935182\r\ncauchy, size, elements 1048576 forward 9.505748748779296e-06 bandwidth (GB/s) 441.23867681003264\r\ncauchy, size, elements 2097152 forward 1.5070438385009766e-05 bandwidth (GB/s) 556.6266744001266\r\ncauchy, size, elements 4194304 forward 2.4406909942626954e-05 bandwidth (GB/s) 687.396152951685\r\ncauchy, size, elements 8388608 forward 4.6243667602539064e-05 bandwidth (GB/s) 725.6005792706125\r\ncauchy, size, elements 16777216 forward 9.100198745727539e-05 bandwidth (GB/s) 737.4439380404413\r\ncauchy, size, elements 33554432 forward 0.00017449140548706055 bandwidth (GB/s) 769.1939188944922\r\n```\r\n### Double Type\r\n#### Before:\r\n```\r\ncauchy, size, elements 65536 forward 4.885196685791015e-06 bandwidth (GB/s) 53.660889593753055\r\ncauchy, size, elements 131072 forward 6.229877471923828e-06 bandwidth (GB/s) 84.15703235943361\r\ncauchy, size, elements 262144 forward 9.605884552001953e-06 bandwidth (GB/s) 109.15975455706132\r\ncauchy, size, elements 524288 forward 1.5976428985595704e-05 bandwidth (GB/s) 131.26537863315923\r\ncauchy, size, elements 1048576 forward 2.9621124267578124e-05 bandwidth (GB/s) 141.59840666786866\r\ncauchy, size, elements 2097152 forward 5.5103302001953126e-05 bandwidth (GB/s) 152.23421637604707\r\ncauchy, size, elements 4194304 forward 0.00010124444961547851 bandwidth (GB/s) 165.70998275677383\r\ncauchy, size, elements 8388608 forward 0.0001944279670715332 bandwidth (GB/s) 172.58027487195184\r\ncauchy, size, elements 16777216 forward 0.00034950494766235353 bandwidth (GB/s) 192.01119883668116\r\ncauchy, size, elements 33554432 forward 0.0007002186775207519 bandwidth (GB/s) 191.67973135938277\r\n```\r\n#### After:\r\n```\r\ncauchy, size, elements 65536 forward 5.91278076171875e-06 bandwidth (GB/s) 44.33514628129032\r\ncauchy, size, elements 131072 forward 6.234645843505859e-06 bandwidth (GB/s) 84.09266751632889\r\ncauchy, size, elements 262144 forward 7.433891296386719e-06 bandwidth (GB/s) 141.05344807902503\r\ncauchy, size, elements 524288 forward 1.1401176452636719e-05 bandwidth (GB/s) 183.94171941045587\r\ncauchy, size, elements 1048576 forward 1.960039138793945e-05 bandwidth (GB/s) 213.99082890665372\r\ncauchy, size, elements 2097152 forward 3.434181213378906e-05 bandwidth (GB/s) 244.26806504326578\r\ncauchy, size, elements 4194304 forward 6.517410278320313e-05 bandwidth (GB/s) 257.4215107465028\r\ncauchy, size, elements 8388608 forward 0.0001229524612426758 bandwidth (GB/s) 272.9057365819818\r\ncauchy, size, elements 16777216 forward 0.00023239374160766602 bandwidth (GB/s) 288.77225150621814\r\ncauchy, size, elements 33554432 forward 0.00046050310134887696 bandwidth (GB/s) 291.4589013773367\r\n```"}
{"number": 20623, "title": "Move THCTensor_(exponential) to ATen", "time": "2019-05-17T04:07:07Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* #20886 Remove curandStateMTGP32 usage\r\n* #20626 Speedup bernoulli_scalar_cuda_kernel with grid-stride loop\r\n* #20625 Move THCTensor_(geometric) to ATen\r\n* #20624 Move THCTensor_(lognormal) to ATen\r\n* **#20623 Move THCTensor_(exponential) to ATen**\r\n* #20622 Move THCTensor_(cauchy) to ATen\r\n* #20621 Move THCTensor_{normal, normal_means, normal_stddevs, normal_means_stddevs} to ATen\r\n* #20620 Move THCTensor_{random, clampedRandom, cappedRandom} to ATen\r\n\r\nDifferential Revision: [D15454047](https://our.internmc.facebook.com/intern/diff/D15454047)\r\n\r\n## Effective Bandwidth Benchmark\r\n- using https://gist.github.com/syed-ahmed/f8b7384d642f4bce484228b508b4bc68\r\n- on V100\r\n### Float Type\r\n#### Before:\r\n```\r\nexponential, size, elements 65536 forward 4.951953887939453e-06 bandwidth (GB/s) 52.937488097063074\r\nexponential, size, elements 131072 forward 5.1164627075195315e-06 bandwidth (GB/s) 102.47079476011184\r\nexponential, size, elements 262144 forward 7.412433624267578e-06 bandwidth (GB/s) 141.46177263119975\r\nexponential, size, elements 524288 forward 1.1911392211914063e-05 bandwidth (GB/s) 176.06271061265014\r\nexponential, size, elements 1048576 forward 2.077341079711914e-05 bandwidth (GB/s) 201.90733437869852\r\nexponential, size, elements 2097152 forward 3.968000411987305e-05 bandwidth (GB/s) 211.4064296631136\r\nexponential, size, elements 4194304 forward 7.367134094238281e-05 bandwidth (GB/s) 227.73056368176054\r\nexponential, size, elements 8388608 forward 0.0001375126838684082 bandwidth (GB/s) 244.00972372926472\r\nexponential, size, elements 16777216 forward 0.0002710747718811035 bandwidth (GB/s) 247.5658783526883\r\nexponential, size, elements 33554432 forward 0.0005277013778686524 bandwidth (GB/s) 254.34409237682056\r\n```\r\n#### After:\r\n```\r\nexponential, size, elements 65536 forward 5.60760498046875e-06 bandwidth (GB/s) 46.74794335782313\r\nexponential, size, elements 131072 forward 7.700920104980468e-06 bandwidth (GB/s) 68.0812153421672\r\nexponential, size, elements 262144 forward 6.551742553710938e-06 bandwidth (GB/s) 160.04536066608443\r\nexponential, size, elements 524288 forward 6.9427490234375e-06 bandwidth (GB/s) 302.0636340043956\r\nexponential, size, elements 1048576 forward 9.472370147705077e-06 bandwidth (GB/s) 442.7935072845709\r\nexponential, size, elements 2097152 forward 1.4712810516357422e-05 bandwidth (GB/s) 570.1567345459731\r\nexponential, size, elements 4194304 forward 2.4566650390625e-05 bandwidth (GB/s) 682.9264768795031\r\nexponential, size, elements 8388608 forward 4.60505485534668e-05 bandwidth (GB/s) 728.6434810009216\r\nexponential, size, elements 16777216 forward 9.00864601135254e-05 bandwidth (GB/s) 744.9384060094111\r\nexponential, size, elements 33554432 forward 0.00017408370971679687 bandwidth (GB/s) 770.9953344764326\r\n```\r\n### Double Type\r\n#### Before:\r\n```\r\nexponential, size, elements 65536 forward 4.985332489013672e-06 bandwidth (GB/s) 52.58305250004783\r\nexponential, size, elements 131072 forward 6.051063537597656e-06 bandwidth (GB/s) 86.64394229913319\r\nexponential, size, elements 262144 forward 9.377002716064453e-06 bandwidth (GB/s) 111.82421843640988\r\nexponential, size, elements 524288 forward 1.549959182739258e-05 bandwidth (GB/s) 135.30369208134132\r\nexponential, size, elements 1048576 forward 2.866983413696289e-05 bandwidth (GB/s) 146.2967654421289\r\nexponential, size, elements 2097152 forward 5.302190780639648e-05 bandwidth (GB/s) 158.2102256793561\r\nexponential, size, elements 4194304 forward 9.615898132324219e-05 bandwidth (GB/s) 174.47372849762968\r\nexponential, size, elements 8388608 forward 0.00018301725387573242 bandwidth (GB/s) 183.34026595537955\r\nexponential, size, elements 16777216 forward 0.0003589057922363281 bandwidth (GB/s) 186.98183604629858\r\nexponential, size, elements 33554432 forward 0.000672616958618164 bandwidth (GB/s) 199.5455604862227\r\n```\r\n#### After:\r\n```\r\nexponential, size, elements 65536 forward 5.755424499511719e-06 bandwidth (GB/s) 45.547291954266775\r\nexponential, size, elements 131072 forward 6.275177001953125e-06 bandwidth (GB/s) 83.54951578844985\r\nexponential, size, elements 262144 forward 7.97271728515625e-06 bandwidth (GB/s) 131.52052963827754\r\nexponential, size, elements 524288 forward 1.2047290802001953e-05 bandwidth (GB/s) 174.07664797561844\r\nexponential, size, elements 1048576 forward 2.0439624786376954e-05 bandwidth (GB/s) 205.20454968407793\r\nexponential, size, elements 2097152 forward 3.5920143127441405e-05 bandwidth (GB/s) 233.53492691379267\r\nexponential, size, elements 4194304 forward 6.896495819091797e-05 bandwidth (GB/s) 243.27160401598564\r\nexponential, size, elements 8388608 forward 0.00012843608856201173 bandwidth (GB/s) 261.2539230653945\r\nexponential, size, elements 16777216 forward 0.0002438235282897949 bandwidth (GB/s) 275.23539041005995\r\nexponential, size, elements 33554432 forward 0.00046614646911621096 bandwidth (GB/s) 287.93037573462635\r\n```"}
{"number": 20624, "title": "Move THCTensor_(lognormal) to ATen", "time": "2019-05-17T04:07:21Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* #20886 Remove curandStateMTGP32 usage\r\n* #20626 Speedup bernoulli_scalar_cuda_kernel with grid-stride loop\r\n* #20625 Move THCTensor_(geometric) to ATen\r\n* **#20624 Move THCTensor_(lognormal) to ATen**\r\n* #20623 Move THCTensor_(exponential) to ATen\r\n* #20622 Move THCTensor_(cauchy) to ATen\r\n* #20621 Move THCTensor_{normal, normal_means, normal_stddevs, normal_means_stddevs} to ATen\r\n* #20620 Move THCTensor_{random, clampedRandom, cappedRandom} to ATen\r\n\r\nDifferential Revision: [D15454051](https://our.internmc.facebook.com/intern/diff/D15454051)\r\n\r\n## Effective Bandwidth Benchmark\r\n- using https://gist.github.com/syed-ahmed/f8b7384d642f4bce484228b508b4bc68\r\n- on V100\r\n### Float Type\r\n#### Before:\r\n```\r\nlog_normal, size, elements 65536 forward 4.84466552734375e-06 bandwidth (GB/s) 54.1098242015748\r\nlog_normal, size, elements 131072 forward 5.0425529479980465e-06 bandwidth (GB/s) 103.97273075895983\r\nlog_normal, size, elements 262144 forward 7.326602935791016e-06 bandwidth (GB/s) 143.11898832098927\r\nlog_normal, size, elements 524288 forward 1.1749267578125e-05 bandwidth (GB/s) 178.49214736623378\r\nlog_normal, size, elements 1048576 forward 2.05230712890625e-05 bandwidth (GB/s) 204.37019103643124\r\nlog_normal, size, elements 2097152 forward 3.9284229278564456e-05 bandwidth (GB/s) 213.53627534643442\r\nlog_normal, size, elements 4194304 forward 7.281541824340821e-05 bandwidth (GB/s) 230.40746595613766\r\nlog_normal, size, elements 8388608 forward 0.00013544559478759766 bandwidth (GB/s) 247.7336531514311\r\nlog_normal, size, elements 16777216 forward 0.0002670741081237793 bandwidth (GB/s) 251.27431659866272\r\nlog_normal, size, elements 33554432 forward 0.0005250406265258789 bandwidth (GB/s) 255.63303336753222\r\n```\r\n#### After:\r\n```\r\nlog_normal, size, elements 65536 forward 5.47647476196289e-06 bandwidth (GB/s) 47.86728897588159\r\nlog_normal, size, elements 131072 forward 6.859302520751953e-06 bandwidth (GB/s) 76.43459351936045\r\nlog_normal, size, elements 262144 forward 7.7056884765625e-06 bandwidth (GB/s) 136.07817175445544\r\nlog_normal, size, elements 524288 forward 8.029937744140625e-06 bandwidth (GB/s) 261.1666574289786\r\nlog_normal, size, elements 1048576 forward 1.1892318725585938e-05 bandwidth (GB/s) 352.6901773138733\r\nlog_normal, size, elements 2097152 forward 1.9683837890625e-05 bandwidth (GB/s) 426.1672975875969\r\nlog_normal, size, elements 4194304 forward 3.241539001464844e-05 bandwidth (GB/s) 517.5694629130921\r\nlog_normal, size, elements 8388608 forward 5.803346633911133e-05 bandwidth (GB/s) 578.1910700272298\r\nlog_normal, size, elements 16777216 forward 0.00011091709136962891 bandwidth (GB/s) 605.0362768381755\r\nlog_normal, size, elements 33554432 forward 0.00021491527557373046 bandwidth (GB/s) 624.5146029834174\r\n```\r\n### Double Type\r\n#### Before:\r\n```\r\nlog_normal, size, elements 65536 forward 5.793571472167969e-06 bandwidth (GB/s) 45.247392089547326\r\nlog_normal, size, elements 131072 forward 8.199214935302735e-06 bandwidth (GB/s) 63.943682918057576\r\nlog_normal, size, elements 262144 forward 1.3582706451416015e-05 bandwidth (GB/s) 77.19934195373004\r\nlog_normal, size, elements 524288 forward 2.3326873779296876e-05 bandwidth (GB/s) 89.90283137988553\r\nlog_normal, size, elements 1048576 forward 4.379749298095703e-05 bandwidth (GB/s) 95.76584673062604\r\nlog_normal, size, elements 2097152 forward 8.105754852294922e-05 bandwidth (GB/s) 103.48953493979646\r\nlog_normal, size, elements 4194304 forward 0.0001421213150024414 bandwidth (GB/s) 118.04855590951854\r\nlog_normal, size, elements 8388608 forward 0.00027796506881713865 bandwidth (GB/s) 120.71456367804988\r\nlog_normal, size, elements 16777216 forward 0.0005494546890258789 bandwidth (GB/s) 122.13721229493271\r\nlog_normal, size, elements 33554432 forward 0.0010767412185668946 bandwidth (GB/s) 124.65179718729368\r\n```\r\n#### After:\r\n```\r\nlog_normal, size, elements 65536 forward 5.91278076171875e-06 bandwidth (GB/s) 44.33514628129032\r\nlog_normal, size, elements 131072 forward 7.789134979248047e-06 bandwidth (GB/s) 67.31017005056627\r\nlog_normal, size, elements 262144 forward 9.219646453857422e-06 bandwidth (GB/s) 113.73277763392811\r\nlog_normal, size, elements 524288 forward 1.5113353729248047e-05 bandwidth (GB/s) 138.7615242500079\r\nlog_normal, size, elements 1048576 forward 2.7089118957519532e-05 bandwidth (GB/s) 154.83353321964444\r\nlog_normal, size, elements 2097152 forward 4.64177131652832e-05 bandwidth (GB/s) 180.71997580169503\r\nlog_normal, size, elements 4194304 forward 8.719682693481446e-05 bandwidth (GB/s) 192.40626740399748\r\nlog_normal, size, elements 8388608 forward 0.0001693272590637207 bandwidth (GB/s) 198.16320293339717\r\nlog_normal, size, elements 16777216 forward 0.00033437252044677735 bandwidth (GB/s) 200.70089464986953\r\nlog_normal, size, elements 33554432 forward 0.0006206154823303223 bandwidth (GB/s) 216.26551676737367\r\n```"}
{"number": 20625, "title": "Move THCTensor_(geometric) to ATen", "time": "2019-05-17T04:07:34Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* #20886 Remove curandStateMTGP32 usage\r\n* #20626 Speedup bernoulli_scalar_cuda_kernel with grid-stride loop\r\n* **#20625 Move THCTensor_(geometric) to ATen**\r\n* #20624 Move THCTensor_(lognormal) to ATen\r\n* #20623 Move THCTensor_(exponential) to ATen\r\n* #20622 Move THCTensor_(cauchy) to ATen\r\n* #20621 Move THCTensor_{normal, normal_means, normal_stddevs, normal_means_stddevs} to ATen\r\n* #20620 Move THCTensor_{random, clampedRandom, cappedRandom} to ATen\r\n\r\nDifferential Revision: [D15454049](https://our.internmc.facebook.com/intern/diff/D15454049)\r\n\r\n## Effective Bandwidth Benchmark\r\n- using https://gist.github.com/syed-ahmed/f8b7384d642f4bce484228b508b4bc68\r\n- on V100\r\n### Float Type\r\n#### Before:\r\n```\r\ngeometric, size, elements 65536 forward 4.827976226806641e-06 bandwidth (GB/s) 54.296870507456795\r\ngeometric, size, elements 131072 forward 5.9986114501953125e-06 bandwidth (GB/s) 87.40156023656598\r\ngeometric, size, elements 262144 forward 9.603500366210938e-06 bandwidth (GB/s) 109.18685479404171\r\ngeometric, size, elements 524288 forward 1.6007423400878906e-05 bandwidth (GB/s) 131.01121570163838\r\ngeometric, size, elements 1048576 forward 2.911090850830078e-05 bandwidth (GB/s) 144.08014778391484\r\ngeometric, size, elements 2097152 forward 5.525588989257812e-05 bandwidth (GB/s) 151.81382502947878\r\ngeometric, size, elements 4194304 forward 0.00010294198989868164 bandwidth (GB/s) 162.9773818877273\r\ngeometric, size, elements 8388608 forward 0.0001985597610473633 bandwidth (GB/s) 168.98908330170744\r\ngeometric, size, elements 16777216 forward 0.00038609743118286135 bandwidth (GB/s) 173.8132879941806\r\ngeometric, size, elements 33554432 forward 0.0007671475410461426 bandwidth (GB/s) 174.9568639912085\r\n```\r\n#### After:\r\n```\r\ngeometric, size, elements 65536 forward 5.98907470703125e-06 bandwidth (GB/s) 43.7703673477707\r\ngeometric, size, elements 131072 forward 5.676746368408203e-06 bandwidth (GB/s) 92.3571295905922\r\ngeometric, size, elements 262144 forward 6.127357482910156e-06 bandwidth (GB/s) 171.13021443984437\r\ngeometric, size, elements 524288 forward 7.076263427734375e-06 bandwidth (GB/s) 296.3643201552561\r\ngeometric, size, elements 1048576 forward 1.0535717010498046e-05 bandwidth (GB/s) 398.1033275495814\r\ngeometric, size, elements 2097152 forward 1.7604827880859376e-05 bandwidth (GB/s) 476.49474659848323\r\ngeometric, size, elements 4194304 forward 2.9888153076171875e-05 bandwidth (GB/s) 561.333313478494\r\ngeometric, size, elements 8388608 forward 5.422115325927734e-05 bandwidth (GB/s) 618.8439378916895\r\ngeometric, size, elements 16777216 forward 0.00010248422622680665 bandwidth (GB/s) 654.8213951626288\r\ngeometric, size, elements 33554432 forward 0.00019872665405273437 bandwidth (GB/s) 675.388657046396\r\n```\r\n### Double Type\r\n#### Before:\r\n```\r\ngeometric, size, elements 65536 forward 7.531642913818359e-06 bandwidth (GB/s) 34.80568622272872\r\ngeometric, size, elements 131072 forward 7.486343383789062e-06 bandwidth (GB/s) 70.03258775643313\r\ngeometric, size, elements 262144 forward 1.2500286102294922e-05 bandwidth (GB/s) 83.8841600439443\r\ngeometric, size, elements 524288 forward 2.1970272064208986e-05 bandwidth (GB/s) 95.45407511891482\r\ngeometric, size, elements 1048576 forward 4.1151046752929686e-05 bandwidth (GB/s) 101.9246004890846\r\ngeometric, size, elements 2097152 forward 7.607698440551757e-05 bandwidth (GB/s) 110.26472809812907\r\ngeometric, size, elements 4194304 forward 0.00013311147689819335 bandwidth (GB/s) 126.03883895625013\r\ngeometric, size, elements 8388608 forward 0.00026131629943847655 bandwidth (GB/s) 128.40543078293493\r\ngeometric, size, elements 16777216 forward 0.0005186843872070312 bandwidth (GB/s) 129.38284948456277\r\ngeometric, size, elements 33554432 forward 0.0010293865203857423 bandwidth (GB/s) 130.38613323759532\r\n```\r\n#### After:\r\n```\r\ngeometric, size, elements 65536 forward 6.048679351806641e-06 bandwidth (GB/s) 43.33904721229799\r\ngeometric, size, elements 131072 forward 7.328987121582031e-06 bandwidth (GB/s) 71.5362152098894\r\ngeometric, size, elements 262144 forward 1.009225845336914e-05 bandwidth (GB/s) 103.89904349407041\r\ngeometric, size, elements 524288 forward 1.6951560974121092e-05 bandwidth (GB/s) 123.71438849800283\r\ngeometric, size, elements 1048576 forward 3.087997436523438e-05 bandwidth (GB/s) 135.82601949054973\r\ngeometric, size, elements 2097152 forward 5.675792694091797e-05 bandwidth (GB/s) 147.7962366161136\r\ngeometric, size, elements 4194304 forward 0.00010924100875854492 bandwidth (GB/s) 153.57983408119776\r\ngeometric, size, elements 8388608 forward 0.0002037382125854492 bandwidth (GB/s) 164.69385675957594\r\ngeometric, size, elements 16777216 forward 0.0003897523880004883 bandwidth (GB/s) 172.18332989384\r\ngeometric, size, elements 33554432 forward 0.0007770538330078125 bandwidth (GB/s) 172.72642164375063\r\n```"}
{"number": 20626, "title": "Speedup bernoulli_scalar_cuda_kernel with grid-stride loop", "time": "2019-05-17T04:07:46Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* #20886 Remove curandStateMTGP32 usage\r\n* **#20626 Speedup bernoulli_scalar_cuda_kernel with grid-stride loop**\r\n* #20625 Move THCTensor_(geometric) to ATen\r\n* #20624 Move THCTensor_(lognormal) to ATen\r\n* #20623 Move THCTensor_(exponential) to ATen\r\n* #20622 Move THCTensor_(cauchy) to ATen\r\n* #20621 Move THCTensor_{normal, normal_means, normal_stddevs, normal_means_stddevs} to ATen\r\n* #20620 Move THCTensor_{random, clampedRandom, cappedRandom} to ATen\r\n\r\nDifferential Revision: [D15454046](https://our.internmc.facebook.com/intern/diff/D15454046)\r\n\r\n\r\n## Effective Bandwidth Benchmark\r\n- using https://gist.github.com/syed-ahmed/f8b7384d642f4bce484228b508b4bc68\r\n- on V100\r\n### Float Type\r\n#### Before:\r\n```\r\nbernoulli, size, elements 65536 forward 5.810260772705078e-06 bandwidth (GB/s) 45.117424200902754\r\nbernoulli, size, elements 131072 forward 5.700588226318359e-06 bandwidth (GB/s) 91.97085970522794\r\nbernoulli, size, elements 262144 forward 7.650852203369141e-06 bandwidth (GB/s) 137.0534905298847\r\nbernoulli, size, elements 524288 forward 1.1038780212402343e-05 bandwidth (GB/s) 189.98041084682507\r\nbernoulli, size, elements 1048576 forward 1.817464828491211e-05 bandwidth (GB/s) 230.77772588765578\r\nbernoulli, size, elements 2097152 forward 3.152847290039063e-05 bandwidth (GB/s) 266.06451972800966\r\nbernoulli, size, elements 4194304 forward 5.8722496032714846e-05 bandwidth (GB/s) 285.7033868358262\r\nbernoulli, size, elements 8388608 forward 0.0001120924949645996 bandwidth (GB/s) 299.3459286511284\r\nbernoulli, size, elements 16777216 forward 0.0002196049690246582 bandwidth (GB/s) 305.58900510336235\r\nbernoulli, size, elements 33554432 forward 0.0004137754440307617 bandwidth (GB/s) 324.3733525907877\r\n```\r\n#### After:\r\n```\r\nbernoulli, size, elements 65536 forward 5.7387351989746094e-06 bandwidth (GB/s) 45.679751881013715\r\nbernoulli, size, elements 131072 forward 5.600452423095703e-06 bandwidth (GB/s) 93.61529397837378\r\nbernoulli, size, elements 262144 forward 6.201267242431641e-06 bandwidth (GB/s) 169.09060019623223\r\nbernoulli, size, elements 524288 forward 6.272792816162109e-06 bandwidth (GB/s) 334.3250863629039\r\nbernoulli, size, elements 1048576 forward 8.275508880615235e-06 bandwidth (GB/s) 506.83336342310577\r\nbernoulli, size, elements 2097152 forward 1.2857913970947266e-05 bandwidth (GB/s) 652.4081603714445\r\nbernoulli, size, elements 4194304 forward 2.348184585571289e-05 bandwidth (GB/s) 714.4760298270282\r\nbernoulli, size, elements 8388608 forward 4.356622695922851e-05 bandwidth (GB/s) 770.1936647257047\r\nbernoulli, size, elements 16777216 forward 8.656024932861328e-05 bandwidth (GB/s) 775.2850126994326\r\nbernoulli, size, elements 33554432 forward 0.0001675891876220703 bandwidth (GB/s) 800.8734328534002\r\n```\r\n### Double Type\r\n#### Before:\r\n```\r\nbernoulli, size, elements 65536 forward 5.733966827392578e-06 bandwidth (GB/s) 45.717739200665285\r\nbernoulli, size, elements 131072 forward 6.6208839416503905e-06 bandwidth (GB/s) 79.18700956254952\r\nbernoulli, size, elements 262144 forward 1.0859966278076171e-05 bandwidth (GB/s) 96.55425929975851\r\nbernoulli, size, elements 524288 forward 1.7333030700683594e-05 bandwidth (GB/s) 120.99165092445668\r\nbernoulli, size, elements 1048576 forward 3.1557083129882816e-05 bandwidth (GB/s) 132.91165038090057\r\nbernoulli, size, elements 2097152 forward 5.902767181396485e-05 bandwidth (GB/s) 142.11314358523305\r\nbernoulli, size, elements 4194304 forward 0.00011337995529174805 bandwidth (GB/s) 147.9733869785806\r\nbernoulli, size, elements 8388608 forward 0.00022054195404052734 bandwidth (GB/s) 152.14534643070206\r\nbernoulli, size, elements 16777216 forward 0.0004380941390991211 bandwidth (GB/s) 153.18366079491483\r\nbernoulli, size, elements 33554432 forward 0.0008704972267150879 bandwidth (GB/s) 154.1851299245198\r\n```\r\n#### After:\r\n```\r\nbernoulli, size, elements 65536 forward 5.877017974853515e-06 bandwidth (GB/s) 44.60493418969575\r\nbernoulli, size, elements 131072 forward 5.819797515869141e-06 bandwidth (GB/s) 90.08698302138468\r\nbernoulli, size, elements 262144 forward 6.091594696044922e-06 bandwidth (GB/s) 172.1348928025049\r\nbernoulli, size, elements 524288 forward 8.232593536376953e-06 bandwidth (GB/s) 254.73770698546193\r\nbernoulli, size, elements 1048576 forward 1.3000965118408203e-05 bandwidth (GB/s) 322.6148183461581\r\nbernoulli, size, elements 2097152 forward 2.2871494293212892e-05 bandwidth (GB/s) 366.7713133413114\r\nbernoulli, size, elements 4194304 forward 4.316329956054687e-05 bandwidth (GB/s) 388.69169342501107\r\nbernoulli, size, elements 8388608 forward 8.46099853515625e-05 bandwidth (GB/s) 396.5776835981966\r\nbernoulli, size, elements 16777216 forward 0.00016601085662841796 bandwidth (GB/s) 404.2438269577137\r\nbernoulli, size, elements 33554432 forward 0.00031869888305664063 bandwidth (GB/s) 421.14276244936264\r\n```"}
{"number": 20627, "title": "How to transfer tf.layers.dense to pytorch?", "time": "2019-05-17T04:40:19Z", "body": "How to transfer tf.layers.dense to pytorch?\r\n\r\n~~~\r\ntf.layers.dense(post_outputs, hp.num_freq)\r\n~~~"}
{"number": 20628, "title": "tf.layers.dense", "time": "2019-05-17T04:41:54Z", "body": "## ‚ùì Questions and Help\r\n\r\n### Please note that this issue tracker is not a help form and this issue will be closed.\r\n\r\nWe have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:\r\n\r\n- [Discussion Forum](https://discuss.pytorch.org/)\r\n"}
{"number": 20629, "title": "profiler seems not print all op calls", "time": "2019-05-17T06:45:13Z", "body": "## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nHello everyone, I use pytorch profiler to get my program performance, but seems profiler not print all op function calls. \r\nMy program has 53 conv2d, but profiler only print 6 conv2d calls, can anyone help on that? Thanks!\r\n![image](https://user-images.githubusercontent.com/9839536/57908296-4645c480-78b2-11e9-8a57-24c5e771c60b.png)\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n                 with torch.autograd.profiler.profile(use_cuda=True) as prof:\r\n                    outputs = model(inputs)\r\n                    _, preds = torch.max(outputs.data, 1)\r\n                    loss = criterion(outputs, labels)\r\n                 print(prof)\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0):1.1.0a0+e8fb5f3\r\n - OS (e.g., Linux):Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): source\r\n - Build command you used (if compiling from source): python setup.py install\r\n - Python version: 3.5\r\n - CUDA/cuDNN version: 9.2\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen"}
{"number": 20630, "title": "distributed all_reduce deadlocks in v1.1", "time": "2019-05-17T06:46:49Z", "body": "## üêõ Bug\r\n\r\nI'm doing multi-node training (8 nodes, 8 gpu's each, NCCL backend) and am using `DistributedDataParallel` for syncing grads and `distributed.all_reduce()` calls to log losses. I recently upgraded from Pytorch v1.0 to v1.1 and after doing so, my training script hangs at a `distributed.all_reduce()` call. The hang doesn't occur if I downgrade pytorch to v1.0. It also hangs if I use the pytorch-nightly version.\r\nSome observations about the deadlock that might be useful:\r\n\r\n1. The script deadlocks exactly after the same number of training iterations (7699). Changing the model architecture changed this number, but it's still the same for different runs of the same architecture.\r\n\r\n2. In all the runs, the hang occurs at an all_reduce of a single element gpu tensor created as follows:\r\n```\r\nloss = loss.item()\r\nsum = torch.tensor(loss * batch).float().cuda() //as batch could be different on each rank\r\ndistributed.all_reduce(sum)\r\nsum = sum.item()\r\n```\r\nAll ranks complete line 3, but only ranks = {0,8,16,...,56} ie ranks with local_rank=0 complete line 4. I checked if different processes are accessing the same GPU, however each process is using it's corresponding gpu (set using `torch.cuda.set_device(local_rank)`, though all GPU's are visible to each process). I tried adding a `dist.barrier()` after the `all_reduce()`, or using the same tensor for sum always (instead of creating a new one), however it still hangs, and always after the same number of iterations (7699). \r\n3. The reduction within the DDP itself hasn't hung in any of my runs yet. I tried setting `find_unused_parameters=True`, however that didn't help.\r\n4. I looked at NCCL debug logs using `NCCL_DEBUG_SUBSYS=COLL NCCL_DEBUG=INFO`, and no errors were thrown. The last calls corresponded to all reduce calls for a 1 element tensor. It's fewer calls than the previous iteration though. \r\n## To Reproduce\r\n\r\nIt takes about 7 hrs for the deadlock to happen, and it's hard for me to share the code. I'll try see if I can come up with a simpler script that reproduces the deadlock.\r\n\r\n## Environment\r\n```\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.11.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\n\r\nGPU models and configuration:                                                                                                          GPU 0: Tesla V100-SXM2-16GB                                                                                                            GPU 1: Tesla V100-SXM2-16GB\r\nGPU 2: Tesla V100-SXM2-16GB\r\nGPU 3: Tesla V100-SXM2-16GB\r\nGPU 4: Tesla V100-SXM2-16GB\r\nGPU 5: Tesla V100-SXM2-16GB\r\nGPU 6: Tesla V100-SXM2-16GB\r\nGPU 7: Tesla V100-SXM2-16GB\r\n\r\nNvidia driver version: 410.79\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.4\r\n\r\nVersions of relevant libraries:                                                                                                        [pip3] numpy==1.16.3                                                                                                                   [pip3] tcop-pytorch==0.0.0\r\n[pip3] torch==1.1.0\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.3                      199\r\n[conda] mkl_fft                   1.0.12           py36ha843d7b_0\r\n[conda] mkl_random                1.0.2            py36hd81dba3_0\r\n[conda] pytorch                   1.1.0           py3.6_cuda10.0.130_cudnn7.5.1_0    pytorch                                           [conda] tcop-pytorch              0.0.0                    pypi_0    pypi\r\n[conda] torchvision               0.2.2.post3              pypi_0    pypi\r\n```"}
{"number": 20631, "title": "Error: Could not find a version that satisfies the requirement torch==0.3.1 (from versions: 0.1.2, 0.1.2.post1) ERROR: No matching distribution found for torch==0.3.1", "time": "2019-05-17T07:09:37Z", "body": "I am trying to install torch=0.3.1\r\n\r\n>pip install torch==0.3.1\r\nCollecting torch==0.3.1\r\n  ERROR: Could not find a version that satisfies the requirement torch==0.3.1 (from versions: 0.1.2, 0.1.2.post1)\r\nERROR: No matching distribution found for torch==0.3.1\r\n\r\nI tried all the available answers to this questions but still i am unable to install it.\r\nI am not getting what \"**matching environment** and **what version** means.\r\nI am using python=3.6"}
{"number": 20632, "title": "Minor typo in wiki page", "time": "2019-05-17T07:38:46Z", "body": "## üìö Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nI don't seem to be able to edit directly, so I'm filing an issue:\r\n\r\nhttps://github.com/pytorch/pytorch/wiki/Writing-Python-in-cpp-(a-manifesto)\r\n\r\ns/solves are problem/solves our problem/"}
{"number": 20633, "title": "Split out gpu/cpu targets based on gpu_library_targets", "time": "2019-05-17T07:43:26Z", "body": "Summary: Merge the c2_gpu and is_amd_build logic in targets files.\n\nReviewed By: xw285cornell\n\nDifferential Revision: D15176621\n\n"}
{"number": 20634, "title": "SyncBN doesn't support fp16 inputs with fp32 parameters", "time": "2019-05-17T08:15:32Z", "body": "## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Convert network to 16 bits while keeping SyncBN in 32 bits\r\n2. Get `RuntimeError: expected scalar type Half but found Float` error\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\nBatchNorm works with different types of input and parameters.\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\nCollecting environment information...\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.105\r\nGPU models and configuration: \r\nGPU 0: GeForce RTX 2080 Ti\r\nGPU 1: GeForce RTX 2080 Ti\r\nGPU 2: GeForce RTX 2080 Ti\r\nGPU 3: GeForce RTX 2080 Ti\r\nGPU 4: GeForce RTX 2080 Ti\r\nGPU 5: GeForce RTX 2080 Ti\r\nGPU 6: GeForce RTX 2080 Ti\r\nGPU 7: GeForce RTX 2080 Ti\r\n\r\nNvidia driver version: 418.56\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.5.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.3\r\n[pip3] torch==1.1.0\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] Could not collect\r\n\n\ncc @csarofeen @ptrblck"}
{"number": 20635, "title": "CUDA error: unknown error on Windows", "time": "2019-05-17T09:25:40Z", "body": "## üêõ Bug\r\n\r\n`CUDA error: unknown error` after trying to convert a `torch.float32` tensor to a `torch.cuda.FloatTensor` using `x.type(torch.cuda.FloatTensor)`\r\n\r\n## Call stack: \r\n![Capture](https://user-images.githubusercontent.com/15847892/57917902-feb13f80-7895-11e9-8f54-bfd07ac3a832.PNG)\r\n\r\nVariables:\r\n- item2 tensor: https://user-images.githubusercontent.com/15847892/57917623-631fcf00-7895-11e9-9e66-726352e9baaa.PNG\r\n\r\nCode:\r\n![Capture](https://user-images.githubusercontent.com/15847892/57918096-636c9a00-7896-11e9-80e1-f62e13bb7cd8.PNG)\r\n\r\n\r\n## To Reproduce\r\n\r\nHard to reproduce, error only appears using a specific libray, if I try to reproduce the error in the REPL it works:\r\n\r\n```\r\nC:\\Users\\Jonas>python\r\nPython 3.6.7 (v3.6.7:6ec5cf24b7, Oct 20 2018, 13:35:33) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import torch\r\n>>> torch.__version__\r\n'1.1.0.dev20190516'\r\n>>> a = torch.randn(1)\r\n>>> a\r\ntensor([-1.1125])\r\n>>> a.dtype\r\ntorch.float32\r\n>>> a.type(torch.cuda.FloatTensor)\r\ntensor([-1.1125], device='cuda:0')\r\n>>>\r\n```\r\n\r\n## Environment\r\n - PyTorch Version (e.g., 1.0): '1.1.0.dev20190516'\r\n - OS (e.g., Linux): windows 10 17763\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.6.7\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: 940mx\r\n"}
{"number": 20636, "title": "Add Tensor.ndim property for the benefit of matplotlib", "time": "2019-05-17T10:12:11Z", "body": "`matplotlib.pyplot` almost likes our tensors thanks to the `__array__`, but it also uses the `ndim` property.\r\n\r\nExample:\r\n\r\n```\r\nimport torch\r\nfrom matplotlib import pyplot\r\na = torch.randn(5)\r\npyplot.plot(a)\r\npyplot.show()\r\n```\r\n\r\nCredit: I saw that fastai monkeypatches Tensor.\r\n"}
{"number": 20637, "title": "Forward hook, Activation holder", "time": "2019-05-17T10:50:43Z", "body": "## üöÄ Feature\r\n\r\nAdd `nn.Module.activation` field to hold the last activation (or tuple, etc) returned from forward().\r\n\r\n```py\r\nwith torch.capture_activations():\r\n    x = net(imgs)\r\n\r\n    # last activation from net.conv layer\r\n    loss = cross_entropy(x,y) + L2(net.conv.activation) \r\n\r\n    for (name, activation) in net.named_activations():\r\n        writer.add_histogram('activation_'+name, activation)\r\n\r\n# net.conv.activation is released\r\n\r\n# capture only desired activations to save memory\r\nwith torch.capture_activations(pattern='net/batch_norm_*'):  \r\n    ....\r\n```\r\n\r\n## Motivation\r\n\r\n- Visualizing activation histogram in tensorboard is not easy.\r\n- Capturing intermediate activations from a network for regularization loss is not easy.\r\n\r\n## Pitch\r\n\r\nWeight tensors can be enumerated and put into tensorboard like the following:\r\n\r\n```py\r\nfor (name, param) in model.named_parameters():\r\n    writer.add_histogram('activation_' + name, param, global_step=global_step)\r\n```\r\n\r\nHowever it is not easy to enumerate activations. One needs to insert `writer.add_histogram` in every `forward()` function. For example, capturing intermediate activations of `nn.Sequential` is impossible, and capturing from nn.Module is cumborsome.\r\n\r\n```py\r\ndef forward(self, x):\r\n    self.memory = []\r\n    x = self.layer1(x)\r\n    self.memory.append(x)\r\n    x = self.layer2(x)\r\n    self.memory.append(x)\r\n    ...\r\n    return x\r\n```\r\n\r\n## Alternatives\r\n\r\nAdd a forward hook with pattern filter. It does not hold the tensor and saves memory for some cases.\r\n"}
{"number": 20638, "title": "Please add support for python primitive type", "time": "2019-05-17T11:00:49Z", "body": "e.g. `torch.sin(1)` or `torch.sin([1,2,3])`\r\n\r\nNumpy supports this feature, which makes programming very convenient.\r\n\r\nThx!"}
{"number": 20639, "title": "The item() operation changes the value?Is this a bug?", "time": "2019-05-17T11:12:05Z", "body": "## ‚ùì Questions and Help\r\n\r\n### Please note that this issue tracker is not a help form and this issue will be closed.\r\n\r\nWe have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:\r\n\r\n- [Discussion Forum](https://discuss.pytorch.org/)\r\n"}
{"number": 20640, "title": "Error while installing PyTorch ", "time": "2019-05-17T11:46:08Z", "body": "This is the stack trace generated \r\n\r\n```\r\nCopying extension caffe2.python.caffe2_pybind11_state\r\nCopying caffe2.python.caffe2_pybind11_state from torch/lib/python3.6/site-packages/caffe2/python/caffe2_pybind11_state.cpython-36m-darwin.so to /Users/hrishikesh/Hrishikesh/Projects/py/pytorch/build/lib.macosx-10.7-x86_64-3.6/caffe2/python/caffe2_pybind11_state.cpython-36m-darwin.so\r\ncopying torch/lib/python3.6/site-packages/caffe2/python/caffe2_pybind11_state.cpython-36m-darwin.so -> /Users/hrishikesh/Hrishikesh/Projects/py/pytorch/build/lib.macosx-10.7-x86_64-3.6/caffe2/python\r\nbuilding 'torch._C' extension\r\ngcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/hrishikesh/anaconda3/envs/pytorch/include -arch x86_64 -I/Users/hrishikesh/anaconda3/envs/pytorch/include -arch x86_64 -I/Users/hrishikesh/anaconda3/envs/pytorch/include/python3.6m -c torch/csrc/stub.cpp -o build/temp.macosx-10.7-x86_64-3.6/torch/csrc/stub.o -std=c++11 -Wall -Wextra -Wno-strict-overflow -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-unknown-pragmas -Wno-deprecated-declarations -fno-strict-aliasing -Wno-missing-braces\r\ng++ -bundle -undefined dynamic_lookup -L/Users/hrishikesh/anaconda3/envs/pytorch/lib -arch x86_64 -L/Users/hrishikesh/anaconda3/envs/pytorch/lib -arch x86_64 -arch x86_64 build/temp.macosx-10.7-x86_64-3.6/torch/csrc/stub.o -L/Users/hrishikesh/Hrishikesh/Projects/py/pytorch/torch/lib -L/Users/hrishikesh/anaconda3/envs/pytorch/lib -lshm -ltorch_python -o build/lib.macosx-10.7-x86_64-3.6/torch/_C.cpython-36m-darwin.so -Wl,-rpath,@loader_path/lib\r\nclang: warning: libstdc++ is deprecated; move to libc++ with a minimum deployment target of OS X 10.9 [-Wdeprecated]\r\nld: library not found for -lstdc++\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nerror: command 'g++' failed with exit status 1\r\n\r\n```"}
{"number": 20641, "title": "Main page broadcasting (?) example image bug", "time": "2019-05-17T11:57:15Z", "body": "## üêõ Bug\r\n\r\nThere is an image on the main page of this repo:\r\n![image](https://user-images.githubusercontent.com/5480856/57926458-fc5dde00-78b3-11e9-819c-96119d99026e.png)\r\n\r\nLooks like the numbers on the result tensor are wrong (and pretty unreadable btw).\r\n"}
{"number": 20642, "title": "Build error with MSVC (aten\\src\\ATen\\native\\quantized\\Copy.cpp)", "time": "2019-05-17T12:32:27Z", "body": "## üêõ Bug\r\n\r\n```\r\n..\\aten\\src\\ATen\\native\\quantized\\Copy.cpp(18): error C2065: 'src_data': undeclared identifier\r\n..\\aten\\src\\ATen\\native\\quantized\\Copy.cpp(18): error C2672: 'at::quantize_val': no matching overloaded function found\r\n..\\aten\\src\\ATen\\native\\quantized\\Copy.cpp(26): error C2780: 'T at::quantize_val(float,int32_t,float)': expects 3 arguments - 2 provided\r\n```\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```\r\nset CMAKE_GENERATOR=\"Visual Studio 16 2019\" x64\r\n\r\nset DISTUTILS_USE_SDK=1\r\n\r\n\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64\r\n\r\nset CMAKE_GENERATOR=Ninja\r\n\r\nset REL_WITH_DEB_INFO=1\r\n\r\npython setup.py install\r\n```\r\n"}
{"number": 20643, "title": "RuntimeError: ONNX export failed: Couldn't export operator aten::softmax", "time": "2019-05-17T12:42:14Z", "body": "## ‚ùì Questions and Help\r\n\r\n### Please note that this issue tracker is not a help form and this issue will be closed.\r\nCurrently I am facing the errorRuntimeError: ONNX export failed: Couldn't export operator aten::softmax  According to the folks at ONNX, this is a bug in PyTorch\r\n"}
{"number": 20644, "title": "[jit] support ModuleList.append inside ScriptModule constructor", "time": "2019-05-17T13:21:29Z", "body": "## üêõ Bug\r\n\r\nCurrently one can't append modules to a `ModuleList` inside the constructor as the `ModuleList` is converted to a `_ConstModuleList`.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\n```python\r\nclass MultiBox(torch.jit.ScriptModule):\r\n\r\n    __constants__ = ['loc_layers']\r\n\r\n    def __init__(self):\r\n        super(MultiBox, self).__init__()\r\n\r\n        self.loc_layers = nn.ModuleList()\r\n        for i in range(4):\r\n            self.loc_layers.append(nn.Conv2d(64, 4, kernel_size=1))\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, x):\r\n        return x\r\n```\r\n\r\nResults in this error:\r\n```\r\n  File \"C:\\Python36\\lib\\site-packages\\torch\\jit\\__init__.py\", line 1232, in __getattr__\r\n    return Module.__getattr__(self, attr)\r\n  File \"C:\\Python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 539, in __getattr__\r\n    type(self).__name__, name))\r\nAttributeError: '_ConstModuleList' object has no attribute 'append'\r\n```\r\n\r\n## Expected behavior\r\n\r\nThis should be possible inside the constructor.\r\n\r\n## Environment\r\n - PyTorch Version: 1.1.0\r\n\r\n\r\n## Additional context\r\n\r\nA workaround for this problem is the following code:\r\n\r\n```python\r\nclass MultiBox2(torch.jit.ScriptModule):\r\n\r\n    __constants__ = ['loc_layers']\r\n\r\n    def __init__(self):\r\n        super(MultiBox2, self).__init__()\r\n\r\n        loc_layers = nn.ModuleList()\r\n        for i in range(4):\r\n            loc_layers.append(nn.Conv2d(64, 4, kernel_size=1))\r\n\r\n        self.loc_layers = loc_layers\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, x):\r\n        return x\r\n```\r\n\r\n"}
{"number": 20645, "title": "Fix missing env for update_binary_size job", "time": "2019-05-17T16:29:22Z", "body": ""}
{"number": 20646, "title": "Remove unnecessary format literals from error message.", "time": "2019-05-17T16:49:05Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20646 Remove unnecessary format literals from error message.**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15394795/)\n\n\n\nDifferential Revision: [D15394795](https://our.internmc.facebook.com/intern/diff/D15394795/)"}
{"number": 20647, "title": "Renaming the relu kernel and adding hypothesis tests", "time": "2019-05-17T16:51:21Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20474 Quantized Max Pool op&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15327923/)\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20647 Renaming the relu kernel and adding hypothesis tests**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15332106/)\n\nThe initial assumption was that `qint8` would be unsigned. After introduction of `quint8` and `qint8`, some tests break.\n\nDifferential Revision: [D15332106](https://our.internmc.facebook.com/intern/diff/D15332106/)"}
{"number": 20648, "title": "Fix optimizer type hint", "time": "2019-05-17T17:31:19Z", "body": "Fixes https://github.com/pytorch/pytorch/issues/20548"}
{"number": 20649, "title": "Audit AT_ASSERT sites in TensorImpl.h; doc improvements", "time": "2019-05-17T17:34:52Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20649 Audit AT_ASSERT sites in TensorImpl.h; doc improvements**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15395576/)\n\nI went through every occurrence of AT_ASSERT in this file and\nthought about whether or not it should be TORCH_INTERNAL_ASSERT\nor TORCH_CHECK.  I think I did a good job at it.  Some thoughts:\n\n- In order to decide if a check is \"internal\" or not, we must\n  think about where the separation between userspace and our internals\n  are.  I think any code that utilizes the PyTorch or Caffe2 C++ frontends\n  count as userspace.  An important collorary is that the majority of operator\n  code \"counts\" as userspace, even though it lives in our repository.  This\n  is inline with TCB (trusted computing base) thinking: you want the TCB to\n  be as small as possible, and because we have a *lot* of operator\n  implementations, they should not count as TCB.\n\n- The primary test I applied when considering an AT_ASSERT was whether or\n  not I could trigger this error by just making method calls on caffe2::Tensor\n  or at::Tensor.  If I could, that made it a TORCH_CHECK.  This covers most\n  of the misapplications of TORCH_INTERNAL_ASSERT.  One place I didn't\n  do this was the \"is variable\" checks; I think you have to work a bit\n  harder to trigger this case, and userspace code is not mixing up\n  Variables and Tensros.\n\n- I updated the docs for device_opt_, explaining when it could be nullopt.\n  (The nullopt checks here are TORCH_CHECK, because you can trigger them\n  by taking an undefined tensor and poking the methods.)\n\nDifferential Revision: [D15395576](https://our.internmc.facebook.com/intern/diff/D15395576/)"}
{"number": 20650, "title": "Added some extra tests for std_mean and var_mean for multiple dims.", "time": "2019-05-17T17:52:22Z", "body": "Added some extra tests for std_mean and var_mean for multiple dims.\r\nSome refactoring of previously created tests based on PR comments: https://github.com/pytorch/pytorch/pull/18731\r\n\r\n"}
{"number": 20651, "title": "torch.distributed.reduce empty tensor bug", "time": "2019-05-17T18:14:45Z", "body": "## üêõ Bug\r\n\r\nFunction call:\r\n```torch.distributed.reduce(packed, dst=0)```\r\n\r\nInputs:\r\n```packed = torch.cuda.FloatTensor([])```\r\n\r\nTrace:\r\n```\r\n File \"/home/xxxx/src/utils.py\", line 875, in log_step_end\r\n    torch.distributed.reduce(packed, dst=src.distributed.get_master_rank())\r\n  File \"/home/xxxx/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py\", line 1002, in reduce\r\n    work = _default_pg.reduce([tensor], opts)\r\nRuntimeError: invalid device pointer: %p0 (recordStream at /pytorch/c10/cuda/CUDACachingAllocator.cpp:384)\r\nframe #0: std::function<std::string ()>::operator()() const + 0x11 (0x7f6d5d33d441 in /home/michaelp/.local/lib/python3.6/site-packages/torch/lib/libc10.so)\r\nframe #1: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x2a (0x7f6d5d33cd7a in /home/michaelp/.local/lib/python3.6/site-packages/torch/lib/libc10.so)\r\nframe #2: c10::cuda::CUDACachingAllocator::recordStream(void*, c10::cuda::CUDAStream) + 0x1f5 (0x7f6d5ae272b5 in /home/michaelp/.local/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)\r\nframe #3: <unknown function> + 0x741bf2 (0x7f6d5deafbf2 in /home/michaelp/.local/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\r\nframe #4: c10d::ProcessGroupNCCL::reduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::ReduceOptions const&) + 0x35 (0x7f6d5deb08b5 in /home/michaelp/.local/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\r\nframe #5: <unknown function> + 0x6c327c (0x7f6d5de3127c in /home/michaelp/.local/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\r\nframe #6: <unknown function> + 0x130cfc (0x7f6d5d89ecfc in /home/michaelp/.local/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\r\nframe #7: _PyCFunction_FastCallDict + 0x35c (0x565d5c in /usr/bin/python3)\r\nframe #8: /usr/bin/python3() [0x503073]\r\nframe #9: _PyEval_EvalFrameDefault + 0x449 (0x506859 in /usr/bin/python3)\r\nframe #10: /usr/bin/python3() [0x504c28]\r\nframe #11: /usr/bin/python3() [0x502540]\r\nframe #12: /usr/bin/python3() [0x502f3d]\r\nframe #13: _PyEval_EvalFrameDefault + 0x1231 (0x507641 in /usr/bin/python3)\r\nframe #14: /usr/bin/python3() [0x502209]\r\nframe #15: /usr/bin/python3() [0x502f3d]\r\nframe #16: _PyEval_EvalFrameDefault + 0x449 (0x506859 in /usr/bin/python3)\r\nframe #17: /usr/bin/python3() [0x502209]\r\nframe #18: /usr/bin/python3() [0x502f3d]\r\nframe #19: _PyEval_EvalFrameDefault + 0x449 (0x506859 in /usr/bin/python3)\r\nframe #20: /usr/bin/python3() [0x504c28]\r\nframe #21: /usr/bin/python3() [0x58650d]\r\nframe #22: PyObject_Call + 0x3e (0x59ebbe in /usr/bin/python3)\r\nframe #23: _PyEval_EvalFrameDefault + 0x1807 (0x507c17 in /usr/bin/python3)\r\nframe #24: /usr/bin/python3() [0x504c28]\r\nframe #25: /usr/bin/python3() [0x502540]\r\nframe #26: /usr/bin/python3() [0x502f3d]\r\nframe #27: _PyEval_EvalFrameDefault + 0x1231 (0x507641 in /usr/bin/python3)\r\nframe #28: /usr/bin/python3() [0x504c28]\r\nframe #29: /usr/bin/python3() [0x502540]\r\nframe #30: /usr/bin/python3() [0x502f3d]\r\nframe #31: _PyEval_EvalFrameDefault + 0x449 (0x506859 in /usr/bin/python3)\r\nframe #32: /usr/bin/python3() [0x504c28]\r\nframe #33: /usr/bin/python3() [0x502540]\r\nframe #34: /usr/bin/python3() [0x502f3d]\r\nframe #35: _PyEval_EvalFrameDefault + 0x1231 (0x507641 in /usr/bin/python3)\r\nframe #36: /usr/bin/python3() [0x504c28]\r\nframe #37: /usr/bin/python3() [0x511eca]\r\nframe #38: /usr/bin/python3() [0x502d6f]\r\nframe #39: _PyEval_EvalFrameDefault + 0x449 (0x506859 in /usr/bin/python3)\r\nframe #40: /usr/bin/python3() [0x504c28]\r\nframe #41: /usr/bin/python3() [0x502540]\r\nframe #42: /usr/bin/python3() [0x502f3d]\r\nframe #43: _PyEval_EvalFrameDefault + 0x449 (0x506859 in /usr/bin/python3)\r\nframe #44: /usr/bin/python3() [0x504c28]\r\nframe #45: /usr/bin/python3() [0x58659d]\r\nframe #46: PyObject_Call + 0x3e (0x59ebbe in /usr/bin/python3)\r\nframe #47: /usr/bin/python3() [0x63835b]\r\nframe #48: Py_Main + 0x448 (0x639028 in /usr/bin/python3)\r\nframe #49: main + 0xe0 (0x4a6f10 in /usr/bin/python3)\r\nframe #50: __libc_start_main + 0xe7 (0x7f6d65d39b97 in /lib/x86_64-linux-gnu/libc.so.6)\r\nframe #51: _start + 0x2a (0x5afa0a in /usr/bin/python3)\r\n```\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.1 \r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Python version: 3.6.7\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: 2 Tesla P100"}
{"number": 20652, "title": "Really forcing gcc ABI in devtoolset7 builds", "time": "2019-05-17T18:16:37Z", "body": "Trying one last time with devtoolset7 before trying other compiler toolchains.\r\nAnother attempt at https://github.com/pytorch/pytorch/issues/17492"}
{"number": 20653, "title": "Remove functionality unsupported by the JIT from multi_head_attention_forward.", "time": "2019-05-17T18:33:27Z", "body": "Remove the internal functions in multi_head_attention_forward. Those internal functions cause 10-15% performance regression and there is possibly a JIT issue. "}
{"number": 20654, "title": "make magic methods work with casts too", "time": "2019-05-17T19:08:23Z", "body": "Previous implementation of magic methods extended from BuiltinOperators, but it should be able to work with other sugared values, such as casts.\r\n\r\nI was also considering making CastValue's and BuiltinOperators's extend from a MagicMethod super class, and having them try to call into the super's before their own call. However, not all Builtin Operators have corresponding magic methods so i did it this way instead (although there are workarounds for that). "}
{"number": 20655, "title": "nn.Embedding backwards slow under high row contention", "time": "2019-05-17T19:13:30Z", "body": "## üêõ Bug\r\n\r\nnn.Embedding backward is very slow when there is high contention for a small number of rows in the embedding matrix. This can be ameliorated with different approaches to backward in this case.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.nn import Parameter\r\n\r\nclass ParameterEmb(nn.Module):\r\n    def __init__(self, n, dim):\r\n        super().__init__()\r\n        self.emb = Parameter(torch.zeros(n, dim))\r\n\r\n    def forward(self, input):\r\n        return self.emb.index_select(0, input)\r\n\r\nclass OneHotEmb(nn.Module):\r\n    def __init__(self, n, dim):\r\n        super().__init__()\r\n        self.n = n\r\n        self.emb = nn.Linear(n, dim, bias=False)\r\n\r\n    def forward(self, input):\r\n        onehot = torch.zeros(input.size(0), self.n, device=input.device)\r\n        onehot.scatter_(1, input.unsqueeze(1), 1)\r\n        return self.emb(onehot)\r\n\r\ndef forward_backward(M, x):\r\n    return M(x).sum().backward()\r\n\r\n\r\nN, D = 100, 100\r\nE = nn.Embedding(N, D)\r\nPE = ParameterEmb(N, D)\r\nOHE = OneHotEmb(N, D)\r\n```\r\n\r\nLets first time everything with a uniform distribution over input rows:\r\n```\r\n  x = (torch.arange(10000, dtype=torch.long) % N)\r\n\r\n  %timeit forward_backward(E, x)\r\n  %timeit forward_backward(PE, x)\r\n  %timeit forward_backward(OHE, x)\r\n\r\n  E.cuda()\r\n  PE.cuda()\r\n  OHE.cuda()\r\n  x = x.cuda()\r\n\r\n  %timeit forward_backward(E, x)\r\n  %timeit forward_backward(PE, x)\r\n  %timeit forward_backward(OHE, x)\r\n```\r\nEverything is fine here\r\n```\r\n  CPU Emb     2.87 ms ¬± 53.8 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\r\n  CPU Param   12.8 ms ¬± 496 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\r\n  CPU OneHOt  2.9 ms ¬± 288 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\r\n  GPU Emb     343 ¬µs ¬± 660 ns per loop (mean ¬± std. dev. of 7 runs, 1000 loops each)\r\n  GPU Param   150 ¬µs ¬± 238 ns per loop (mean ¬± std. dev. of 7 runs, 10000 loops each)\r\n  GPU OneHOt  231 ¬µs ¬± 983 ns per loop (mean ¬± std. dev. of 7 runs, 1000 loops each)\r\n```\r\nBut if I have high contention on a single row, Embedding becomes 20x slower while other approaches do not.\r\n\r\n```\r\n  x = torch.zeros(10000, dtype=torch.long)\r\n\r\n\r\n  CPU Emb     31.5 ms ¬± 248 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\r\n  CPU Param   12.8 ms ¬± 66.9 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\r\n  CPU OneHOt  2.52 ms ¬± 95.6 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\r\n  GPU Emb     6.34 ms ¬± 348 ns per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\r\n  GPU Param   513 ¬µs ¬± 198 ns per loop (mean ¬± std. dev. of 7 runs, 1000 loops each)\r\n  GPU OneHOt  205 ¬µs ¬± 500 ns per loop (mean ¬± std. dev. of 7 runs, 1000 loops each)\r\n```\r\n\r\n## Environment\r\n```\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.12.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: Quadro GP100\r\n...\r\n\r\nNvidia driver version: 410.79\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.2\r\n[pip3] torch==1.0.1.post2\r\n[pip3] torchbiggraph==1.dev0\r\n[pip3] torchvision==0.2.1\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.0                      118\r\n[conda] mkl_fft                   1.0.6            py37h7dd41cf_0\r\n[conda] mkl_random                1.0.1            py37h4414c95_1\r\n[conda] torch                     1.0.1.post2              pypi_0    pypi\r\n[conda] torchvision               0.2.1                    py37_1    pytorch\r\n ```"}
{"number": 20656, "title": "[pt1][quant] int_repr for different quantized types", "time": "2019-05-17T19:22:40Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20656 [pt1][quant] int_repr for different quantized types**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15398134/)\n\natt\n\nDifferential Revision: [D15398134](https://our.internmc.facebook.com/intern/diff/D15398134/)"}
{"number": 20657, "title": "Use python type string for user facing error msgs", "time": "2019-05-17T19:46:26Z", "body": "Otherwise users see something like (Tensor, Tensor)? and don't know what the ? means. \r\n\r\nFirst commit is formatting. "}
{"number": 20658, "title": "Make CUDACachingAllocator::recordStream() a no-op on null ptrs", "time": "2019-05-17T20:26:40Z", "body": "Fixes #20651 \r\n\r\nCommunication collectives in `torch.distributed` call `CUDACachingAllocator::recordStream()` on input and output tensors to prevent their memory blocks being freed too early. `CUDACachingAllocator` uses tensor's data pointer to track memory blocks, which does not accept null pointers. However, empty tensor's `storage().data()` might be null. In this case, as there is no associated memory block for the empty tensor, it should be fine to make `recordStream()` a no-op.\r\n\r\nTests only cover `broadcast` empty tensors for GLOO backend, because GLOO does not support empty inputs (facebookincubator/gloo/issues/179). It can be addressed in either `ProcessGroupGloo` or GLOO itself. Will add more tests when that gap is filled. "}
{"number": 20659, "title": "Add get/set_num_interop_threads into torch.h include", "time": "2019-05-17T20:41:56Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* **#20659 Add get/set_num_interop_threads into torch.h include**\r\n\r\nSummary:\r\nAdding these functions into torch namespace\r\n\r\nTest Plan:\r\ntest/cpp/api/torch_include\r\n./build/bin/test_api\r\nReviewers:cpuhrsch, vitalyf, soumith, dzhulgakov\r\n\r\n\r\nDifferential Revision: [D15399780](https://our.internmc.facebook.com/intern/diff/D15399780)"}
{"number": 20660, "title": "[jit] Add support for recursive compilation on Modules", "time": "2019-05-17T21:05:28Z", "body": ""}
{"number": 20661, "title": "[jit] delete brodcasting ops from shape analysis resize aliasing", "time": "2019-05-17T21:17:42Z", "body": "According to https://pytorch.org/docs/stable/notes/broadcasting.html, in-place operations do not allow the in-place tensor to change shape as a result of the broadcast. Therefore our shape analysis could keep the shape information on inputs."}
{"number": 20662, "title": "compile with -fcolor-diagnostics", "time": "2019-05-17T21:48:14Z", "body": "Let there be color!\r\n\r\n"}
{"number": 20663, "title": "torch.cat disconnecting graph ", "time": "2019-05-17T22:04:24Z", "body": "### Bug\r\ntorch.cat appears to disconnect the graph. In effect, graph of each variable is cutoff when concatenated with another variable. \r\n\r\n### To Reproduce\r\n\r\n    import torch\r\n    from torch.autograd import Variable\r\n    \r\n    a = Variable(torch.Tensor([1., 3., 5.]), requires_grad=True)\r\n    b = Variable(torch.Tensor([2., 4., 6.]), requires_grad=True)\r\n    a = a.unsqueeze(1)\r\n    b = b.unsqueeze(1)\r\n    c = torch.matmul(a.t(), b)\r\n\r\n    \r\n    print(torch.autograd.grad(c, a, retain_graph=True)[0]) #returns b\r\n    print(torch.autograd.grad(c, b, retain_graph=True)[0]) #returns a\r\n    z = torch.cat((a,b), dim=1)\r\n    print(torch.autograd.grad(c, z)[0]) \r\n\r\nThe last line of code gives the error: \"RuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.\"\r\n\r\n### Expected Result\r\nPrints a gradient corresponding to the gradient of the individual components\r\n\r\n"}
{"number": 20664, "title": "[JIT] List python builtin has wrong casting behavior", "time": "2019-05-17T22:43:32Z", "body": "## üêõ Bug\r\n\r\nList builtin doesn't preserve the type of its inputs.\r\n\r\n## To Reproduce\r\n```\r\n@torch.jit.script\r\ndef test(x, y):\r\n    return list(x, y)\r\n\r\nprint(test.graph)\r\n```\r\n\r\n```\r\ngraph(%x : Tensor,\r\n      %y : Tensor):\r\n  %2 : int = prim::ImplicitTensorToNum(%x)\r\n  %3 : int = prim::ImplicitTensorToNum(%y)\r\n  %4 : int[] = prim::ListConstruct(%2, %3)\r\n  %5 : int[] = aten::list(%4)\r\n  return (%5)\r\n```\r\n\n\ncc @ezyang @gchanan @zou3519 @suo"}
{"number": 20665, "title": "Add gelu activation in pytorch", "time": "2019-05-17T22:58:29Z", "body": "Summary:\nAdd gelu activation forward on CPU in pytorch\n\nCompare to current python implemented version of gelu in BERT model like\n\n  def gelu(self, x):\n      x * 0.5 * (1.0 + torch.erf(x / self.sqrt_two))\n\nThe torch.gelu function can reduce the forward time from 333ms to 112ms (with MKL) / 133ms (without MKL) for input size = [64, 128, 56, 56] on a devvm.\n\nDifferential Revision: D15400974\n\n"}
{"number": 20666, "title": " [jit] Refactor builtin ops", "time": "2019-05-17T23:04:33Z", "body": "\n\nDifferential Revision: [D15404419](https://our.internmc.facebook.com/intern/diff/15404419/)"}
{"number": 20667, "title": "Remove unused var (ws_) and use vars in undefined case for compile", "time": "2019-05-17T23:05:41Z", "body": "Summary:\nI'm trying to get xplat/caffe2 compiling in ovrsource. I was running into undefined and unused private field compiler warnings and decided it would be easier to fix these then to add Wno equivalents of those flags into the caffe2 buck file.\n\n```\n ‚úò dxy@dxy-mbp ÓÇ∞ ~/ovrsource ÓÇ∞ buck build mode/android32/coretech/mac/dev-release //xplat/caffe2:caffe2\nIn file included from xplat/caffe2/caffe2/utils/signal_handler.cc:1:\nxplat/caffe2/caffe2/utils/signal_handler.h:31:10: error: private field 'SIGINT_action_' is not used [-Werror,-Wunused-private-field]\n  Action SIGINT_action_;\n         ^\nxplat/caffe2/caffe2/utils/signal_handler.h:32:10: error: private field 'SIGHUP_action_' is not used [-Werror,-Wunused-private-field]\n  Action SIGHUP_action_;\n         ^\nxplat/caffe2/caffe2/utils/signal_handler.h:33:17: error: private field 'my_sigint_count_' is not used [-Werror,-Wunused-private-field]\n  unsigned long my_sigint_count_;\n                ^\nxplat/caffe2/caffe2/utils/signal_handler.h:34:17: error: private field 'my_sighup_count_' is not used [-Werror,-Wunused-private-field]\n  unsigned long my_sighup_count_;\n                ^\n4 errors generated.\ncommand exited with code 1: third-party/toolchains/android-ndk/r18b/toolchains/llvm/prebuilt/darwin-x86_64/bin/clang++ --target=armv7-none-linux-android --sysroot third-party/toolchains/android-ndk/r18b/sysroot --gcc-toolchain=third-party/toolchains/android-ndk/r18b/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64 -D__ANDROID_API__=24 @/Users/dxy/ovrsource/buck-out/bin/xplat/caffe2/caffe2#compile-signal_handler.cc.oe8090f90,default/ppandcompile.argsfile -isystem third-party/toolchains/android-ndk/r18b/sysroot/usr/include/arm-linux-androideabi -isystem third-party/toolchains/android-ndk/r18b/sources/cxx-stl/llvm-libc++/include -isystem third-party/toolchains/android-ndk/r18b/sources/cxx-stl/llvm-libc++abi/include -isystem third-party/toolchains/android-ndk/r18b/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/lib/gcc/arm-linux-androideabi\nxplat/caffe2/caffe2/share/fb/stylizer/median_blur_ops.cc:593:14: error: private field 'ws_' is not used [-Werror,-Wunused-private-field]\n  Workspace* ws_;\n             ^\n1 error generated.\ncommand exited with code 1: third-party/toolchains/android-ndk/r18b/toolchains/llvm/prebuilt/darwin-x86_64/bin/clang++ --target=armv7-none-linux-android --sysroot third-party/toolchains/android-ndk/r18b/sysroot --gcc-toolchain=third-party/toolchains/android-ndk/r18b/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64 -D__ANDROID_API__=24 @/Users/dxy/ovrsource/buck-out/bin/xplat/caffe2/caffe2#compile-median_blur_ops.cc.o0cb98141,default/ppandcompile.argsfile -isystem third-party/toolchains/android-ndk/r18b/sysroot/usr/include/arm-linux-androideabi -isystem third-party/toolchains/android-ndk/r18b/sources/cxx-stl/llvm-libc++/include -isystem third-party/toolchains/android-ndk/r18b/sources/cxx-stl/llvm-libc++abi/include -isystem third-party/toolchains/android-ndk/r18b/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/lib/gcc/arm-linux-androideabi\nBuilding: finished in 4.9 sec (100%) 211/210 jobs, 2 updated\n  Total time: 4.9 sec\nMore details at https://our.intern.facebook.com/intern/buck/build/774d0dc6-8e29-437f-97af-31a6f341991a\nCommand failed with exit code 1.\nstderr: In file included from xplat/caffe2/caffe2/utils/signal_handler.cc:1:\nxplat/caffe2/caffe2/utils/signal_handler.h:31:10: error: private field 'SIGINT_action_' is not used [-Werror,-Wunused-private-field]\n  Action SIGINT_action_;\n         ^\nxplat/caffe2/caffe2/utils/signal_handler.h:32:10: error: private field 'SIGHUP_action_' is not used [-Werror,-Wunused-private-field]\n  Action SIGHUP_action_;\n         ^\nxplat/caffe2/caffe2/utils/signal_handler.h:33:17: error: private field 'my_sigint_count_' is not used [-Werror,-Wunused-private-field]\n  unsigned long my_sigint_count_;\n                ^\nxplat/caffe2/caffe2/utils/signal_handler.h:34:17: error: private field 'my_sighup_count_' is not used [-Werror,-Wunused-private-field]\n  unsigned long my_sighup_count_;\n                ^\n4 errors generated.\ncommand exited with code 1: third-party/toolchains/android-ndk/r18b/toolchains/llvm/prebuilt/darwin-x86_64/bin/clang++ --target=armv7-none-linux-android --sysroot third-party/toolchains/android-ndk/r18b/sysroot --gcc-toolchain=third-party/toolchains/android-ndk/r18b/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64 -D__ANDROID_API__=24 @/Users/dxy/ovrsource/buck-out/bin/xplat/caffe2/caffe2#compile-signal_handler.cc.oe8090f90,default/ppandcompile.argsfile -isystem third-party/toolchains/android-ndk/r18b/sysroot/usr/include/arm-linux-androideabi -isystem third-party/toolchains/android-ndk/r18b/sources/cxx-stl/llvm-libc++/include -isystem third-party/toolchains/android-ndk/r18b/sources/cxx-stl/llvm-libc++abi/include -isystem third-party/toolchains/android-ndk/r18b/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/lib/gcc/arm-linux-androideabi\n    When running <c++ preprocess_and_compile>.\n    When building rule //xplat/caffe2:caffe2#compile-signal_handler.cc.oe8090f90,default.\n```\n\nDifferential Revision: D15402928\n\n"}
{"number": 20668, "title": "Replace AT_ASSERT with TORCH_INTERNAL_ASSERT/TORCH_CHECK", "time": "2019-05-17T23:54:00Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20677 Unify the addQuantDequantNode api for inputs and outputs from quant nodes&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15406354/)\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20668 Replace AT_ASSERT with TORCH_INTERNAL_ASSERT/TORCH_CHECK**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15404401/)\n\nAT_ASSERT is deprecated. Replace it wil the new exception handling macro.\n\nDifferential Revision: [D15404401](https://our.internmc.facebook.com/intern/diff/D15404401/)"}
{"number": 20669, "title": "Dict is a reference type", "time": "2019-05-18T00:20:03Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20871 Use c10::List&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15476433/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20870 List&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15476434/)\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20669 Dict is a reference type**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15404911/)\n\nBefore, Dict was a value type, i.e. copying it did a deep copy.\nUnfortunately, this doesn't work well with storing and passing Dicts around in IValues because IValues are reference types.\nThis diff changes Dict to be a reference type.\n\nDifferential Revision: [D15404911](https://our.internmc.facebook.com/intern/diff/D15404911/)"}
{"number": 20670, "title": "[jit] make wildcards alias only each other", "time": "2019-05-18T00:57:00Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20670 [jit] make wildcards alias only each other**\n* #20556 [jit] Mark values entering containers as wildcards\n\nContext found at the bottom of this stack. This PR makes wildcards alias\nonly each other (instead of any value in the graph). Special \"wildcard\"\nmemoryDAG elements are created to model this aliasing relationship.\nGraph inputs also now alias the same wildcard set.\n\nSince the rules for wildcards aren't special anymore, we can eliminate a\nton of special-casing code for wildcards!\n\nDifferential Revision: [D15447567](https://our.internmc.facebook.com/intern/diff/D15447567)"}
{"number": 20671, "title": "Increase static tolerance for negative feature ids", "time": "2019-05-18T00:58:32Z", "body": "Summary:\nThis is a hack for mitigating s178018. Full details are in sev summary.\n\nTL;DR is that due to a bit flip somewhere between training data generation and it landing to hive causes ~ 0.0000001% rows to have negative dense feature ids.\n\nThis threshold is still arbitrary, based on the last few days of corruption (we see ~ 6 negative ids per day), if we train on 30 days of data then the limit should be 180 ensure workflows never fail, given that we have multiple readers its unlikely that a single reader will see all rows (checked that negative ids arent concentrated in a single ts partition etc)\n\nHopefully in the future,we can remove this altogether when checksum support is implemented (scribe/logdevice/fbetl) to prevent such rows from being written to warehouse. This is being tracked as a follow up item to the sev\n\nReviewed By: Wakeupbuddy\n\nDifferential Revision: D15401078\n\n"}
{"number": 20672, "title": "Modify the Insert quant-dequant test cases to look for q-dq pattern", "time": "2019-05-18T01:14:39Z", "body": "Stack:\r\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20672 Modify the Insert quant-dequant test cases to look for q-dq pattern**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15405606/)\r\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20107 [pt1][quant] Add dequantize_linear for JIT pass&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15202187/)\r\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #19984 [pt1][quant] Add qint8 type (int8_t)&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15150715/)\r\n\r\nCurrent test case looks for q->int_repr->dq pattern and constant nodes also.\r\nThe prim::Constant nodes are not guaranteed to be present at same point in graph.\r\nSo we modify the test case to only look for the q->int_repr->dq nodes.\r\n\r\nTest Cases:\r\n./buck-out/gen/caffe2/test/jit#binary.par -r \"test_insert_quantdequant_consecutive_qnodes_script\"\r\ntest_insert_quantdequant_consecutive_qnodes_script (test_jit.TestJit) ... ok\r\n\r\n----------------------------------------------------------------------\r\nRan 1 test in 0.599s\r\n\r\nOK\r\n[npandit@devvm6468.prn2 /data/users/npandit/fbsource/fbcode] ./buck-out/gen/caffe2/test/jit#binary.par -r \"test_insert_quantdequant_consecutive_qnodes_trace\"\r\ntest_insert_quantdequant_consecutive_qnodes_trace (test_jit.TestJit) ... ok\r\n\r\n----------------------------------------------------------------------\r\nRan 1 test in 0.269s\r\n\r\nOK\r\n[npandit@devvm6468.prn2 /data/users/npandit/fbsource/fbcode] ./buck-out/gen/caffe2/test/jit#binary.par -r \"test_insert_quantdequant_single_qnode\"test_insert_quantdequant_single_qnode (test_jit.TestJit) ... ok\r\n\r\n----------------------------------------------------------------------\r\nRan 1 test in 0.678s\r\n\r\nOK\r\n[npandit@devvm6468.prn2 /data/users/npandit/fbsource/fbcode] ./buck-out/gen/caffe2/test/jit#binary.par -r \"test_insert_quantdequant_alternate_qnode\"\r\ntest_insert_quantdequant_alternate_qnode (test_jit.TestJit) ... ok\r\n\r\n----------------------------------------------------------------------\r\nRan 1 test in 0.688s\r\n\r\nOK\r\n\r\nDifferential Revision: [D15405606](https://our.internmc.facebook.com/intern/diff/D15405606/)"}
{"number": 20673, "title": "Add hashing to bucket-weighted pooling", "time": "2019-05-18T01:29:59Z", "body": "Summary: Add option to bucket-weighted pooling to hash the bucket so that any cardinality score can be used.\n\nDifferential Revision: D15003509\n\n"}
{"number": 20674, "title": "Split cpu/gpu in caffe2/distributed + some clean up", "time": "2019-05-18T02:29:26Z", "body": "Summary: A few targets in caffe2/caffe2/distribute needs to be split too, otherwise won't compile. Also some clean ups and make select_gpu_type to gpu_library_selector\n\nDifferential Revision: D15406019\n\n"}
{"number": 20675, "title": "Windows 10 CUDA 9 CUDNN 7.5 Pytorch 1.1 CUDNN_STATUS_EXECUTION_FAILED", "time": "2019-05-18T02:58:06Z", "body": "## ‚ùì Questions and Help\r\n\r\n### Please note that this issue tracker is not a help form and this issue will be closed.\r\n\r\nWe have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:\r\n\r\n- [Discussion Forum](https://discuss.pytorch.org/)\r\nFirstly I tested tensorflow-gpu 1.12, it works on CUDA GPU well. \r\nThen I followed pytorch tutorial to setup a simple CIFAR10 net, it works well on cpu. I tested cuda&cudnn is_available, both of them return True, however the gpu-enabled CIFAR10 net failed on gpu with exception CUDNN_STATUS_EXECUTION_FAILED.\r\n\r\nInputs is cudnn acceptable:  True\r\nTraceback (most recent call last):\r\n  File \"D:/Python/pytorch/tutorial/tut_4_classifier_gpu.py\", line 97, in <module>\r\n    outputs = net(inputs)\r\n  File \"D:\\Python\\Python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 489, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"D:/Python/pytorch/tutorial/tut_4_classifier_gpu.py\", line 62, in forward\r\n    x = self.pool(F.relu(self.conv1(x)))\r\n  File \"D:\\Python\\Python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 489, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"D:\\Python\\Python36\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 320, in forward\r\n    self.padding, self.dilation, self.groups)\r\nRuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED\r\n"}
{"number": 20676, "title": "Inconsistent behavior between jit loaded model and the one to be saved", "time": "2019-05-18T03:39:12Z", "body": "## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```\r\nimport torch\r\nfrom torch import jit\r\nfrom torch import nn\r\nimport numpy as np\r\n\r\n\r\nclass SubContainsVariable(jit.ScriptModule):\r\n    def __init__(self):\r\n        super(SubContainsVariable, self).__init__()\r\n        self.v = nn.Parameter(torch.from_numpy(np.array(1.)))\r\n\r\n    @jit.script_method\r\n    def forward(self, x):\r\n        return self.v * x\r\n\r\n\r\nclass ContainsVariable(jit.ScriptModule):\r\n    def __init__(self):\r\n        super(ContainsVariable, self).__init__()\r\n        self.submodule = SubContainsVariable()\r\n        self.submodule1 = jit.ScriptModule()\r\n        self.submodule1.submodule2 = self.submodule\r\n        self.v = nn.Parameter(torch.from_numpy(np.array(2.)))\r\n\r\n    @jit.script_method\r\n    def forward(self, x):\r\n        return x + self.v + self.submodule(x) + self.submodule1.submodule2(x)\r\n\r\n\r\ncv = ContainsVariable()\r\n# both print True\r\nprint(cv.submodule is cv.submodule1.submodule2)\r\nprint(cv.submodule.v is cv.submodule1.submodule2.v)\r\n\r\njit.save(cv, \"save/torchmodel.zip\")\r\nload = jit.load(\"save/torchmodel.zip\")\r\n# both print False\r\nprint(load.submodule is load.submodule1.submodule2)\r\nprint(load.submodule.v is load.submodule1.submodule2.v)\r\n```\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\nThe results should be consistent between the saved and loaded\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.0.1.post2\r\n - OS (e.g., Linux): mac osx\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):  from binary\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: None\r\n - GPU models and configuration:None\r\n - Any other relevant information:None\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"}
{"number": 20677, "title": "Unify the addQuantDequantNode api for inputs and outputs from quant nodes", "time": "2019-05-18T04:06:44Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20677 Unify the addQuantDequantNode api for inputs and outputs from quant nodes**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15406354/)\n\nWith new changes in IR, it is possible to insert nodes after param\nnodes in graph. Thus we do not need to have two methods for inserting q-dq\nnodes to input or output to quantizable nodes.\n\nDifferential Revision: [D15406354](https://our.internmc.facebook.com/intern/diff/D15406354/)"}
{"number": 20678, "title": "RuntimeError: the derivative for 'qr' is not implemented", "time": "2019-05-18T05:19:33Z", "body": "## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.\r\n1.\r\n1.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux):\r\n - How you installed PyTorch (`conda`, `pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"}
{"number": 20679, "title": "Fix Binomimal overflow when logits is large", "time": "2019-05-18T06:55:30Z", "body": "This PR fixes  #17843. In addition (test locally), this still maintains the continuity of log_prob which is addressed in https://github.com/pytorch/pytorch/pull/15962 \r\n\r\ncc @neerajprad "}
{"number": 20680, "title": "CUDA ", "time": "2019-05-18T08:06:25Z", "body": "## ‚ùì Questions and Help\r\n\r\nwhy couldn't I set the used GPU by writing following code \r\nos.environ['CUDA_VISIBLE_DEVICES']='3' ?"}
{"number": 20681, "title": "Caffe2 build on windows 10 fails with \"ninja: build stopped: subcommand failed.\"", "time": "2019-05-18T10:49:31Z", "body": "## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nfollowing steps to build caffe2 on windows per the caffe2 install page\r\nbuild_windows.bat runs for a good while then fails with\r\n\r\nMicrosoft (R) C/C++ Optimizing Compiler Version 19.20.27508.1 for x64\r\nCopyright (C) Microsoft Corporation.  All rights reserved.\r\n\r\n**ninja: build stopped: subcommand failed.\r\nTraceback (most recent call last):\r\n  File \"tools\\build_libtorch.py\", line 22, in <module>\r\n    build_python=False, rerun_cmake=True, build_dir='.')\r\n  File \"c:\\Users\\garry\\Downloads\\pytorch\\tools\\build_pytorch_libs.py\", line 286, in build_caffe2\r\n    check_call(build_cmd, cwd=build_dir, env=my_env)\r\n  File \"C:\\Users\\garry\\AppData\\Local\\Programs\\Python\\Python37\\lib\\subprocess.py\", line 347, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['cmake', '--build', '.', '--target', 'install', '--config', 'Release', '--', '-j', '7']' returned non-zero exit status 1.\r\n\"Caffe2 building failed\"**\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. followed instructions on https://caffe2.ai/docs/getting-started.html?platform=windows&configuration=compile\r\n\r\n1.\r\n1.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nexpected build script to complete build\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux): windows 10 home\r\n - How you installed PyTorch (`conda`, `pip`, source): clone from github\r\n - Build command you used (if compiling from source): pytorch/scripts/build_windows.bat\r\n - Python version:3.7\r\n - CUDA/cuDNN version: NA\r\n - GPU models and configuration: AMD Radeon 8700M\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"}
{"number": 20682, "title": "[distribution] Support for various domain for AffineTransform", "time": "2019-05-18T14:49:12Z", "body": "## üöÄ Feature\r\nCurrently, AffineTransform's domain and codomain are `real`. So transformed distribution with affine transform will return `real` support by default. It would be great if we allow several more domains such as `greater_than` or `interval`. cc @neerajprad who is also interested in this feature.\r\n\r\nRelated issue: currently, `transform_to(constraints.interval(a, b))`'s codomain is real; while it should be `constraints.interval(a, b)`.\r\n\r\n## Motivation\r\n\r\nThis will be helpful so that users don't have to redefine `support` for such transformed distributions.\r\n\r\n## Alternatives\r\n\r\nA simple solution is to allow a `domain` arg in AffineTransform initialization. Then we can define `codomain` as a dependent property. This way, we can fix the above issue at `constraint_registry`  by using\r\n```\r\ndef _transform_to_greater_than(constraint):\r\n    return transforms.ComposeTransform([transforms.ExpTransform(),\r\n        transforms.AffineTransform(constraint.lower_bound, 1,\r\n                                  domain=constraint.positive)])\r\n```\r\nor\r\n```\r\ndef _transform_to_interval(constraint):\r\n    ...\r\n    return transforms.ComposeTransform([transforms.SigmoidTransform(),\r\n        transforms.AffineTransform(loc, scale,\r\n                domain=constraint.unit_interval)])\r\n```\r\n\r\ncc @fritzo @neerajprad @alicanb @vishwakftw who are maintainers of distributions library\r\n"}
{"number": 20683, "title": "random behavior is not able to be controlled", "time": "2019-05-18T15:08:25Z", "body": "## üêõ Bug\r\n\r\nWhen  use gpu to train a two-convolution-layer model, the random behavior is different in the running of two times.\r\n\r\n## To Reproduce\r\n\r\n1. write this code in some file, on my computer the source code is named `try.py`\r\n```python\r\nimport torch\r\nimport numpy as np\r\nimport random\r\n\r\ntorch.manual_seed(123)\r\nrandom.seed(123)\r\nnp.random.seed(123)\r\ntorch.cuda.manual_seed(123)\r\ntorch.backends.cudnn.enabled=False\r\ntorch.backends.cudnn.deterministic=True\r\n\r\nconv1 = torch.nn.Conv2d(3, 32, 3, 1, 1).cuda()\r\nconv2 = torch.nn.Conv2d(32, 32, 3, stride=1, padding=1, dilation=1).cuda()\r\n#  fc = torch.nn.Linear(32, 10)\r\ncriteria = torch.nn.CrossEntropyLoss()\r\n\r\ntorch.nn.init.kaiming_normal_(conv1.weight, 1)\r\ntorch.nn.init.kaiming_normal_(conv2.weight, 1)\r\nparams = list(conv1.parameters()) + list(conv2.parameters())\r\noptim = torch.optim.SGD(params, lr=1e-4, momentum=0.9, weight_decay=1e-4)\r\n\r\nfor i in range(10):\r\n    inten = torch.randn(16, 3, 64, 64).cuda()\r\n    lb = torch.randint(0, 10, (16, 64, 64)).cuda()\r\n\r\n    optim.zero_grad()\r\n    feat = conv1(inten)\r\n    feat = conv2(feat)\r\n    loss = criteria(feat, lb)\r\n    loss.backward()\r\n    optim.step()\r\n    print(loss.item())\r\n```\r\n2. run the above code for twice, and see the loss printed to the screen:\r\n```\r\nCUDA_VISIBLE_DEVICES=0 python try.py\r\n3.9505960941314697\r\n3.962101936340332\r\n3.944667100906372\r\n3.9629740715026855\r\n3.9574058055877686\r\n3.9502787590026855\r\n3.956238031387329\r\n3.9543356895446777\r\n3.950388193130493\r\n3.946221351623535\r\n\r\nCUDA_VISIBLE_DEVICES=0 python try.py\r\n3.9505960941314697\r\n3.962101697921753\r\n3.944666862487793\r\n3.9629745483398438\r\n3.9574055671691895\r\n3.9502787590026855\r\n3.956238031387329\r\n3.9543352127075195\r\n3.950387954711914\r\n3.9462215900421143\r\n```\r\nSince my practical model has a big variance, this random behavior can bring a final score variance of as much as 0.5%, which makes it too difficult for me to tune the hyper-parameters. What is the cause of this randomness, and how could I reduce it please?\r\n\r\n\r\n\r\n## Environment\r\nCollecting environment information...\r\nPyTorch version: 1.1.0a0+95ce796\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 5.4.0\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 390.67\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.3\r\n[pip] torch==1.1.0\r\n[pip] torchvision==0.2.2.post3\r\n[pip] torchvision-nightly==0.2.3\r\n[conda] Could not collect\r\n"}
{"number": 20684, "title": "Add dilation to _grad_input_padding, closes #16012", "time": "2019-05-18T22:05:08Z", "body": "Implemented fix to #16012 and #13008 according to the proposal of @ozzzp"}
{"number": 20685, "title": "Refactor CUDA copy and general copy dispatch", "time": "2019-05-19T00:15:42Z", "body": "Copy.cu goes from 308 to 190 lines of code. In general it uses, the same\r\ncopy strategy, using cudaMempcyAsync, a pointwise kernel, or a copy\r\nusing temporary buffers. The pointwise kernel has slightly improved\r\nperformance when broadcasting due to faster index calculation.\r\n\r\nThis deletes \"`s_copy_`\", \"`_s_copy_from`\", and \"`_copy_same_type_`\". The only\r\nentry-point now is \"`copy_`\".\r\n\r\nA mini-benchmark is here:\r\nhttps://gist.github.com/colesbury/706de1d4e8260afe046020988410b992\r\n\r\nBefore:\r\nhttps://gist.github.com/colesbury/ab454b6fe3791bff420d7bcf8c041f18\r\nAfter:\r\nhttps://gist.github.com/colesbury/9024d242b56ab09a9ec985fa6d1620bc\r\n\r\nResults were measured on 2.2 GHz Broadwell; no-turbo; one thread;\r\ncompiled with GCC 7.3.0. (Results are slower than typical usage due to\r\nturbo being off.)\r\n\r\nThe only significant differences is in the CUDA [1024] -> [1024, 1024]\r\nbroadcasting copy which is ~25% faster. I don't expect a noticeable\r\ndifference in real programs.\r\n\r\nCPU copy overhead is a tiny bit (~200 ns) faster, but I don't expect\r\nanyone to notice that.\r\n\r\n"}
{"number": 20686, "title": "In-source build causes repeating filename annotations (Windows doesn't support out-of-source build)", "time": "2019-05-19T03:07:09Z", "body": "We are seeing filenames in `aten/src/ATen/native/cpu/` like the following cases:\r\n```\r\nActivation.cpp\r\nActivation.cpp..cpp\r\nActivation.cpp..cpp..cpp\r\nActivation.cpp.AVX.cpp\r\nActivation.cpp.AVX.cpp..cpp\r\nActivation.cpp.AVX.cpp..cpp..cpp\r\nActivation.cpp.AVX.cpp.AVX.cpp\r\nUnaryOpsKernel.cpp.DEFAULT.cpp.DEFAULT.cpp.AVX2.cpp.AVX2.cpp\r\nUnaryOpsKernel.cpp.DEFAULT.cpp.DEFAULT.cpp.DEFAULT.cpp.AVX2.cpp\r\nUnaryOpsKernel.cpp.DEFAULT.cpp.AVX.cpp.DEFAULT.cpp.AVX2.cpp\r\n```\r\nThey will slow down the build, and sometimes even make it hang if we don't clean the build cache and do the in-source build.\r\n\r\nEdit: This can also cause multiple symbol definition problem\n\ncc @ezyang @gchanan @zou3519 @peterjc123"}
{"number": 20687, "title": "data loader wait for all workers", "time": "2019-05-19T10:14:19Z", "body": "i am testing pytorch data loader in google colab platform\r\nit seems higher workers have marginal improvement\r\n\r\nthis code is printing the time when each batch is delivered:\r\n\r\n```\r\nimport time\r\nt0=time.time()\r\nfor x in batch_gen:\r\n  print('%.1fs from start' % (time.time()-t0))\r\n```\r\n\r\nhere is the result for 2 and 4 workers\r\n\r\nnum_workers=2 :\r\n10.2s from start\r\n10.4s from start\r\n21.3s from start\r\n21.3s from start\r\n32.0s from start\r\n32.0s from start\r\n42.6s from start\r\n42.9s from start\r\n...\r\n\r\nnum_workers=4 :\r\n22.7s from start\r\n22.7s from start\r\n22.7s from start\r\n22.7s from start\r\n45.6s from start\r\n45.6s from start\r\n45.8s from start\r\n45.8s from start\r\n67.4s from start\r\n67.8s from start\r\n67.8s from start\r\n67.8s from start\r\n....\r\n\r\n\r\ni can understand this kind of threading can improve speed in some cases (not mine)\r\nbut it is kind of bad practice and have lower value then queue threading\r\n\r\nhere is another test:\r\n\r\n```\r\nt0=time.time()\r\nfor x in batch_gen:\r\n  time.sleep(2)\r\n  print('%.1fs from start' % (time.time()-t0))\r\n```\r\n\r\nwith num_workers=4 :\r\nbatch1: 24.1s from start\r\nbatch2: 26.1s from start\r\nbatch3: 28.1s from start\r\nbatch4: 30.1s from start\r\nbatch5: 45.2s from start\r\nbatch6: 47.3s from start\r\nbatch7: 49.3s from start\r\nbatch8: 51.3s from start\r\nbatch9: 66.7s from start\r\nbatch10: 68.7s from start\r\nbatch11: 70.7s from start\r\nbatch12: 72.8s from start\r\n...\r\n\r\nwith num_workers=2:\r\nbatch1: 13.7s from start\r\nbatch2: 15.7s from start\r\nbatch3: 25.4s from start\r\nbatch4: 27.4s from start\r\nbatch5: 35.9s from start\r\nbatch6: 37.9s from start\r\nbatch7: 46.8s from start\r\nbatch8: 48.8s from start\r\nbatch9: 57.3s from start\r\nbatch10: 59.4s from start\r\nbatch11: 68.0s from start\r\nbatch12: 70.0s from start\r\n\r\npytorch version  :  '1.1.0'  \r\n\r\n\r\nhope this help"}
{"number": 20688, "title": "[tensorboard] fix add_histogram_raw", "time": "2019-05-19T17:56:21Z", "body": "This is a porting of the fix from:\r\nhttps://github.com/lanpa/tensorboardX/issues/421\r\n\r\ncc @orionr "}
{"number": 20689, "title": "Enable batched QR decomposition and add a `some` option", "time": "2019-05-19T17:58:01Z", "body": "This PR covers two important points with respect to the QR decomposition:\r\n- batching of input matrices (#7500)\r\n- adding `some` as an option in `torch.qr` akin to NumPy's `mode` option (#10538)\r\n\r\nChangelog:\r\n- Enable batching for inputs to `torch.qr`\r\n- Move QR decomposition implementation to ATen (CPU and CUDA)\r\n- Remove existing implementations in TH/THC\r\n- Add a `some` option to `torch.qr` that will enable users to switch between complete and reduced decomposition\r\n- Modify doc strings\r\n\r\nTest plan:\r\n- Add new tests, remove old ones.\r\n\r\nCloses #10538"}
{"number": 20690, "title": "Use Device instead of Backend in TensorIterator", "time": "2019-05-19T18:59:46Z", "body": "This PR also moves Device::validate into the header file, which makes\r\nstatements like `Device d = kCPU` effectively free.\r\n\r\nDevice includes the device's index, so TensorIterator::compute_types\r\nnow implicitly checks that all CUDA inputs are on the same GPU.\r\nPreviously, this was done ad-hoc in places like TensorIterator::binary_op.\r\n\r\nNote that zero-dim Tensor (scalars) are NOT required to be on the\r\nsame device as other inputs because they behave almost like Python numbers.\r\nTensorIterator handles copying zero-dim Tensors to the common device.\r\n\r\nPrior to this PR, TensorIterator would copy zero-dim Tensors between CPU\r\nand GPU, but not between different GPUs (because Backend didn't encode\r\nthe GPU index). This removes that restriction."}
{"number": 20691, "title": "Port dilated_max_pool2d() to ATen", "time": "2019-05-19T22:48:43Z", "body": ""}
{"number": 20692, "title": "Compiling NCCL does not obey MAX_JOBS, makes machine unresponsive", "time": "2019-05-19T23:29:40Z", "body": "## üêõ Bug\r\n\r\nCompiling pytorch causes my laptop to become unresponsive, requiring a hard restart. This is because NCCL is compiled with `make -j` (without a number) which is [dangerous behaviour](https://unix.stackexchange.com/questions/316644/is-make-j-with-no-argument-dangerous) since it causes an unbounded number of processes to be spawned.\r\n\r\nThis is the line:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/7b9ee598d64bd35cad9afea8276ebd1c069620d9/cmake/External/nccl.cmake#L45\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Compile on any machine with modest resources, and when the bundled NCCL is built rather than an existing one used.\r\n\r\n## Expected behavior\r\n\r\nThe machine should not grind to a halt. It should probably obey the MAX_JOBS environment variable used elsewhere, and have a reasonable default.\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.1.0\r\n - OS (e.g., Linux): Ubuntu 16.04.6\r\n - How you installed PyTorch (`conda`, `pip`, source): source\r\n - Build command you used (if compiling from source): `make all` or `MAX_JOBS=1 make all` or `python setup.py install`\r\n - Python version: 3.7.3\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: Quadro K2000M\r\n - Any other relevant information:\r\n"}
{"number": 20693, "title": "bound nccl build to NCORES on the system", "time": "2019-05-19T23:51:29Z", "body": "Fixes https://github.com/pytorch/pytorch/issues/20692"}
{"number": 20694, "title": "nn.functional.conv functions do not take a padding_mode argument", "time": "2019-05-20T00:18:00Z", "body": "## üìö Documentation\r\n\r\nAs mentioned in [this post ](https://discuss.pytorch.org/t/a-possible-bug-in-torch-nn-functional-conv1d-2d/44066)\r\n\r\n[These docs for functional conv opperations](https://pytorch.org/docs/stable/nn.html?highlight=conv1d#torch.nn.functional.conv1d) show a optional argument `padding_mode`. However when specifying this argument an exception is raised:\r\n\r\n`TypeError: conv1d() got an unexpected keyword argument 'padding_mode'`\r\n\r\nUpon inspection of the modular Conv layers code, it can be seen that the `padding_mode` argument is used to apply padding before calling the functional conv:\r\nhttps://github.com/pytorch/pytorch/blob/v1.1.0/torch/nn/modules/conv.py#L192\r\n\r\nSo I believe the `padding_mode` argument should be removed from the **functional** conv docs, as this argument applies to the **modular** Conv layers only\r\n\r\nThanks "}
{"number": 20695, "title": "Py_NewInterpreter()", "time": "2019-05-20T00:29:09Z", "body": "Now used PyGILState_Ensure() / PyGILState_Release() turns deadlock.\r\n\r\n> Note that the PyGILState_*() functions assume there is only one global interpreter (created automatically by Py_Initialize()). Python supports the creation of additional interpreters (using Py_NewInterpreter()), but mixing multiple interpreters and the PyGILState_*() API is unsupported.\r\n\r\nYou must use PyEval_SaveThread() / PyEval_RestoreThread() to support Py_NewInterpreter().\r\n\r\nhttps://github.com/pytorch/pytorch/blob/7b9ee598d64bd35cad9afea8276ebd1c069620d9/torch/csrc/cuda/Module.cpp#L233\r\n\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/csrc/utils/auto_gil.h\r\n"}
{"number": 20696, "title": "#20028", "time": "2019-05-20T04:20:54Z", "body": "Hi, @ezyang\r\nSorry to trouble you. "}
{"number": 20697, "title": "Indexing by 1-dimension `int64` tensor returns incorrect shape", "time": "2019-05-20T04:37:35Z", "body": "## üêõ Bug\r\n\r\nSometimes indexing by a 1-dimension `int64` tensor with a single element returns a tensor with reduced rank.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n<img width=\"498\" alt=\"Screenshot 2019-05-19 21 41 52\" src=\"https://user-images.githubusercontent.com/630490/57996908-ec462a00-7a7e-11e9-8ba2-1a7bf5f71bca.png\">\r\n\r\nThis bug gets even better:\r\n\r\n<img width=\"228\" alt=\"Screenshot 2019-05-19 21 44 09\" src=\"https://user-images.githubusercontent.com/630490/57996986-4515c280-7a7f-11e9-962d-75f60589a4a2.png\">\r\n\r\n\r\n```python\r\nobs.dtype\r\nOut[28]: dtype('float32')\r\nobs.shape\r\nOut[29]: (54, 1, 64, 64)\r\na.shape\r\nOut[30]: torch.Size([2])\r\nobs.shape\r\nOut[31]: (54, 1, 64, 64)\r\nb = torch.tensor([3])\r\nobs[b].shape\r\nOut[33]: (1, 64, 64)\r\na = torch.tensor([3, 3])\r\nobs[a].shape\r\nOut[35]: (2, 1, 64, 64)\r\n```\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: Quadro GP100\r\nGPU 1: Quadro GP100\r\n\r\nNvidia driver version: 410.79\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.2\r\n[pip] torch==1.1.0\r\n[pip] torchvision==0.2.2\r\n[conda] mkl                       2019.3                      199\r\n[conda] pytorch                   1.1.0           py3.7_cuda10.0.130_cudnn7.5.1_0    pytorch\r\n[conda] pytorch-nightly           1.1.0.dev20190411 py3.7_cuda10.0.130_cudnn7.4.2_0    pytorch\r\n[conda] torchvision               0.2.2                      py_3    pytorch"}
{"number": 20698, "title": "Lightweight at-most-once logging for API usage", "time": "2019-05-20T06:10:45Z", "body": "Idea is that when PyTorch is used in a custom build environment (e.g. Facebook), it's useful to track usage of various APIs centrally. This PR introduces a simple very lightweight mechanism to do so - only first invocation of a trigger point would be logged. This is significantly more lightweight than https://github.com/pytorch/pytorch/pull/18235 and thus we can allow to put logging in e.g. TensorImpl.\r\n\r\nAlso adds an initial list of trigger points. Trigger points are added in such a way that no static initialization triggers them, i.e. just linking with libtorch.so will not cause any logging. Further suggestions of what to log are welcomed.\r\n\r\nTest plan:\r\nUsed `PYTORCH_API_USAGE_STDERR=1` env with various scenarios, verified that logging is indeed triggered.\r\nGiven the only-once nature of logging, I'm not sure adding unittest would be that beneficial as it might be impact by how multiple unittests are linked together in one binary."}
{"number": 20699, "title": "tf.nn.embedding_lookup_sparse C++ ", "time": "2019-05-20T09:52:29Z", "body": "I have two questions:\r\nquestion one : I decide to  use libtorch for my  training problems, and I want to use the function that have implemented in tensorflow,that is sparse tensor embedding. and I want to know if libtorch have the same interface. \r\n\r\nquestion two: tensorflow have the function tf.concat\r\n,and I want to know if libtorch have the similar interface."}
{"number": 20700, "title": "batched cholesky decomposition GPU vs. CPU speed", "time": "2019-05-20T11:24:39Z", "body": "## üêõ Bug\r\n\r\nBatched Cholesky Decomposition is much slower on GPU than on CPU. For small number of batches, CPU is orders of magnitude faster than the corresponding GPU implementation\r\n## To Reproduce\r\n```\r\ndef cholesky_speed_test(device=\"cpu\"):\r\n    assert device in (\"cpu\",\"cuda\")\r\n    np.random.seed(123)\r\n    n_rep = 10000\r\n    dim = 16\r\n    batches = 10\r\n\r\n    A = np.random.normal(0,1,(batches,dim,dim))\r\n    cov = np.matmul(A,A.swapaxes(1,2))\r\n\r\n    pt_cov = torch.tensor(cov.astype(np.float32),device=device)\r\n    L = torch.cholesky(pt_cov)\r\n\r\n    torch.cuda.nvtx.range_push(\"CholeskyDecomposition\")\r\n    start_time = time.time()\r\n    for i in range(n_rep):\r\n        torch.cuda.nvtx.range_push(\"Iter{}\".format(i))\r\n        torch.cholesky(pt_cov, out=L)\r\n        torch.cuda.nvtx.range_pop()\r\n    duration = time.time() - start_time\r\n    print(\"Duration: {:.3f}s on device {}\".format(duration,device))\r\n    torch.cuda.nvtx.range_pop()\r\n```\r\n\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Execute the above function for device \"cpu\" and \"cuda\"\r\n2. We observe the following times on a 1080Ti\r\n```\r\nDuration: 5.007s on device cuda\r\nDuration: 0.253s on device cpu\r\n```\r\n3. Investigating Nvidia Nsight logs:\r\n![image](https://user-images.githubusercontent.com/8858505/58018090-2f95ac80-7b02-11e9-9d2f-dcf236588716.png)\r\nShows that `torch.cholesky` is allocating and freeing memory constantly.\r\n\r\n## Expected behavior\r\n\r\nI would expect the GPU code to be much faster and not spend so much time on memory allocation\r\n\r\n## Environment\r\n\r\nCollecting environment information... \r\nPyTorch version: 1.1.0 \r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: GPU 0: GeForce GTX 1080 Ti\r\nNvidia driver version: 384.130\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.0.5\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.4\r\n[pip3] torch==1.1.0\r\n[pip3] torchvision==0.2.2\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.1                      144\r\n[conda] mkl_fft                   1.0.10           py36ha843d7b_0\r\n[conda] mkl_random                1.0.2            py36hd81dba3_0\r\n[conda] pytorch                   1.1.0           py3.6_cuda9.0.176_cudnn7.5.1_0    pytorch\r\n[conda] torchvision               0.2.2                      py_3    pytorch\r\n\r\n\r\nAny help or insight is appreciated!\r\n\r\nMatthias\r\n\n\ncc @vincentqb @vishwakftw @jianyuh @nikitaved @pearu @mruberry @heitorschueroff @VitalyFedyunin @ngimel"}
{"number": 20701, "title": "DataLoader Problem", "time": "2019-05-20T13:07:54Z", "body": "I have training , validation and test dataset. I have created dataloaders for three of them with training shuffle = True and for valdiation and test shuffle = False . So in one of the code epoch loop contains iterator over train and validation and in another one , epoch loop contains iterator over train , validation and test. \r\n\r\nIt's obvious that train example id's must be same in both the cases but they are different. \r\n\r\nThe code is given below :\r\n\r\n-  train + validation : https://www.kaggle.com/suchith0312/pytorch-dataloader-testing?scriptVersionId=14425205\r\n\r\n- train + validation + test : https://www.kaggle.com/suchith0312/pytorch-dataloader-testing?scriptVersionId=14425182"}
{"number": 20702, "title": "Re-sync with internal repository", "time": "2019-05-20T13:19:44Z", "body": "The internal and external repositories are out of sync. This attempts to brings them back in sync by patching the GitHub repository. Please carefully review this patch. You must disable ShipIt for your project in order to merge this pull request. DO NOT IMPORT this pull request. Instead, merge it directly on GitHub using the MERGE BUTTON. Re-enable ShipIt after merging."}
{"number": 20703, "title": "[Feature request] Zero diagonal tensor elements", "time": "2019-05-20T15:02:54Z", "body": "## üöÄ Feature\r\nRemove diagonal elements or replace diagonal elements with zero\r\n\r\n## Motivation\r\nUsing tensors to represent network structure (e.g. sociomatrix) it is often useful to remove information on the diagonal.\r\n\r\n## Alternatives\r\nOne simple alternative is to subtract the diagonal from the tensor itself.\r\n\r\n    def nodiag(input):\r\n        return input.sub(input.diag().diag())\r\n"}
{"number": 20704, "title": "Remove unpack() in torch/csrc/nn/type_checks.h and its caller functions in the codebase", "time": "2019-05-20T16:34:38Z", "body": "The `unpack()` function in torch/csrc/nn/type_checks.h is not used anymore because we had already killed torch.legacy.nn and all the normal NN ops go through ATen bindings right now. A proper removal of the `unpack()` function would also involve removing all of its caller functions."}
{"number": 20705, "title": "How to reserve negative values of features extracting by register_forward_hook?", "time": "2019-05-20T17:04:31Z", "body": "I am trying to extract features of a certain layer of a pretrained model. The fellowing code does work, however, the values of template_feature_map changed and I did nothing of it.\r\n\r\n    vgg_feature = models.vgg13(pretrained=True).features\r\n    template_feature_map=[]\r\n    def save_template_feature_map(self, input, output):\r\n        template_feature_map.append(output.detach())\r\n        print(template_feature_map)\r\n    template_handle = vgg_feature[5].register_forward_hook(save_template_feature_map)\r\n    vgg_feature(template[0])\r\n    print(template_feature_map)\r\n\r\nThe output of 6th layer of the model should have negative values, as first print(template_feature_map) shows. But, the negative values which should maintain in second print(template_feature_map) are changed to zeros, I don‚Äôt know why. If you know the mechanism of this, please tell me how to keep the negative values.\r\n\r\nThe output of two print(template_feature_map):\r\n\r\n` [tensor([[[[-5.7389e-01, -2.7154e+00, -4.0990e+00,  ...,  4.1902e+00,\r\n            3.1757e+00,  2.2461e+00],\r\n          [-2.2217e+00, -4.3395e+00, -6.8158e+00,  ..., -1.4454e+00,\r\n            9.8012e-01, -2.3653e+00],\r\n          [-4.1940e+00, -6.3235e+00, -6.8422e+00,  ..., -2.8329e+00,\r\n            2.5570e+00, -2.7704e+00],\r\n          ...,\r\n          [-3.3250e+00,  1.3792e-01,  5.4926e+00,  ..., -4.1722e+00,\r\n           -6.1008e-01, -2.6037e+00],\r\n          [ 1.5377e+00,  6.0671e-01,  2.0974e+00,  ...,  1.2441e+00,\r\n            1.5033e+00, -2.7246e+00],\r\n          [ 6.8857e-01, -3.5160e-02,  6.7858e-01,  ...,  1.2052e+00,\r\n            1.4533e+00, -1.4160e+00]],\r\n\r\n         [[ 6.8798e-01,  1.6971e+00,  2.1629e+00,  ...,  3.1701e-01,\r\n            8.5424e-01,  2.8768e+00],\r\n          [ 1.4013e+00,  2.7217e+00,  2.1476e+00,  ...,  3.1156e+00,\r\n            4.4858e+00,  3.6936e+00],\r\n          [ 3.1807e+00,  2.2245e+00,  2.4665e+00,  ...,  1.3838e+00,\r\n            1.0580e-02, -3.1445e-03],\r\n          ...,\r\n          [-4.7298e+00, -3.3037e+00, -1.2982e+00,  ...,  2.3266e-01,\r\n            6.7711e+00,  3.8166e+00],\r\n          [-4.7972e+00, -5.4591e+00, -2.5201e+00,  ...,  3.7584e+00,\r\n            5.1524e+00,  2.3072e+00],\r\n          [-2.4306e+00, -2.8033e+00, -2.0912e+00,  ...,  1.9888e+00,\r\n            2.0582e+00,  1.9266e+00]],\r\n\r\n         [[-4.4257e+00, -4.6331e+00, -3.3580e-03,  ..., -8.2233e+00,\r\n           -7.4645e+00, -1.7361e+00],\r\n          [-4.5593e+00, -8.4195e+00, -8.8428e+00,  ..., -6.7950e+00,\r\n           -1.4665e+01, -2.5335e+00],\r\n          [-2.3481e+00, -3.8543e+00, -3.5965e+00,  ..., -1.5105e+00,\r\n           -1.6923e+01, -5.9852e+00],\r\n          ...,\r\n          [-8.0165e+00,  8.0185e+00,  6.5506e+00,  ...,  5.3241e+00,\r\n            3.3854e+00, -1.6342e+00],\r\n          [-1.3689e+01, -2.2930e+00,  4.7097e+00,  ...,  3.2021e+00,\r\n            2.9208e+00, -8.0228e-01],\r\n          [-1.3055e+01, -1.1470e+01, -8.4442e+00,  ...,  1.8155e-02,\r\n           -6.2866e-02, -2.0333e+00]],\r\n\r\n         ...,\r\n\r\n         [[ 3.4622e+00, -1.2417e+00, -5.0749e+00,  ...,  5.3184e+00,\r\n            1.4744e+01,  8.3968e+00],\r\n          [-2.7820e+00, -9.1911e+00, -1.1069e+01,  ...,  2.5380e+00,\r\n            9.8336e+00,  4.0623e+00],\r\n          [-3.9794e+00, -1.0140e+01, -9.9133e+00,  ...,  3.0999e+00,\r\n            5.5936e+00,  2.5775e+00],\r\n          ...,\r\n          [ 2.0299e+00,  2.1304e-01, -2.2307e+00,  ...,  1.1388e+01,\r\n            8.8098e+00,  1.8991e+00],\r\n          [ 8.0663e-01, -1.5073e+00,  3.3977e-01,  ...,  8.5316e+00,\r\n            4.9923e+00, -3.6818e-01],\r\n          [-3.5146e+00, -7.2647e+00, -5.4331e+00,  ..., -1.9781e+00,\r\n           -3.4463e+00, -4.9034e+00]],\r\n\r\n         [[-3.2915e+00, -7.3263e+00, -6.8458e+00,  ...,  2.3122e+00,\r\n            9.7774e-01, -1.3498e+00],\r\n          [-4.5396e+00, -8.6832e+00, -8.8582e+00,  ...,  7.1535e-02,\r\n           -4.1133e+00, -4.4045e+00],\r\n          [-4.8781e+00, -7.0239e+00, -4.7350e+00,  ..., -3.6954e+00,\r\n           -9.6687e+00, -8.8289e+00],\r\n          ...,\r\n          [-4.7072e+00, -4.4823e-01,  1.7099e+00,  ...,  3.7923e+00,\r\n            1.6887e+00, -4.3305e+00],\r\n          [-5.5120e+00, -3.2324e+00,  2.3594e+00,  ...,  4.6031e+00,\r\n            1.8856e+00, -4.0147e+00],\r\n          [-5.1355e+00, -5.5335e+00, -1.7738e+00,  ...,  1.6159e+00,\r\n           -1.3950e+00, -4.1055e+00]],\r\n\r\n         [[-2.0252e+00, -2.3971e+00, -1.6477e+00,  ..., -3.3740e+00,\r\n           -4.9965e+00, -2.1219e+00],\r\n          [-7.6059e-01, -3.3901e-01, -1.8980e-01,  ..., -4.3286e+00,\r\n           -7.1350e+00, -3.9186e+00],\r\n          [ 8.4101e-01,  1.3403e+00,  2.5821e-01,  ..., -5.1847e+00,\r\n           -7.1829e+00, -3.7724e+00],\r\n          ...,\r\n          [-6.0619e+00, -5.6475e+00, -1.6446e+00,  ..., -9.2322e+00,\r\n           -9.1981e+00, -5.5239e+00],\r\n          [-7.4606e+00, -7.6054e+00, -5.8401e+00,  ..., -7.6998e+00,\r\n           -6.4111e+00, -2.9374e+00],\r\n          [-6.4147e+00, -7.2813e+00, -6.1880e+00,  ..., -4.6726e+00,\r\n           -3.1090e+00, -7.8383e-01]]]])]\r\n[tensor([[[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.1902e+00,\r\n           3.1757e+00, 2.2461e+00],\r\n          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\r\n           9.8012e-01, 0.0000e+00],\r\n          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\r\n           2.5570e+00, 0.0000e+00],\r\n          ...,\r\n          [0.0000e+00, 1.3792e-01, 5.4926e+00,  ..., 0.0000e+00,\r\n           0.0000e+00, 0.0000e+00],\r\n          [1.5377e+00, 6.0671e-01, 2.0974e+00,  ..., 1.2441e+00,\r\n           1.5033e+00, 0.0000e+00],\r\n          [6.8857e-01, 0.0000e+00, 6.7858e-01,  ..., 1.2052e+00,\r\n           1.4533e+00, 0.0000e+00]],\r\n\r\n         [[6.8798e-01, 1.6971e+00, 2.1629e+00,  ..., 3.1701e-01,\r\n           8.5424e-01, 2.8768e+00],\r\n          [1.4013e+00, 2.7217e+00, 2.1476e+00,  ..., 3.1156e+00,\r\n           4.4858e+00, 3.6936e+00],\r\n          [3.1807e+00, 2.2245e+00, 2.4665e+00,  ..., 1.3838e+00,\r\n           1.0580e-02, 0.0000e+00],\r\n          ...,\r\n          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.3266e-01,\r\n           6.7711e+00, 3.8166e+00],\r\n          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.7584e+00,\r\n           5.1524e+00, 2.3072e+00],\r\n          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.9888e+00,\r\n           2.0582e+00, 1.9266e+00]],\r\n\r\n         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\r\n           0.0000e+00, 0.0000e+00],\r\n          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\r\n           0.0000e+00, 0.0000e+00],\r\n          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\r\n           0.0000e+00, 0.0000e+00],\r\n          ...,\r\n          [0.0000e+00, 8.0185e+00, 6.5506e+00,  ..., 5.3241e+00,\r\n           3.3854e+00, 0.0000e+00],\r\n          [0.0000e+00, 0.0000e+00, 4.7097e+00,  ..., 3.2021e+00,\r\n           2.9208e+00, 0.0000e+00],\r\n          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.8155e-02,\r\n           0.0000e+00, 0.0000e+00]],\r\n\r\n         ...,\r\n\r\n         [[3.4622e+00, 0.0000e+00, 0.0000e+00,  ..., 5.3184e+00,\r\n           1.4744e+01, 8.3968e+00],\r\n          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.5380e+00,\r\n           9.8336e+00, 4.0623e+00],\r\n          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.0999e+00,\r\n           5.5936e+00, 2.5775e+00],\r\n          ...,\r\n          [2.0299e+00, 2.1304e-01, 0.0000e+00,  ..., 1.1388e+01,\r\n           8.8098e+00, 1.8991e+00],\r\n          [8.0663e-01, 0.0000e+00, 3.3977e-01,  ..., 8.5316e+00,\r\n           4.9923e+00, 0.0000e+00],\r\n          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\r\n           0.0000e+00, 0.0000e+00]],\r\n\r\n         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.3122e+00,\r\n           9.7774e-01, 0.0000e+00],\r\n          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.1535e-02,\r\n           0.0000e+00, 0.0000e+00],\r\n          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\r\n           0.0000e+00, 0.0000e+00],\r\n          ...,\r\n          [0.0000e+00, 0.0000e+00, 1.7099e+00,  ..., 3.7923e+00,\r\n           1.6887e+00, 0.0000e+00],\r\n          [0.0000e+00, 0.0000e+00, 2.3594e+00,  ..., 4.6031e+00,\r\n           1.8856e+00, 0.0000e+00],\r\n          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.6159e+00,\r\n           0.0000e+00, 0.0000e+00]],\r\n\r\n         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\r\n           0.0000e+00, 0.0000e+00],\r\n          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\r\n           0.0000e+00, 0.0000e+00],\r\n          [8.4101e-01, 1.3403e+00, 2.5821e-01,  ..., 0.0000e+00,\r\n           0.0000e+00, 0.0000e+00],\r\n          ...,\r\n          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\r\n           0.0000e+00, 0.0000e+00],\r\n          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\r\n           0.0000e+00, 0.0000e+00],\r\n          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\r\n           0.0000e+00, 0.0000e+00]]]])] `\r\n"}
{"number": 20706, "title": "Add support for Visual Studio 2019", "time": "2019-05-20T17:07:58Z", "body": "## üöÄ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nCan you please add support for building from source in Visual Studio 2019\r\n## Motivation\r\nIt would allow developers who do not have the previous Visual Studio versions to work with and contribute to this repository.\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n## Pitch\r\n\r\nAdd support for building in Visual Studio 2019 to increase potential contributors with a relatively easy fix.  Most of what is needed would be updating scripts/build_windows.bat file to include a code generate for Visual Studio 2019 along with having CMake 3.14+ installed.  This has been done in ML.net and was a relatively simple fix. \r\n[ML.net Solution](https://github.com/PranovD/machinelearning/commit/b0fdb444a4acaf402c706dcbeaf157eb2668ccd3)\r\n\r\n[Warning: CMake 3.14 does not support placing Win32 or other architecture flags directly after named Generator](https://cmake.org/cmake/help/v3.14/generator/Visual%20Studio%2016%202019.html)\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n"}
{"number": 20707, "title": "[DO NOT MERGE] Test different implementation in AccumulateGrad::apply", "time": "2019-05-20T17:40:13Z", "body": ""}
{"number": 20708, "title": " [jit] Add support for recursive compilation on Modules", "time": "2019-05-20T17:48:10Z", "body": "Following on #19747, this implements most of the `torch.jit.script()` changes laid out in #20939.\r\n\r\nStill to do:\r\n* Accessing a method from Python does not add it as a `ScriptMethod` (so only `@export`ed methods and `forward` are compiled)\r\n* Calling a method other than `forward` on a submodule doesn't work\r\n\n\nDifferential Revision: [D15560490](https://our.internmc.facebook.com/intern/diff/15560490/)"}
{"number": 20709, "title": "De-deprecate old list and dict APIs", "time": "2019-05-20T18:00:23Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20709 De-deprecate old list and dict APIs**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15417025/)\n\n- Remove ArrayRef based API. This is neither the old nor the planned new API.\n- De-deprecate kernels based on std::vector and std::unordered_map. We don't have the Dict/List based API figured out entirely yet, so we shouldn't push people towards using them.\n  std::vector and std::unordered_map will get deprecated again once we figured out List/Dict.\n\nDifferential Revision: [D15417025](https://our.internmc.facebook.com/intern/diff/D15417025/)"}
{"number": 20710, "title": "Export ONNX  Dropout for opset 10", "time": "2019-05-20T18:00:24Z", "body": "Remove Dropout from the opset 10 blacklist.\r\nONNX Dropout was modified in opset 10, but only the output \"mask\" was modified, which is not exported in pytorch opset 9. So we can still fallback on the opset 9 op."}
{"number": 20711, "title": "[pt1][quant] Fix a bug in quantize_linear", "time": "2019-05-20T18:09:48Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20713 [pt1][quant] Change Bias to QTensor with qint32(int32_t)&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15410734/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20712 [pt1][quant] Change Weight to QTensor with qint8(int8_t)&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15410696/)\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20711 [pt1][quant] Fix a bug in quantize_linear**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15410695/)\n\nFor uint8_t, ```std::numeric_limits::digits``` returns 8;\nFor int8_t, ```std::numeric_limits::digits``` returns 7.\n\nFBGEMM wants to get the ```qparams.precision``` to be always 8 for both int8_t and uint8_t.\n\nDifferential Revision: [D15410695](https://our.internmc.facebook.com/intern/diff/D15410695/)"}
{"number": 20712, "title": "[pt1][quant] Change Weight to QTensor with qint8(int8_t)", "time": "2019-05-20T18:09:56Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20713 [pt1][quant] Change Bias to QTensor with qint32(int32_t)&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15410734/)\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20712 [pt1][quant] Change Weight to QTensor with qint8(int8_t)**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15410696/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20711 [pt1][quant] Fix a bug in quantize_linear&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15410695/)\n\nAs Title says.\n\nDifferential Revision: [D15410696](https://our.internmc.facebook.com/intern/diff/D15410696/)"}
{"number": 20713, "title": "[pt1][quant] Change Bias to QTensor with qint32(int32_t)", "time": "2019-05-20T18:10:04Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20713 [pt1][quant] Change Bias to QTensor with qint32(int32_t)**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15410734/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20712 [pt1][quant] Change Weight to QTensor with qint8(int8_t)&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15410696/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20711 [pt1][quant] Fix a bug in quantize_linear&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15410695/)\n\nAs Title says.\n\nDifferential Revision: [D15410734](https://our.internmc.facebook.com/intern/diff/D15410734/)"}
{"number": 20714, "title": "[WIP] Interp1d for unstructured 1-D torch.tensor on CPU", "time": "2019-05-20T18:19:24Z", "body": "As requested by https://github.com/pytorch/pytorch/issues/1552, an 1-D interpolation function is created, similar to numpy.interp function (https://github.com/numpy/numpy/blob/v1.16.1/numpy/lib/function_base.py#L1282-L1412). The function currently supports CPU tentors only. Please feel free to leave comments as work progresses."}
{"number": 20715, "title": "Remove tab.", "time": "2019-05-20T18:30:18Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20715 Remove tab.**\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nDifferential Revision: [D15417984](https://our.internmc.facebook.com/intern/diff/D15417984)"}
{"number": 20716, "title": "[pt1][quant] Rename FC to Linear in the test routine", "time": "2019-05-20T18:31:38Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20716 [pt1][quant] Rename FC to Linear in the test routine**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15410823/)\n\nAs Title says.\n\nDifferential Revision: [D15410823](https://our.internmc.facebook.com/intern/diff/D15410823/)"}
{"number": 20717, "title": "Problem with DataLoader", "time": "2019-05-20T19:10:04Z", "body": "## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\nI have training , validation and test dataset. I have created dataloaders for three of them with training shuffle = True and for valdiation and test shuffle = False . So in one of the code epoch loop contains iterator over train and validation and in another one , epoch loop contains iterator over train , validation and test.\r\n\r\nCheck the output of the code 1 and 2\r\n1. https://www.kaggle.com/suchith0312/pytorch-dataloader-testing?scriptVersionId=14425205\r\n2. https://www.kaggle.com/suchith0312/pytorch-dataloader-testing?scriptVersionId=14425182\r\n\r\n## Expected behavior\r\n\r\nThe train id's from output of files must be same but they are different. \r\n\r\n## Environment\r\n - PyTorch Version : 1.0.1.post2\r\n - Python version: 3.6.6\r\n - CUDA version : 10.0\r\n- Cudnn version 7.4.2 "}
{"number": 20718, "title": "Update references to minimum CUDA and cuDNN version", "time": "2019-05-20T19:14:39Z", "body": "I didn't update the Windows references because I wasn't sure if they apply to CUDA 9. @peterjc123 what should the Windows section say?"}
{"number": 20719, "title": "use tensoriterator instead of th for fill_ implementation.", "time": "2019-05-20T19:40:55Z", "body": "Moves fill_ to aten as suggested in:\r\nhttps://github.com/pytorch/pytorch/pull/20336#issuecomment-493260729\r\n\r\nborrows from @cpuhrsch's PR: https://github.com/pytorch/pytorch/pull/18876/files#diff-0d1178f1a4ce15aeb760d251974e6924\r\n\r\nCo-authored-by: @cpuhrsch\r\n"}
{"number": 20720, "title": "onnx/caffe2 tests: Do not execute models with CPU-only operators on GPU.", "time": "2019-05-20T20:14:00Z", "body": ""}
{"number": 20721, "title": "[pt1][quant] Add quantized::fbgemm_linear_unpack operator for serialization", "time": "2019-05-20T20:40:13Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20721 [pt1][quant] Add quantized::fbgemm_linear_unpack operator for serialization**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15314568/)\n\nPull Request resolved: https://github.com/pytorch/FBGEMM/pull/97\n\n- FBGEMM: Add unpack function for PackBMatrix class: Unpack pmat buffer to the origin_buf (Used for the serialization to recover weight matrix).\n- PyTorch Quantizer: Add quantized::fbgemm_linear_unpack operator for serialization.\n\nDifferential Revision: [D15314568](https://our.internmc.facebook.com/intern/diff/D15314568/)"}
{"number": 20722, "title": "MultiheadAttention is not scriptable", "time": "2019-05-20T20:50:09Z", "body": "## üêõ Bug\r\n\r\nWhen trying to use MultiheadAttention in torch.jit.ScriptModule, errors result\r\n## To Reproduce\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\n\r\n\r\nclass MyModule(torch.jit.ScriptModule):\r\n#class MyModule(nn.Module):\r\n   def __init__(self,embed_dim, num_heads):\r\n       super(MyModule, self).__init__()\r\n       self.mod = nn.MultiheadAttention(embed_dim, num_heads)\r\n\r\n   def forward(self, q,k,v):\r\n       return self.mod(q,k,v)\r\n\r\n\r\nembed_dim = 1024\r\nnum_heads = 16\r\nsl=30\r\nbs=20\r\nmodel = MyModule(embed_dim, num_heads).cuda()\r\nq=torch.randn(sl,bs,embed_dim, device=\"cuda\")\r\nk=torch.randn(sl,bs,embed_dim, device=\"cuda\")\r\nv=torch.randn(sl,bs,embed_dim, device=\"cuda\")\r\n\r\nout = model(q,k,v)\r\nprint(out[0].size())\r\n```\r\n```\r\nTraceback (most recent call last):\r\n  File \"/workspace/ALL/playground/attentionscripting.py\", line 19, in <module>\r\n    model = MyModule(embed_dim, num_heads).cuda()\r\n  File \"/workspace/ALL/pytorch_upstream/torch/jit/__init__.py\", line 1202, in init_then_register\r\n    original_init(self, *args, **kwargs)\r\n  File \"/workspace/ALL/playground/attentionscripting.py\", line 9, in __init__\r\n    self.attn = nn.MultiheadAttention(embed_dim, num_heads)\r\n  File \"/workspace/ALL/pytorch_upstream/torch/jit/__init__.py\", line 1397, in __setattr__\r\n    value = _make_strong(value)\r\n  File \"/workspace/ALL/pytorch_upstream/torch/jit/__init__.py\", line 1586, in _make_strong\r\n    proxy = weak_type(mod, stubs)\r\n  File \"/workspace/ALL/pytorch_upstream/torch/jit/__init__.py\", line 1202, in init_then_register\r\n    original_init(self, *args, **kwargs)\r\n  File \"/workspace/ALL/pytorch_upstream/torch/jit/__init__.py\", line 1202, in init_then_register\r\n    original_init(self, *args, **kwargs)\r\n  File \"/workspace/ALL/pytorch_upstream/torch/jit/__init__.py\", line 1512, in __init__\r\n    _create_methods_from_stubs(self, stubs)\r\n  File \"/workspace/ALL/pytorch_upstream/torch/jit/__init__.py\", line 1163, in _create_methods_from_stubs\r\n    self._c._create_methods(self, defs, rcbs, defaults)\r\n  File \"/workspace/ALL/pytorch_upstream/torch/jit/__init__.py\", line 898, in _try_compile_weak_script\r\n    compiled_fn = torch.jit.script(fn, True, 0, entry[\"rcb\"])\r\n  File \"/workspace/ALL/pytorch_upstream/torch/jit/__init__.py\", line 974, in script\r\n    ast = get_jit_def(obj)\r\n  File \"/workspace/ALL/pytorch_upstream/torch/jit/frontend.py\", line 156, in get_jit_def\r\n    type_line = torch.jit.annotations.get_type_line(source)\r\n  File \"/workspace/ALL/pytorch_upstream/torch/jit/annotations.py\", line 136, in get_type_line\r\n    raise RuntimeError(\"Return type line '# type: (...) -> ...' not found on multiline \"\r\nRuntimeError: Return type line '# type: (...) -> ...' not found on multiline type annotation\r\n```\r\n## Expected behavior\r\nMultiheadAttention module can be used in ScriptModule. After the error above is resolved, the next one I think will be `.new_zeros` not being scriptable. \r\n\r\n## Environment\r\nFresh compile from master (09f22d10) python 3.6\r\n\r\ncc @zhangguanheng66 "}
{"number": 20723, "title": "Publish torch::Dict and torch::OperatorKernel", "time": "2019-05-20T21:13:17Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20723 Publish torch::Dict and torch::OperatorKernel**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15421575/)\n\nThese classes already existed but only as c10::Dict and c10::OperatorKernel.\nSince they're now part of torch::RegisterOperators(), they should also live in the torch namespace.\n\nDifferential Revision: [D15421575](https://our.internmc.facebook.com/intern/diff/D15421575/)"}
{"number": 20724, "title": "[wip][jit] Recursive modules", "time": "2019-05-20T22:08:47Z", "body": "Same as #20708 but prints errors and adds a Python op instead of stopping"}
{"number": 20725, "title": "Hipify fb/quantize", "time": "2019-05-20T22:09:34Z", "body": "Reviewed By: bddppq\n\nDifferential Revision: D15407710\n\n"}
{"number": 20726, "title": "Update Conda description in PyTorch README", "time": "2019-05-20T22:23:45Z", "body": "Summary:\nEdward says it doesn't actually provide compilers,\nbut it does provide dependencies, so let's mention that instead.\n\nDifferential Revision: D15423316\n\n"}
{"number": 20727, "title": "Fixing missing miniconda path in macos smokes", "time": "2019-05-20T22:28:07Z", "body": ""}
{"number": 20728, "title": "pythonb -m torch.distributed.launch doesn't propagate non-0 status of child process", "time": "2019-05-20T22:29:39Z", "body": "Right now if child process crashes, with `python -m torch.distributed.launch script.py` will return with a 0 return code. It should instead propagate non-zero exit status so that the launching environment could detect the failure\r\n\r\n```\r\nsubprocess.CalledProcessError: Command '['/home/ubuntu/anaconda3/envs/pytorch_p36/bin/python', '-u', 'pytorch_bench.py', '--local_rank=0', '--num_tasks=2', '--logdir', '/ncluster/runs/unnamedrun-0-ogf2l.06']' returned non-zero exit status 1.\r\n(pytorch_p36) ubuntu@ip-172-31-12-237:~$ echo $?\r\n0\r\n(pytorch_p36) ubuntu@ip-172-31-12-237:~$ python\r\nPython 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56)\r\n[GCC 7.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\nim>>> import torch\r\n>>> torch.__version__\r\n'1.1.0'\r\n\r\n```"}
{"number": 20729, "title": "Make nn functions configurable for different scalar types", "time": "2019-05-20T22:39:39Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20729 Make nn functions configurable for different scalar types**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15423752/)\n\nCurrently there is no way to specify what scalar types each nn function will support.\n\nThis change will allow to specify supported scalar types for each function/backward function and device. By default each function will support Float, Double, Half.\n\nIf you want to scpecify any extra supported scalar types, other then default, you will need to change nn.yalm:\n\n- name: _some_func(Tensor self)\n  cname: SomeFunction\n  CPU:\n    forward_scalar_types: ['Float', 'Double', 'Long']\n    backward_scalar_types: ['Float', 'Double']\n\nDifferential Revision: [D15423752](https://our.internmc.facebook.com/intern/diff/D15423752/)"}
{"number": 20730, "title": "conv2d/conv3d for LongTensor", "time": "2019-05-20T22:39:47Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20730 conv2d/conv3d for LongTensor**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15423753/)\n\nGenerates forward conv2d function for LongTensor\n\nDifferential Revision: [D15423753](https://our.internmc.facebook.com/intern/diff/D15423753/)"}
{"number": 20731, "title": "sum_pool2d/sum_pool3d for LongTensor", "time": "2019-05-20T22:39:52Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20731 sum_pool2d/sum_pool3d for LongTensor**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15423751/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20730 conv2d/conv3d for LongTensor&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15423753/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20729 Make nn functions configurable for different scalar types&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15423752/)\n\nGenerates sum_pool2d function for LongTensor\n\nDifferential Revision: [D15423751](https://our.internmc.facebook.com/intern/diff/D15423751/)"}
{"number": 20732, "title": "Fix missing cudatoolkit dependency in binary linux tests", "time": "2019-05-20T22:46:50Z", "body": ""}
{"number": 20733, "title": "[wip][jit] Remove _pack and _unpack", "time": "2019-05-20T23:01:47Z", "body": ""}
{"number": 20734, "title": "[jit] set up views for Autodiff and autograd hooks ", "time": "2019-05-20T23:08:58Z", "body": "Derived from the bug finding in #19769 , Here is what we found is missing in our Autodiff infrastructure. In Autodiff, we don't set up the views for inputs/outputs after we execute the forward graph(see this [TODO](https://github.com/pytorch/pytorch/blob/be33434d85953fe6f237d475cc8ae2e9a822ed34/torch/csrc/jit/graph_executor.cpp#L350)). This disables us from adding complicated Ad formulas (such as `linear` and `layer_norm`, etc.), because when we calculate higher level gradients, we usually ended up capturing multiple outputs, and some of the output is just a view of another output, and we want the gradient to flow to the original output, which we could not do in the current autodiff infrastructure. \r\n\r\nTaking a example from a graph after we added `linear` in AD, the first order backward graph of a simple model might (second order gradient forward graph) looks like the blow:\r\n\r\n```\r\ngraph(%1 : int[],\r\n      %4 : Float(*, *),\r\n      %8 : int[],\r\n      %11 : Float(*, *),\r\n      %15 : int[],\r\n      %17 : Float(*, *),\r\n      %19 : Float(*, *),\r\n      %21 : Float(*, *),\r\n      %22 : Tensor):\r\n  %28 : int = prim::Constant[value=1]()\r\n  %146 : int[] = aten::size(%22)\r\n  %24 : Tensor = aten::add(%21, %22, %28)\r\n  %grad_input.2 : Float(*, *) = aten::mm(%24, %19)\r\n  %grad_input.3 : Float(*, *) = aten::mm(%grad_input.2, %17)\r\n  %16 : Tensor = aten::reshape(%grad_input.3, %15)\r\n  %self_size.6 : int[] = aten::size(%grad_input.3)\r\n  %97 : Float(*, *) = aten::t(%grad_input.2)\r\n  %grad_weight.3 : Float(*, *) = aten::mm(%97, %11)\r\n  %self_size.4 : int[] = aten::size(%grad_input.2)\r\n  %72 : Tensor = aten::_grad_sum_to_size(%grad_input.2, %8)\r\n  %63 : Tensor = aten::t(%24)\r\n  %grad_weight.1 : Float(*, *) = aten::mm(%63, %4)\r\n  %self_size.2 : int[] = aten::size(%24)\r\n  %38 : Tensor = aten::_grad_sum_to_size(%24, %1)\r\n  return (%38, %grad_weight.1, %72, %grad_weight.3, %16, %146, %24, %grad_input.2, %self_size.6, %97, %self_size.4, %63, %self_size.2)\r\n```\r\nThe reason why we have so much outputs is that the second order backward need to know the intermediate outputs of this graph, and autodiff captures them during the reverse of the forward graph, which can bee seen as below:\r\n```\r\n%grad_bias.4 : Tensor, %grad_weight.1 : Float(*, *), %grad_bias.1 : Tensor, %grad_weight.3 : Float(*, *), %102 : Tensor = prim::DifferentiableGraph[Subgraph=<Graph>, ReverseSubgraph=<Graph>, f_real_outputs=5, df_input_vjps=[0, 1, 2, 3, 4, 6, 7], df_input_captured_inputs=[1, 3, 5, 6]\r\n, df_input_captured_outputs=[5, 6, 7, 8, 9, 10, 11, 12], df_output_vjps=[1, 3, 5, 6, 8]](%22, %input.2, %34, %input.1, %self_size.1, %6, %4, %1, %14)\r\n```\r\n\r\nWe can see that in the above forward graph, `%97` is just a view variable of `%grad_input.2`, but our Autodiff will create a new variable (instead of a view variable) for `%97` which will make this variable be a leaf that trying to flow the gradients back to it, which is wrong. This could be fixed if we correctly set up the views for `df_input_vjps` and `caputuredOutputs`. \r\n\r\nBecause we allow tensor lists to show up in the symbolic differentiable graph in https://github.com/pytorch/pytorch/pull/16784, we could not scan the lists to do the view discovery otherwise we will make the compiler slow on execution. It is possible after the Tensor/Variable merge, we can refactor the autograd to make it safe for tensors to be constructed and automatically track the viewing relationship. \r\n\r\nCreating this issue to keep track of the content discovered and potential solutions. \r\n\r\nCC @zdevito @apaszke @ailzhang @yf225 @bwasti @bddppq "}
{"number": 20735, "title": "[fix] Fix up repr on scripted modules", "time": "2019-05-20T23:15:52Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #20791 [jit][fix] Add len to OrderedDict types\n* **#20735 [fix] Fix up repr on scripted modules**\n\nDifferential Revision: [D15764815](https://our.internmc.facebook.com/intern/diff/D15764815)"}
{"number": 20736, "title": "Fixing upload_binary_htmls again", "time": "2019-05-20T23:44:09Z", "body": ""}
{"number": 20737, "title": "Throw error if multiple kernels registered", "time": "2019-05-20T23:49:55Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20737 Throw error if multiple kernels registered**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15425660/)\n\nIf someone tries to register multiple kernels in the same .op() call, we're now throwing an error.\n\nDifferential Revision: [D15425660](https://our.internmc.facebook.com/intern/diff/D15425660/)"}
{"number": 20738, "title": "fix soft_nms_cpu call in BoxWithNMSLimit", "time": "2019-05-20T23:58:07Z", "body": "Summary: D15348610 introduces a bug of misaligned arguments.\n\nReviewed By: isameer\n\nDifferential Revision: D15425627\n\n"}
{"number": 20739, "title": "When detecting numpy, assign relavant variables outside the try block", "time": "2019-05-21T00:40:36Z", "body": "When detecting the presence of NumPy using import, move numpy-related variable assignments outside the try block (i.e., to an else block) to improve readability.\r\n\r\n"}
{"number": 20740, "title": "[pt1][quant] Add as_quantized_tensor", "time": "2019-05-21T01:47:42Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20740 [pt1][quant] Add as_quantized_tensor**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15232416/)\n\nProvide a way to assemble quantized Tensor from int8 Tensor, scale and zero point.\n\nDifferential Revision: [D15232416](https://our.internmc.facebook.com/intern/diff/D15232416/)"}
{"number": 20741, "title": "[DO NOT MERGE] Try to use \"impl_ =\" in set_data()", "time": "2019-05-21T02:57:07Z", "body": ""}
{"number": 20742, "title": "Libtorch C++ Silently Fails to Generate Static Library libtorch.a", "time": "2019-05-21T03:21:01Z", "body": "## üêõ Bug\r\n\r\nBelow is all the library generated which does not contain the libtorch.a. No error messages are observed.\r\n\r\nlibprotobuf-lite.a\r\nlibclog.a\r\nlibcpuinfo_internals.a\r\nlibcpuinfo.a\r\nlibpthreadpool.a\r\nlibqnnpack.a\r\nlibnnpack_reference_layers.a\r\nlibonnxifi_loader.a\r\nlibonnxifi_dummy.so\r\nlibfoxi_loader.a\r\nlibonnxifi.so\r\nlibfoxi_dummy.so\r\nlibfoxi.so\r\nlibgloo.a\r\nlibprotobuf.a\r\nlibgloo_builder.a\r\nlibprotoc.a\r\nlibonnx_proto.a\r\nlibc10.a\r\nlibonnx.a\r\nlibCaffe2_perfkernels_avx2.a\r\nlibCaffe2_perfkernels_avx.a\r\nlibcaffe2_protos.a\r\nlibnnpack.a\r\nlibmkldnn.a\r\nlibcaffe2.a\r\nlibTHD.a\r\nlibc10d.a\r\nlibtorch.so\r\nlibcaffe2_detectron_ops.so\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\nI follow offical steps to clone the lastest code\r\n```\r\nbranch master\r\nlast commit 7b9ee598d64bd35cad9afea8276ebd1c069620d9\r\nAuthor: Jongsoo Park <jongsoo@fb.com>\r\n```\r\n\r\nConfigure environment variables\r\n\r\n```\r\nexport NO_CUDA=1  //do not need GPU here\r\nexport BUILD_SHARED_LIBS=OFF //this is instructed here https://github.com/pytorch/pytorch/blob/master/docs/libtorch.rst\r\nexport BUILD_TEST=False //I set this, b/c test binary is not needed here\r\n```\r\n\r\n\r\nCommand to build the library\r\n```\r\nmkdir build && cd build\r\npython ../tools/build_libtorch.py\r\n```\r\n\r\nEnsure that dependencies are installed \r\n`conda install numpy ninja pyyaml mkl mkl-include setuptools cmake cffi typing`\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\nlibtorch.a is generated\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.1.0a0+7b9ee59\r\nIs debug build: No\r\nCUDA used to build PyTorch: Could not collect\r\n\r\nOS:  Red Hat\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-5)\r\nCMake version: version 3.12.0\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: Could not collect\r\n\r\nNvidia driver version: 410.104\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.0\r\n[pip3] numpydoc==0.7.0\r\n[pip3] torch==1.1.0\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] mkl                       2019.3                      199  \r\n[conda] mkl-include               2019.3                      199  \r\n[conda] mkl-service               1.1.2                    py36_3  \r\n[conda] mkl_fft                   1.0.1            py36h3010b51_0  \r\n[conda] mkl_random                1.0.1            py36h629b387_0  \r\n[conda] scikit-learn              0.19.1          py36_nomklh6cfcb94_0  \r\n[conda] scipy                     1.1.0           py36_nomklh9c1e066_0  \r\n[conda] torch                     1.1.0a0+7b9ee59           <pip>\r\n[conda] torch                     1.1.0                     <pip>\r\n[conda] torchvision               0.2.2.post3               <pip>\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"}
{"number": 20743, "title": "[caffe2] convert Onnx DynamicSlice operator to caffe2  failed", "time": "2019-05-21T03:25:26Z", "body": "## üêõ Bug\r\nWhen I convert onnx dynamicSlice operator to caffe2, it failed.\r\n```\r\n[libprotobuf FATAL /root/pytorch/third_party/protobuf/src/google/protobuf/repeated_field.h:1506] CHECK failed: (index) < (current_size_):\r\nONNX FATAL: CHECK failed: (index) < (current_size_):\r\nTraceback (most recent call last):\r\n  File \"/root/pytorch-env3-tf1.12/bin/convert-onnx-to-caffe2\", line 10, in <module>\r\n    sys.exit(onnx_to_caffe2())\r\n  File \"/root/pytorch-env3-tf1.12/lib/python3.6/site-packages/click/core.py\", line 764, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/root/pytorch-env3-tf1.12/lib/python3.6/site-packages/click/core.py\", line 717, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/root/pytorch-env3-tf1.12/lib/python3.6/site-packages/click/core.py\", line 956, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/root/pytorch-env3-tf1.12/lib/python3.6/site-packages/click/core.py\", line 555, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"/root/pytorch-env3-tf1.12/lib/python3.6/site-packages/caffe2/python/onnx/bin/conversion.py\", line 87, in onnx_to_caffe2\r\n    init_net, predict_net = c2.onnx_graph_to_caffe2_net(onnx_model_proto)\r\n  File \"/root/pytorch-env3-tf1.12/lib/python3.6/site-packages/caffe2/python/onnx/backend.py\", line 924, in onnx_graph_to_caffe2_net\r\n    return cls._onnx_model_to_caffe2_net(model, device=device, opset_version=opset_version, include_initializers=True)\r\n  File \"/root/pytorch-env3-tf1.12/lib/python3.6/site-packages/caffe2/python/onnx/backend.py\", line 917, in _onnx_model_to_caffe2_net\r\n    raise RuntimeError('ONNX conversion failed')\r\nRuntimeError: ONNX conversion failed\r\n```\r\nI found the reason is DynamicSlice's input_size is 3, and we should get the inputs from index 0-2 rather then 3:\r\ncaffe2/onnx/backend.cc\r\n```\r\n@@ -1080,7 +1108,7 @@ Caffe2Ops Caffe2Backend::CreateDynamicSlice(\r\n   // arguments to the caffe2 Slice operator.\r\n   std::string axes_tensor;\r\n   if (onnx_node->node.input_size() > 2) {\r\n-    axes_tensor = onnx_node->node.input(3);\r\n+    axes_tensor = onnx_node->node.input(2);\r\n   } else {\r\n     axes_tensor = dummy_->NewDummyName();\r\n     auto* c2_op = ret.ops.Add();\r\n```\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n1.Original model:    tensorflow's [inceptionv4 model](https://github.com/tensorflow/models/tree/master/research/slim)\r\n2.convert to onnx\r\n3.onnx model convert to caffe2:\r\n```\r\nconvert-onnx-to-caffe2 ./inception_v4/inception_v4.onnx --output ./inception_v4/onnx_predict_net.pb --init-net-output ./inception_v4/onnx_init_net.pb\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nproduce onnx_predict_net.pb and onnx_init_net.pb successfully.\r\nSo I found that we can fix this issue by modify file:\r\ncaffe2/onnx/backend.cc\r\n```\r\n@@ -1080,7 +1108,7 @@ Caffe2Ops Caffe2Backend::CreateDynamicSlice(\r\n   // arguments to the caffe2 Slice operator.\r\n   std::string axes_tensor;\r\n   if (onnx_node->node.input_size() > 2) {\r\n-    axes_tensor = onnx_node->node.input(3);\r\n+    axes_tensor = onnx_node->node.input(2);\r\n   } else {\r\n     axes_tensor = dummy_->NewDummyName();\r\n     auto* c2_op = ret.ops.Add();\r\n```\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n\r\n\r\nPython version: 3.6\r\nIs CUDA available: N/A\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration:\r\nGPU 0: Tesla K80\r\nGPU 1: Tesla K80\r\nGPU 2: Tesla K80\r\nGPU 3: Tesla K80\r\n\r\nNvidia driver version: 410.48\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.2.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.2\r\n[pip3] torch==1.1.0a0+8036af3\r\n[pip3] torchvision==0.2.3a0+ccbb322\r\n[conda] Could not collect\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"}
{"number": 20744, "title": "Use std::set to track wildcard nodes so that \"last\" queries are fast.", "time": "2019-05-21T04:00:53Z", "body": "Cuts load time on ResNet18 by 40%.\r\n\r\n"}
{"number": 20745, "title": "Lightweight at-most-once logging for API usage", "time": "2019-05-21T05:06:52Z", "body": "Resubmit #20698 which got messed up.\r\n\r\nIdea is that when PyTorch is used in a custom build environment (e.g. Facebook), it's useful to track usage of various APIs centrally. This PR introduces a simple very lightweight mechanism to do so - only first invocation of a trigger point would be logged. This is significantly more lightweight than #18235 and thus we can allow to put logging in e.g. TensorImpl.\r\n\r\nAlso adds an initial list of trigger points. Trigger points are added in such a way that no static initialization triggers them, i.e. just linking with libtorch.so will not cause any logging. Further suggestions of what to log are welcomed.\r\n\r\nTest plan:\r\nUsed PYTORCH_API_USAGE_STDERR=1 env with various scenarios, verified that logging is indeed triggered.\r\nGiven the only-once nature of logging, I'm not sure adding unittest would be that beneficial as it might be impact by how multiple unittests are linked together in one binary.\r\n\r\nAdded a tiny benchmark to verify overhead. Since we're talking about static variable only, it seems to be 0.3ns with warm cache (note it's 1000 invocations reported below). Didn't test with cold cache as it's pretty hard to capture such a small overhead:\r\n\r\n```\r\nRunning bin/core_overhead_benchmark\r\nRun on (24 X 2394.5 MHz CPU s)\r\nCPU Caches:\r\n  L1 Data 32K (x24)\r\n  L1 Instruction 32K (x24)\r\n  L2 Unified 4096K (x24)\r\n  L3 Unified 16384K (x24)\r\n-------------------------------------------------------\r\nBenchmark                Time           CPU Iterations\r\n-------------------------------------------------------\r\nBM_APILogging         2816 ns       2815 ns     244608\r\nBM_NoAPILogging       2471 ns       2470 ns     277368\r\n```"}
{"number": 20746, "title": "[WIP] [ignore] test commit", "time": "2019-05-21T05:40:35Z", "body": ""}
{"number": 20747, "title": "kernel and padding doesn't work properly", "time": "2019-05-21T05:47:16Z", "body": "## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nself.model = nn.Sequential((nn.AvgPool2d(kernel_size = 6 )))(x)\r\n1.x = [16,1,3,6]\r\n1.No error message despite the fact that no padding was allowed by configuration.\r\n\r\n\r\n## Expected behavior\r\n\r\nI was expecting an error message like in TF\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\nenvironment collection script\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\nPyTorch Version (e.g., 1.0):\r\nOS (e.g., Linux):\r\nHow you installed PyTorch (conda, pip, source):\r\nBuild command you used (if compiling from source):\r\nPython version:\r\nCUDA/cuDNN version:\r\nGPU models and configuration:\r\nAny other relevant information:"}
{"number": 20748, "title": "Segmentation fault when use torch::from_blob", "time": "2019-05-21T08:09:28Z", "body": "my code like this:\r\n\r\n\r\n```c++\r\n#include <torch/script.h>\r\n#include <torch/torch.h> \r\n#include <opencv2/opencv.hpp>\r\n#include <opencv2/imgproc/imgproc.hpp>\r\n\r\n#include <iostream>\r\n#include <memory>\r\n#include <string>\r\n#include <vector>\r\n\r\n/* main */\r\nint main(int argc, const char* argv[]) {\r\n  if (argc < 3) {\r\n    std::cerr << \"usage: example-app <path-to-exported-script-module> \"\r\n      << \"<path-to-image>\\n\";\r\n    return -1;\r\n  }\r\n\r\n  // Deserialize the ScriptModule from a file using torch::jit::load().\r\n  std::shared_ptr<torch::jit::script::Module> module = torch::jit::load(argv[1]);\r\n\r\n  assert(module != nullptr);\r\n  std::cout << \"load model ok\\n\";\r\n\r\n  // Create a vector of inputs.\r\n  std::vector<torch::jit::IValue> inputs;\r\n\r\n  // load image with opencv and transform\r\n  cv::Mat image;\r\n  image = cv::imread(argv[2]);\r\n  cv::cvtColor(image, image, CV_BGR2RGB);\r\n  cv::Mat img_float;\r\n  image.convertTo(img_float, CV_32F, 1.0);\r\n  cv::Mat img_resize;\r\n  cv::resize(img_float, img_resize, cv::Size(144, 288));\r\n  //std::cout << img_float.at<cv::Vec3f>(56,34)[1] << std::endl;\r\n  cv::imwrite(\"1.png\",img_resize);\r\n  auto img_tensor = torch::from_blob(img_resize.data, {1, 144, 288, 3});\r\n  img_tensor = img_tensor.permute({0,3,1,2});\r\n  img_tensor[0][0] = img_tensor[0][0].sub_(0.485).div_(0.229);\r\n  img_tensor[0][1] = img_tensor[0][1].sub_(0.456).div_(0.224);\r\n  img_tensor[0][2] = img_tensor[0][2].sub_(0.406).div_(0.225);\r\n  // //auto img_var = torch::autograd::make_variable(img_tensor, false).data();\r\n  inputs.push_back(img_tensor);\r\n  \r\n  // Execute the model and turn its output into a tensor.\r\n  torch::Tensor out_tensor = module->forward(inputs).toTensor();\r\n  std::cout << out_tensor.slice(/*dim=*/1, /*start=*/0, /*end=*/23) << '\\n';\r\n\r\n  printf(\"ok ok ok ok ok ok ok ok \\n\");\r\n  return 0;\r\n}\r\n```\r\n\r\nand the enc is: ubuntu16.04 cuda10 pytorch1.0\r\n\r\nmy input image size is 108*271\r\n\r\nmy network is resnet50\r\n\r\nthe code can print \"ok ok ok ok ok ok ok ok\",  then the error happens like this:\r\n```\r\nThread 1 \"example-app\" received signal SIGSEGV, Segmentation fault.\r\nstd::_Function_handler<void (void*), torch::from_blob(void*, c10::ArrayRef<long>, c10::TensorOptions const&)::{lambda(void*)#1}>::_M_invoke(std::_Any_data const&, void*&&) (\r\n    __functor=..., __args#0=<unknown type in /home/ulsee/often/lqc/Pedestrain attribute/Person-Reid/cpp/attribute-recognition/build/example-app, CU 0x0, DIE 0xc3f92>)\r\n    at /usr/include/c++/5/functional:1871\r\n1871            (*_Base::_M_get_pointer(__functor))(\r\n\r\n```"}
{"number": 20749, "title": "torch.utils.data.Dataloader: base seed is determined inside if loop", "time": "2019-05-21T08:55:15Z", "body": "the outcome from the pytorch forum issue: https://discuss.pytorch.org/t/dataloader-problem-problem-arises-when-shuffle-true/45631\r\n\r\nThe rng state is updated (unnecessarily?) by the test data loader, even if the option shuffle=False. This may hinder reproducibility.\r\n"}
{"number": 20750, "title": "libcurand.so.8.0: cannot open shared object file", "time": "2019-05-21T09:40:05Z", "body": "## üêõ Bug\r\nimport torch\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.5/dist-packages/torch/__init__.py\", line 79, in <module>\r\n    from torch._C import *\r\nImportError: libcurand.so.8.0: cannot open shared object file: No such file or directory\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.1.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source): sudo pip3.5 install https://download.pytorch.org/whl/cu100/torch-1.1.0-cp35-cp35m-linux_x86_64.whl\r\n - Python version: 3.5\r\n - CUDA/cuDNN version: cuda(10.0),cuDNN(7.5.1)\r\n\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"}
{"number": 20751, "title": "tensor_illustration with correct numbers and better fonts for README file", "time": "2019-05-21T09:41:25Z", "body": "Fix of README tensor image for issue #20641 \r\nNumbers are fixed, symbols made more readable."}
{"number": 20752, "title": "Add support for CMake switches for VS 2019", "time": "2019-05-21T10:07:47Z", "body": "Appending `arch` to the generator name is not supported for VS starting from VS 2019."}
{"number": 20753, "title": "cmake for Torch unusable in archlinux", "time": "2019-05-21T12:49:29Z", "body": "## üêõ Bug\r\n\r\nWhen running the example of using libtorch with cmake, it errors about missing `libc10_cuda` during `find_library(Torch REQUIRED)`:\r\n\r\n```\r\nCMake Error at /usr/lib64/cmake/Caffe2/Caffe2Targets.cmake:107 (message):\r\n  The imported target \"c10_cuda\" references the file\r\n\r\n     \"/usr/lib/libc10_cuda.so\"\r\n\r\n  but this file does not exist.  Possible reasons include:\r\n\r\n  * The file was deleted, renamed, or moved to another location.\r\n\r\n  * An install or uninstall procedure did not complete successfully.\r\n\r\n  * The installation package was faulty and contained\r\n\r\n     \"/usr/lib64/cmake/Caffe2/Caffe2Targets.cmake\"\r\n\r\n  but not all the files it references.\r\n\r\nCall Stack (most recent call first):\r\n  /usr/lib64/cmake/Caffe2/Caffe2Config.cmake:116 (include)\r\n  /usr/lib64/cmake/Torch/TorchConfig.cmake:39 (find_package)\r\n  CMakeLists.txt:4 (find_package)\r\n\r\n\r\n-- Configuring incomplete, errors occurred!\r\n```\r\n\r\nIt tries to find it under `/usr/lib`, but arch's package manager installs it to `/usr/lib/pytorch`. So it's either the packager didn't install it to the correct place, or the  `Caffe2Targets.cmake` file not rigorous enough about ELF location.\r\n\r\nWhat's your thought on this? Is it proper to modify `Caffe2Targets.cmake` so it can find the lib correctly under this circumstance, or at least provides a variable or something to workaround this please.\r\n\r\n\r\n## To Reproduce\r\n```cmake\r\ncmake_minimum_required(VERSION 3.0 FATAL_ERROR)\r\nproject(custom_ops)\r\n\r\n\r\nfind_package(Torch REQUIRED)\r\n\r\nadd_executable(test test.cc)\r\n# target_link_libraries(test \"-L/usr/lib/pytorch -lc10 -lc10_cuda -lcaffe2 -lmklml_intel -ltorch -lshm -L/opt/cuda/targets/x86_64-linux/lib -lnvrtc -lcuda\")\r\ntarget_link_libraries(test \"${TORCH_LIBRARIES}\")\r\nset_property(TARGET test PROPERTY CXX_STANDARD 11)\r\n\r\n```\r\n\r\n```\r\ncmake ..\r\n```\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.105\r\n\r\nOS: Arch Linux\r\nGCC version: (GCC) 8.3.0\r\nCMake version: version 3.14.4\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.168\r\nGPU models and configuration: GPU 0: GeForce GTX 1060 6GB\r\nNvidia driver version: 430.14\r\ncuDNN version: /usr/lib/libcudnn.so.7.5.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.3\r\n[pip3] torch==1.0.1\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] Could not collect\r\n\r\n\r\n## Additional context\r\nthe package I install is named \"python-pytorch-cuda\" under arch.\r\n"}
{"number": 20754, "title": "Pytorch 1.1 change the pre-model path?", "time": "2019-05-21T12:50:37Z", "body": "## ‚ùì Questions and Help\r\n\r\n### Please note that this issue tracker is not a help form and this issue will be closed.\r\n\r\nWe have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:\r\n\r\n- [Discussion Forum](https://discuss.pytorch.org/)\r\nthe previous path is  root/.torch/model , now in Pytorch 1.1  I found that the path is he root/.cache/torch/checkpoints/. How to change this to the previous one?"}
{"number": 20755, "title": "Type conversion from float64 to float32 (cpu) sometimes crashes", "time": "2019-05-21T13:13:30Z", "body": "```python\r\nimport torch\r\n\r\na = torch.rand(3, 3, dtype = torch.float64)\r\nprint(a.dtype, a.device) # torch.float64 cpu\r\nc = a.to(torch.float32)\r\n#works\r\n\r\nb = torch.load('bug.pt')\r\nprint(b.dtype, b.device) # torch.float64 cpu\r\nc = b.to(torch.float32)\r\n# RuntimeError: expected scalar type Float but found Double\r\n\r\nd = b.clone().to(torch.float32)\r\n# works\r\n```\r\n\r\n[bug.zip](https://github.com/pytorch/pytorch/files/3202664/bug.zip)\r\n"}
{"number": 20756, "title": "AttributeError: 'Conv2d' object has no attribute 'padding_mode' when loading model from pytorch 1.0 to 1.1", "time": "2019-05-21T13:37:31Z", "body": "Loading a model created with pytorch 1.0 fails on pytorch 1.1 with the error \"AttributeError: 'Conv2d' object has no attribute 'padding_mode'\".\r\n\r\nThis issue was also reported on https://github.com/jacquelinelala/GFN/issues/11 .\r\n\r\ncomplete backtrace:\r\n```\r\nTraceback (most recent call last):\r\n  File \"run_nn.py\", line 213, in <module>\r\n    loss = criterion(model(batch_y)[:,:,loss_crop_lb:loss_crop_up, loss_crop_lb:loss_crop_up], batch_x[:,:,loss_cro\r\np_lb:loss_crop_up, loss_crop_lb:loss_crop_up]).cuda()\r\n  File \"/usr/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 493, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/orb/Dev/mthesis-denoise/networks/Hul.py\", line 511, in forward\r\n    l126 = self.enc128to126std(x)\r\n  File \"/usr/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 493, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/usr/lib/python3.7/site-packages/torch/nn/modules/container.py\", line 92, in forward\r\n    input = module(input)\r\n  File \"/usr/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 493, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/usr/lib/python3.7/site-packages/torch/nn/modules/conv.py\", line 331, in forward\r\n    if self.padding_mode == 'circular':\r\n  File \"/usr/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 539, in __getattr__\r\n    type(self).__name__, name))\r\nAttributeError: 'Conv2d' object has no attribute 'padding_mode'\r\n\r\n```\r\nExample of problematic model: https://github.com/trougnouf/mthesis-denoise/blob/master/networks/Hul.py , loaded with model = torch.load(args.load_g_path, map_location=device)\r\n\r\nA workaround is to comment out the lines that use self.padding_mode in module.py, the model can then be imported, and saving the state_dictionary instead of the whole model allows loading into an unmodified version of pytorch 1.1."}
{"number": 20757, "title": "torch.hub does not close the resource before removing", "time": "2019-05-21T13:57:34Z", "body": "## üêõ Bug\r\n\r\nIn the hub.py, line 169 should be preceded by:\r\ncached_zipfile.close()\r\nOtherwise on Windows, it will crash with PermissionException\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Call torch.hub.load() on external repo on windows\r\n\r\nFile \"...\\python36pytorch\\lib\\site-packages\\torch\\hub.py\", line 83, in _remove_if_exists\r\n    os.remove(path)\r\nPermissionError: [WinError 32] The process cannot access the file because it is being used by another process: '.../.cache\\\\torch\\\\hub\\\\master.zip'\r\n\r\n"}
{"number": 20758, "title": "Instructions for how to update pytorch-ci-hud when updating binary builds", "time": "2019-05-21T14:03:35Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20758 Instructions for how to update pytorch-ci-hud when updating binary builds**\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nDifferential Revision: [D15435639](https://our.internmc.facebook.com/intern/diff/D15435639)"}
{"number": 20759, "title": "Fix copy_transpose_valid check", "time": "2019-05-21T16:39:21Z", "body": "Fixes #20755\r\n\r\n(Was broken in #20685)\r\n\r\ncc @vadimkantorov"}
{"number": 20760, "title": "[jit] dropout symbolic_script should respect the training flag", "time": "2019-05-21T16:46:12Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20760 [jit] dropout symbolic_script should respect the training flag**\n\nas title\n\nDifferential Revision: [D15486511](https://our.internmc.facebook.com/intern/diff/D15486511)"}
{"number": 20761, "title": "[JIT] add str comparisons", "time": "2019-05-21T17:21:40Z", "body": "add string comparisons"}
{"number": 20762, "title": "Use SmallVector to allocate Compound operands inline.", "time": "2019-05-21T17:25:25Z", "body": "Reduces load time for serialized ResNet-18 by 5.5%.\r\n\r\n"}
{"number": 20763, "title": "Automatic update of fbcode/onnx to cc2333a3f929caca7223b98699237f19388dd585", "time": "2019-05-21T17:43:54Z", "body": "Summary:\nPrevious import was ead449a30d026a7a0a59e2ba0a42ca8e52ec2359\n\nIncluded changes:\n- **[cc2333a3](https://github.com/onnx/onnx/commit/cc2333a3)**: Version Conversion of Min, Max, Mean from opset 7 to 8 (#2032) <Ksenija Stanojevic>\n- **[5d0975f4](https://github.com/onnx/onnx/commit/5d0975f4)**: Fix auto_pad shape inference bug (#2028) <stevenlix>\n- **[819afd05](https://github.com/onnx/onnx/commit/819afd05)**: Version Conversion from opset 8 to 9 (#2007) <Ksenija Stanojevic>\n- **[6c913669](https://github.com/onnx/onnx/commit/6c913669)**: fix macro ONNX_DISALLOW_COPY_AND_ASSIGN bug (#2017) <one-hello>\n\nDifferential Revision: D15425957\n\n"}
{"number": 20764, "title": "[pt1][quant] Add PerChannelAffineQuantizer", "time": "2019-05-21T18:08:08Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20765 [pt1][quant] Add `quantize_linear_per_channel`&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15435455/)\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20764 [pt1][quant] Add PerChannelAffineQuantizer**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15367364/)\n\natt\n\nDifferential Revision: [D15367364](https://our.internmc.facebook.com/intern/diff/D15367364/)"}
{"number": 20765, "title": "[pt1][quant] Add `quantize_linear_per_channel`", "time": "2019-05-21T18:09:32Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20765 [pt1][quant] Add `quantize_linear_per_channel`**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15435455/)\n\natt\n\nDifferential Revision: [D15435455](https://our.internmc.facebook.com/intern/diff/D15435455/)"}
{"number": 20766, "title": "PyTorch ThroughputBenchmark", "time": "2019-05-21T18:13:00Z", "body": "Summary:\nThis is useful for measuring inference performance of your\nmodels. This is a very basic benchmark for now. We don't support\nbatching on the benchmark side, no inter and intra op parallelizm is\nsupported yet, just caller based parallelizm.\n\nMain phylosophy here is that user should be able to provide inputs\nfrom python and just stack them within the benchmark. API should be\nexactly the same as passing inputs to module.forward.\n\nDifferential Revision: D15435461\n\n"}
{"number": 20767, "title": "Change comparison ops result dtype to bool [part 1]", "time": "2019-05-21T18:21:44Z", "body": "This is the first part of the planned changes to change the comparison operations result tensor dtype from Byte to Bool. You can see the whole list of changes (not cleaned up) [here](https://github.com/pytorch/pytorch/pull/19332). As the PR is too big for a single review im breaking it into pieces. \r\n\r\n**Changes in this PR:** \r\n1. Enable these methods for bool tensors:\r\n- maskedSelect\r\n- maskedSelectBool\r\n- bitand\r\n- cbitand\r\n- bitor\r\n- cbitor\r\n- bitxor\r\n- cbitxor\r\n- sign\r\n- equal\r\n- neg\r\n\r\n2. Add bool clause for the TH version of sign method.\r\n\r\n\r\nEverything except the TH sign method is just a code movement with no logic change."}
{"number": 20768, "title": "Improve the recommended citation", "time": "2019-05-21T18:32:16Z", "body": ""}
{"number": 20769, "title": "[JIT] Better Python String Support", "time": "2019-05-21T18:36:17Z", "body": "## [JIT] Support All Python String methods\r\n\r\nTracking issue for parity with python strings. \r\n\r\n- [x] capitalize()\r\n- [ ] casefold() - skipped\r\n- [x] center()\r\n- [x] count()\r\n- [ ] ~~encode()~~ - maybe skip this one since everything is already unicode\r\n- [x] endswith()\r\n- [x] expandtabs()\r\n- [x] find()\r\n- [x] format()\r\n- [ ] format_map() - skipped\r\n- [x] index()\r\n- [x] isalnum()\r\n- [x] isalpha()\r\n- [x] isdecimal()\r\n- [x] isdigit()\r\n- [x] isidentifier()\r\n- [x] islower()\r\n- [x] isnumeric()\r\n- [x] isprintable()\r\n- [x] isspace()\r\n- [x] istitle()\r\n- [x] isupper()\r\n- [ ] join()\r\n- [x] ljust()\r\n- [x] lower()\r\n- [x] lstrip()\r\n- [ ] maketrans() - skipped\r\n- [x] partition()\r\n- [x] replace()\r\n- [x] rfind()\r\n- [x] rindex()\r\n- [x] rjust()\r\n- [x] rpartition()\r\n- [x] rsplit()\r\n- [x] rstrip()\r\n- [x] split()\r\n- [x] splitlines()\r\n- [x] startswith()\r\n- [x] strip()\r\n- [x] swapcase()\r\n- [x] title()\r\n- [ ] translate() - skipped\r\n- [x] upper()\r\n- [x] zfill()\r\n\r\n\r\n## Motivation\r\n\r\nUsers have run been blocked by lack of these methods in the past. \r\n\n\ncc @suo"}
{"number": 20770, "title": "Add DictType to Metadata", "time": "2019-05-21T18:43:32Z", "body": "Summary: Add dict type since it's part of the pytorch built-in system, and sparse features and text features will be converted to Dict\n\nDifferential Revision: D15436255\n\n"}
{"number": 20771, "title": "Leave it as an option for whether to colorize output during build", "time": "2019-05-21T19:00:55Z", "body": "Currently PyTorch forces color output due to #20662. But users should be left an option to turn it off because redirection of the output to a file would be messed if color output is forced.\r\n\r\n"}
{"number": 20772, "title": "Quantized Conv2d operator", "time": "2019-05-21T19:29:05Z", "body": "Summary:\nCopy of D15178352\n\nA conflicting commit landed at the same time as D15178352 that removed registering kernels using IntArrayRef, Hence, D15178352 was revered. Using std::vector instead.\n\nDifferential Revision: D15437237\n\n"}
{"number": 20773, "title": "Catchall kernels instead of fallback kernels", "time": "2019-05-21T20:45:13Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20887 New torch assertion macros&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15484658/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20821 Specify dispatch key with kernel&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15455790/)\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20773 Catchall kernels instead of fallback kernels**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15438977/)\n\nThis removes the feature to register fallback kernels that are called when no other kernel matches.\nInstead, we introduce the concept of catchall kernels that are always called independent of inputs.\nIf you only have a fallback/catchall kernel and no kernels with concrete dispatch keys, then both concepts behave in the same way.\nThe difference is that we now disallow operators to have both, a catchall kernel and kernels with concrete dispatch keys.\nThis was possible before when they have been fallback kernels.\n\nThe reason for this change is that we anticipate needing a method_missing feature in backends, i.e. a backend-wide fallback to call when the backend doesn't specify a kernel for an operator.\nWe are not clear on precendence between this backend-wide fallback and an operator level fallback. Disallow fallbacks for now so we are free to choose later without breaking backwards compatibility.\n\nDifferential Revision: [D15438977](https://our.internmc.facebook.com/intern/diff/D15438977/)"}
{"number": 20774, "title": "Torch rename", "time": "2019-05-21T21:13:22Z", "body": "This renames the CMake `caffe2` target to `torch`, as well as renaming `caffe2_gpu` to `torch_gpu` (and likewise for other gpu target variants).  Many intermediate variables that don't manifest as artifacts of the build remain for now with the \"caffe2\" name; a complete purge of `caffe2` from CMake variable names is beyond the scope of this PR.\r\n\r\nThe shell `libtorch` library that had been introduced as a stopgap in #17783 is again flattened in this PR."}
{"number": 20775, "title": "Reduce set of build/tests which run on PRs.", "time": "2019-05-21T21:20:42Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20775 Reduce set of build/tests which run on PRs.**\n\nI ran an analysis of historical build data to find out which\nbuild jobs success/failures where correlated with each other.\nThe analysis can be found here:\n\n  https://github.com/ezyang/pytorch-ci-deduplication/blob/master/PyTorch%20CI%20deduplication%20(1).pdf\n\nBased on this analysis, I've revamped our logic for whether or not to\nrun a PR or not to only run builds in our default set.  This replaces\nour previous logic for doing [ci slow] or [ci xla]; now, you can place\nany string [ci foobar] and it will cause any build which matches that\nsubstring to be run on your run.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nDifferential Revision: [D15499918](https://our.internmc.facebook.com/intern/diff/D15499918)"}
{"number": 20776, "title": "Caffe2: executor_test.py::ExecutorGPUResNetTest fails with an AssertionError", "time": "2019-05-21T21:32:10Z", "body": "## üêõ Bug\r\n\r\nThe `executor_test.py::ExecutorGPUResNetTest::test_executor` test case fails on both x86 and Power9 with an error about a blob be completely different on different runs.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Install Caffe2\r\n2. Run `python caffe2/python/test/executor_test.py ExecutorGPUResNetTest`\r\n\r\nThe `test_executor` case fails with an error about `comp_2_sum_3_grad_autosplit_0` differing in parallel runs. The full output is below:\r\n\r\n```\r\n$ python executor_test.py ExecutorGPUResNetTest\r\nIgnoring @/caffe2/caffe2/contrib/nccl:nccl_ops as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops_gpu as it is not a valid file.\r\nINFO:caffe2.python.net_drawer:Cannot import pydot, which is required for drawing a network. This can usually be installed in python with \"pip install pydot\". Also, pydot requires graphviz to convert dot files to pdf: in ubuntu, this can usually be installed with \"sudo apt-get install graphviz\".\r\nnet_drawer will not run correctly. Please install the correct dependencies.\r\n[E init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\n[E init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\n[E init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\nTrying example: test_executor(self=<__main__.ExecutorGPUResNetTest testMethod=test_executor>, executor='parallel', num_workers=8)\r\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1]\r\nINFO:data_parallel_model:Create input and model training operators\r\nINFO:data_parallel_model:Model for GPU : 0\r\n/home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/caffe2/python/brew.py:97: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\r\n  var_names, _, varkw, _= inspect.getargspec(func)\r\nINFO:data_parallel_model:Model for GPU : 1\r\nINFO:data_parallel_model:Adding gradient operators\r\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\r\nINFO:data_parallel_model:Post-iteration operators for updating params\r\nINFO:data_parallel_model:Calling optimizer builder function\r\nINFO:data_parallel_model:Add initial parameter sync\r\nWARNING:data_parallel_model:------- DEPRECATED API, please use data_parallel_model.OptimizeGradientMemory() ----- \r\n/home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/caffe2/python/memonger.py:55: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\r\n  log.warn(\"NOTE: Executing memonger to optimize gradient memory\")\r\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\r\n[I memonger.cc:236] Remapping 108 using 14 shared blobs.\r\nINFO:memonger:Memonger memory optimization took 0.030774593353271484 secs\r\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\r\n[I memonger.cc:236] Remapping 108 using 14 shared blobs.\r\nINFO:memonger:Memonger memory optimization took 0.04712033271789551 secs\r\nFinished iteration 1/1 (1.90 images/sec)\r\n[I net_dag_utils.cc:102] Operator graph pruning prior to chain compute took: 0.00131458 secs\r\n[I net_parallel.cc:87] Initialized parallel net: 'test_resnet50', #ops: 1675, #chains: 1524, #workers: 8, dfs scheduling: 0, task graph engine: futures\r\n[I net_async_base.h:205] Using specified CUDA pool size: 8; device id: 0\r\n[I net_async_base.h:218] Created shared CUDA pool, size: 8; device id: 0\r\n[I net_async_base.h:205] Using specified CPU pool size: 8; device id: -1\r\n[I net_async_base.h:218] Created shared CPU pool, size: 8; device id: -1\r\n[I net_async_base.h:205] Using specified CUDA pool size: 8; device id: 1\r\n[I net_async_base.h:218] Created shared CUDA pool, size: 8; device id: 1\r\nFinished iteration 1/1 (44.82 images/sec)\r\nTraceback (most recent call last):\r\n  File \"/home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/hypothesis/core.py\", line 672, in evaluate_test_data\r\n    result = self.execute(data, collect=True)\r\n  File \"/home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/hypothesis/core.py\", line 587, in execute\r\n    result = self.test_runner(data, run)\r\n  File \"/home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/hypothesis/executors.py\", line 58, in default_new_style_executor\r\n    return function(data)\r\n  File \"/home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/hypothesis/core.py\", line 583, in run\r\n    return test(*args, **kwargs)\r\n  File \"executor_test.py\", line 53, in test_executor\r\n    num_workers=st.sampled_from([8]))\r\n  File \"/home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/hypothesis/core.py\", line 525, in test\r\n    result = self.test(*args, **kwargs)\r\n  File \"executor_test.py\", line 67, in test_executor\r\n    model_run_func=run_model,\r\n  File \"/home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/caffe2/python/test/executor_test_util.py\", line 268, in compare_executors\r\n    \"Blob {} differs in {} run\".format(blob_name, test_executor))\r\n  File \"/home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/numpy/testing/nose_tools/utils.py\", line 857, in assert_array_equal\r\n    verbose=verbose, header='Arrays are not equal')\r\n  File \"/home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/numpy/testing/nose_tools/utils.py\", line 781, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not equal\r\nBlob _gpu_0/comp_2_sum_3_grad_autosplit_0 differs in parallel run\r\n(mismatch 100.0%)\r\n x: array([[[[ 1.008181e-04, -6.411409e-05,  5.045671e-05, ...,\r\n          -6.273524e-05,  1.340966e-04,  6.698444e-05],\r\n         [-1.703523e-04, -2.880513e-04,  3.850183e-05, ...,...\r\n y: array([[[[-3.788031e-05,  1.195416e-04,  5.127888e-05, ...,\r\n          -1.146530e-04,  1.209393e-04,  3.692786e-05],\r\n         [-6.415213e-05,  2.887475e-05,  1.446379e-04, ...,...\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nTest case should pass\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.168\r\n\r\nOS: Red Hat\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)\r\nCMake version: version 2.8.12.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: \r\nGPU 0: Tesla V100-PCIE-16GB\r\nGPU 1: Tesla V100-PCIE-16GB\r\n\r\nNvidia driver version: 418.40.04\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.14.6\r\n[pip] torch==1.1.0\r\n[pip] torchvision==0.2.2\r\n[conda] blas                      1.0                         mkl  \r\n[conda] magma                     2.5.0\r\n[conda] mkl                       2018.0.3\r\n[conda] mkl_fft                   1.0.6\r\n[conda] mkl_random                1.0.1\r\n[conda] pytorch                   1.1.0\r\n\r\n### Power\r\n\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.168\r\n\r\nOS: Red Hat Enterprise Linux Server 7.6 (Maipo)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.105\r\nGPU models and configuration: \r\nGPU 0: Tesla V100-SXM2-16GB\r\nGPU 1: Tesla V100-SXM2-16GB\r\nGPU 2: Tesla V100-SXM2-16GB\r\nGPU 3: Tesla V100-SXM2-16GB\r\n\r\nNvidia driver version: 418.67\r\ncuDNN version: 7.5.1\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.14.5\r\n[pip] torch==1.1.0\r\n[pip] torchvision==0.2.2\r\n[conda] magma                     2.5.0\r\n[conda] pytorch                   1.1.0"}
{"number": 20777, "title": "Caffe2: operator_test/instance_norm_test.py::TestInstanceNorm::test_instance_norm_gradients test case fails with \"RuntimeError: dim() called on undefined TensorError from operator\"", "time": "2019-05-21T21:44:07Z", "body": "## üêõ Bug\r\n\r\nThe `operator_test/instance_norm_test.py::TestInstanceNorm::test_instance_norm_gradients` test case fails with a RuntimeError about ` test case fails intermittently\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Install Caffe2\r\n2. Run `python caffe2/python/operator_test/instance_norm_test.py TestInstanceNorm`\r\n\r\nThe test fails on `test_instance_norm_gradients` with the following error:\r\n\r\n```\r\nERROR: test_instance_norm_gradients (__main__.TestInstanceNorm)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"instance_norm_test.py\", line 54, in test_instance_norm_gradients\r\n    dc=hu.gcs['dc'],\r\n  File \"/home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/hypothesis/core.py\", line 1046, in wrapped_test\r\n    state.run()\r\n  File \"/home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/hypothesis/core.py\", line 814, in run\r\n    falsifying_example.__expected_traceback,\r\n  File \"/home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/hypothesis/core.py\", line 587, in execute\r\n    result = self.test_runner(data, run)\r\n  File \"/home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/hypothesis/executors.py\", line 58, in default_new_style_executor\r\n    return function(data)\r\n  File \"/home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/hypothesis/core.py\", line 578, in run\r\n    return test(*args, **kwargs)\r\n  File \"instance_norm_test.py\", line 54, in test_instance_norm_gradients\r\n    dc=hu.gcs['dc'],\r\n  File \"/home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/hypothesis/core.py\", line 525, in test\r\n    result = self.test(*args, **kwargs)\r\n  File \"instance_norm_test.py\", line 92, in test_instance_norm_gradients\r\n    threshold=0.01)\r\n  File \"/home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/caffe2/python/hypothesis_test_util.py\", line 458, in assertGradientChecks\r\n    input_device_options=input_device_options\r\n  File \"/home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/caffe2/python/gradient_checker.py\", line 281, in CheckSimple\r\n    outputs_with_grads\r\n  File \"/home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/caffe2/python/gradient_checker.py\", line 200, in GetLossAndGrad\r\n    workspace.RunOperatorsOnce(grad_ops)\r\n  File \"/home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/caffe2/python/workspace.py\", line 199, in RunOperatorsOnce\r\n    success = RunOperatorOnce(op)\r\n  File \"/home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/caffe2/python/workspace.py\", line 190, in RunOperatorOnce\r\n    return C.run_operator_once(StringifyProto(operator))\r\nRuntimeError: dim() called on undefined TensorError from operator: \r\ninput: \"input\" input: \"scale\" input: \"bias\" input: \"output_grad\" output: \"input_grad\" output: \"scale_grad\" output: \"bias_grad\" name: \"\" type: \"InstanceNormGradient\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"epsilon\" f: 1e-06 } device_option { device_type: 1 } is_gradient_op: true (dim at ../c10/core/UndefinedTensorImpl.cpp:24)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x6d (0x7f7bffeae80d in /home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/caffe2/python/../../torch/lib/libc10.so)\r\nframe #1: c10::UndefinedTensorImpl::dim() const + 0xc3 (0x7f7bffea7aa3 in /home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/caffe2/python/../../torch/lib/libc10.so)\r\nframe #2: void c10::TensorImpl::Resize<int, int>(int, int) + 0x160 (0x7f7c11980680 in /home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/caffe2/python/../../torch/lib/libcaffe2.so)\r\nframe #3: caffe2::InstanceNormGradientOp<float, caffe2::CUDAContext>::RunOnDeviceWithOrderNCHW() + 0x821 (0x7f7c02d097e1 in /home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/caffe2/python/../../torch/lib/libcaffe2_gpu.so)\r\nframe #4: <unknown function> + 0x27a449c (0x7f7c02d0a49c in /home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/caffe2/python/../../torch/lib/libcaffe2_gpu.so)\r\nframe #5: <unknown function> + 0x267cb28 (0x7f7c02be2b28 in /home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/caffe2/python/../../torch/lib/libcaffe2_gpu.so)\r\nframe #6: caffe2::Workspace::RunOperatorOnce(caffe2::OperatorDef const&) + 0x4a (0x7f7c11617f7a in /home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/caffe2/python/../../torch/lib/libcaffe2.so)\r\nframe #7: <unknown function> + 0x4fd9d (0x7f7c138e4d9d in /home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/caffe2/python/caffe2_pybind11_state_gpu.cpython-36m-x86_64-linux-gnu.so)\r\nframe #8: <unknown function> + 0x4ff17 (0x7f7c138e4f17 in /home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/caffe2/python/caffe2_pybind11_state_gpu.cpython-36m-x86_64-linux-gnu.so)\r\nframe #9: <unknown function> + 0x8dd0e (0x7f7c13922d0e in /home/cphofer/anaconda3/envs/pytorch_env/lib/python3.6/site-packages/caffe2/python/caffe2_pybind11_state_gpu.cpython-36m-x86_64-linux-gnu.so)\r\n<omitting python frames>\r\n```\r\n\r\n## Expected behavior\r\n\r\nThe test case should pass\r\n\r\n## Environment\r\n\r\n### x86\r\n\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.168\r\n\r\nOS: Red Hat\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)\r\nCMake version: version 2.8.12.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: Tesla V100-PCIE-16GB\r\nGPU 1: Tesla V100-PCIE-16GB\r\n\r\nNvidia driver version: 418.40.04\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.14.6\r\n[pip] torch==1.1.0\r\n[pip] torchvision==0.2.2\r\n[conda] blas 1.0 mkl\r\n[conda] magma 2.5.0\r\n[conda] mkl 2018.0.3\r\n[conda] mkl_fft 1.0.6\r\n[conda] mkl_random 1.0.1\r\n[conda] pytorch 1.1.0\r\n\r\n### Power\r\n\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.168\r\n\r\nOS: Red Hat Enterprise Linux Server 7.6 (Maipo)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.105\r\nGPU models and configuration: \r\nGPU 0: Tesla V100-SXM2-16GB\r\nGPU 1: Tesla V100-SXM2-16GB\r\nGPU 2: Tesla V100-SXM2-16GB\r\nGPU 3: Tesla V100-SXM2-16GB\r\n\r\nNvidia driver version: 418.67\r\ncuDNN version: 7.5.1\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.14.5\r\n[pip] torch==1.1.0\r\n[pip] torchvision==0.2.2\r\n[conda] magma                     2.5.0\r\n[conda] pytorch                   1.1.0\r\n"}
{"number": 20778, "title": "Support One step transform to numpy", "time": "2019-05-21T21:46:04Z", "body": "## üöÄ Feature\r\nWriting         `print tensor.detach().cpu().numpy()` gets really annoying. Can you provide a method that just takes you to numpy in one step?\r\n\r\nLike `fnumpy` (force to numpy). So if it has gradient, it will detach it for you, if it is cuda, it will move to CPU for you etc."}
{"number": 20779, "title": "[jit] Improve error message for missing attribute", "time": "2019-05-21T22:03:43Z", "body": "Fixes #20495 .\r\n\r\nNow for\r\n```python\r\n        class A(torch.jit.ScriptModule):\r\n            def __init__(self):\r\n                super(A, self).__init__()\r\n\r\n            @torch.jit.script_method\r\n            def forward(self, x):\r\n                return x + self.whatisgoingon\r\n\r\n        class B(A):\r\n            def __init__(self):\r\n                super(B, self).__init__()\r\n            @torch.jit.script_method\r\n            def bar(self, x):\r\n                return x * x \r\n        \r\n        A()\r\n```\r\nit does\r\n```\r\nRuntimeError: \r\nattribute 'whatisgoingon' does not exist:\r\n@torch.jit.script_method\r\ndef forward(self, x):\r\n    return x + self.whatisgoingon\r\n               ~~~~~~~~~~~~~~~~~~ <--- HERE\r\n\r\n```\r\n\r\nI added a test in `test_jit.py` as well."}
{"number": 20780, "title": "Fixes a bug when incorrect number of bytes are copied.", "time": "2019-05-21T22:12:16Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20780 Fixes a bug when incorrect number of bytes are copied.**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15440923/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20656 [pt1][quant] int_repr for different quantized types&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15398134/)\n\n`memcpy`'s third argument is a count of bytes to copy. When using non-byte length data, it might cause an error.\n\nDifferential Revision: [D15440923](https://our.internmc.facebook.com/intern/diff/D15440923/)"}
{"number": 20781, "title": "python2 imp.find_module cannot find .egg packages", "time": "2019-05-21T22:27:19Z", "body": "This is fixed in python3 import logic following [PEP0302](https://www.python.org/dev/peps/pep-0302/) but in python2 relies on `imp.find_module` to find dependencies. This affects `torch.hub`.\r\nWe will have to work around it using https://stackoverflow.com/questions/28962344/imp-find-module-which-supports-zipped-eggs?lq=1. \r\nI will send a fix. "}
{"number": 20782, "title": "Workaround python2.7 find_module limitation / explicitly close file", "time": "2019-05-21T23:15:43Z", "body": "fix #20781 #20757 \r\nhmm I don't know an easy way to add a test to make sure it runs against a package installed as .egg. But i tested it locally with torchvision. \r\n"}
{"number": 20783, "title": "Add back at::_copy_from for use by XLA", "time": "2019-05-21T23:19:52Z", "body": "XLA needs a way to override CPUTensor.copy_(XLATensor), but we only\r\ndispatch on the \"self\" argument. This inverts the dispatch order when\r\n\"src\" is an unhandled type.\r\n\r\nNote that things like XLATensor.copy_(CPUTensor) never enter this\r\nimplementation.\r\n\r\ncc @dlibenzi\r\n"}
{"number": 20784, "title": "[ONNX] Support tuples in ScriptModule inputs/outputs", "time": "2019-05-21T23:36:25Z", "body": "- [x] Add tests after https://github.com/pytorch/pytorch/pull/20256 is merged\r\n\r\n- Support exporting ScriptModule with inputs/outputs of arbitrarily constructed tuples.\r\n\r\n- Moved the assigning of output shapes to after graph conversion to ONNX is completed. By then all tuples in the IR has already been lowered by the pass ```_jit_pass_lower_all_tuples```. If assigning output shapes is required to happen before that, we'll need to hand parse the tuple structures in the graph, and repeat the same logic in ```_jit_pass_lower_all_tuples```. Handling inputs is easier because all tuple information is encoded within the input tensor type.\r\n\r\n- Swap the order of ```_jit_pass_lower_all_tuples``` and ```_jit_pass_erase_number_types```. Ops like ```prim::TupleIndex``` relies on index being a scalar. ```_jit_pass_erase_number_types``` will convert these kind of scalars to tensors."}
{"number": 20785, "title": "grid_sample is not aligned", "time": "2019-05-21T23:55:01Z", "body": "`F.interpolate` has been changed to use `align_corners=False` by default, which is also the pixel model used by opencv/PIL, etc.\r\n\r\nThe input to `grid_sample` is in range `[-1, 1]`, and therefore should ideally be scale-invariant.\r\nHowever, `F.grid_sample` still uses the old pixel model: https://github.com/pytorch/pytorch/blob/65b00aa5972e23b2a70aa60dec5125671a3d7153/aten/src/ATen/native/cuda/GridSampler.cu#L161-L162, and as a result it is not properly aligned under the scaling of `F.interpolate(align_corners=False)`. \r\n\r\nExamples:\r\n\r\n```python\r\nimport torch\r\nfrom torch.nn import functional as F\r\n\r\ndef grid_sample_v2(input, grid):\r\n    # grid: [-1, 1]\r\n    N, C, H, W = input.shape\r\n    gridx = grid[:, :, :, 0]\r\n    gridy = grid[:, :, :, 1]\r\n    gridx = ((gridx + 1) / 2 * W - 0.5) / (W - 1) * 2 - 1\r\n    gridy = ((gridy + 1) / 2 * H - 0.5) / (H - 1) * 2 - 1\r\n    newgrid = torch.stack([gridx, gridy], dim=-1)\r\n    return F.grid_sample(input, newgrid)\r\n\r\nN = 500\r\ninput = torch.rand(N, 1, 32, 74)\r\ninput2 = F.interpolate(input, scale_factor=5, align_corners=False, mode='bilinear')\r\n\r\ngrid = torch.rand(N, 18, 17, 2) * 1.8 - 0.9  # use coordinates in -0.9, 0.9 to avoid boundary effects\r\n\r\noutput = F.grid_sample(input, grid)\r\noutput2 = F.grid_sample(input2, grid)\r\ndiff = torch.abs(output2 - output)\r\nprint(\"DIFFv1:\", diff.mean())\r\n\r\noutput = grid_sample_v2(input, grid)\r\noutput2 = grid_sample_v2(input2, grid)\r\ndiff = torch.abs(output2 - output)\r\nprint(\"DIFFv2:\", diff.mean())\r\n```\r\nThe above code, when run with `align_corners=True`, prints:\r\n```\r\nDIFFv1: tensor(0.0032)\r\nDIFFv2: tensor(0.0675)\r\n```\r\nwhen run with `align_corners=False`, prints:\r\n```\r\nDIFFv1: tensor(0.0689)\r\nDIFFv2: tensor(5.3741e-07)\r\n```\r\nIn the example, I provide a new implementation of `grid_sample`, which leads to smaller reconstruction error when `align_corners=False`. Given that `align_corners=False` is now the default for `interpolate`, similar options should be built into `grid_sample` as well."}
{"number": 20786, "title": "Add ability to filter metric schema in LayerModelHelper", "time": "2019-05-22T00:47:03Z", "body": "Summary: Add a method to LayerModelHelper to filter metrics_schema. A general model builder may add metric schema that is not needed in some situations. This change add the ability to skip those unneeded.\n\nDifferential Revision: D15418140\n\n"}
{"number": 20787, "title": "Some minor fix to unblock the Bert model quantization", "time": "2019-05-22T00:50:31Z", "body": "Summary:\nSet requires_grad=False for bias: this will block the jit tracing.\nThe as_type fix: The input tensor shape and output tensor shape will be different, which will trigger the assertion failure at https://fburl.com/0m8xy7tc.\n\nReviewed By: jamesr66a\n\nDifferential Revision: D15445092\n\n"}
{"number": 20788, "title": "terminate called after throwing an instance of 'c10::Error'", "time": "2019-05-22T00:54:24Z", "body": "## üêõ Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.\r\n1.\r\n1.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux):\r\n - How you installed PyTorch (`conda`, `pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"}
{"number": 20789, "title": "terminate called after throwing an instance of 'c10", "time": "2019-05-22T00:54:49Z", "body": "## ‚ùì Questions and Help\r\n\r\n### Please note that this issue tracker is not a help form and this issue will be closed.\r\n\r\nWe have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:\r\n\r\n- [Discussion Forum](https://discuss.pytorch.org/)\r\n"}
{"number": 20790, "title": "[pt1][quant][fix] fbgemm precision argument", "time": "2019-05-22T01:20:20Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20790 [pt1][quant][fix] fbgemm precision argument**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15445903/)\n\natt\n\nDifferential Revision: [D15445903](https://our.internmc.facebook.com/intern/diff/D15445903/)"}
{"number": 20791, "title": "[jit][fix] Add len to OrderedDict types", "time": "2019-05-22T02:15:16Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20791 [jit][fix] Add len to OrderedDict types**\n* #20735 [fix] Fix up repr on scripted modules\n\n"}
{"number": 20792, "title": "libtorch terminate called after throwing an instance of 'c10::Error'", "time": "2019-05-22T02:36:50Z", "body": "## üêõ Bug\r\nI use the libtorch(version 1.1.0) inference faceboxes(https://github.com/XiaXuehai/faceboxes),it can normally generate faceboxes.pt model,but when I inference the model  faceboxes.pt ,it report some error.my code:\r\n`from networks import FaceBox\r\nfrom encoderl import DataEncoder\r\nimport torch\r\nimport numpy as np\r\nimport torch.nn.functional as F\r\nimport cv2\r\nimport time\r\n\r\nimport torchvision\r\nim=cv2.imread(\"/home/tao/useful/faceboxes_tensorrt_cuda/pytorch/picture/22.jpg\")\r\nh, w, _ = im.shape\r\ndim_diff = np.abs(h - w)\r\npad1, pad2 = dim_diff // 2, dim_diff - dim_diff // 2\r\npad = ((pad1, pad2), (0, 0), (0, 0)) if h <= w else ((0, 0), (pad1, pad2), (0, 0))\r\ninput_img = np.pad(im, pad, 'constant', constant_values=128)\r\ninput_img = cv2.resize(input_img,(1024,1024))/255\r\nnet = FaceBox()\r\nuse_gpu = torch.cuda.is_available()\r\nif use_gpu:\r\n   net.cuda()\r\nnet.load_state_dict(torch.load('weight/faceboxes.pt', map_location=lambda storage, loc:storage), strict=False) \r\ndata_encoder = DataEncoder()\r\nim_tensor = torch.from_numpy(input_img.transpose((2,0,1))).float()\r\nverbose=True,input_names=input_names,output_names=output_names)\r\ntraced_script_module = torch.jit.trace(net, im_tensor.unsqueeze(0).cuda())\r\ntraced_script_module.save(\"faceboxes.pt\")\r\n`\r\nthe libtorch code:\r\n`#include <torch/script.h>\r\n#include <opencv2/highgui/highgui.hpp>\r\n#include <opencv2/imgproc/imgproc.hpp>\r\n#include <opencv2/opencv.hpp>\r\n#include <cmath>\r\n#include <iostream>\r\n#include <memory>\r\n#include <string>\r\n#include <vector>\r\n\r\n#define kIMAGE_SIZE 1024\r\n#define kCHANNELS 3\r\n\r\n\r\nbool LoadImage(std::string file_name, cv::Mat &image) {\r\n  image = cv::imread(file_name);  // CV_8UC3\r\n  if (image.empty() || !image.data) {\r\n    return false;\r\n  }\r\n  cv::cvtColor(image, image, CV_BGR2RGB);\r\n  std::cout << \"== image size: \" << image.size() << \" ==\" << std::endl;\r\n\r\n  // scale image to fit\r\n  cv::Size scale(kIMAGE_SIZE, kIMAGE_SIZE);\r\n  cv::resize(image, image, scale);\r\n  std::cout << \"== simply resize: \" << image.size() << \" ==\" << std::endl;\r\n\r\n  // convert [unsigned int] to [float]\r\n  image.convertTo(image, CV_32FC3, 1.0f / 255.0f);\r\n\r\n  return true;\r\n}\r\nint main(int argc, const char *argv[]) {\r\n\r\n  std::string modelpath=\"/home/tao/libtorch/example/faceboxes_pytorch_c++/pytorch/faceboxes.pt\";\r\n  std::shared_ptr<torch::jit::script::Module> module =\r\n      torch::jit::load(modelpath);\r\n  std::cout << \"== Switch to GPU mode\" << std::endl;\r\n  // to GPU\r\n  module->to(at::kCUDA);\r\n\r\n  assert(module != nullptr);\r\n\r\n  std::string file_name = \"/home/tao/libtorch/example/faceboxes_pytorch_c++/pytorch/picture/22.jpg\";\r\n  cv::Mat image;\r\n  if (LoadImage(file_name, image)) {\r\n      auto input_tensor = torch::from_blob(image.data, {1, kIMAGE_SIZE, kIMAGE_SIZE, kCHANNELS});\r\n      input_tensor = input_tensor.permute({0, 3, 1, 2});\r\n      input_tensor[0][0] = input_tensor[0][0];\r\n      input_tensor[0][1] = input_tensor[0][1];\r\n      input_tensor[0][2] = input_tensor[0][2];\r\n\r\n      // to GPU\r\n      input_tensor = input_tensor.to(at::kCUDA);\r\n      std::cout<<\"it is here \"<<std::endl;\r\n      torch::Tensor out_tensor = module->forward({input_tensor}).toTensor();\r\n      std::cout<<out_tensor<<std::endl;\r\n  }\r\n}\r\n`\r\nerrors:\r\n`terminate called after throwing an instance of 'c10::Error'\r\n  what():  isTensor() ASSERT FAILED at /home/tao/libtorch/include/ATen/core/ivalue.h:205, please report a bug to PyTorch. (toTensor at /home/tao/libtorch/include/ATen/core/ivalue.h:205)\r\nframe #0: std::function<std::string ()>::operator()() const + 0x11 (0x7f9a840d4441 in /home/tao/libtorch/lib/libc10.so)\r\nframe #1: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x2a (0x7f9a840d3d7a in /home/tao/libtorch/lib/libc10.so)\r\nframe #2: c10::IValue::toTensor() && + 0x1f0 (0x40f300 in /home/tao/libtorch/example/build-faceboxes_pytorch_c++-Desktop_Qt_5_7_0_GCC_64bit-Default/facebox)\r\nframe #3: main + 0x66a (0x40abfa in /home/tao/libtorch/example/build-faceboxes_pytorch_c++-Desktop_Qt_5_7_0_GCC_64bit-Default/facebox)\r\nframe #4: __libc_start_main + 0xf0 (0x7f9a20c67830 in /lib/x86_64-linux-gnu/libc.so.6)\r\nframe #5: _start + 0x29 (0x40b139 in /home/tao/libtorch/example/build-faceboxes_pytorch_c++-Desktop_Qt_5_7_0_GCC_64bit-Default/facebox)`\r\n\r\n##CMakeLists.txt:\r\ncmake_minimum_required(VERSION 2.8)\r\nproject(faceboxes)\r\nset(CMAKE_BUILD_TYPE \"Debug\")\r\nSET(CMAKE_CXX_FLAGS ${CMAKE_CXX_FLAGS} \"-std=c++11 -O3\")\r\nset(Torch_DIR /home/tao/libtorch/share/cmake/Torch)\r\nset(OpenCV_DIR /home/tao/opencv-3.4.2/build)\r\nfind_package(OpenCV REQUIRED)\r\nfind_package(Torch REQUIRED)\r\ninclude_directories( ${OpenCV_INCLUDE_DIRS} )\r\nadd_executable(facebox prediction.cpp)\r\ntarget_link_libraries(facebox ${OpenCV_LIBS} ${TORCH_LIBRARIES})\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0):1.1.0\r\n - OS (e.g., Linux):ubuntu 16.04\r\n - How you installed PyTorch (`conda`, `pip`, source): pip3\r\n - Build command you used (if compiling from source):no\r\n - Python version:python 3.5\r\n - CUDA/cuDNN version: 9.0/9.2\r\n - GPU models and configuration: GeForce GTX 930MX 2GB\r\n \r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"}
{"number": 20793, "title": "Update Convolution.cpp", "time": "2019-05-22T02:43:45Z", "body": "it just strikes to me that the computation process of gI under non-transposed conv is different from the comments above?\r\nI just make it consistent with the code.\r\n\r\n"}
{"number": 20794, "title": "pytorch1.1.0 windows than one operator \" \" matches these operands", "time": "2019-05-22T03:05:14Z", "body": "When I compile the c++ extension on windows with pytorch1.1.0. I get the following error.\r\nHow to resolve the question? Thanks!\r\nE:/Program Files/Python35/lib/site-packages/torch/include\\THC/THCNumerics.cuh(190 than one operator \"<\" matches these operands:\r\n            built-in operator \"arithmetic < arithmetic\"\r\n            function \"operator<(const __half &, const __half &)\"\r\n            operand types are: c10::Half < c10::Half\r\n\r\nE:/Program Files/Python35/lib/site-packages/torch/include\\THC/THCNumerics.cuh(191 than one operator \"<=\" matches these operands:\r\n            built-in operator \"arithmetic <= arithmetic\"\r\n            function \"operator<=(const __half &, const __half &)\"\r\n            operand types are: c10::Half <= c10::Half\r\n\r\nE:/Program Files/Python35/lib/site-packages/torch/include\\THC/THCNumerics.cuh(192 than one operator \">\" matches these operands:\r\n            built-in operator \"arithmetic > arithmetic\"\r\n            function \"operator>(const __half &, const __half &)\"\r\n            operand types are: c10::Half > c10::Half\r\n\r\nE:/Program Files/Python35/lib/site-packages/torch/include\\THC/THCNumerics.cuh(193 than one operator \">=\" matches these operands:\r\n            built-in operator \"arithmetic >= arithmetic\"\r\n            function \"operator>=(const __half &, const __half &)\"\r\n            operand types are: c10::Half >= c10::Half\r\n\r\nE:/Program Files/Python35/lib/site-packages/torch/include\\THC/THCNumerics.cuh(194 than one operator \"==\" matches these operands:\r\n            built-in operator \"arithmetic == arithmetic\"\r\n            function \"operator==(const __half &, const __half &)\"\r\n            operand types are: c10::Half == c10::Half\r\n\r\nE:/Program Files/Python35/lib/site-packages/torch/include\\THC/THCNumerics.cuh(196 than one operator \"!=\" matches these operands:\r\n            built-in operator \"arithmetic != arithmetic\"\r\n            function \"operator!=(const __half &, const __half &)\"\r\n            operand types are: c10::Half != c10::Half"}
{"number": 20795, "title": "[jit] Add start and step parameters for range in torchscript", "time": "2019-05-22T03:05:28Z", "body": "Fixes #18440\r\n\r\nI calculate a derived index from `start,stop,step` as `start + step*index`. When `start=0` and `step=1` (the defaults/`range(n)`), this is the same behavior as before.\r\n\r\nUnluckily, it seems that we do not optimize out operations like `x*1` or `x+0`. That means that we're doing lots of redundant operations when we don't need to. EDIT: More specifically, it seems like we only do this optimization for (tensor, scalar): https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/passes/peephole.cpp#L128 \r\n\r\nThe most annoying part of this code is calculating the number of iterations, given `start, stop, step`. I ended up going with the formula `(abs(stop-start) + abs(step)-1)//abs(step)`. Other intuitively appealing formulas like `(stop-start + step -1)//step` don't work for negative numbers.\r\n\r\nI tried using `SymbolicVariable` for the calculations, but it seems that `symbolicvariable` only outputs ops for `tensors`, not the integers we have. "}
{"number": 20796, "title": "[jit] allow pass ordered dict for nn sequential", "time": "2019-05-22T05:03:21Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20796 [jit] allow pass ordered dict for nn sequential**\n\nDifferential Revision: [D15505330](https://our.internmc.facebook.com/intern/diff/D15505330)"}
{"number": 20797, "title": "Modify cos to cosh in Vec256", "time": "2019-05-22T05:14:11Z", "body": "Minor typo fix."}
{"number": 20798, "title": "Version 1.1.0 get \"No grad accumulator for a saved leaf!\" in ScriptModule", "time": "2019-05-22T05:45:18Z", "body": "## üêõ Bug\r\n\r\nWhen using LayerNorm in ScriptModule in version 1.1.0, I get \"RuntimeError: No grad accumulator for a saved leaf!\". But it's OK in 1.0.1.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nimport torch\r\nfrom torch.nn import LayerNorm, Linear\r\nfrom torch.jit import ScriptModule, script_method\r\n\r\nclass Test(ScriptModule):\r\n    def __init__(self, dim):\r\n        super().__init__()\r\n        self.layer_norm = LayerNorm(dim)\r\n        self.projection = Linear(dim, dim)\r\n    \r\n    @script_method\r\n    def forward(self, inputs):\r\n        return self.layer_norm(inputs + self.projection(inputs))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    m = Test(512)\r\n    input_tensor = torch.randn((10, 11, 512))\r\n    output_tensor = m(input_tensor)\r\n    output_tensor.sum().backward()\r\n```\r\n\r\n## Expected behavior\r\n\r\nGet \"RuntimeError: No grad accumulator for a saved leaf!\" in 1.1.0. But it's OK in 1.0.1,or with torch.jit removed in 1.1.0.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 6.5.0-2ubuntu1~18.04) 6.5.0 20181026\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 980 Ti\r\nGPU 1: GeForce GTX TITAN X\r\n\r\nNvidia driver version: 390.116\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.4\r\n\r\nVersions of relevant libraries:\r\n[pip3] msgpack-numpy==0.4.3.1\r\n[pip3] numpy==1.16.3\r\n[pip3] numpydoc==0.8.0\r\n[pip3] torch==1.1.0\r\n[pip3] torchfile==0.1.0\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.3                      199  \r\n[conda] mkl-service               1.1.2            py37he904b0f_5  \r\n[conda] mkl_fft                   1.0.10           py37ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] pytorch                   1.1.0           py3.7_cuda9.0.176_cudnn7.5.1_0    pytorch\r\n[conda] pytorch-pretrained-bert   0.6.2                    pypi_0    pypi\r\n[conda] torchvision               0.2.2                      py_3    pytorch"}
{"number": 20799, "title": "Add support for save and load mkldnn modules", "time": "2019-05-22T06:08:08Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20820 Add mkldnn sigmoid operator&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15455866/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20800 Enable torch.jit.trace for mkldnn modules&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15447892/)\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20799 Add support for save and load mkldnn modules**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15447891/)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/20799\n\nDifferential Revision: [D15447891](https://our.internmc.facebook.com/intern/diff/D15447891/)"}
{"number": 20800, "title": "Enable torch.jit.trace for mkldnn modules", "time": "2019-05-22T06:08:14Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20820 Add mkldnn sigmoid operator&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15455866/)\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20800 Enable torch.jit.trace for mkldnn modules**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15447892/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20799 Add support for save and load mkldnn modules&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15447891/)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/20800\n\nDifferential Revision: [D15447892](https://our.internmc.facebook.com/intern/diff/D15447892/)"}
{"number": 20801, "title": "Add warning about memory overhead when using multiple tiny tensors", "time": "2019-05-22T06:34:55Z", "body": "added note in docs regarding #19408"}
{"number": 20802, "title": "Exposing LengthsSum in pytorch", "time": "2019-05-22T07:56:12Z", "body": "Summary: Need this for sequence model\n\nDifferential Revision: D15448529\n\n"}
{"number": 20803, "title": "Add empty check for InferenceLSTM", "time": "2019-05-22T09:09:04Z", "body": "Summary: Throw an exception when the input to the InferenceLSTM layer is empty instead of crashing\n\nDifferential Revision: D15448784\n\n"}
{"number": 20804, "title": "expected ) but found 'ident' here: quantized::fake_quantize_per_tensor_affine_forward", "time": "2019-05-22T10:57:21Z", "body": "## üêõ Bug\r\n\r\nGot an error after transition from cuda 9.0 to 10.1.  The error occurs when I call methods from shared library built with cmake.   \r\n\r\n## To Reproduce\r\n \r\nSteps to reproduce the behavior:\r\n\r\n```\r\ncmake_minimum_required(VERSION 3.5.1)\r\nproject(java_torch_lib)\r\n\r\nset(PROJECT_VERSION_MAJOR 0)\r\nset(PROJECT_VERSION_MINOR 0)\r\nset(PROJECT_VERSION_PATCH 0)\r\n\r\nset(CMAKE_CXX_STANDARD 14)\r\n\r\nfind_package(Torch REQUIRED)\r\n\r\nmessage (STATUS \"TORCH_LIBRARIES=${TORCH_LIBRARIES}\")\r\nmessage (STATUS \"TORCH_INCLUDE_DIRS=${TORCH_INCLUDE_DIRS}\")\r\n\r\ninclude_directories(${TORCH_INCLUDE_DIRS})\r\n\r\nfind_package(JNI)\r\n\r\nif (${JNI_FOUND})\r\n    message (STATUS \"JNI_INCLUDE_DIRS=${JNI_INCLUDE_DIRS}\")\r\n    message (STATUS \"JNI_LIBRARIES=${JNI_LIBRARIES}\")\r\nendif()\r\n\r\ninclude_directories(${JNI_INCLUDE_DIRS})\r\n\r\nset (LIB_NAME ${PROJECT_NAME}${PROJECT_VERSION_MAJOR})\r\nadd_library(${LIB_NAME} SHARED jnijavacpp.cpp java_torch_lib.cpp)\r\ntarget_link_libraries(${LIB_NAME}  \"${TORCH_LIBRARIES}\")\r\ntarget_link_libraries(${LIB_NAME} ${JNI_LIBRARIES})\r\n```\r\n\r\nTHE ERROR:\r\n\r\nWhen I execute any method from the builded ${LIB_NAME}\r\n\r\n```\r\nterminate called after throwing an instance of 'std::runtime_error'\r\n  what():  expected ) but found 'ident' here:\r\nquantized::fake_quantize_per_tensor_affine_forward(Tensor X, float scale, int zero_point, int num_bits = 8, int quant_delay = 0, int iter = 0) -> Tensor\r\n                                                                                                            ~~~ <--- HERE\r\n```\r\n\r\n## Expected behavior\r\n\r\nno error \r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.168\r\nGPU models and configuration: \r\nGPU 0: Tesla V100-DGXS-16GB\r\nGPU 1: Tesla V100-DGXS-16GB\r\nGPU 2: Tesla V100-DGXS-16GB\r\nGPU 3: Tesla V100-DGXS-16GB\r\n\r\nNvidia driver version: 418.67\r\ncuDNN version: /usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudnn.so.7.5.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.3\r\n[pip3] torch==1.1.0\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] Could not collect\r\n\r\n\r\n"}
{"number": 20805, "title": "LibTorch :About torch.jit.trace generate model.pt", "time": "2019-05-22T11:11:10Z", "body": "## üêõ Bug\r\n\r\nI use libtorch inference faceboxes,when I test generated model ,I have a problem.\r\n\r\n1.torch.save(net1.state_dict(), \\'net_params.pth\\')  \r\n2.torch.save(net1, \\'net.pth\\') \r\nThe first way save the model,libtorch inference the model ,it make Incorrect result,the second way can make correct result. why?\r\n\r\n\r\n\r\n\r\n - PyTorch Version (e.g., 1.0):1.1.0\r\n - OS (e.g., Linux):ubuntu 16.04\r\n - How you installed PyTorch (`conda`, `pip`, source):pip\r\n - Build command you used (if compiling from source):no\r\n - Python version:python3.5\r\n - CUDA/cuDNN version:9.0/9.2\r\n - GPU models and configuration:gtx 930mx\r\n - Any other relevant information:\r\n\r\n"}
{"number": 20806, "title": "Stabilize gradient for unfolded Tensor", "time": "2019-05-22T13:05:08Z", "body": "## üöÄ Feature\r\n\r\nI propose a `stabilize_grad` flag for the `Tensor.unfold` method.\r\n\r\n## Motivation\r\n\r\nImagine you want to calculate a loss between different patches of two images. Consider the following snippet:\r\n```python\r\nheight = 8\r\nwidth = 8\r\nsize = 3\r\nstep = 2\r\n\r\ninput_unstabilized = torch.ones(1, 1, height, width).requires_grad_(True)\r\ntarget = torch.zeros(1, 1, height, width).detach()\r\n\r\ninput_unstabilized_patches = input_unstabilized\r\ntarget_patches = target\r\nfor dim in range(2, 4):\r\n    input_unstabilized_patches = input_unstabilized_patches.unfold(dim, size, step)\r\n\r\n    target_patches = target_patches.unfold(dim, size, step)\r\n\r\nloss = torch.sum((input_unstabilized_patches - target_patches) ** 2.0)\r\nloss.backward()\r\nprint(input_unstabilized.grad[0, 0, :, :])\r\n```\r\nThis results in:\r\n```\r\ntensor([[2., 2., 4., 2., 4., 2., 2., 0.],\r\n        [2., 2., 4., 2., 4., 2., 2., 0.],\r\n        [4., 4., 8., 4., 8., 4., 4., 0.],\r\n        [2., 2., 4., 2., 4., 2., 2., 0.],\r\n        [4., 4., 8., 4., 8., 4., 4., 0.],\r\n        [2., 2., 4., 2., 4., 2., 2., 0.],\r\n        [2., 2., 4., 2., 4., 2., 2., 0.],\r\n        [0., 0., 0., 0., 0., 0., 0., 0.]])\r\n```\r\nAs you can see, some elements receive a higher gradient update, simply because they appear in multiple patches. This effect might lead to unwanted results.\r\n\r\n## Pitch\r\n\r\nWith `stabilize_grad=True` the `unfold` method should track how often each element appears in a patch and in the backward pass average the gradient with this number.\r\n\r\n## Alternatives\r\n\r\nRight now I achieve this functionality as follows:\r\n```python\r\nclass _UnfoldGradStabilizer(torch.autograd.Function):\r\n    @staticmethod\r\n    def forward(ctx, input, dim, size, step):\r\n        ctx.needs_stabilizing = step < size\r\n        if ctx.needs_stabilizing:\r\n            stabilizer = torch.zeros_like(input)\r\n            item = [slice(None) for _ in range(input.dim())]\r\n            for idx in range(0, stabilizer.size()[dim] - size, step):\r\n                item[dim] = slice(idx, idx + size)\r\n                stabilizer[item].add_(1.0)\r\n\r\n            # clamping to avoid zero division\r\n            ctx.save_for_backward(torch.clamp(stabilizer, min=1.0))\r\n        return input\r\n\r\n    @staticmethod\r\n    def backward(ctx, grad_output):\r\n        if ctx.needs_stabilizing:\r\n            stabilizer, = ctx.saved_tensors\r\n            grad_input = grad_output / stabilizer\r\n        else:\r\n            grad_input = grad_output.clone()\r\n        return grad_input, None, None, None\r\n\r\n\r\nunfold_grad_stabilizer = _UnfoldGradStabilizer.apply\r\n\r\n\r\ndef stable_unfold(input, dim, size, step):\r\n    return unfold_grad_stabilizer(input, dim, size, step).unfold(dim, size, step)\r\n\r\n\r\ninput_stabilized = torch.ones(1, 1, height, width).requires_grad_(True)\r\n\r\ninput_stabilized_patches = input_stabilized\r\nfor dim in range(2, 4):\r\n    input_stabilized_patches = stable_unfold(input_stabilized_patches, dim, size, step)\r\n\r\nloss = torch.sum((input_stabilized_patches - target_patches) ** 2.0)\r\nloss.backward()\r\nprint(input_stabilized.grad[0, 0, :, :])\r\n```\r\nThis results in\r\n```\r\ntensor([[2., 2., 2., 2., 2., 2., 2., 0.],\r\n        [2., 2., 2., 2., 2., 2., 2., 0.],\r\n        [2., 2., 2., 2., 2., 2., 2., 0.],\r\n        [2., 2., 2., 2., 2., 2., 2., 0.],\r\n        [2., 2., 2., 2., 2., 2., 2., 0.],\r\n        [2., 2., 2., 2., 2., 2., 2., 0.],\r\n        [2., 2., 2., 2., 2., 2., 2., 0.],\r\n        [0., 0., 0., 0., 0., 0., 0., 0.]])\r\n```\r\n\r\nI have not yet timed itin a larger context, but I suspect the creation of the `stabilizer` with a `for` loop in every forward pass might be slow. If you can think of a better way, please let me know.\r\n"}
{"number": 20807, "title": "Revert \"Split ATen/Parallel into interface and backend (#20057)\", \"Use AT_INTERNAL_ASSERT in test_base (#20555)\"", "time": "2019-05-22T14:40:30Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20807 Revert \"Split ATen/Parallel into interface and backend (#20057)\", \"Use AT_INTERNAL_ASSERT in test_base (#20555)\"**\n\nThis reverts commit c4a3b4d528db2e2a5c1c466e61fb740ee6ce0b03.\nThis reverts commit 70eb315da441701e0026e2127c466cf8eef4d005."}
{"number": 20808, "title": "rename Half.x to be clearer", "time": "2019-05-22T14:57:37Z", "body": "Seems like it would be clearer to use a more descriptive name for the variable holding the underlying data in Half.h. Open to other suggestions.\r\n"}
{"number": 20809, "title": "[TEST] Stop installing torchvision", "time": "2019-05-22T15:38:18Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20809 [TEST] Stop installing torchvision**\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>"}
{"number": 20810, "title": "Make torchvision install chatty.", "time": "2019-05-22T15:40:18Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20810 Make torchvision install chatty.**\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>"}
{"number": 20811, "title": "Pin torchvision version.", "time": "2019-05-22T15:45:56Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20811 Pin torchvision version.**\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nDifferential Revision: [D15779416](https://our.internmc.facebook.com/intern/diff/D15779416)"}
{"number": 20812, "title": "Update version strings to 1.2", "time": "2019-05-22T16:03:36Z", "body": ""}
{"number": 20813, "title": "Dictionarize check_inputs coming from `trace`", "time": "2019-05-22T17:21:55Z", "body": ""}
{"number": 20814, "title": "10% difference noticed between jit and python model", "time": "2019-05-22T17:52:35Z", "body": "## üêõ Bug\r\n\r\nWe recently develop a jit test for torch.nn.functional.multi_head_attention_forward function. The test fails due to the numerical discrepancy between the jit version and the python version. \r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Check out the branch in PR (https://github.com/pytorch/pytorch/pull/20653)\r\n2. Run the unit test \"python test/test_jit.py TestScript.test_torchscript_multi_head_attn\"\r\n3. (Optional) check the rel. error by printout.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nThe jit version and python version are expected to generate very close results.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0a0+8f9f7ed\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.2.88\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.88\r\nGPU models and configuration:\r\nGPU 0: Quadro GP100\r\nGPU 1: Quadro GP100\r\n\r\nNvidia driver version: 410.79\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.15.4\r\n[pip] numpydoc==0.8.0\r\n[pip] torch==1.1.0a0+8f9f7ed\r\n[conda] blas                      1.0                         mkl\r\n[conda] magma-cuda90              2.5.0                         1    pytorch\r\n[conda] mkl                       2019.1                      144\r\n[conda] mkl-include               2019.3                      199\r\n[conda] mkl-service               1.1.2            py37he904b0f_5\r\n[conda] mkl_fft                   1.0.6            py37hd81dba3_0\r\n[conda] mkl_random                1.0.2            py37hd81dba3_0\r\n[conda] torch                     1.1.0a0+8f9f7ed           dev_0    <develop>"}
{"number": 20815, "title": "Convert Tree to use intrusive_ptr instead of shared_ptr.", "time": "2019-05-22T17:52:53Z", "body": ""}
{"number": 20816, "title": "Allow both Variables and Tensors in c10 kernel interface", "time": "2019-05-22T18:01:35Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20816 Allow both Variables and Tensors in c10 kernel interface**&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15453963/)\n\nPreviously, the c10 dispatcher expected ops to be called with Variables and unwrapped them to Tensors before calling into the kernel.\nThe kernel was expected to return Tensors that were re-wrapped into Variables before passing them on into the system.\n\nHowever, that doesn't work with kernels that call other operators. One recent example was a kernel that returned the result of `torch::ones()` as output.\nNow, with this diff, the c10 dispatcher still passes Tensors to the kernel and Variables back into the system, but it accepts ops to be called with both Tensors or Variables\nand kernels are also allowed to return either.\n\nAfter https://github.com/pytorch/pytorch/pull/17072 , we should be able to get rid of the whole wrapping/unwrapping logic.\n\nDifferential Revision: [D15453963](https://our.internmc.facebook.com/intern/diff/D15453963/)"}
{"number": 20817, "title": "Raise TypeError when the argument to isinf and isfinite is not a tensor", "time": "2019-05-22T18:06:29Z", "body": "Currently when the argument to isinf and isfinite is not tensor, a ValueError is raised. This, however, should be a TypeError, because the error is a type mismatch.\r\n\r\nIn the error message, \"str(tensor)\" is replaced by \"repr(tensor)\" because, when an error occurs, a printable representation of the object is likely more useful than the \"informal\" string version of the object.\r\n\r\n"}
{"number": 20818, "title": "awscli install is flaky", "time": "2019-05-22T18:13:28Z", "body": "If we fail to download, we end up with this error:\r\n\r\n```\r\nsudo pip -q install awscli==1.16.35\r\nException:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python2.7/dist-packages/pip/basecommand.py\", line 209, in main\r\n    status = self.run(options, args)\r\n  File \"/usr/lib/python2.7/dist-packages/pip/commands/install.py\", line 317, in run\r\n    requirement_set.prepare_files(finder)\r\n  File \"/usr/lib/python2.7/dist-packages/pip/req/req_set.py\", line 360, in prepare_files\r\n    ignore_dependencies=self.ignore_dependencies))\r\n  File \"/usr/lib/python2.7/dist-packages/pip/req/req_set.py\", line 512, in _prepare_file\r\n    finder, self.upgrade, require_hashes)\r\n  File \"/usr/lib/python2.7/dist-packages/pip/req/req_install.py\", line 273, in populate_link\r\n    self.link = finder.find_requirement(self, upgrade)\r\n  File \"/usr/lib/python2.7/dist-packages/pip/index.py\", line 442, in find_requirement\r\n    all_candidates = self.find_all_candidates(req.name)\r\n  File \"/usr/lib/python2.7/dist-packages/pip/index.py\", line 400, in find_all_candidates\r\n    for page in self._get_pages(url_locations, project_name):\r\n  File \"/usr/lib/python2.7/dist-packages/pip/index.py\", line 545, in _get_pages\r\n    page = self._get_page(location)\r\n  File \"/usr/lib/python2.7/dist-packages/pip/index.py\", line 648, in _get_page\r\n    return HTMLPage.get_page(link, session=self.session)\r\n  File \"/usr/lib/python2.7/dist-packages/pip/index.py\", line 757, in get_page\r\n    \"Cache-Control\": \"max-age=600\",\r\n  File \"/usr/share/python-wheels/requests-2.9.1-py2.py3-none-any.whl/requests/sessions.py\", line 480, in get\r\n    return self.request('GET', url, **kwargs)\r\n  File \"/usr/lib/python2.7/dist-packages/pip/download.py\", line 378, in request\r\n    return super(PipSession, self).request(method, url, *args, **kwargs)\r\n  File \"/usr/share/python-wheels/requests-2.9.1-py2.py3-none-any.whl/requests/sessions.py\", line 468, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/usr/share/python-wheels/requests-2.9.1-py2.py3-none-any.whl/requests/sessions.py\", line 597, in send\r\n    history = [resp for resp in gen] if allow_redirects else []\r\n  File \"/usr/share/python-wheels/requests-2.9.1-py2.py3-none-any.whl/requests/sessions.py\", line 195, in resolve_redirects\r\n    **adapter_kwargs\r\n  File \"/usr/share/python-wheels/requests-2.9.1-py2.py3-none-any.whl/requests/sessions.py\", line 576, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/usr/share/python-wheels/CacheControl-0.11.5-py2.py3-none-any.whl/cachecontrol/adapter.py\", line 46, in send\r\n    resp = super(CacheControlAdapter, self).send(request, **kw)\r\n  File \"/usr/share/python-wheels/requests-2.9.1-py2.py3-none-any.whl/requests/adapters.py\", line 376, in send\r\n    timeout=timeout\r\n  File \"/usr/share/python-wheels/urllib3-1.13.1-py2.py3-none-any.whl/urllib3/connectionpool.py\", line 610, in urlopen\r\n    _stacktrace=sys.exc_info()[2])\r\n  File \"/usr/share/python-wheels/urllib3-1.13.1-py2.py3-none-any.whl/urllib3/util/retry.py\", line 228, in increment\r\n    total -= 1\r\nTypeError: unsupported operand type(s) for -=: 'Retry' and 'int'\r\n```\r\n\r\nThis is due to pip version being old. We should probably just manually retry in this case"}
{"number": 20819, "title": "fix a couple of typos in README markdown", "time": "2019-05-22T18:21:40Z", "body": "was reading the README on github and came across a couple of typos.\r\n"}
{"number": 20820, "title": "Add mkldnn sigmoid operator", "time": "2019-05-22T19:15:17Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20820 Add mkldnn sigmoid operator**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15455866/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20800 Enable torch.jit.trace for mkldnn modules&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15447892/)\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20799 Add support for save and load mkldnn modules&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15447891/)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/20820\n\nDifferential Revision: [D15455866](https://our.internmc.facebook.com/intern/diff/D15455866/)"}
{"number": 20821, "title": "Specify dispatch key with kernel", "time": "2019-05-22T19:20:02Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:white_circle:&nbsp; #20887 New torch assertion macros&nbsp;&nbsp;[:yellow_heart:](https://our.internmc.facebook.com/intern/diff/D15484658/)\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20821 Specify dispatch key with kernel**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15455790/)\n\nChange registration API. Instead of\n\n    static auto registry = torch::RegisterOperators()\n      .op(\"my::op\", torch::RegisterOperators::options()\n        .kernel<Kernel>()\n        .dispatchKey(CPUTensorId()));\n\nit is now\n\n    static auto registry = torch::RegisterOperators()\n      .op(\"my::op\", torch::RegisterOperators::options()\n        .kernel<Kernel>(CPUTensorId()));\n\nThis binds kernel and dispatch key together, allowing them to be separate from other future configuration options like alias analysis or autograd wrappers.\n\nThe semantic problem behind this is that the dispatch key is a *kernel config parameter* and not an *operator config parameter* while things like autograd wrappers, alias info, and actually the kernel itself are *operator config parameters*. And while previously, the different kind of config parameters have been mixed, this diff now separates them.\n\nBefore this change, it wouldn't have been well defined if you specified a dispatchKey together with an autogradWrapper or aliasInfo for example.\n\n    // what is this supposed to do?\n    static auto registry = torch::RegisterOperators()\n      .op(\"my::op\", torch::RegisterOperators::options()\n        .aliasInfo(DEFAULT)\n        .dispatchKey(CPUTensorId()));\n\nIf we get more kernel config parameters in the future, we could introduce something like this\n\n    static auto registry = torch::RegisterOperators()\n      .op(\"my::op\", torch::RegisterOperators::options()\n        .kernel<Kernel>(torch::RegisterOperators::kernelOptions()\n            .dispatchKey(CPUTensorId())\n            .otherConfig());\n\nbut that's overkill as long as dispatch keys are the only kernel config parameter, and we can introduce that later without breaking backwards compatibility.\n\nA nice side effect of this is that people can register multiple kernels to the same operator in the same `.op()` call:\n\n    static auto registry = torch::RegisterOperators()\n      .op(\"my::op\", torch::RegisterOperators::options()\n        .kernel<Kernel1>(CPUTensorId())\n        .kernel<Kernel2>(CUDATensorId()));\n\nDifferential Revision: [D15455790](https://our.internmc.facebook.com/intern/diff/D15455790/)"}
{"number": 20822, "title": "[Proposal] Data reading framework for PyTorch (Hive, MySQL, S3 etc.) ", "time": "2019-05-22T19:36:25Z", "body": "At Facebook we are building a data reading framework for PyTorch which can efficiently read from data stores like Hive, MySQL, our internal blob store and any other tabular data sources. The framework allows for specifying complex input pipelines to read from different sources. For example if you have a table which stores handles for images, you can write SQL like code to read from the table, apply filters to select certain handles and then retrieve those handles from another data source with a few lines of code.\r\n\r\nIn addition to this, the framework supports running user-defined transforms which can either be pure python (ex: [torchvision.transforms](https://pytorch.org/docs/stable/torchvision/transforms.html)) or torchscript code. This framework can also be used with the [torch.distributed](https://pytorch.org/docs/stable/distributed.html) package to distribute the data across multiple nodes for training. The input pipeline that the user specifies can be defined once, serialized as a plan and run on multiple remote machines if required.\r\n\r\nThe framework builds upon the OSS dataloader and dataset framework. In particular it uses [IterableDataset](https://github.com/pytorch/pytorch/pull/19228) to provide a stream based interface for data retrieved from input pipelines.\r\n\r\nSample code to illustrate what reading and pre-processing images would look like:\r\n\r\n```\r\n# Hive table has columns handle and partition_no. The partition column \r\n# for the table is partition_no\r\ndf = data.data_warehouse(\"mynamespace\", \"mytable\")\r\n\r\n# Filter to partition the data across multiple workers.\r\npartition_filter = \"hash(partition_no) % {0} = {1}\".format(worker_info.num_workers, worker_id)\r\ndf = df.filter(partition_filter)\r\n\r\n# Fetch the handle from a blobstore\r\ndf = df.map([\"fetch_handle(handle) as img\"])\r\n\r\n# Rebatch the data\r\ndf = df.rebatch(batch_size=16)\r\n\r\n# transform_image is a user supplied function to run image transforms.\r\nds = MyDataset(df=df, transforms=transform_image)\r\ndl = torch.utils.data.DataLoader(ds)\r\n\r\nfor batch in dl:\r\n    pass\r\n```\r\nWe are evaluating whether it makes sense to open source this framework. For OSS users, this framework might be useful for training jobs which store large amount of data in Hive or S3 (images). Although, we would love to hear from the community whether this would be useful and also some use cases that might benefit from a framework like this.\r\n\r\n@dzhulgakov @jspisak @aartibasant @apaszke @SsnL "}
{"number": 20823, "title": "torch.Size is not pickleable in Python 2", "time": "2019-05-22T19:51:51Z", "body": "## üêõ Bug\r\n\r\ntorch.Size objects (returned by Tensor.shape) are not pickleable in Python 2.7. They can be pickled in Python 3.\r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport torch\r\nimport pickle\r\n\r\npickle.dumps(torch.randn(10).shape)  # TypeError: can't pickle Size objects\r\n```\r\n\r\n## Environment\r\n\r\nPyTorch 1.0.1.post2 (but I believe it also is present in master as of 5/22/19)\r\nPython 2.7.15 on Mac OS X"}
{"number": 20824, "title": "[jit] fix pow bug on overloads and clean up", "time": "2019-05-22T20:10:15Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#20824 [jit] fix pow bug on overloads and clean up**\n\nDifferential Revision: [D15458009](https://our.internmc.facebook.com/intern/diff/D15458009)"}
{"number": 20825, "title": "Resend \"Split ATen/Parallel into interface and backend\"", "time": "2019-05-22T20:41:04Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #20157 Support for Eigen thread pool\n* #20480 Native TBB parallel backend\n* #20454 Restore TBB module\n* #20087 Native ATen/Parallel backend\n* **#20825 Resend \"Split ATen/Parallel into interface and backend\"**\n* #20848 Fix init_thread calls in thread pool initialization\n\nSummary:\nResending https://github.com/pytorch/pytorch/pull/20057\n\nTest Plan:\nhttps://github.com/pytorch/pytorch/pull/20057\n\nDifferential Revision: [D15458073](https://our.internmc.facebook.com/intern/diff/D15458073)"}
{"number": 20826, "title": "add batch of string ops", "time": "2019-05-22T21:07:44Z", "body": "First batch of https://github.com/pytorch/pytorch/issues/20769, handles `isupper`, `islower`, `isdigit`, `isspace`, `isalnum`, `isalpha`, `upper`, `lower`"}
{"number": 20827, "title": "Cuda persistent softmax", "time": "2019-05-22T22:24:36Z", "body": "Adds persistent cuda kernels that speed up SoftMax applied over the fast dimension, i.e. torch.nn.Softmax(dim=-1) and torch.nn.LogSoftmax(dim=-1). When the size is <= 1024, this code is 2-10x faster than the current code, speedup is higher for smaller sizes. This code works for half, float and double tensors with 1024 or fewer elements in the fast dimension. Numerical accuracy is on par with the current code, i.e. relative error is ~1e-8 for float tensors and ~1e-17 for double tensors. Relative error was computed against the CPU code.\r\n\r\nThe attached image shows kernel time in us for torch.nn.Softmax(dim=-1) applied to a half precision tensor of shape [16384,n], n is plotted along the horizontal axis. Similar uplifts can be seen for the backward pass and for LogSoftmax.\r\n\r\n![image](https://user-images.githubusercontent.com/41591019/58212822-b63ebb00-7cb5-11e9-910d-1fc7d8585d58.png)\r\n"}
{"number": 20828, "title": "replaces torch.Size() inside with a tuple", "time": "2019-05-22T22:31:51Z", "body": "Summary: replaces torch.Size() inside with a tuple\n\nDifferential Revision: D15460726\n\n"}
{"number": 20829, "title": "fix the input/output type mismatch", "time": "2019-05-22T23:13:23Z", "body": "Summary: as title\n\nDifferential Revision: D15461937\n\n"}
{"number": 20830, "title": "[pt1][quant] Add torch.load/torch.save for QTensor", "time": "2019-05-22T23:42:57Z", "body": "Stack:\n&nbsp;&nbsp;&nbsp;&nbsp;:black_circle:&nbsp; **#20830 [pt1][quant] Add torch.load/torch.save for QTensor**&nbsp;&nbsp;[:green_heart:](https://our.internmc.facebook.com/intern/diff/D15340701/)\n\natt\n\nDifferential Revision: [D15340701](https://our.internmc.facebook.com/intern/diff/D15340701/)"}
{"number": 20831, "title": "[WIP] Test PR to showcase the comparison ops return type change", "time": "2019-05-23T00:16:43Z", "body": "Updated indexing for scalars. TODO: add tests \r\nAdded support for add for bool. TODO: add tests."}
{"number": 20832, "title": "Make LayerNorm.normalized_shape a tuple", "time": "2019-05-23T00:22:17Z", "body": "\n\nDifferential Revision: [D15464693](https://our.internmc.facebook.com/intern/diff/15464693/)"}
{"number": 20833, "title": "Fix oscillation in coalesceInsertedDataDependencies", "time": "2019-05-23T00:24:01Z", "body": "Summary: Att. The algorithm is still \"horrendously inefficient\". But since we are sunsetting Nomnigraph, I just did the minimal fix here.\n\nDifferential Revision: D15463880\n\n"}
{"number": 20834, "title": "Make LayerNorm.normalized_shape a tuple", "time": "2019-05-23T00:50:18Z", "body": "Some other info"}
{"number": 20835, "title": "Make LayerNorm.normalized_shape a tuple", "time": "2019-05-23T00:52:03Z", "body": "Some other info"}
{"number": 20836, "title": "Install torchvision from master", "time": "2019-05-23T00:55:42Z", "body": ""}
{"number": 20837, "title": "[feature request] a function to empty all the cuda buffer", "time": "2019-05-23T00:59:17Z", "body": "## üöÄ Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nAt present the pytorch process uses about 900MB after `lazy_init` cuda context. I was searching on internet whether the CUDA context can be shutdown, but seems like such API is not provided yet. Here I'm asking if we can do a `pytorch` side context clear, so the minimal CUDA memory should be allocated to pytorch runtime.\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nI'm developing an interesting function that each pytorch worker interacts with a scheduling server, dynamically moving the workload from/to GPU, so that the CUDA memory can be used for tasks with higher priority. \r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n```python\r\ntensor = torch.Tensor(100,100).cuda()  # lazy_init happens here\r\ndel tensor\r\ntorch.cuda.de_init()  # clear all the CUDA memory maintained by pytorch runtime\r\n```\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n"}
{"number": 20838, "title": "Make LayerNorm.normalized_shape a tuple", "time": "2019-05-23T00:59:49Z", "body": "Some other info"}
{"number": 20839, "title": "Make LayerNorm.normalized_shape a tuple", "time": "2019-05-23T01:00:38Z", "body": "Some other info"}
{"number": 20840, "title": "Make LayerNorm.normalized_shape a tuple", "time": "2019-05-23T01:01:36Z", "body": "Some other info\n\nDifferential Revision: [D15464788](https://our.internmc.facebook.com/intern/diff/15464788/)"}
{"number": 20841, "title": "Make LayerNorm.normalized_shape a tuple", "time": "2019-05-23T01:03:41Z", "body": "Some other info\n\nDifferential Revision: [D15464820](https://our.internmc.facebook.com/intern/diff/15464820/)"}
{"number": 20842, "title": "try to fix windows build by including ctype", "time": "2019-05-23T01:33:38Z", "body": "Fix windows build for std::isdigit etc, see if build fixes"}
{"number": 20843, "title": "torch.cuda.set_rng_state() followed by torch.cuda.get_rng_state() return different result", "time": "2019-05-23T03:15:38Z", "body": "## üêõ Bug\r\n\r\n```python\r\ntorch.cuda.set_rng_state(stored_state)\r\nretrieved_state = torch.cuda.get_rng_state() \r\n```\r\n\r\nstored_state and retrieved_state differ in 400 positions out of 824016 in:\r\npytorch 1.0.0\r\nCuda 10.0.130\r\nGeForce GTX 1070 Max-Q\r\n\r\nthey **are** the same in:\r\npytorch 1.1.0\r\nCuda 10.0.130\r\nTesla K80\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\n\\# contents of rng_test.sh\r\n```python\r\npython - <<EOF\r\nimport torch\r\ntorch.save(torch.cuda.get_rng_state(), 'cuda_rng_state.tch')\r\ntorch.save(torch.get_rng_state(), 'rng_state.tch')\r\nEOF\r\n\r\npython - <<EOF\r\nimport torch\r\ncuda_pre_state = torch.load('cuda_rng_state.tch')\r\npre_state = torch.load('rng_state.tch')\r\ntorch.cuda.set_rng_state(cuda_pre_state)\r\ntorch.set_rng_state(pre_state)\r\ncuda_cur_state = torch.cuda.get_rng_state()\r\ncur_state = torch.get_rng_state()\r\nprint('cuda state tensors differ in {} positions out of {}.'.format(\r\n    cuda_pre_state.ne(cuda_cur_state).sum(),\r\n    cuda_pre_state.nelement()))\r\nprint('cpu state tensors differ in {} positions out of {}.'.format(\r\n    pre_state.ne(cur_state).sum(),\r\n    pre_state.nelement()))\r\n\r\nEOF\r\n```\r\n$ . rng_test.sh \r\ncuda state tensors differ in **400** positions out of **824016**.\r\ncpu state tensors differ in **0** positions out of **5048**.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nI would expect the states to be the same.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\nThis is the environment failing the test:\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 6.5.0-2ubuntu1~18.04) 6.5.0 20181026\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 1070 with Max-Q Design\r\nNvidia driver version: 410.104\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.2\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.15.4\r\n[pip] torch==1.0.0\r\n[pip] torchvision==0.2.1\r\n[conda] blas                      1.0                         mkl  \r\n[conda] cuda100                   1.0                           0    pytorch\r\n[conda] mkl                       2018.0.3                      1  \r\n[conda] mkl_fft                   1.0.6            py36h7dd41cf_0  \r\n[conda] mkl_random                1.0.1            py36h4414c95_1  \r\n[conda] pytorch                   1.0.0           py3.6_cuda10.0.130_cudnn7.4.1_1  [cuda100]  pytorch\r\n[conda] torchvision               0.2.1                      py_2    pytorch\r\n(pytorch) henry@henry-gs65:~$ \r\n\r\n## Additional context\r\n\r\nIn this environment, the test works properly.\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Debian GNU/Linux 9.9 (stretch)\r\nGCC version: (Debian 6.3.0-18+deb9u1) 6.3.0 20170516\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: Tesla K80\r\nNvidia driver version: 410.72\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] intel-numpy==1.15.1\r\n[pip3] numpy==1.15.1\r\n[pip3] torch==1.1.0\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.3                      199  \r\n[conda] mkl-service               1.1.2            py37he904b0f_5  \r\n[conda] mkl_fft                   1.0.10           py37ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] pytorch                   1.1.0           py3.7_cuda10.0.130_cudnn7.5.1_0    pytorch\r\n[conda] torchvision               0.2.2                      py_3    pytorch\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"}
{"number": 20844, "title": "possible slip on cudnn version?", "time": "2019-05-23T03:26:47Z", "body": "## üêõ Bug\r\n\r\nI think I may have just had a problem related to cudnn version.  Did a `conda install` with ` cudatoolkit=9.0`; CPU-only code was working (simple conv+mlp), GPU code was running but failing to learn; `torch.backends.cudnn.version()` returned `7501`, but I had `7003` in my `/usr/local/cuda/` (and cuda 9.0 installed).  Since upgrading that to `7501`, GPU code is learning fine.  :)\r\n\r\nNothing in my `.bashrc` was pointing to any specific cuda directory, but from the environment printout below, it's linking to my pre-installed cudnn,  despite the conda install supposedly including everything?  (Wish I had seen your `collect_env.py` previously!)\r\n\r\nThis was confusing and took a long time to figure out--maybe would be helpful to have an extra check in there somewhere on cudnn version?\r\n\r\nThanks!\r\n-Adam\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.3 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080\r\nGPU 1: GeForce GTX 1080\r\n\r\nNvidia driver version: 384.130\r\ncuDNN version: /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.3\r\n[pip] torch==1.1.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.3                      199  \r\n[conda] mkl_fft                   1.0.12           py37ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] pytorch                   1.1.0           py3.7_cuda9.0.176_cudnn7.5.1_0    pytorch\r\n\r\n"}

{"number": 90531, "title": "[LTC] Make DeviceContextArena protected", "time": "2022-12-09T09:01:32Z", "body": "Summary:\r\nThis patch makes DeviceContextArena protected such that XLAGraphExecutor can reuse it. In addition, it makes all methods that utilize DeviceContextArena virtual such that XLAGraphExecutor can override them to provide its own DeviceContextArena.\r\n\r\nTest Plan:\r\nCI.\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90529, "title": "Update to ROCm5.4", "time": "2022-12-09T06:34:57Z", "body": "cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport", "label": [{"id": 1078897659, "node_id": "MDU6TGFiZWwxMDc4ODk3NjU5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20rocm", "name": "module: rocm", "color": "f7e101", "default": false, "description": "AMD GPU support for Pytorch"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90528, "title": "Completely redo how ShapeEnv guards are generated", "time": "2022-12-09T05:17:11Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #90528\r\n* #90381\r\n* #90295\r\n\r\nInstead of inferring shape mappings from a bunch of data structures that were plumbed in InstructionTranslator, we instead work out mappings by just iterating over the GraphArgs and mapping symbols to arguments as they show up. If multiple argument sizes/strides/offset map to the same symbol, this means they are duck sized, so we also generate extra equality tests that they must be equal. Finally, we generate 0/1 specialization guards. The resulting code is much shorter, and I think also easier to understand.\r\n\r\nTODO: Delete all the tensor ref tracking code, it's unnecessary\r\n\r\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\r\n\r\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90524, "title": "fix some building warnings", "time": "2022-12-09T04:32:13Z", "body": null, "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 90523, "title": "[FSDP] Allow nested FSDP wrapper to use different mixed precision", "time": "2022-12-09T04:24:12Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90523\n\nThe main change is to move `args` and `kwargs` dtype convertion\nfrom `_root_pre_forward` to `_pre_forward`, so that every\nFSDP has a chance to apply its own precision.", "label": [{"id": 3769221312, "node_id": "LA_kwDOA-j9z87gqcDA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(fsdp)", "name": "release notes: distributed (fsdp)", "color": "C1F551", "default": false, "description": "release notes category"}]}
{"number": 90522, "title": "fix cudnn RNN reproducibility problem", "time": "2022-12-09T03:56:44Z", "body": "Fixes #74177\r\n\r\nSince RNN code use static variables to cache state, we store a flag in RNG generator to notify new seed changes. And that generator is used to generate seed for RNN.  This may be ugly but it is the best way currently with out large code refactoring", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 90519, "title": "[Release only change] Turn off failing tests in release", "time": "2022-12-09T02:04:04Z", "body": "These tests where OFF during the release however later where enabled.\r\nFollowing issues where created to turn this test off:\r\n#77338\r\n#77337\r\n#77305\r\n", "label": [{"id": 3769210091, "node_id": "LA_kwDOA-j9z87gqZTr", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20python_frontend", "name": "release notes: python_frontend", "color": "87E01C", "default": false, "description": "release notes category"}]}
{"number": 90518, "title": "[inductor] Enable test_nn and test_torch", "time": "2022-12-09T01:53:14Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90518\n* #90494\n\nSummary: Skipping failures in those tests so that CI can guard other\npassing cases.", "label": [{"id": 3769207236, "node_id": "LA_kwDOA-j9z87gqYnE", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20nn", "name": "release notes: nn", "color": "29E07F", "default": false, "description": "release notes category"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90517, "title": "Move _test_inductor_realize into python", "time": "2022-12-09T01:18:13Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90517\n\nAddresses https://github.com/pytorch/pytorch/pull/90014/files#r1043625932\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90512, "title": "Add alltoall_ to CommTensor", "time": "2022-12-09T00:02:18Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90512\n\nThis PR adds alltoall_ to the CommTensor", "label": [{"id": 3769218061, "node_id": "LA_kwDOA-j9z87gqbQN", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(c10d)", "name": "release notes: distributed (c10d)", "color": "EE86D4", "default": false, "description": "release notes category"}]}
{"number": 90510, "title": "[reland] add save and load stats in memory_tracker", "time": "2022-12-08T23:54:54Z", "body": "reland https://github.com/pytorch/pytorch/pull/90144, this PR removed temporary path \"memory.trace\" in the unit test", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90508, "title": "Introduce causal mask", "time": "2022-12-08T23:43:49Z", "body": "Summary: Introduce causal mask\n\nTest Plan: sandcastle & github ci/cd\n\nDifferential Revision: D41723137\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}]}
{"number": 90505, "title": "Fix non-existing parameters in docstrings", "time": "2022-12-08T22:45:28Z", "body": "Continuation after https://github.com/pytorch/pytorch/pull/90163.\r\n\r\nHere is a script I used to find all the non-existing arguments in the docstrings (the script can give false positives in presence of *args/**kwargs or decorators):\r\n\r\n_Edit:_ \r\nI've realized that the indentation is wrong for the last `break` in the script, so the script only gives output for a function if the first docstring argument is wrong. I'll create a separate PR if I find more issues with corrected script.\r\n\r\n``` python\r\nimport ast\r\nimport os\r\nimport docstring_parser\r\n\r\nfor root, dirs, files in os.walk('.'):\r\n    for name in files:\r\n        if root.startswith(\"./.git/\") or root.startswith(\"./third_party/\"):\r\n            continue\r\n        if name.endswith(\".py\"):\r\n            full_name = os.path.join(root, name)\r\n            with open(full_name, \"r\") as source:\r\n                tree = ast.parse(source.read())\r\n                for node in ast.walk(tree):\r\n                    if isinstance(node, ast.FunctionDef):\r\n                        all_node_args = node.args.args\r\n                        if node.args.vararg is not None:\r\n                            all_node_args.append(node.args.vararg)\r\n                        if node.args.kwarg is not None:\r\n                            all_node_args.append(node.args.kwarg)\r\n                        if node.args.posonlyargs is not None:\r\n                            all_node_args.extend(node.args.posonlyargs)\r\n                        if node.args.kwonlyargs is not None:\r\n                            all_node_args.extend(node.args.kwonlyargs)\r\n                        args = [a.arg for a in all_node_args]\r\n                        docstring = docstring_parser.parse(ast.get_docstring(node))\r\n                        doc_args = [a.arg_name for a in docstring.params]\r\n                        clean_doc_args = []\r\n                        for a in doc_args:\r\n                            clean_a = \"\"\r\n                            for c in a.split()[0]:\r\n                                if c.isalnum() or c == '_':\r\n                                    clean_a += c\r\n                            if clean_a:\r\n                                clean_doc_args.append(clean_a)\r\n                        doc_args = clean_doc_args\r\n                        for a in doc_args:\r\n                            if a not in args:\r\n                                print(full_name, node.lineno, args, doc_args)\r\n                            break\r\n\r\n```", "label": [{"id": 1447309924, "node_id": "MDU6TGFiZWwxNDQ3MzA5OTI0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/better-engineering", "name": "better-engineering", "color": "94f76a", "default": false, "description": "Relatively self-contained tasks for better engineering contributors"}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}, {"id": 4726472156, "node_id": "LA_kwDOA-j9z88AAAABGbg93A", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20AO%20frontend", "name": "release notes: AO frontend", "color": "ededed", "default": false, "description": null}]}
{"number": 90502, "title": "[Dynamo] Support access nn.Module keys", "time": "2022-12-08T22:14:44Z", "body": "Fixes https://github.com/pytorch/torchdynamo/issues/1973\r\n\n\ncc @mlazos @soumith @voznesenskym @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90501, "title": "Serialize dynamo/inductor config for minifier", "time": "2022-12-08T22:10:41Z", "body": "Fixes https://github.com/pytorch/torchdynamo/issues/1965\r\n\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}, {"id": 4705034454, "node_id": "LA_kwDOA-j9z88AAAABGHEg1g", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20dynamo", "name": "release notes: dynamo", "color": "b60205", "default": false, "description": ""}]}
{"number": 90496, "title": "[not for land] debug state_dict", "time": "2022-12-08T21:31:17Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90496\n* #90436\n\n", "label": [{"id": 3769221312, "node_id": "LA_kwDOA-j9z87gqcDA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(fsdp)", "name": "release notes: distributed (fsdp)", "color": "C1F551", "default": false, "description": "release notes category"}]}
{"number": 90494, "title": "[inductor][Reland] Use decomposition for _to_copy", "time": "2022-12-08T21:21:51Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #90518\n* __->__ #90494\n\nSummary: also contains a fix for https://github.com/pytorch/pytorch/issues/89633\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90493, "title": "Respect the global std option if set.", "time": "2022-12-08T20:31:52Z", "body": "This target-level setting is probably not needed anyway since caffe2/CMakeLists.txt is the only one using it and it does not create additional project.\r\nBut not a bad idea to be explicit either.\r\nThis change redirects it to use the global setup.\r\n\r\nBackground:\r\nStatic constexpr member `caffe2::TensorProto::UNDEFINED` in `caffe2/onnx/backend.cc` and `caffe2/onnx/onnx_exporter.cc` seems to be ODR-used.\r\nBy C++14, it has to be exported, but C++17 relaxed that requirement.\r\nIf we set C++17 at top level but 14 here, on CentOS 7 + gcc-9 (devtoolset-9), this leads to undefined references.\r\n\r\nRelated PR:\r\n- https://github.com/pytorch/pytorch/pull/75519\r\n\r\nThis was originally opened since early 2022 but got ignored:\r\n- https://github.com/pytorch/pytorch/pull/75729\r\n\r\nRebased to master with C++17.", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 90489, "title": "Automated submodule update: FBGEMM", "time": "2022-12-08T19:21:10Z", "body": "This is an automated pull request to update the first-party submodule for [pytorch/FBGEMM](https://github.com/pytorch/FBGEMM).\n\nNew submodule commit: https://github.com/pytorch/FBGEMM/commit/81ba6c51ecece07190601a09e8d88a04d0581dbc\n\nTest Plan: Ensure that CI jobs succeed on GitHub before landing.", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90483, "title": "Use some `if constexpr` in the code", "time": "2022-12-08T18:18:58Z", "body": "As PyTorch is C++17 project now. Replace `c10::guts::if_constexpr` with `if constexpr`\r\n\r\nDeliberately delaying changes in headers until at least one nightly\r\ncycle is complete.\r\n\r\n\r\n\r\ncc @VitalyFedyunin @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90482, "title": "[inductor] Rewrite Triton templates + epilogue fusion", "time": "2022-12-08T17:57:48Z", "body": "Test Plan: Tests added, run torchbench with `TORCHINDUCTOR_MAX_AUTOTUNE=1`\n\nDifferential Revision: D41474473\n\n\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 4443556792, "node_id": "LA_kwDOA-j9z88AAAABCNtLuA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/skip-pr-sanity-checks", "name": "skip-pr-sanity-checks", "color": "BED7D5", "default": false, "description": ""}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90478, "title": "[pthreadpool] Don't recreate threadpool if the counts are same", "time": "2022-12-08T16:56:05Z", "body": "Summary: Don't do anything if the incoming count and current threadpool size are same\n\nTest Plan: CI\n\nReviewed By: salilsdesai\n\nDifferential Revision: D41628132\n\n", "label": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false, "description": ""}, {"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}]}
{"number": 90477, "title": "[stateless] add weight tying support", "time": "2022-12-08T16:27:51Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90477\n* #90476\n\n", "label": [{"id": 3769207236, "node_id": "LA_kwDOA-j9z87gqYnE", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20nn", "name": "release notes: nn", "color": "29E07F", "default": false, "description": "release notes category"}, {"id": 3773059130, "node_id": "LA_kwDOA-j9z87g5FA6", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20deprecation", "name": "topic: deprecation", "color": "2824A8", "default": false, "description": "topic category"}]}
{"number": 90476, "title": "[stateless] fix functional call docs", "time": "2022-12-08T16:25:21Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #90477\n* __->__ #90476\n\n", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769207236, "node_id": "LA_kwDOA-j9z87gqYnE", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20nn", "name": "release notes: nn", "color": "29E07F", "default": false, "description": "release notes category"}, {"id": 3773063595, "node_id": "LA_kwDOA-j9z87g5GGr", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20documentation", "name": "topic: documentation", "color": "B2E89B", "default": false, "description": "topic category"}]}
{"number": 90475, "title": "Disable BUILD_CAFFE2 from ONNX builds", "time": "2022-12-08T16:22:36Z", "body": "Fixes https://github.com/microsoft/onnx-converters-private/issues/132\r\n\r\n@kit1980 and @malfet agreed in disabling ONNX tests for Caffe2 builds.\r\nWith this change, exporting models with `operator+export_type=ONNX_ATEN_FALLBACK` will properly test non-caffe2 builds, which is the only scenario for aten fallback after caffe2 deprecation\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90474, "title": "[dynamo] Get GPU names without calling nvidia-smi", "time": "2022-12-08T16:16:08Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90474\n* #90473\n* #90472\n\n\n\nBelieve it or not, inductor can sometimes be used on machines that\nhave CUDA GPUs but no nvidia-smi.  Let's use torch APIs instead of subprocess.\n\nDifferential Revision: [D41841930](https://our.internmc.facebook.com/intern/diff/D41841930/)\n\nDifferential Revision: [D41841930](https://our.internmc.facebook.com/intern/diff/D41841930)", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90473, "title": "[pt2] Reset dynamo log level when exiting inductor debug context", "time": "2022-12-08T16:16:02Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #90474\n* __->__ #90473\n* #90472\n\n\n\nWhen entering an inductor debug context we increase the log level of\ndynamo; I guess this makes sense, since if we're debugging inductor, and\ninductor calls into dynamo, we probably want visibility into what dynamo is\ndoing.\n\nBut when we exit that context, we probably want to go back to whatever level of\ndynamo-specific logging was in place before.  Dynamo generates lots of debug\ninfo (guards, bytecode), and it's a lot to sift through if you're not\nspecifically interested in it.\n\nDifferential Revision: [D41841879](https://our.internmc.facebook.com/intern/diff/D41841879/)\n\nDifferential Revision: [D41841879](https://our.internmc.facebook.com/intern/diff/D41841879)", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90472, "title": "Emit torch.cuda.synchronize() after every kernel call in inductor", "time": "2022-12-08T16:15:56Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #90474\n* #90473\n* __->__ #90472\n\n\n\nDebugging illegal memory access is hard; even CUDA_LAUNCH_BLOCKING=1\nand using C10_CUDA_KERNEL_LAUNCH_CHECK doesn't guarantee a useful stack trace.\ndoesn't necessarily guarantee that you'll get a stack trace pointing to the\nright kernel.  This diff adds a config option to force a CUDA synchronize after\nevery kernel call in inductor, for debugging those tricky cases.\n\nDifferential Revision: [D41744967](https://our.internmc.facebook.com/intern/diff/D41744967/)\n\nDifferential Revision: [D41744967](https://our.internmc.facebook.com/intern/diff/D41744967)", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90470, "title": "Fixed output memory format mismatch for bicubic2d", "time": "2022-12-08T15:50:51Z", "body": "Description:\r\n\r\n- output memory format is matching input for bicubic2d\r\n\r\nProblem: output tensor's memory format does not match input format for bicubic2d\r\n\r\n```python\r\nimport torch\r\n\r\ni = torch.rand(1, 3, 32, 32).contiguous(memory_format=torch.channels_last)\r\nassert i.is_contiguous(memory_format=torch.channels_last)\r\no = torch.nn.functional.interpolate(i, size=(4, 4), mode=\"bicubic\")\r\nassert o.is_contiguous(memory_format=torch.channels_last), f\"Should be channels last but given channels first ({o.is_contiguous(memory_format=torch.contiguous_format)})\"\r\n\r\n> AssertionError: Should be channels last but given channels first (True)\r\n```\r\n\r\nRelated PR fixing bilinear ops: https://github.com/pytorch/pytorch/pull/53535 (cc @VitalyFedyunin @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @bdhirsh )\r\n\r\nDiscovered together with @NicolasHug while working on https://github.com/pytorch/pytorch/tree/interpolate_uint8_images_linear_cpu_support_dev\r\n\r\n- Updated code to match grad input / output memory formats\r\n- temporary tensor creation matches memory format in `separable_upsample_generic_Nd_kernel_impl`\r\n- Updated tests\r\n- Added missing forward AD support for bicubic with antialiasing\r\n", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3769207236, "node_id": "LA_kwDOA-j9z87gqYnE", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20nn", "name": "release notes: nn", "color": "29E07F", "default": false, "description": "release notes category"}]}
{"number": 90468, "title": "Stop populating real_value_cache eagerly", "time": "2022-12-08T15:26:31Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90468\n* #90442\n\nFixes https://github.com/pytorch/torchdynamo/issues/1950\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 3773061952, "node_id": "LA_kwDOA-j9z87g5FtA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20bug%20fixes", "name": "topic: bug fixes", "color": "A44870", "default": false, "description": "topic category"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}, {"id": 4705034454, "node_id": "LA_kwDOA-j9z88AAAABGHEg1g", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20dynamo", "name": "release notes: dynamo", "color": "b60205", "default": false, "description": ""}]}
{"number": 90463, "title": "Fix dynamo handling for tensor attributes: T, H, mT, mH", "time": "2022-12-08T12:17:39Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90463\n\r\nFixes https://github.com/pytorch/pytorch/issues/88843\r\n\r\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}, {"id": 4705034454, "node_id": "LA_kwDOA-j9z88AAAABGHEg1g", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20dynamo", "name": "release notes: dynamo", "color": "b60205", "default": false, "description": ""}]}
{"number": 90461, "title": "Make dynamo return unimplemented for ops whose out shape depends on the data", "time": "2022-12-08T11:24:19Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90461\n\n\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90457, "title": "[LTC] Make some LazyGraphExecutor private data structures protected", "time": "2022-12-08T07:18:39Z", "body": "Summary:\r\nThis pull request makes some LazyGraphExecutor private data structures protected such that XLAGraphExecutor can reuse them.\r\n\r\nHere is the list:\r\n1. DeviceLocker.\r\n2. DeviceLockerArena.\r\n3. DataCacheArena.\r\n\r\nIn addition, it also introduces LazyGraphExecutor::ResetTrimCounter() such that XLAGraphExecutor can reuse the trim counter.\r\n\r\nTest Plan:\r\nCI.\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90456, "title": "Populate Canonical Aten Ops (Batch 2)", "time": "2022-12-08T07:07:17Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90456\n\r\nacos\r\nargmax\r\nargmin\r\nacosh\r\nasinh\r\natanh\r\nasin\r\natan\r\nlogical_not\r\nlogical_and\r\nlogical_or\r\ncos\r\ncosh\r\nempty_strided\r\nfull\r\nisnan\r\nsin\r\nsinh\r\nscatter_reduce.two\r\nbitwise_xor.Tensor\r\nsign\r\nfmod.Tensor\r\nremainder.Tensor\r\npow.Tensor_Tensor\r\nis_inf\r\nne.Scalar\r\nne.Tensor\r\neq.Tensor\r\nge.Tensor\r\nle.Tensor\r\ngt.Tensor\r\nlt.Tensor\r\n", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90455, "title": "[fix] type-promotion : efficientzerotensor", "time": "2022-12-08T06:49:56Z", "body": "Fixes https://github.com/pytorch/pytorch/issues/90065\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 90452, "title": "Better __repr__ for ModuleList", "time": "2022-12-08T05:56:44Z", "body": "## Problem\r\nWhen models have a lot of complex repeated layers, `print(module)` output becomes unfeasible to work with. For example, current output of `__repr__` for `t5-small` is `715 ` lines long.\r\n\r\n## Solution\r\nUsing better `__repr__` it becomes `135`. For `t5-large`, current `__repr__` prints `1411` lines. Better `__repr__` — `135`. Same numer as for t5-small, because most of the layers are just repeated. For `EleutherAI/gpt-j-6B` number of lines reduces form `483` to just `24`.\r\n\r\nHere's how it works: when ModuleList items have exactly the same `__repr__` instead of printing both of them, it prints f`N x {repr(item)}`. Current code supports cases when the same ModuleList has multiple repeating items, which is especially useful when first/last layer of a block is different from the reset of them.\r\n\r\nBetter `__repr__` should make model prints smaller, more beautiful and significantly more useful by highlighting the difference between repeated blocks instead of losing it in a wall of text.\r\n\r\n## Motivating real-life example.\r\n\r\nYou can try it out in this [colab notebook](https://colab.research.google.com/drive/1PscpX_K1UemIDotl2raC4QMy_pTqDq7p?usp=sharing).\r\n\r\nCurrent `__repr__` of gpt-j-6b output it too big to add it to this PR description:\r\n```\r\nGPTJModel(\r\n  (wte): Embedding(50400, 4096)\r\n  (drop): Dropout(p=0.0, inplace=False)\r\n  (h): ModuleList(\r\n    (0): GPTJBlock(\r\n      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\r\n      (attn): GPTJAttention(\r\n        (attn_dropout): Dropout(p=0.0, inplace=False)\r\n        (resid_dropout): Dropout(p=0.0, inplace=False)\r\n        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\r\n        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\r\n        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\r\n        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\r\n      )\r\n      (mlp): GPTJMLP(\r\n        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\r\n        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\r\n        (act): NewGELUActivation()\r\n        (dropout): Dropout(p=0.0, inplace=False)\r\n      )\r\n    )\r\n    (1): GPTJBlock(\r\n      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\r\n      (attn): GPTJAttention(\r\n        (attn_dropout): Dropout(p=0.0, inplace=False)\r\n        (resid_dropout): Dropout(p=0.0, inplace=False)\r\n        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\r\n        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\r\n        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\r\n        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\r\n      )\r\n      (mlp): GPTJMLP(\r\n        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\r\n        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\r\n        (act): NewGELUActivation()\r\n        (dropout): Dropout(p=0.0, inplace=False)\r\n      )\r\n    )\r\n    (2): GPTJBlock(\r\n...\r\n```\r\n\r\nBetter `__repr__` output looks like this:\r\n```\r\nGPTJModel(\r\n  (wte): Embedding(50400, 4096)\r\n  (drop): Dropout(p=0.0, inplace=False)\r\n  (h): ModuleList(\r\n    28 x GPTJBlock(\r\n      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\r\n      (attn): GPTJAttention(\r\n        (attn_dropout): Dropout(p=0.0, inplace=False)\r\n        (resid_dropout): Dropout(p=0.0, inplace=False)\r\n        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\r\n        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\r\n        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\r\n        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\r\n      )\r\n      (mlp): GPTJMLP(\r\n        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\r\n        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\r\n        (act): NewGELUActivation()\r\n        (dropout): Dropout(p=0.0, inplace=False)\r\n      )\r\n    )\r\n  )\r\n  (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\r\n)\r\n```\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 90450, "title": "[dtensor] refactor dispatch and add cached propagator", "time": "2022-12-08T05:39:10Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90450\n* #90449\n\n", "label": [{"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 90449, "title": "[dtensor] delete unused torch_function", "time": "2022-12-08T05:38:59Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #90450\n* __->__ #90449\n\ntorch_function is not actually getting used yet today, deleting\nit first and we can revisit once we really need it", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 4766195119, "node_id": "LA_kwDOA-j9z88AAAABHBZdrw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(dtensor)", "name": "release notes: distributed (dtensor)", "color": "0e8a16", "default": false, "description": "release notes category"}]}
{"number": 90446, "title": "[NO COMMIT] Test https://github.com/pytorch/kineto/pull/704 on Windows", "time": "2022-12-08T04:45:33Z", "body": null, "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90444, "title": "Add dynamic shapes benchmark accuracy to CI", "time": "2022-12-08T04:07:12Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90444\n* #90443\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90442, "title": "SymIntify resize_", "time": "2022-12-08T03:52:55Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #90468\n* __->__ #90442\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769203231, "node_id": "LA_kwDOA-j9z87gqXof", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20composability", "name": "release notes: composability", "color": "A5282C", "default": false, "description": "release notes category"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90441, "title": "[Vulkan] Enable QInt8 weights and test quantized convolution with QInt8 weights and QInt32 bias", "time": "2022-12-08T03:45:23Z", "body": "Summary:\n- Enable convolution with QInt8 weights\n- Modify test_quantized_conv2d function to allow testing with QInt8 weights and QInt32 bias.\n- Added multiple tests for regular, depthwise and pointwise convolution with QInt8 weights and QInt32 bias.\n\nTest Plan:\nOn Mac\n```\ncd ~/fbsource\nbuck1 run -c pt.vulkan_full_precision=1 //xplat/caffe2:pt_vulkan_quantized_api_test_binAppleMac\\#macosx-arm64\n```\n\nOn Android\n```\ncd ~/fbsource\nbuck1 build -c ndk.custom_libcxx=false -c pt.enable_qpl=0 -c pt.vulkan_full_precision=1 //xplat/caffe2:pt_vulkan_quantized_api_test_binAndroid\\#android-arm64 --show-output\nadb push buck-out/gen/xplat/caffe2/pt_vulkan_quantized_api_test_binAndroid\\#android-arm64 /data/local/tmp/vulkan_quantized_api_test\nadb shell \"/data/local/tmp/vulkan_quantized_api_test\"\n```\n\nReviewed By: kimishpatel\n\nDifferential Revision: D41562053\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 3769206412, "node_id": "LA_kwDOA-j9z87gqYaM", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20vulkan", "name": "release notes: vulkan", "color": "23DD30", "default": false, "description": "release notes category"}]}
{"number": 90436, "title": "Adopt state_dict_pre_hook in FSDP", "time": "2022-12-08T02:12:22Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90436\n\r\nUse register_state_dict_pre_hook in FSDP to simplify state_dict implementations & remove hacks.\r\n\r\n", "label": [{"id": 3769221312, "node_id": "LA_kwDOA-j9z87gqcDA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(fsdp)", "name": "release notes: distributed (fsdp)", "color": "C1F551", "default": false, "description": "release notes category"}]}
{"number": 90434, "title": "[inductor] Fix edge case in stride matching", "time": "2022-12-08T01:25:06Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89877\n* __->__ #90434\n\nFor the example `index=371712*c0 + 352*c2 + 1`, `maybe_stride_vars`\nreturned `[371712, 1/c1, 352]` where `1 == (1/c1) * c1` technically matches the\npattern but is not the expected result.\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90432, "title": "[fx] Added better tests to pass infra", "time": "2022-12-08T01:06:44Z", "body": null, "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}]}
{"number": 90431, "title": "Only turn off caching allocator when PYTORCH_NO_CUDA_MEMORY_CACHING=1", "time": "2022-12-08T00:44:07Z", "body": "Differential Revision: D41799279\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}]}
{"number": 90430, "title": "[dynamo][ez] Store nn_module_stack in node directly", "time": "2022-12-08T00:41:16Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90430\n* #90429\n\nSummary:\natt\n\nTest Plan:\nNA, will be tested in downstream code\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90428, "title": "[MPS] Add Inverse op.", "time": "2022-12-08T00:17:17Z", "body": null, "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 4164781683, "node_id": "LA_kwDOA-j9z874PYZz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20mps", "name": "release notes: mps", "color": "1d76db", "default": false, "description": "Release notes category"}, {"id": 4417837777, "node_id": "LA_kwDOA-j9z88AAAABB1La0Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/mps", "name": "ciflow/mps", "color": "02EDA7", "default": false, "description": "Run MPS tests (subset of trunk)"}]}
{"number": 90427, "title": "[CUDA] Use accumulate type to improve accuracy of grid_sample on half precision inputs", "time": "2022-12-08T00:11:40Z", "body": "Fixes https://github.com/pytorch/pytorch/issues/89836\r\n\r\nThis PR changes the CUDA kernels of grid_sample 2d and 3d, forward, to use accumulate type to improve accuracy on half precision inputs.\r\n\r\nAlso, the backward error on grad with half input is in the order of 1e-4, unlike 1e2 in forward process. The backward kernels are thus unchanged.", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3769207236, "node_id": "LA_kwDOA-j9z87gqYnE", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20nn", "name": "release notes: nn", "color": "29E07F", "default": false, "description": "release notes category"}]}
{"number": 90425, "title": "[Inductor] GEMM Shape Padding Optimization", "time": "2022-12-07T23:51:18Z", "body": "Summary:\nOptimize the shape padding in the following perspectives:\n- Add BFloat16 support for AMP training and Float16 support for inference\n- Optimize microbenchmark to avoid peak memory issue, and include profiling memory ops to make more accurate decision\n- Turn off padding dims N and M in `torch.bmm` due to expensive memory copy of `.contiguous` to avoid peak memory issue\n\nTest Plan: CI\n\nDifferential Revision: D41724868\n\n\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90422, "title": "Log1p complex for CUDA", "time": "2022-12-07T23:08:21Z", "body": "Another pull request in the direction of solving #89205: log1p for complex numbers in CUDA.", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 90418, "title": "[pyfunctorch] Generate a more meaningful name for _SingleLevelAutogradFunction", "time": "2022-12-07T22:07:35Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90418\n* #90415\n* #90077\n* #90240\n* #90037\n* #89860\n* #89859\n* #89858\n\nThe API to do this is not pretty, but at least it works.\n\nTest Plan:\n- new test", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90417, "title": "Fakify params and weights under private config", "time": "2022-12-07T21:55:58Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90417\n\r\nPreviously, we planned to lift the parameters and weights while exporting and implement our own transformer to \"unlift\" the lifted weights and params back to the graph as attributes. But this is bit challenging because:\r\n\r\n- We need to maintain correct ordering for weights and parameters that are passed as inputs so that we know how to map them back. \r\n- Some weights are unused in the graph, so our transformer needs to be aware of which weights and parameters are not used in the graph. And we need to distinguish which are real user input and which are parameters. \r\n- There can be more edge cases we haven't seen in other models yet. \r\n\r\nI am aware that @Chillee  and @bdhirsh mentioned that functionalization won't work with fake-tensor attributes but this is fine for the short term as we don't expect users to be modifying weights and params in inference mode. In fact, we explicitly disable attribute mutation in torchdynamo export mode right now. \r\n\r\nGiven above condition, it might be ok to just fakify params when we need. I use a flag to guard against this change. \r\n", "label": [{"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90415, "title": "Fix circular import in torch/autograd/function.py", "time": "2022-12-07T21:34:42Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #90418\n* __->__ #90415\n* #90077\n* #90240\n* #90037\n* #89860\n* #89859\n* #89858\n\nIt turns out it is possible to break cycles by not directly importing a\nmodule:\n- there's a problem that torch.jit imports torch._ops and torch._ops\nimport torch.jit\n- there's another problem that torch.autograd.function imports\ncustom_function_call but torch._functorch.autograd_function imports\ntorch.autograd.function\n\nThe \"better\" way to handle all of this is to do some large refactoring so\nthat torch._functorch.autograd_function imports some file that has\n_SingleLevelAutogradFunction and then have torch.autograd.function\ndepend on torch.functorch.autograd_function... (and ditto for torch.jit\nvs torch._ops), but I'm scared to move code around too much for BC\nreasons and the fix in this PR works well.\n\nTest Plan:\n- import torch", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90413, "title": "[SDP] Fix alignment check for efficient_attention", "time": "2022-12-07T21:28:06Z", "body": "Fixes a bug found using head_dim_size==100 on an a100 gpu. This PR contains stricter guards on the input shape. These constraints are taken from xformers: https://github.com/facebookresearch/xformers/blob/gh/danthe3rd/60/orig/xformers/ops/fmha/cutlass.py#L23", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 4890079485, "node_id": "LA_kwDOA-j9z88AAAABI3iw_Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20multi-headed-attention", "name": "module: multi-headed-attention", "color": "bfdadc", "default": false, "description": ""}]}
{"number": 90410, "title": "TEMP", "time": "2022-12-07T21:07:16Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90410\n* #90336\n\n\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90409, "title": "[dynamo] constant fold cudnn.is_acceptable into graph", "time": "2022-12-07T20:39:49Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* __->__ #90409\n\nsee the implementation of [is_acceptable](https://github.com/pytorch/pytorch/blob/6dcc214ac273d594b8a9a30e1f90e30a3c1e40c8/torch/backends/cudnn/__init__.py#L87):\n\n```\ndef is_acceptable(tensor):\n    if not torch._C._get_cudnn_enabled():\n        return False\n    if tensor.device.type != 'cuda' or tensor.dtype not in CUDNN_TENSOR_DTYPES:\n        return False\n    if not is_available():\n        warnings.warn(\n            \"PyTorch was compiled without cuDNN/MIOpen support. To use cuDNN/MIOpen, rebuild \"\n            \"PyTorch making sure the library is visible to the build system.\")\n        return False\n    if not _init():\n        warnings.warn('cuDNN/MIOpen library not found. Check your {libpath}'.format(\n            libpath={\n                'darwin': 'DYLD_LIBRARY_PATH',\n                'win32': 'PATH'\n            }.get(sys.platform, 'LD_LIBRARY_PATH')))\n        return False\n    return True\n```\n\n- _get_cudnn_enabled: property of pytorch build\n- tensor.device.type and tensor.dtype are guarded by tensor_match guards\n- is_available(): property of pytorch build\n- _init(): run-once initialization of cudnn, the return value is a property of python build\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 3773061952, "node_id": "LA_kwDOA-j9z87g5FtA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20bug%20fixes", "name": "topic: bug fixes", "color": "A44870", "default": false, "description": "topic category"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}, {"id": 4705034454, "node_id": "LA_kwDOA-j9z88AAAABGHEg1g", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20dynamo", "name": "release notes: dynamo", "color": "b60205", "default": false, "description": ""}]}
{"number": 90408, "title": "Ignore shape inference exception from Caffe2 ATen fallback", "time": "2022-12-07T20:26:41Z", "body": "Fixes #87318\r\n", "label": [{"id": 693805995, "node_id": "MDU6TGFiZWw2OTM4MDU5OTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20onnx", "name": "module: onnx", "color": "f7e101", "default": false, "description": "Related to torch.onnx"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3768821874, "node_id": "LA_kwDOA-j9z87go6hy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20onnx", "name": "release notes: onnx", "color": "B4EB93", "default": false, "description": "torch.onnx related changes that should show up in the release notes"}]}
{"number": 90407, "title": "Workaround to let functionalization work with sparse tensors; fixes dlrm", "time": "2022-12-07T20:23:50Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90407\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>", "label": []}
{"number": 90406, "title": "add a device keyword argument", "time": "2022-12-07T20:22:44Z", "body": "fix issue #89054\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 90405, "title": "Add missing infer_size_symdimvector implementation.", "time": "2022-12-07T20:20:29Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90405\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769203231, "node_id": "LA_kwDOA-j9z87gqXof", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20composability", "name": "release notes: composability", "color": "A5282C", "default": false, "description": "release notes category"}, {"id": 3773061952, "node_id": "LA_kwDOA-j9z87g5FtA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20bug%20fixes", "name": "topic: bug fixes", "color": "A44870", "default": false, "description": "topic category"}]}
{"number": 90404, "title": "[Dynamo][TIMM][Benchmarks] Fix TIMM `0.8.0dev` breaking the `timm_models.py` script's data config", "time": "2022-12-07T20:03:02Z", "body": "It seems `0.8.0dev` breaks the current argument passing by expecting a dictionary instead of a namespace after https://github.com/rwightman/pytorch-image-models/commit/0dadb4a6e9e245c30653db2a48752423df98fa44\r\n\r\nCC @desertfire @ngimel \n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90403, "title": "[pruning][docs] Update README.md for structured pruning", "time": "2022-12-07T19:50:48Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90403\n\nSummary:\n\nI wrote a tutorial of how to use structured pruning flow as part of BE week\n\nTest Plan:\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90402, "title": "test meta kernel for quantize_per_tensor.tensor_qparams", "time": "2022-12-07T19:45:02Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}]}
{"number": 90396, "title": "[WIP] torch.func nested TOC documentation", "time": "2022-12-07T17:45:45Z", "body": "Fixes #ISSUE_NUMBER\r\n\n\ncc @svekars @carljparker", "label": [{"id": 897287230, "node_id": "MDU6TGFiZWw4OTcyODcyMzA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20docs", "name": "module: docs", "color": "f7e101", "default": false, "description": "Related to our documentation, both in docs/ and docblocks"}]}
{"number": 90390, "title": "[dynamo] Fix rst syntax for list", "time": "2022-12-07T16:13:55Z", "body": null, "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 90381, "title": "Revert guaranteed symint allocation", "time": "2022-12-07T14:57:57Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #90528\n* __->__ #90381\n* #90295\n\r\nSo, uh, I have a new strategy for generating dupe guards, one where I don't actually need to allocate symints for every tensor that is fakeified. So I'm reverting the changes I made from earlier PRs in this one.\r\n\r\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>", "label": [{"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}]}
{"number": 90380, "title": "[BE] Raise exception on unknown types in Caffe2Backend::CreateCast", "time": "2022-12-07T14:46:15Z", "body": "Rather than assigning dtype to unknown and then raising exception on condition, raise it during default case of switch statement that performs the conversion.\r\n", "label": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false, "description": ""}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90372, "title": "fix a memory leak on return without free", "time": "2022-12-07T09:31:42Z", "body": "This issue is found by static analysis. The allocated object by new may be leaked on early return.\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 90370, "title": "Initial Guard Env, duplicate tensor guards", "time": "2022-12-07T09:15:24Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90370\n* #90130\n* #90129\n\r\nAdds the guard env defined here: https://docs.google.com/document/d/1VbiXkIhKy744Q7Lx2e8UUBvP_5RH_WU0FEc3VadVX4U/edit#heading=h.bk4qqlofhsq\r\n\r\ncc @mlazos @soumith @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}, {"id": 4726472156, "node_id": "LA_kwDOA-j9z88AAAABGbg93A", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20AO%20frontend", "name": "release notes: AO frontend", "color": "ededed", "default": false, "description": null}]}
{"number": 90368, "title": "replace skipIf with xfailif", "time": "2022-12-07T08:28:24Z", "body": "Replace skips with xfails.", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90364, "title": "[Quant] Add fused conv2d_add_relu op for onednn backend", "time": "2022-12-07T07:35:48Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #90364\r\n* #90262\r\n* #90157\r\n\r\n**Summary**\r\nPost op fusion can reduce data movement overhead and improve inference performance. This PR adds fused conv2d_add_relu op for onednn backend, which will be used for int8 inference with onednn backend. Cannot call this op with other quantization backends otherwise an error is thrown.\r\n\r\n**Test Plan**\r\n```\r\npython -m pytest test_quantization.py::TestQuantizedConv\r\n```\r\n\r\n**TODO**\r\nThere is a oneDNN issue which may cause the kernel core dump with some input shapes. This PR should be merged after the issue resolved.\r\n\r\ncc @VitalyFedyunin @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 90359, "title": "[ao] making _is_activation_post_process private with BC", "time": "2022-12-07T05:29:34Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90359\n\nsame function in observer and quantize, consolidated to a\nsingle function\n\nnote: this is a recreation of D40709276 which caused severa breakages due to not maintaining BC for models with cached code with calls to the old function name\n\nDifferential Revision: [D41793604](https://our.internmc.facebook.com/intern/diff/D41793604/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D41793604/)!", "label": [{"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}]}
{"number": 90358, "title": "fix segfault for EmbeddingBag on CPU slow path when include_last_offset is true", "time": "2022-12-07T04:52:34Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* __->__ #90358\r\n\r\nThis PR is to fix the segfault reported at https://github.com/pytorch/pytorch/issues/89677, this is a `double free` issue caused by `invalid read`.\r\n\r\nThe reported issue broke at slow path for `EmbeddingBag` on float32, at [EmbeddingBag.cpp#L451](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/EmbeddingBag.cpp#L451)\r\n\r\nRoot cause is that `add_indices` has index which exceeds range of `output_data`, for the reported case.\r\n\r\nThe offsets are given as\r\n```\r\n{0,  6, 12, 15, 25, 32, 40, 42, 46, 53, 53}\r\n```\r\n\r\nThe `indices` has 55 elements and `offsets[-1] != indices.size(0)`.\r\n\r\nWhen `include_last_offset` is true, the `output` will be in the shape of {offsets.size(0) - 1, weight.sizes()[1]}, which will be {10, 5}.\r\nOriginally, `add_indices` will be (i re-arange the 1D tensor by rows, so here 10 rows in total)\r\n```\r\n### this is 55 elements\r\n  0 0 0 0 0 0\r\n  1 1 1 1 1 1\r\n  2 2 2\r\n  3 3 3 3 3 3 3 3 3 3\r\n  4 4 4 4 4 4 4\r\n  5 5 5 5 5 5 5 5\r\n  6 6\r\n  7 7 7 7\r\n  8 8 8 8 8 8 8\r\n  10 10\r\n```\r\nThe last row has index of 10 which is out of range of output tensor whose size is [10, 5].\r\n\r\nThe reason is `make_offset2bag` at [EmbeddingBag.cpp#L66](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/EmbeddingBag.cpp#L66) would give the following `offset2bag`:\r\n```\r\n### this is 55 + 1 elements:\r\n0 0 0 0 0 0 1\r\n0 0 0 0 0 1\r\n0 0 1\r\n0 0 0 0 0 0 0 0 0 1\r\n0 0 0 0 0 0 1\r\n0 0 0 0 0 0 0 1\r\n0 1\r\n0 0 0 1\r\n0 0 0 0 0 0 2\r\n0 0\r\n```\r\n\r\nNotice for index 53, it is added twice.\r\n\r\nThe fix is ignore the last index from `offsets` when `include_last_offset` is true, also this behavior aligns with CUDA, quote from https://github.com/pytorch/pytorch/pull/57208#issuecomment-1021727378", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 90356, "title": "Make Inductor accepts opaque callable", "time": "2022-12-07T04:17:57Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90356\n\r\nThis is an experiment to make Inductor accepts opaque callable. \r\nOpaque callable could used for custom op, kernels generated by another backend. \r\nOpaque callable should fallback, and wrapper codegen need handle forward the call the original callable. \r\n\r\nFor following fx graph, \r\n```\r\nclass <lambda>(torch.nn.Module):\r\n    def forward(self, arg0_1: f32[3, 3]):\r\n        # No stacktrace found for following nodes\r\n        sin: f32[3, 3] = inductor_test_torchinductor_opaque_func(arg0_1);  arg0_1 = None\r\n        mm: f32[3, 3] = torch.ops.aten.mm.default(sin, sin);  sin = None\r\n        return [mm]\r\n\r\n```\r\nInductor would generate\r\n```\r\ndef call(args):\r\n    arg0_1, = args\r\n    args.clear()\r\n    buf0 = opaque_func_8736875164326(arg0_1)\r\n    del arg0_1\r\n    buf1 = buf0\r\n    assert_size_stride(buf1, (3, 3), (3, 1))\r\n    del buf0\r\n    buf2 = empty_strided((3, 3), (3, 1), device='cpu', dtype=torch.float32)\r\n    aten.mm.out(buf1, buf1, out=buf2)\r\n    return (buf2, )\r\n```\r\n\r\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90355, "title": "[ao] fix incorrect integer cast on histogram observer bounds", "time": "2022-12-07T04:09:03Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #90355\r\n\r\nSummary: A cast to int was added in\r\nhttps://github.com/pytorch/pytorch/pull/45630 to make mypy not complain.\r\nHowever this leads to unexpected behavior where the histogram doesn't\r\nactually capture the full range of activation values.\r\n\r\nnote1: the test_histogram_observer_against_reference test was secretly\r\nbroken, on master. The random parameters that normally get run apparently don't cause a test failure but if you make a loop repeatedly run the test, it would\r\neventually fail. This was due to in some cases\r\nsum(<tensor>)!=torch.sum(<tensor>).item(). I was not able to reproduce\r\nthis with a toy example but running this test in a loop and editing\r\neither observer to print the calculation for 'total' would break the\r\ntest and show different behaviors. Fixing this test was necessary to\r\nland this PR since the changing histogram bounds changed things enough\r\nthat this test would error.\r\n\r\nnote2: updating histogram observer breaks some BC tests unless I regenerate the\r\nmodel using the HistogramObserver from this PR\r\n\r\nTest Plan: python test/test_quantization.py TestHistogramObserver.test_histogram_observer_correct_numel\r\n\r\npython test/test_quantization -k histogram\r\n\r\nReviewers:\r\n\r\nSubscribers:\r\n\r\nTasks:\r\n\r\nTags:\n\ncc @gujinghui @PenghuiCheng @XiaobingSuper @jianyuh @VitalyFedyunin @jgong5 @mingfeima @sanchitintel @ashokei @jingxu10 @min-jean-cho @yanbing-j @Guobing-Chen @Xia-Weiwen", "label": [{"id": 1303456553, "node_id": "MDU6TGFiZWwxMzAzNDU2NTUz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20mkldnn", "name": "module: mkldnn", "color": "f7e101", "default": false, "description": "Related to Intel IDEEP/MKL-DNN (mkldnn) integration"}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}, {"id": 3773061952, "node_id": "LA_kwDOA-j9z87g5FtA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20bug%20fixes", "name": "topic: bug fixes", "color": "A44870", "default": false, "description": "topic category"}]}
{"number": 90354, "title": "[Quant] onednn backend switch to ideep new api without affacting performance", "time": "2022-12-07T03:56:52Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #90354\r\n\r\n**Summary**\r\nOnednn quantization backend switch to new API in `third_party/ideep`. It won't impact correctness. Performance should be unaffected or slightly better.\r\nFBGEMM and QNNPACK backends not affected.\r\n\r\nPerformance results are given below.\r\n1. End-to-end performance of static quantized models (from torchvision)\r\n(throughput: fps, higher is better)\r\n![image](https://user-images.githubusercontent.com/12522207/206105879-45c59996-9804-4531-aa1f-dc962e6db5ab.png)\r\n\r\n2. Op benchmark of dynamic quantized linear\r\n(Latency: ms, lower is better)\r\n![image](https://user-images.githubusercontent.com/12522207/206124949-77352991-0fda-4285-a484-e20a5797262b.png)\r\n\r\nTest method & env:\r\n- Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz\r\n- Run multi-instances on a single node. Use one core for each instance.\r\n- Use Jemalloc and Intel OpenMP\r\n\r\n**Test plan**\r\npython test/test_quantization.py\r\n\r\ncc @VitalyFedyunin @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 ", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 90353, "title": "Fixes #90305, modify aten/src/ATen/core/ivalue.cpp & Dict_inl.h to have same hashable classes", "time": "2022-12-07T03:23:55Z", "body": "Fixes #90305, modify aten/src/ATen/core/ivalue.cpp & Dict_inl.h to have same hashable classes\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 90351, "title": "[Quant][fx][bc-breaking] Add simpler BackendConfig pattern format", "time": "2022-12-07T03:14:57Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90351\n* #90304\n\nSummary: The existing BackendConfig fusion pattern\nuses a \"reversed nested tuple\" format that is highly\nunintuitive. For example,\n```\nlinear-relu -> (nn.ReLU, nn.Linear)\nconv-bn-relu -> (nn.ReLU, (nn.BatchNorm2d, nn.Conv2d))\n```\nThis pattern format also complicates the signatures\nof the user specified \"fuser methods\", which needed\nto accept arguments in reverse nested order to match\nthe patterns:\n```\ndef fuse_linear_relu(is_qat, relu, linear):\n    ...\n\ndef fuse_conv_bn_relu(is_qat, relu, bn_conv):\n    (bn, conv) = bn_conv\n    ...\n```\nInstead, this commit introduces a new pattern format that\nsimply specifies the ops in forward order with no nesting:\n```\nlinear-relu -> (nn.Linear, nn.ReLU)\nconv-bn-relu -> (nn.Conv2d, nn.BatchNorm2d, nn.ReLU)\n\ndef fuse_linear_relu(is_qat, linear, relu):\n    ...\n\ndef fuse_conv_bn_relu(is_qat, conv, bn, relu):\n    ...\n```\nNote that the legacy \"reversed nested tuple\" is still\nused internally since it is more general. In the\nfuture, we should replace it with the format used in\nthe subgraph rewriter in `torch.fx`, and simplify the\nexisting pattern matching code to handle the new\nformat added in this commit.\n\nBC-breaking Notes:\n\nBefore:\n```\nimport torch as nn\nimport torch.ao.nn.intrinsic as nni\nfrom torch.ao.quantization.backend_config import BackendPatternConfig\n\ndef fuse_linear_relu(is_qat, relu, bn_conv):\n    (bn, conv) = bn_conv\n    return nni.ConvBnReLU2d(conv, bn, relu)\n\nconfig = BackendPatternConfig((nn.ReLU, (nn.BatchNorm2d, nn.Conv2d))) \\\n    .set_dtype_configs(...) \\\n    .set_fuser_method(fuse_conv_bn_relu) \\\n    .set_fused_module(nni.ConvBnReLU2d)\n```\n\nAfter:\n```\ndef fuse_linear_relu(is_qat, conv, bn, relu):\n    return nni.ConvBnReLU2d(conv, bn, relu)\n\nconfig = BackendPatternConfig((nn.Conv2d, nn.BatchNorm2d, nn.ReLU)) \\\n    .set_dtype_configs(...) \\\n    .set_fuser_method(fuse_conv_bn_relu) \\\n    .set_fused_module(nni.ConvBnReLU2d)\n```\n\nOR (for backward-compatibility)\n\n```\ndef fuse_linear_relu(is_qat, relu, bn_conv):\n    (bn, conv) = bn_conv\n    return nni.ConvBnReLU2d(conv, bn, relu)\n\nconfig = BackendPatternConfig((nn.ReLU, (nn.BatchNorm2d, nn.Conv2d))) \\\n    .set_dtype_configs(...) \\\n    .set_fuser_method(fuse_conv_bn_relu) \\\n    .set_fused_module(nni.ConvBnReLU2d) \\\n    ._set_use_legacy_pattern_format(True)\n```\n\nTest Plan:\npython test/test_quantization.py TestQuantizeFx\npython test/test_quantization.py TestQuantizeFxOps\npython test/test_quantization.py TestBackendConfig\n\nReviewers: jerryzh168, vkuzo\n\nSubscribers: jerryzh168, vkuzo\n\nDifferential Revision: [D41854096](https://our.internmc.facebook.com/intern/diff/D41854096)", "label": [{"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}]}
{"number": 90349, "title": "WIP try to get Fake Work working", "time": "2022-12-07T02:30:24Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90349\n* #90024\n* #89984\n\n", "label": [{"id": 3769218061, "node_id": "LA_kwDOA-j9z87gqbQN", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(c10d)", "name": "release notes: distributed (c10d)", "color": "EE86D4", "default": false, "description": "release notes category"}]}
{"number": 90348, "title": "Add a way to visualize memory snapshot traces", "time": "2022-12-07T02:26:34Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90348\n\nThis adds a d3-based interactive visualization for exploring the memory\nallocation traces that the caching allocator can capture. This visualization\ncode can also be attached to kineto trace information in the future to also\nprovide visualization for the memory events captured there, which come with\naddition information about the graph.", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90345, "title": "[Profiler] Include more uncategorized events in memory profile.", "time": "2022-12-07T02:02:55Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90345\n\nThis PR adds handling for allocations / frees which we cannot prove are for Tensors. (And thus aren't assigned an ID.) These events are still important for judging overall utilization.\n\nDifferential Revision: [D41790860](https://our.internmc.facebook.com/intern/diff/D41790860/)", "label": []}
{"number": 90337, "title": "Update ONNX version", "time": "2022-12-06T23:49:27Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* #84789\r\n* #83186\r\n* __->__ #90337\r\n\r\nWhy:\r\n\r\nONNX had mismatch checker usage between cpp and python and it's later fixed by https://github.com/onnx/onnx/pull/4386. And since torch.onnx.export is using cpp checker for graph-level check with older version of ONNX,this improvement should be added. Also, this version bump enables https://github.com/pytorch/pytorch/pull/83186\r\n\r\nUpdated 12/5/2022:\r\nThis PR includes ONNX 1.13.0 pre-release (https://github.com/onnx/onnx/tree/rel-1.13.0)\r\n", "label": [{"id": 693805995, "node_id": "MDU6TGFiZWw2OTM4MDU5OTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20onnx", "name": "module: onnx", "color": "f7e101", "default": false, "description": "Related to torch.onnx"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3768821874, "node_id": "LA_kwDOA-j9z87go6hy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20onnx", "name": "release notes: onnx", "color": "B4EB93", "default": false, "description": "torch.onnx related changes that should show up in the release notes"}]}
{"number": 90335, "title": "Fix issue 38095 TODO in test_multiprocessing.py", "time": "2022-12-06T23:26:54Z", "body": "Fix TODO related to https://github.com/pytorch/pytorch/issues/38095", "label": [{"id": 1447309924, "node_id": "MDU6TGFiZWwxNDQ3MzA5OTI0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/better-engineering", "name": "better-engineering", "color": "94f76a", "default": false, "description": "Relatively self-contained tasks for better engineering contributors"}, {"id": 2404913419, "node_id": "MDU6TGFiZWwyNDA0OTEzNDE5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Merged", "name": "Merged", "color": "ededed", "default": false, "description": null}, {"id": 2510927053, "node_id": "MDU6TGFiZWwyNTEwOTI3MDUz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Reverted", "name": "Reverted", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4332276215, "node_id": "LA_kwDOA-j9z88AAAABAjlJ9w", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo-elimination", "name": "todo-elimination", "color": "82AC80", "default": false, "description": ""}]}
{"number": 90334, "title": "[Pytorch Edge] Whitelist -> Allowlist in selective build", "time": "2022-12-06T23:10:10Z", "body": "Summary: .\n\nTest Plan: ci\n\nReviewed By: larryliu0820\n\nDifferential Revision: D41784620\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}]}
{"number": 90333, "title": "Add comment to output_code in dynamo config", "time": "2022-12-06T23:04:13Z", "body": "Title.\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 3773063595, "node_id": "LA_kwDOA-j9z87g5GGr", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20documentation", "name": "topic: documentation", "color": "B2E89B", "default": false, "description": "topic category"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90332, "title": "Bump to ONNX 1.13.0 RC", "time": "2022-12-06T23:03:15Z", "body": "ONNX had mismatch checker usage between cpp and python and it's later fixed by https://github.com/onnx/onnx/pull/4386. And since `torch.onnx.export` is using cpp checker for graph-level check with older version of ONNX,this improvement should be added. Also, this version bump enables #83186 \r\n\r\nUpdated 12/5/2022:\r\nThis PR includes ONNX 1.13.0 pre-release (https://github.com/onnx/onnx/tree/rel-1.13.0)", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90331, "title": "Create torch.func, add APIs from torch._functorch.eager_transforms", "time": "2022-12-06T22:51:59Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* __->__ #90331\n\nThis PR creates the torch.func APIs. (If we don't like the name, we have\nuntil the next release to change them).\n\nThe APIs are a thin wrapper around the implementation of the APIs in\ntorch._functorch.eager_transforms + documentation. This is to satisfy\nPyTorch public API rules.\n\nThe documentation was copy-pasted from their original locations, with\nslight modifications:\n- I renamed all instances of functorch to torch.func.\n\nTest Plan:\n- wait for tests", "label": [{"id": 4395386783, "node_id": "LA_kwDOA-j9z88AAAABBfxHnw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20functorch", "name": "release notes: functorch", "color": "957DAE", "default": false, "description": "release notes category"}]}
{"number": 90329, "title": "[dynamo] Add is_compiling for dynamo", "time": "2022-12-06T22:20:31Z", "body": "`is_tracing` returns True during dynamo tracing and False when run in Eager\n\ncc @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}, {"id": 4705034454, "node_id": "LA_kwDOA-j9z88AAAABGHEg1g", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20dynamo", "name": "release notes: dynamo", "color": "b60205", "default": false, "description": ""}]}
{"number": 90323, "title": "[dynamo] mark torch.backends.cudnn.is_acceptable as unimplemented", "time": "2022-12-06T21:23:17Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* __->__ #90323\n\nTracing `torch.backends.cudnn.is_acceptable(Tensor) -> bool:` fails with:\n\n```\n...\n  File \"/scratch/dberard/dynamo38/pytorch/torch/_dynamo/variables/functions.py\", line 196, in call_function\n    return super(UserFunctionVariable, self).call_function(tx, args, kwargs)\n  File \"/scratch/dberard/dynamo38/pytorch/torch/_dynamo/variables/functions.py\", line 67, in call_function\n    return tx.inline_user_function_return(\n  File \"/scratch/dberard/dynamo38/pytorch/torch/_dynamo/symbolic_convert.py\", line 426, in inline_user_function_return\n    result = InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n  File \"/scratch/dberard/dynamo38/pytorch/torch/_dynamo/symbolic_convert.py\", line 1698, in inline_call\n    return cls.inline_call_(parent, func, args, kwargs)\n  File \"/scratch/dberard/dynamo38/pytorch/torch/_dynamo/symbolic_convert.py\", line 1752, in inline_call_\n    tracer.run()\n  File \"/scratch/dberard/dynamo38/pytorch/torch/_dynamo/symbolic_convert.py\", line 485, in run\n    and self.step()\n  File \"/scratch/dberard/dynamo38/pytorch/torch/_dynamo/symbolic_convert.py\", line 455, in step\n    getattr(self, inst.opname)(inst)\n  File \"/scratch/dberard/dynamo38/pytorch/torch/_dynamo/symbolic_convert.py\", line 281, in wrapper\n    return inner_fn(self, inst)\n  File \"/scratch/dberard/dynamo38/pytorch/torch/_dynamo/symbolic_convert.py\", line 912, in CALL_FUNCTION\n    self.call_function(fn, args, {})\n  File \"/scratch/dberard/dynamo38/pytorch/torch/_dynamo/symbolic_convert.py\", line 389, in call_function\n    self.push(fn.call_function(self, args, kwargs))\n  File \"/scratch/dberard/dynamo38/pytorch/torch/_dynamo/variables/torch.py\", line 431, in call_function\n    tensor_variable = wrap_fx_proxy(\n  File \"/scratch/dberard/dynamo38/pytorch/torch/_dynamo/variables/builder.py\", line 662, in wrap_fx_proxy\n    return wrap_fx_proxy_cls(\n  File \"/scratch/dberard/dynamo38/pytorch/torch/_dynamo/variables/builder.py\", line 820, in wrap_fx_proxy_cls\n    raise AssertionError(\nAssertionError: torch.* op returned non-Tensor bool call_function <function is_acceptable at 0x7f00deefb790>\n```\n\nSo, mark it as unimplemented in variables/torch.py so that we don't try to `wrap_fx_proxy`.\n\nNote: this fixes tts_angular with FSDP. This was an issue with FSDP because FSDP modules are interpreted as UnspecializedNNModules, and UnspecializedNNModules try to inline calls. In comparison, NNModules (e.g. when the tts_angular model is not wrapped in FSDP) do not inline calls and instead evaluate subsequent calls. In subsequent calls, cudnn.is_acceptable would be skipped by eval_frame.py:catch_errors because it is not in an allowlist.\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 3773061952, "node_id": "LA_kwDOA-j9z87g5FtA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20bug%20fixes", "name": "topic: bug fixes", "color": "A44870", "default": false, "description": "topic category"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}, {"id": 4705034454, "node_id": "LA_kwDOA-j9z88AAAABGHEg1g", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20dynamo", "name": "release notes: dynamo", "color": "b60205", "default": false, "description": ""}]}
{"number": 90322, "title": "Enable inductor perf CI nightly on GCP A100 ", "time": "2022-12-06T21:22:35Z", "body": "This PR tries to enable inductor performance nightly testing on A100 runner provided by GCP. Currently these GCP runners were created and maintained using scripts in https://github.com/fairinternal/pytorch-gha-infra/pull/82. \r\nFor some reason the artifacts cannot (and does not need to) be uploaded to S3, so adding use-gha parameter to _linux-test.yml to avoid creating a new but mostly identical _linux-test.yml. \r\n\r\nWorkflow test results: https://github.com/pytorch/pytorch/actions/runs/3642340544/jobs/6149691109 ", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90317, "title": "[functorch] Refactor life handle storage", "time": "2022-12-06T20:34:05Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* __->__ #90317\n\nA \"life handle\" is a pointer-to-boolean that says whether or not a\nTensorWrapper is alive. A TensorWrapper is alive if we are currently\ninside of its corresponding transform. An Interpreter is alive if we are\ncurrently inside of its corresponding transform. I.e., for vmap(f)(x),\nthe BatchedTensor(x, level=1) is alive inside of the execution of f; and\nthe corresponding VmapInterpreter is alive inside of f.\n\nPreviously, there was a global map of level to life handle. It is\npossible to get into a state where we have multiple levels that refer to\ndifferent Interpreters (if the implementation of an operator calls into\nfunctorch) and that messes up the global map.\n\nThis PR changes it so that\n- every Interpreter holds a life handle that says if it is alive\n- to construct a TensorWrapper, one must either (a) directly pass it a life\nhandle, or (b) one must create the TensorWrapper when the corresponding\nInterpreter is on the stack (and we will automatically grab the life\nhandle by indexing into the DynamicLayerStack with the level)\n\n(a) is more robust so I changed most of our C++ callsites to do that.\n(b) feels a bit hacky to me, but it seems fine for now:\n- It'll raise a nice error message if the interpreter isn't on the stack\n- all of our Python callsites already follow this convention (we construct\nTensorWrappers after pushing the Interpreter onto the stack).\n\nThe alternative to (b) is that we always do (a), which we can do in the\nfuture if (b) runs us into any problems.\n\nTest Plan:\n- all functorch tests", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90316, "title": "Enable inductor opinfo tests for all inputs", "time": "2022-12-06T20:32:02Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90316\n\n\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90308, "title": "Add torch.compile support to minifier", "time": "2022-12-06T19:25:07Z", "body": "Initial fix for https://github.com/pytorch/torchdynamo/issues/1964. \n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 3773061952, "node_id": "LA_kwDOA-j9z87g5FtA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20bug%20fixes", "name": "topic: bug fixes", "color": "A44870", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}, {"id": 4705034454, "node_id": "LA_kwDOA-j9z88AAAABGHEg1g", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20dynamo", "name": "release notes: dynamo", "color": "b60205", "default": false, "description": ""}]}
{"number": 90306, "title": "Minor fix in package exporter", "time": "2022-12-06T19:06:57Z", "body": "Summary:\nAs title.\nSaw this while working on another diff.\n`storage` won't be defined in the `else` case. But this causes pyre to freak out.\n\nTest Plan: Unit tests.\n\nDifferential Revision: D41751229\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 3769202179, "node_id": "LA_kwDOA-j9z87gqXYD", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20package/deploy", "name": "release notes: package/deploy", "color": "EC030C", "default": false, "description": "release notes category"}]}
{"number": 90303, "title": "[WIP] cuSPARSElt 2:4 sparsity integration", "time": "2022-12-06T18:43:38Z", "body": "Continuation of https://github.com/pytorch/pytorch/pull/84759\r\n\r\nWork in progress", "label": [{"id": 3769217376, "node_id": "LA_kwDOA-j9z87gqbFg", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20sparse", "name": "release notes: sparse", "color": "2A7626", "default": false, "description": "release notes category"}]}
{"number": 90302, "title": "Improve interpolate() speed for channels_last CPU videos", "time": "2022-12-06T18:27:26Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #90302\r\n\r\nThis is the exact same PR as https://github.com/pytorch/pytorch/pull/86361, but on Videos (3D) instead of images (2D).\r\n\r\nFor torchvision training use-cases (num_threads=1), the speed-ups range in 1X-2X.  When num_threads>1 the speed-ups are a lot higher, up to ~30X\r\n\r\nBenchmarks details:\r\n<details >\r\n\r\n```\r\nmain branch=c6942dbbfbf836450898aa9a0c08aefe437d0765\r\ninput shape            output size      mode            dtype     num_threads  speed-up  main   PR\r\n(1, 3, 8, 256, 256) -> (16, 320, 320)  linear          float32    num_threads=1   1.0X  54.7ms vs 55.7ms\r\n(1, 3, 8, 256, 256) -> (16, 320, 320)  nearest         float32    num_threads=1   1.7X  40.5ms vs 24.4ms\r\n(1, 3, 8, 256, 256) -> (16, 320, 320)  nearest         uint8      num_threads=1   1.4X  33.1ms vs 23.7ms\r\n(1, 3, 8, 256, 256) -> (16, 320, 320)  nearest-exact   float32    num_threads=1   2.0X  47.5ms vs 24.3ms\r\n(1, 3, 8, 256, 256) -> (16, 320, 320)  nearest-exact   uint8      num_threads=1   1.7X  39.9ms vs 23.7ms\r\n\r\n(1, 3, 8, 256, 256) -> (16, 320, 320)  linear          float32    num_threads=2   2.2X  54.6ms vs 25.1ms\r\n(1, 3, 8, 256, 256) -> (16, 320, 320)  nearest         float32    num_threads=2   2.3X  21.2ms vs 9.3ms\r\n(1, 3, 8, 256, 256) -> (16, 320, 320)  nearest         uint8      num_threads=2   1.4X  16.5ms vs 12.0ms\r\n(1, 3, 8, 256, 256) -> (16, 320, 320)  nearest-exact   float32    num_threads=2   2.6X  24.3ms vs 9.3ms\r\n(1, 3, 8, 256, 256) -> (16, 320, 320)  nearest-exact   uint8      num_threads=2   1.7X  19.9ms vs 12.0ms\r\n\r\n(1, 3, 8, 256, 256) -> (16, 320, 320)  linear          float32    num_threads=12  10X   54.3ms vs 5.4ms\r\n(1, 3, 8, 256, 256) -> (16, 320, 320)  nearest         float32    num_threads=12  2.5X  4.1ms vs 1.6ms\r\n(1, 3, 8, 256, 256) -> (16, 320, 320)  nearest         uint8      num_threads=12  1.4X  2.9ms vs 2.1ms\r\n(1, 3, 8, 256, 256) -> (16, 320, 320)  nearest-exact   float32    num_threads=12  1.7X  4.8ms vs 2.8ms\r\n(1, 3, 8, 256, 256) -> (16, 320, 320)  nearest-exact   uint8      num_threads=12  1.7X  3.5ms vs 2.1ms\r\n\r\n(1, 3, 8, 256, 256) -> (16, 320, 320)  linear          float32    num_threads=32  20X   54.2ms vs 2.7ms\r\n(1, 3, 8, 256, 256) -> (16, 320, 320)  nearest         float32    num_threads=32  1.5X  2.2ms vs 1.5ms\r\n(1, 3, 8, 256, 256) -> (16, 320, 320)  nearest         uint8      num_threads=32  1.6X  1.3ms vs 0.8ms\r\n(1, 3, 8, 256, 256) -> (16, 320, 320)  nearest-exact   float32    num_threads=32  1.3X  1.8ms vs 1.4ms\r\n(1, 3, 8, 256, 256) -> (16, 320, 320)  nearest-exact   uint8      num_threads=32  1.7X  1.3ms vs 0.8ms\r\n\r\n(1, 3, 16, 320, 320) -> (8, 256, 256)  linear          float32    num_threads=1   1.0X  15.4ms vs 16.0ms\r\n(1, 3, 16, 320, 320) -> (8, 256, 256)  nearest         float32    num_threads=1   2.0X  12.3ms vs 6.0ms\r\n(1, 3, 16, 320, 320) -> (8, 256, 256)  nearest         uint8      num_threads=1   1.6X  12.0ms vs 7.7ms\r\n(1, 3, 16, 320, 320) -> (8, 256, 256)  nearest-exact   float32    num_threads=1   2.2X  13.1ms vs 6.0ms\r\n(1, 3, 16, 320, 320) -> (8, 256, 256)  nearest-exact   uint8      num_threads=1   1.7X  12.8ms vs 7.6ms\r\n\r\n(1, 3, 16, 320, 320) -> (8, 256, 256)  linear          float32    num_threads=2   1.9X  15.5ms vs 8.2ms\r\n(1, 3, 16, 320, 320) -> (8, 256, 256)  nearest         float32    num_threads=2   2.0X  6.1ms vs 3.1ms\r\n(1, 3, 16, 320, 320) -> (8, 256, 256)  nearest         uint8      num_threads=2   1.5X  6.0ms vs 3.9ms\r\n(1, 3, 16, 320, 320) -> (8, 256, 256)  nearest-exact   float32    num_threads=2   2.2X  6.6ms vs 3.0ms\r\n(1, 3, 16, 320, 320) -> (8, 256, 256)  nearest-exact   uint8      num_threads=2   1.7X  6.5ms vs 3.9ms\r\n\r\n(1, 3, 16, 320, 320) -> (8, 256, 256)  linear          float32    num_threads=12  11X   15.5ms vs 1.4ms\r\n(1, 3, 16, 320, 320) -> (8, 256, 256)  nearest         float32    num_threads=12  2.0X  1.1ms vs 0.5ms\r\n(1, 3, 16, 320, 320) -> (8, 256, 256)  nearest         uint8      num_threads=12  1.6X  1.1ms vs 0.7ms\r\n(1, 3, 16, 320, 320) -> (8, 256, 256)  nearest-exact   float32    num_threads=12  2.1X  1.2ms vs 0.5ms\r\n(1, 3, 16, 320, 320) -> (8, 256, 256)  nearest-exact   uint8      num_threads=12  1.5X  1.1ms vs 0.8ms\r\n\r\n(1, 3, 16, 320, 320) -> (8, 256, 256)  linear          float32    num_threads=32  15X   15.4ms vs 1.0ms\r\n(1, 3, 16, 320, 320) -> (8, 256, 256)  nearest         float32    num_threads=32  1.7X  0.7ms vs 0.4ms\r\n(1, 3, 16, 320, 320) -> (8, 256, 256)  nearest         uint8      num_threads=32  1.3X  0.7ms vs 0.5ms\r\n(1, 3, 16, 320, 320) -> (8, 256, 256)  nearest-exact   float32    num_threads=32  3X    0.7ms vs 0.2ms\r\n(1, 3, 16, 320, 320) -> (8, 256, 256)  nearest-exact   uint8      num_threads=32  2.6X  0.7ms vs 0.3ms\r\n\r\n(1, 3, 16, 320, 320) -> (32, 512, 512)  linear          float32    num_threads=1   1.0X  295.6ms vs 304.3ms\r\n(1, 3, 16, 320, 320) -> (32, 512, 512)  nearest         float32    num_threads=1   1.5X  223.2ms vs 144.3ms\r\n(1, 3, 16, 320, 320) -> (32, 512, 512)  nearest         uint8      num_threads=1   1.5X  177.7ms vs 121.0ms\r\n(1, 3, 16, 320, 320) -> (32, 512, 512)  nearest-exact   float32    num_threads=1   1.8X  258.6ms vs 145.3ms\r\n(1, 3, 16, 320, 320) -> (32, 512, 512)  nearest-exact   uint8      num_threads=1   1.6X  203.9ms vs 128.6ms\r\n\r\n(1, 3, 16, 320, 320) -> (32, 512, 512)  linear          float32    num_threads=2   1.8X  295.4ms vs 160.4ms\r\n(1, 3, 16, 320, 320) -> (32, 512, 512)  nearest         float32    num_threads=2   1.5X  119.0ms vs 80.2ms\r\n(1, 3, 16, 320, 320) -> (32, 512, 512)  nearest         uint8      num_threads=2   1.4X  84.8ms vs 60.6ms\r\n(1, 3, 16, 320, 320) -> (32, 512, 512)  nearest-exact   float32    num_threads=2   1.7X  136.1ms vs 80.1ms\r\n(1, 3, 16, 320, 320) -> (32, 512, 512)  nearest-exact   uint8      num_threads=2   1.7X  102.2ms vs 60.5ms\r\n\r\n(1, 3, 16, 320, 320) -> (32, 512, 512)  linear          float32    num_threads=12  9X    295.3ms vs 32.3ms\r\n(1, 3, 16, 320, 320) -> (32, 512, 512)  nearest         float32    num_threads=12  1.4X  25.2ms vs 18.7ms\r\n(1, 3, 16, 320, 320) -> (32, 512, 512)  nearest         uint8      num_threads=12  1.4X  16.5ms vs 11.9ms\r\n(1, 3, 16, 320, 320) -> (32, 512, 512)  nearest-exact   float32    num_threads=12  1.5X  28.1ms vs 18.8ms\r\n(1, 3, 16, 320, 320) -> (32, 512, 512)  nearest-exact   uint8      num_threads=12  1.7X  19.4ms vs 11.5ms\r\n\r\n(1, 3, 16, 320, 320) -> (32, 512, 512)  linear          float32    num_threads=32  18X   294.7ms vs 16.2ms\r\n(1, 3, 16, 320, 320) -> (32, 512, 512)  nearest         float32    num_threads=32  1.2X  14.4ms vs 12.5ms\r\n(1, 3, 16, 320, 320) -> (32, 512, 512)  nearest         uint8      num_threads=32  1.2X  5.9ms vs 4.8ms\r\n(1, 3, 16, 320, 320) -> (32, 512, 512)  nearest-exact   float32    num_threads=32  1.2X  14.5ms vs 12.5ms\r\n(1, 3, 16, 320, 320) -> (32, 512, 512)  nearest-exact   uint8      num_threads=32  1.4X  6.9ms vs 4.8ms\r\n\r\n(1, 3, 32, 512, 512) -> (16, 320, 320)  linear          float32    num_threads=1   0.9X  48.6ms vs 55.1ms\r\n(1, 3, 32, 512, 512) -> (16, 320, 320)  nearest         float32    num_threads=1   2.0X  38.8ms vs 19.2ms\r\n(1, 3, 32, 512, 512) -> (16, 320, 320)  nearest         uint8      num_threads=1   1.6X  37.6ms vs 23.8ms\r\n(1, 3, 32, 512, 512) -> (16, 320, 320)  nearest-exact   float32    num_threads=1   2.1X  41.2ms vs 19.2ms\r\n(1, 3, 32, 512, 512) -> (16, 320, 320)  nearest-exact   uint8      num_threads=1   1.7X  39.9ms vs 23.8ms\r\n\r\n(1, 3, 32, 512, 512) -> (16, 320, 320)  linear          float32    num_threads=2   1.9X  48.8ms vs 25.3ms\r\n(1, 3, 32, 512, 512) -> (16, 320, 320)  nearest         float32    num_threads=2   2.0X  19.2ms vs 9.5ms\r\n(1, 3, 32, 512, 512) -> (16, 320, 320)  nearest         uint8      num_threads=2   1.6X  18.8ms vs 12.0ms\r\n(1, 3, 32, 512, 512) -> (16, 320, 320)  nearest-exact   float32    num_threads=2   2.2X  20.5ms vs 9.5ms\r\n(1, 3, 32, 512, 512) -> (16, 320, 320)  nearest-exact   uint8      num_threads=2   1.7X  20.0ms vs 12.0ms\r\n\r\n(1, 3, 32, 512, 512) -> (16, 320, 320)  linear          float32    num_threads=12  11X   48.6ms vs 4.6ms\r\n(1, 3, 32, 512, 512) -> (16, 320, 320)  nearest         float32    num_threads=12  2.0X  3.4ms vs 1.7ms\r\n(1, 3, 32, 512, 512) -> (16, 320, 320)  nearest         uint8      num_threads=12  1.6X  3.3ms vs 2.1ms\r\n(1, 3, 32, 512, 512) -> (16, 320, 320)  nearest-exact   float32    num_threads=12  2.1X  3.6ms vs 1.7ms\r\n(1, 3, 32, 512, 512) -> (16, 320, 320)  nearest-exact   uint8      num_threads=12  1.7X  3.5ms vs 2.1ms\r\n\r\n(1, 3, 32, 512, 512) -> (16, 320, 320)  linear          float32    num_threads=32  27X   48.3ms vs 1.8ms\r\n(1, 3, 32, 512, 512) -> (16, 320, 320)  nearest         float32    num_threads=32  1.1X  2.2ms vs 2.0ms\r\n(1, 3, 32, 512, 512) -> (16, 320, 320)  nearest         uint8      num_threads=32  2.6X  2.1ms vs 0.8ms\r\n(1, 3, 32, 512, 512) -> (16, 320, 320)  nearest-exact   float32    num_threads=32  2.4X  2.3ms vs 0.9ms\r\n(1, 3, 32, 512, 512) -> (16, 320, 320)  nearest-exact   uint8      num_threads=32  2.6X  2.2ms vs 0.8ms\r\n\r\n```\r\n\r\n\r\n</details>\r\n\r\nCode:\r\n\r\n<details>\r\n\r\n\r\n```py\r\nimport operator_benchmark as op_bench\r\nimport torch\r\n\r\n\"\"\"Microbenchmarks for interpolate operator.\"\"\"\r\n\r\n\r\nclass InterpolateBenchmark(op_bench.TorchBenchmarkBase):\r\n    def init(self, input_size, output_size, channels_last=False, mode='linear', dtype=torch.float):\r\n\r\n        input_image = torch.randint(0, 256, size=input_size, dtype=dtype, device='cpu',\r\n                                    requires_grad=self.auto_set())\r\n        if channels_last:\r\n            if input_image.ndim == 4:\r\n                input_image = input_image.contiguous(memory_format=torch.channels_last)\r\n            elif input_image.ndim == 5:\r\n                input_image = input_image.contiguous(memory_format=torch.channels_last_3d)\r\n            else:\r\n                raise ValueError(\r\n                    f\"Can not set channels_last to the input of {input_image.ndim} dims\"\r\n                )\r\n\r\n\r\n        align_corners = None if \"nearest\" in mode else False\r\n\r\n        if mode == \"linear\":\r\n            mode = {\r\n                3: 'linear',\r\n                4: 'bilinear',\r\n                5: 'trilinear',\r\n            }[input_image.ndim]\r\n\r\n        self.inputs = {\r\n            \"input_image\": input_image,\r\n            \"output_size\": output_size,\r\n            \"mode\": mode,\r\n            \"align_corners\": align_corners,\r\n        }\r\n\r\n        self.set_module_name(\"interpolate\")\r\n\r\n    def forward(self, input_image, output_size, mode, align_corners):\r\n        return torch.nn.functional.interpolate(input_image, size=output_size, mode=mode,\r\n                                               align_corners=align_corners)\r\n\r\n\r\ndef make_config():\r\n    sizes = (\r\n        ((16, 320, 320), (8, 256, 256)),\r\n        ((16, 320, 320), (32, 512, 512)),\r\n    )\r\n\r\n    attrs = []\r\n    for (DHW1, DHW2) in sizes:\r\n        attrs.append([(1, 3, *DHW1), DHW2])\r\n        attrs.append([(1, 3, *DHW2), DHW1])\r\n\r\n\r\n    config = op_bench.config_list(\r\n        attr_names=[\"input_size\", \"output_size\"],\r\n        attrs=attrs,\r\n        cross_product_configs={\r\n            'channels_last': [True],\r\n            'mode': [\"linear\", \"nearest\", \"nearest-exact\"],\r\n            'dtype': [torch.float, torch.uint8]\r\n        },\r\n        tags=[\"short\"],\r\n    )\r\n\r\n    # Need to remove instances with both torch.int and linear\r\n    # Note: this is naaaasty\r\n    def get_mode(l):\r\n        for d in l:\r\n            if \"mode\" in d:\r\n                return d[\"mode\"]\r\n    def get_dtype(l):\r\n        for d in l:\r\n            if \"dtype\" in d:\r\n                return d[\"dtype\"]\r\n    config = [l for l in config if not(get_mode(l) == \"linear\" and get_dtype(l) == torch.uint8)]\r\n    return config\r\n\r\nconfig = make_config()\r\nop_bench.generate_pt_test(config, InterpolateBenchmark)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    op_bench.benchmark_runner.main()\r\n```\r\n\r\n```py\r\nimport re\r\nimport argparse\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\"f3\", nargs=\"?\", default=\"main\")\r\nparser.add_argument(\"f2\", nargs=\"?\", default=\"new\")\r\nargs = parser.parse_args()\r\n\r\nwith open(args.f1) as f:\r\n    main = f.readlines()\r\nwith open(args.f2) as f:\r\n    new = f.readlines()\r\n\r\nout = []\r\n\r\nfor main_line, new_line in zip(main, new):\r\n    # num_threads=1  # TODO: remove\r\n    if main_line.startswith(\"num_threads=\"):\r\n        num_threads = int(main_line.split(\"=\")[-1])\r\n    if main_line.startswith(\"# Input\"):\r\n        deets = f\"{main_line.strip()}, {num_threads=}\"\r\n    if main_line.startswith(\"Forward\"):\r\n        main_time = float(main_line.split()[-1])\r\n        new_time = float(new_line.split()[-1])\r\n        ratio = main_time / new_time\r\n        fmt = \".1f\" if ratio < 3 else \".0f\"\r\n        improv = f\"{ratio:{fmt}}X\"\r\n        time_fmt = \",.3f\" if new_time < 100 else \",.1f\"\r\n        deets = deets.strip().replace(\"# Input: \", \"\")\r\n        deets = deets.replace(\": \", \"=\")\r\n        deets = deets.replace(\"input_size=\", \"\")\r\n        deets = deets.replace(\", output_size=\", \" -> \")\r\n        deets = deets.replace(\"dtype=torch.\", \"\")\r\n        deets = deets.replace(\"mode=\", \"\")\r\n        deets = deets.replace(\"channels_last=True, \", \"\")\r\n        split = deets.split(\",\")\r\n        size = ','.join(split[:-3])\r\n        mode, dtype, threads = split[-3:]\r\n        deets = f\"{size:<30} {mode:<15} {dtype:<10} {threads:<15}\"\r\n\r\n        l = f\"{deets}  {improv:<5} {main_time / 1000:{time_fmt}}ms vs {new_time / 1000:{time_fmt}}ms\"\r\n        out.append(l)\r\n\r\n\r\ndef key(s):\r\n    # s = ''.join(s.split()[1:]) # remove \"N.nX\" part\r\n    num_threads = (int(re.findall(r\"num_threads=(\\d+)\", s)[0]),)\r\n\r\n    input_shape, output_shape = re.findall(\"\\(.*?\\)\", s)\r\n    input_shape = input_shape[1:-1]  # remove parenthesis\r\n    input_HW = tuple(int(x) for x in input_shape.split(\",\")[-2:])\r\n    input_C = (-int(input_shape.split(\",\")[1]),)\r\n\r\n    output_HW = tuple(int(x) for x in output_shape[1:-1].split(\",\"))\r\n    is_downsample = (output_HW[0] < input_HW[0],)\r\n    if \"linear\" in s:\r\n        mode = \"linear\"\r\n    elif \"nearest-exact\" in s:\r\n        mode = \"nearest-exact\"\r\n    else:\r\n        assert \"nearest\" in s\r\n        mode = \"nearest\"\r\n    mode = (mode,)\r\n    return is_downsample + input_HW + output_HW + num_threads + input_C + mode\r\n\r\nfor i, l in enumerate(sorted(out, key=key)):\r\n    if i % 5 == 0:\r\n        print()\r\n    # if i % 10 == 0 and i % 40 != 0:\r\n    #     print()\r\n    # if i % 40 == 0:\r\n    #     print(\"-\" * 100)\r\n    print(l)\r\n```\r\n\r\n\r\n</details >\r\n\r\ncc @VitalyFedyunin @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 3769207236, "node_id": "LA_kwDOA-j9z87gqYnE", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20nn", "name": "release notes: nn", "color": "29E07F", "default": false, "description": "release notes category"}, {"id": 3773062847, "node_id": "LA_kwDOA-j9z87g5F6_", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20performance", "name": "topic: performance", "color": "AE9D13", "default": false, "description": "topic category"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90300, "title": "[FX][Graph] improve attr_node warning logic to avoid console overflow ", "time": "2022-12-06T17:34:09Z", "body": "This PR sets the get_attr node warning to display only once, rather than the continuous stream of warnings that in our use case, represent nothing but console overflow that gets in the way.  \r\n\r\nBackground: \r\nCurrently we (tau compiler) are inserting subgraphs that contain distributed communication collectives, which when traced, translate the distributed classes ReduceOp and ProcessGroup into tensor constants, referenced by get_attr nodes.\r\n\r\nHowever, the graph linter has a warning about get_attr nodes referencing anything but params/buffers. \r\n<img width=\"1124\" alt=\"warning_fx_graph\" src=\"https://user-images.githubusercontent.com/46302957/205981634-574acb4d-f3e1-4cdf-8ed2-c82f0a912987.png\">\r\n\r\n\r\nAs a result, we continuously get a massive console spew of warnings as each and every get_attr node is warned. \r\nExample:\r\n<img width=\"1499\" alt=\"fx_graph_console_overload\" src=\"https://user-images.githubusercontent.com/46302957/205980506-73a8ea7a-674b-45eb-8fc9-f1b138a3f8ec.png\">\r\n\r\n\r\nIn talking with Ansley, have determined that a good compromise here is to enable a warn once functionality to blend the goals of alerting a user, while not overflowing users that intentionally have situations that don't match the current well-intentioned warning.\r\n\r\nThus, this PR simply sets an attr_warning_shown flag to show the warning for the first node detected, and then suppresses continous display after that. \r\nThe warning itself has been updated to note that this is only the first node detected and the user should review their graph, and/or fix this node and re-run the linter to check the next offending node, if any. \r\nThat results in a single console output like this:\r\n<img width=\"1499\" alt=\"fx_graph_single_warning\" src=\"https://user-images.githubusercontent.com/46302957/205980870-c78023ab-2ed6-443f-bf9f-ee0f660c6914.png\">\r\n\r\nwhich is much more helpful towards providing info without overflow (imo). \r\nIf there are better ways to resolve this issue, then pls advise but hopefully this PR achieves the desired compromise between warning the user while not overflowing the console of other users who have legit use for get_attr outside the bounds of this warning. \r\n", "label": [{"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}]}
{"number": 90299, "title": "vetorize zero_ operations", "time": "2022-12-06T17:20:16Z", "body": "There are too many conflicts to solve when I try to rebase the previous PR  https://github.com/pytorch/pytorch/pull/82777 . So I have to open a new PR. \r\n\r\nIn this new PR, I only change the zero_grad in `torch/optim/optimizer.py`. ", "label": []}
{"number": 90295, "title": "Keep track of source name on all allocated SymInts", "time": "2022-12-06T16:55:38Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #90528\n* #90381\n* __->__ #90295\n\r\nWow, I had to sweat so much to get this PR out lol.\r\n\r\nThis PR enforces the invariant that whenever we allocate SymInts as part of fakeification, the SymInt is associated with a Source, and in fact we store the string source name on SymbolWithSourceName. We use 'sname' as the shorthand for source name, as 'name' is already used by sympy to name symbols.\r\n\r\nIn order to store source names, we have to plumb source names from Dynamo to PyTorch. This made doing this PR a bit bone crushing, because there are many points in the Dynamo codebase where we are improperly converting intermediate tensors into fake tensors, where there is no source (and there cannot be, because it's a frickin' intermediate tensor). I've fixed all of the really awful cases in earlier PRs in the stack. This PR is just plumbing in source names from places where we do have it.\r\n\r\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\r\n\r\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90287, "title": "Keep only test config filtering logic in workflow build step", "time": "2022-12-06T15:08:11Z", "body": "Originally, the filtering logic was applied twice, first in the build step (doesn't show up in GitHub signal box) and latter as a separate job before testing (show up in GitHub signal box), so that PR authors had plenty of time to add the necessary label(s).  This PR tweaks this and keeps only the first part:\r\n\r\n* **This is to reduce the number of jobs on GitHub signal box, which might be limited by GitHub**.  This PR is needed only when the limit is too low (100) and cannot be raised.\r\n* After this change, PR authors have a smaller window to set the `test-config` labels, i.e. when creating the PR. This should be ok as an advance feature.  The filter logic is applied dynamically given all labels available at the time it runs.\r\n* This shouldn't change the way `mem_leak_check` and `rerun_disabled_tests` jobs run.  They depends on the filtering step to work.", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}, {"id": 4542782543, "node_id": "LA_kwDOA-j9z88AAAABDsVcTw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/test-config/default", "name": "test-config/default", "color": "3AF818", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}, {"id": 4747152252, "node_id": "LA_kwDOA-j9z88AAAABGvPLfA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/test-config/inductor", "name": "test-config/inductor", "color": "D44EC0", "default": false, "description": ""}]}
{"number": 90283, "title": "inductor(CPU): support vectorization of scalar tensor add when the scalar tensor dtype is not float", "time": "2022-12-06T14:23:24Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90283\n* #90259\n\r\nFor HF transformer model, there has an NumPy operator which introduces an add: float tensor + double scalar tensor:\r\n\r\n```\r\nclass MockModule(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.temperature = np.power(128, 0.5)\r\n\r\n    def forward(self, x):\r\n        return x + self.temperature\r\n```\r\nand inductor always convert the NumPy.number to a scalar-tensor which has a double dtype, and it  generates a non-vec code:\r\n\r\n```\r\nkernel_cpp_0 = async_compile.cpp('''\r\n#include \"/tmp/torchinductor_xiaobing/77/c7773nj5pwikpmm2pwa62rcudlf7p3if7eyqb5k4sjsvewwje4le.h\"\r\nextern \"C\" void kernel(const float* __restrict__ in_ptr0,\r\n                       const double* __restrict__ in_ptr1,\r\n                       float* __restrict__ out_ptr0)\r\n{\r\n    #pragma GCC ivdep\r\n    for(long i0=0; i0<18816; i0+=1)\r\n    {\r\n        {\r\n            {\r\n                auto tmp0 = in_ptr0[i0];\r\n                auto tmp1 = in_ptr1[0];\r\n                auto tmp2 = static_cast<float>(tmp1);\r\n                auto tmp3 = tmp0 + tmp2;\r\n                out_ptr0[i0] = tmp3;\r\n            }\r\n        }\r\n    }\r\n}\r\n''')\r\n\r\n\r\nasync_compile.wait(globals())\r\ndel async_compile\r\n\r\ndef call(args):\r\n    arg0_1, arg1_1 = args\r\n    args.clear()\r\n    buf0 = empty_strided((128, 3, 7, 7), (147, 1, 21, 3), device='cpu', dtype=torch.float32)\r\n    kernel_cpp_0(c_void_p(arg0_1.data_ptr()), c_void_p(arg1_1.data_ptr()), c_void_p(buf0.data_ptr()))\r\n    del arg0_1\r\n    del arg1_1\r\n    return (buf0, )\r\n\r\n```\r\nAfter this PR:\r\n\r\n\r\n```\r\nkernel_cpp_0 = async_compile.cpp('''\r\n#include \"/tmp/torchinductor_xiaobing/tu/ctukqeudfyyazw7deqmkrt5gv5wz5mnyl5tzcgkkz4djjjjbiai7.h\"\r\nextern \"C\" void kernel(const float* __restrict__ in_ptr0,\r\n                       const double* __restrict__ in_ptr1,\r\n                       float* __restrict__ out_ptr0)\r\n{\r\n    #pragma omp parallel num_threads(40)\r\n    {\r\n        #pragma omp for\r\n        for(long i0=0; i0<1176; i0+=1)\r\n        {\r\n            auto tmp0 = at::vec::Vectorized<float>::loadu(in_ptr0 + 16*i0);\r\n            float g_tmp_buffer_in_ptr1[1] = {0};\r\n            to_float(in_ptr1, g_tmp_buffer_in_ptr1, 1);\r\n            auto tmp1 = at::vec::Vectorized<float>(g_tmp_buffer_in_ptr1[0]);\r\n            auto tmp2 = (tmp1);\r\n            auto tmp3 = tmp0 + tmp2;\r\n            tmp3.store(out_ptr0 + 16*i0);\r\n        }\r\n        #pragma omp for simd simdlen(8)\r\n        for(long i0=18816; i0<18816; i0+=1)\r\n        {\r\n            auto tmp0 = in_ptr0[i0];\r\n            auto tmp1 = in_ptr1[0];\r\n            auto tmp2 = static_cast<float>(tmp1);\r\n            auto tmp3 = tmp0 + tmp2;\r\n            out_ptr0[i0] = tmp3;\r\n        }\r\n    }\r\n}\r\n''')\r\n\r\n\r\nasync_compile.wait(globals())\r\ndel async_compile\r\n\r\ndef call(args):\r\n    arg0_1, arg1_1 = args\r\n    args.clear()\r\n    buf0 = empty_strided((128, 3, 7, 7), (147, 49, 7, 1), device='cpu', dtype=torch.float32)\r\n    kernel_cpp_0(c_void_p(arg0_1.data_ptr()), c_void_p(arg1_1.data_ptr()), c_void_p(buf0.data_ptr()))\r\n    del arg0_1\r\n    del arg1_1\r\n    return (buf0, )\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    from torch._dynamo.testing import rand_strided\r\n    from torch._inductor.utils import print_performance\r\n    arg0_1 = rand_strided((128, 3, 7, 7), (147, 49, 7, 1), device='cpu', dtype=torch.float32)\r\n    arg1_1 = rand_strided((), (), device='cpu', dtype=torch.float64)\r\n    print_performance(lambda: call([arg0_1, arg1_1]))\r\n\r\n```\r\n\r\nthe current solution is that manually cast double to float, but I think a better method is that to fully support vectorization of dtype conversion, i.e. make **to_dtype** can be vectorized as aten side.\r\n\r\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}, {"id": 4705034454, "node_id": "LA_kwDOA-j9z88AAAABGHEg1g", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20dynamo", "name": "release notes: dynamo", "color": "b60205", "default": false, "description": ""}]}
{"number": 90281, "title": "to_sparse_XXX: backward support", "time": "2022-12-06T13:59:59Z", "body": "As per title. Fixes https://github.com/pytorch/pytorch/issues/85226\r\n\n\ncc @pearu @cpuhrsch @amjames @bhosmer @ezyang @albanD @zou3519 @gqchen @soulitzer @Lezcano @Varal7", "label": [{"id": 679954154, "node_id": "MDU6TGFiZWw2Nzk5NTQxNTQ=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20sparse", "name": "module: sparse", "color": "f7e101", "default": false, "description": "Related to torch.sparse"}, {"id": 1076922545, "node_id": "MDU6TGFiZWwxMDc2OTIyNTQ1", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20autograd", "name": "module: autograd", "color": "f7e101", "default": false, "description": "Related to torch.autograd, and the autograd engine in general"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3769217376, "node_id": "LA_kwDOA-j9z87gqbFg", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20sparse", "name": "release notes: sparse", "color": "2A7626", "default": false, "description": "release notes category"}]}
{"number": 90274, "title": "[Discussion] Monitor CI/CD Windows jobs", "time": "2022-12-06T11:35:32Z", "body": "# Goal\r\nThe main goal is to create an automatic system that increases awareness of the state of CI/CD pipelines. The system should trigger a notification every time a Windows related job fails.  \r\n\r\n# Proposed solution (only for GitHub Actions)\r\n\r\nUsing a GitHub Action Plugin, every time a job fails, there is an automatic message received in a Slack channel, in the official Slack workplace (this can be further discussed).\r\n\r\n# Required configurations\r\n\r\n## Slack configurations\r\nFollowing the steps described in [Technique 2/Setup](https://github.com/marketplace/actions/slack-send)\r\n\r\n## Workflow modifications\r\nThis plugin can be integrated following the steps described in [Technique 2/Usage](https://github.com/marketplace/actions/slack-send) with slight modifications. The modifications in this PR worked on my fork with a mock workflow. \r\nThe `if: failure()` line will send the notification only if the job fails, so the channel is not spammed with passed situations which do not represent any concern for future development. \r\n\r\n\r\n\r\n\n\ncc @peterjc123 @mszhanyi @skyline75489 @nbcsm @seemethere @malfet @pytorch/pytorch-dev-infra", "label": [{"id": 790080431, "node_id": "MDU6TGFiZWw3OTAwODA0MzE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20windows", "name": "module: windows", "color": "f7e101", "default": false, "description": "Windows support for PyTorch"}, {"id": 1300896147, "node_id": "MDU6TGFiZWwxMzAwODk2MTQ3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20ci", "name": "module: ci", "color": "f7e101", "default": false, "description": "Related to continuous integration"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1360094054, "node_id": "MDU6TGFiZWwxMzYwMDk0MDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20infra", "name": "module: infra", "color": "f7e101", "default": false, "description": "Relates to CI infrastructure"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90270, "title": "[Inductor] fix issue: redeclaration of float g_tmp_buffer_xxx", "time": "2022-12-06T09:24:06Z", "body": "This pr is to fix the issue: redeclaration of 'float g_tmp_buffer_in_ptr1[16] = {0};'\r\nIf a bool or uint8 tensor is used by multiple op, this tensor will be loaded multiple times. On load, it writes the declaration of this variable, i.e., `self.loads.writeline(f\"float {g_tmp_buf}[{nelements}] = {{0}};\")`, which will introduce redeclaration error.\r\n\r\n![image](https://user-images.githubusercontent.com/69951214/205869956-5c325761-dc09-4aa8-a9ed-fad7f4c85917.png)\r\n![image](https://user-images.githubusercontent.com/69951214/205870695-ee252f17-8f54-484f-9b0a-3a424c479327.png)\r\n\r\n\r\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @Guobing-Chen @chunyuan-w @zhuhaozhe @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90269, "title": "Fix missing symbols in guards by forcing all nnmodule associated tensors to be static, rm bad assert", "time": "2022-12-06T07:51:35Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90269\n\nCleanup\n\ncc @mlazos @soumith @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90267, "title": "[inductor] weight prepack for single conv_transpose2d", "time": "2022-12-06T07:34:21Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90267\n* #90266\n* #90265\n* #90264\n\r\nThis PR enables weight prepack for single conv_transpose2d OP.\r\n\r\n\r\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90266, "title": "[inductor] weight prepack for _convolution_transpose_pointwise", "time": "2022-12-06T07:34:11Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #90267\n* __->__ #90266\n* #90265\n* #90264\n\r\n\r\nThis PR implements weight prepack for `_convolution_transpose_pointwise`, similar to https://github.com/pytorch/pytorch/pull/88988.\r\n\r\n\r\nDifferent from Conv2d, once ConvTranspose weight has been prepacked, the size changes:\r\n- Original weight size: `[i, o, ...]`\r\n- Prepacked size:\r\n\r\n  - Groups > 1:  `[g*o, i/g, ...]`\r\n  - Groups == 1: `[o, i, ...]`\r\n\r\nThe  `_convolution_transpose_pointwise` kernel handles the below two situations:\r\n\r\n- During compilation, when running the node of the FX graph, the kernel gets a public weight tensor with prepacked size. The kernel will convert the weight back to the original weight size for computation.\r\n- During execution, it gets a mkldnn tensor and will directly use it for computation.\r\n\r\ncc @VitalyFedyunin @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90265, "title": "[inductor] add conv_transpose2d unary fusion for cpu in inference mode", "time": "2022-12-06T07:34:02Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #90267\n* #90266\n* __->__ #90265\n* #90264\n\r\n\r\nAn FX transformation is added to fuse ConvTranspose2d with eltwise OPs in torchinductor for CPU in inference mode, following the implementation in https://github.com/pytorch/pytorch/pull/87063.\r\n\r\nThe fusion OP is implemented in https://github.com/pytorch/pytorch/pull/90264 and will be treated as an extern kernel call in torchinductor.\r\n\r\nThe fusion of ConvTranspose2d with the below OPs is supported:\r\n\r\n- relu\r\n- sigmoid\r\n- tanh\r\n- hardswish\r\n- leaky_relu\r\n- hardtanh\r\n- gelu\r\n\r\n\r\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90264, "title": "add conv_transpose2d pointwise(unary) fusion kernel", "time": "2022-12-06T07:33:52Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #90267\n* #90266\n* #90265\n* __->__ #90264\n\r\nThis PR adds `torch.ops.mkldnn._convolution_transpose_pointwise` that supports ConvTranspose fusion with the below unary pointwise OPs:\r\n\r\n- relu\r\n- sigmoid\r\n- tanh\r\n- hardswish\r\n- leaky_relu\r\n- hardtanh\r\n- gelu\r\n\r\n\r\ncc @VitalyFedyunin @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90262, "title": "[Quant] Add fused conv2d_add op for onednn backend", "time": "2022-12-06T06:25:44Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #90364\n* __->__ #90262\n* #90157\n\r\n**Summary**\r\nPost op fusion can reduce data movement overhead and improve inference performance. This PR adds fused `conv2d_add` op for onednn backend, which will be used for int8 inference with onednn backend. Cannot call this op with other quantization backends otherwise an error is thrown.\r\n\r\n**Test Plan**\r\n```\r\npython -m pytest test_quantization.py::TestQuantizedConv\r\n```\r\n\r\n**TODO**\r\nThe IDeep change in this PR points to my personal repo currently for test purpose. After the changes landed in IDeep main branch, need to point IDeep changes to the main branch.\r\n\r\ncc @VitalyFedyunin @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @gujinghui @PenghuiCheng @jianyuh @min-jean-cho @yanbing-j @Guobing-Chen @Xia-Weiwen", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1303456553, "node_id": "MDU6TGFiZWwxMzAzNDU2NTUz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20mkldnn", "name": "module: mkldnn", "color": "f7e101", "default": false, "description": "Related to Intel IDEEP/MKL-DNN (mkldnn) integration"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 90259, "title": "inductor(CPU): add Conv+binary+unary fusion filter", "time": "2022-12-06T05:59:59Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #90283\n* __->__ #90259\n\r\n\r\nFor Conv+binary+unary fusion, we only support conv+add+relu, this PR adds a such check to fix TIMM failed models. \r\nTODO: enable more Conv+binary+unary fusion to improve TIMM models' performance.\r\n\r\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}, {"id": 4705034454, "node_id": "LA_kwDOA-j9z88AAAABGHEg1g", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20dynamo", "name": "release notes: dynamo", "color": "b60205", "default": false, "description": ""}]}
{"number": 90257, "title": "[Composable API] `replicate`: cleanup _ddp.py", "time": "2022-12-06T04:37:05Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90257\n\n", "label": [{"id": 4863754865, "node_id": "LA_kwDOA-j9z88AAAABIecCcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(composable)", "name": "release notes: distributed (composable)", "color": "22A265", "default": false, "description": ""}]}
{"number": 90249, "title": "[FSDP] Simplify grad padding logic", "time": "2022-12-06T03:02:45Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#90249 [FSDP] Simplify grad padding logic**\n* #90248 [FSDP] Have the unsharded `flat_param` use the padded size\n* #90251 [FSDP] Clarify loss dtype check in `_test_fsdp_parity`\n* #90250 [FSDP][BE] Clean up dead code from `clip_grad_norm_()` testing\n* #90290 [FSDP] Test `use_orig_params=True` in `test_fsdp_ignored_modules.py`\n* #90252 [FSDP] Fix accidental change in `_test_fsdp_parity`\n\r\nThis is a follow-up to the previous PR to now remove the padding logic completely from the post-backward hook. This works because the unsharded gradient is guaranteed to be padded.", "label": [{"id": 3769221312, "node_id": "LA_kwDOA-j9z87gqcDA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(fsdp)", "name": "release notes: distributed (fsdp)", "color": "C1F551", "default": false, "description": "release notes category"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90248, "title": "[FSDP][Perf] Compute `flat_param.grad` with padded size directly", "time": "2022-12-06T03:02:35Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* #90249 [FSDP] Simplify grad padding logic\r\n* **#90248 [FSDP] Have the unsharded `flat_param` use the padded size**\r\n* #90251 [FSDP] Clarify loss dtype check in `_test_fsdp_parity`\r\n* #90250 [FSDP][BE] Clean up dead code from `clip_grad_norm_()` testing\r\n* #90290 [FSDP] Test `use_orig_params=True` in `test_fsdp_ignored_modules.py`\r\n* #90252 [FSDP] Fix accidental change in `_test_fsdp_parity`\r\n\r\n**Overview**\r\nThis PR enforces that the `flat_param` uses the *padded* unsharded size during FSDP-managed computation, not the *unpadded* unsharded size. As a result, the gradient is computed directly with padding, avoiding the need for manual padding in the post-backward hook, which requires an unsharded memory allocation in the post-backward stream and a copy.\r\n\r\n**Approach**\r\nWe achieve this by defining `FORWARD`, `BACKWARD_PRE`, `BACKWARD_POST`, and `IDLE` to be \"computation\" states. (We include `IDLE` to accommodate prefetching.) Conversely, we define `SUMMON_FULL_PARAMS` to _not_ be a \"computation\" state.\r\n\r\nIn theory, we do not expect users to inspect the unsharded `flat_param` in `summon_full_params()` since they should only care about the unsharded original parameters in that context. However, for the `use_orig_params=False` code path, the `flat_param` is inevitably exposed, and it is reasonable for users to access it in `summon_full_params()`. To handle this, we trim the padding in `summon_full_params()`. Notably, this does not affect gradient computation.\r\n\r\nThe workhorse for creating the unsharded views is the static method `FlatParamHandle_get_unflat_views(flat_param, tensor=None)`, which is for the `FlatParameter`, its gradient, and optimizer states. For simplicity, we want to preserve that the unsharded optimizer state does not need padding. We do this by defining an invariant: If the user specifies a non-`None` `tensor` argument, then `tensor` has the *unpadded* unsharded size, which may require manual trimming (e.g. for the gradient). If the user specifies `tensor=None`, then the method uses the `flat_param` and expects it to be padded.\r\n\r\n**Details**\r\n`flat_param._padded_unsharded_size` is only defined for sharded strategies and after lazy attribute initialization (via `_lazy_init()` -> `init_flat_param_attributes()`). To handle the window between initialization and lazy initialization and the difference between sharded and non-sharded strategies, we define `flat_param._unsharded_size_for_comp` that always return the unsharded size for computation, hiding the complex branching.\r\n\r\nEven with this attribute, developers should be wary which unsharded size they are using. For any all-gathers, the unsharded size is `flat_param._padded_unsharded_size` (which is only defined for sharded strategies). For any uses outside computation or uses that require padding trimmed, the unsharded size is `flat_param._unpadded_unsharded_size` (which is _always_ defined).", "label": [{"id": 3769221312, "node_id": "LA_kwDOA-j9z87gqcDA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(fsdp)", "name": "release notes: distributed (fsdp)", "color": "C1F551", "default": false, "description": "release notes category"}, {"id": 3773061335, "node_id": "LA_kwDOA-j9z87g5FjX", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20improvements", "name": "topic: improvements", "color": "22499E", "default": false, "description": "topic category"}]}
{"number": 90247, "title": "[MKLDNN] Rename pooling_avg to pooling_avg_exclude_padding", "time": "2022-12-06T02:09:02Z", "body": "**Summary**\r\nRename `pooling_avg` to `pooling_avg_exclude_padding` to align with onednn v3.0. It does not affect correctness or performance. Same as https://github.com/pytorch/pytorch/pull/87851 . Looks like https://github.com/pytorch/pytorch/pull/87851 did not cover all occurrences.\r\n\r\n**Test plan**\r\npython test/test_mkldnn.py\r\npython caffe2/python/ideep/pool_op_test.py\n\ncc @VitalyFedyunin @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @gujinghui @PenghuiCheng @jianyuh @min-jean-cho @yanbing-j @Guobing-Chen", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1303456553, "node_id": "MDU6TGFiZWwxMzAzNDU2NTUz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20mkldnn", "name": "module: mkldnn", "color": "f7e101", "default": false, "description": "Related to Intel IDEEP/MKL-DNN (mkldnn) integration"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 90244, "title": "[fx] Have replace_pattern return replaced nodes", "time": "2022-12-06T01:53:17Z", "body": "Summary: Modified replace_pattern in the subgraph rewriter to return a list of pairs of matches along with their corresponding replacement nodes in the modified graph (`List[Tuple[Match, List[Node]]]`). This allows us to easily modify the replaced nodes, including setting the metadata.\n\nTest Plan: CI\n\nDifferential Revision: D41737056\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}]}
{"number": 90240, "title": "Move private forward grad mode helpers to torch.autograd.forward_ad", "time": "2022-12-06T00:40:39Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #90418\n* #90415\n* #90077\n* __->__ #90240\n* #90037\n* #89860\n* #89859\n* #89858\n\nMotivation\n- These were previously defined in functorch. They are not\nfunctorch-specific, so I'm moving them to torch.autograd.forward_ad and\nthe autograd python bindings.\n- I need this to avoid some of my cyclic import problems.\n\nShould these be public APIs? Probably. Though this needs discussion, so\npunting it to the future.\n\nTest Plan:\n- moved the tests of these from test/functorch/test_eager_transforms.py\nto test/test_autograd.py", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90238, "title": "Add merged label to ghstack prs", "time": "2022-12-06T00:12:39Z", "body": "not very elegant\r\n\r\nother option might be adding something to pytorchbot to listen to push events for master?", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90236, "title": "Avoid dependency on kineto headers if USE_KINETO=0", "time": "2022-12-05T23:36:47Z", "body": "When `USE_KINETO=0`, the kineto header is still required to build pytorch. I'd like to avoid that dependency.\r\n\r\nThe only thing needed from the headers is an enum class, which I copied here. Not sure if this would be considered as a bad idea.", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 90232, "title": "[FSDP] Allocate padded unsharded grad in default stream", "time": "2022-12-05T23:06:51Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#90232 [FSDP] Allocate padded unsharded grad in default stream**\n\n", "label": [{"id": 3769221312, "node_id": "LA_kwDOA-j9z87gqcDA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(fsdp)", "name": "release notes: distributed (fsdp)", "color": "C1F551", "default": false, "description": "release notes category"}]}
{"number": 90231, "title": "[Checkpoint] Update test_file_system_checkpoint.py ", "time": "2022-12-05T22:51:09Z", "body": "We updated filesystem.py  and added two params to FileSystemWriter: thread_count and per_thread_copy_ahead in this pr(https://github.com/pytorch/pytorch/pull/87987).\r\n\r\nThis PR updated test_file_system_checkpoint.py to incorporate thread_count in unit tests. \r\n\r\nNote: Using @parametrize would result in timeout in CI so that we are manually creating two tests for thread_count 1 and 2 for each existing test. \r\n\r\n", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90226, "title": "codegen fixes to fix tracing XLA autograd ops", "time": "2022-12-05T22:25:29Z", "body": "Today when XLA registers an autograd.Function to the `AutogradXLA` key, the \"add to the autograd graph\" step and the \"run the forward kernel\" step happen all in one go. That's wrong, and prevents other dispatcher code from executing in the middle.\r\n\r\nWhen trying to fix this, I noticed a bug in the codegen: we register kernels for both the XLA and AutogradXLA dispatch keys to the same class. This prevents XLA from registering a separate kernel to the XLA and Autograd XLA, which is what this PR attempts to address.\r\n\r\nCompanion patch to fix XLA's max_pool2d registration here, which was blocking the dynamo integration: https://github.com/pytorch/xla/pull/4276\r\n\r\nAfter this PR, XLA should generate two separate header files: `XLANativeFunctions.h`, and `AutogradXLANativeFunctions.h` Before, all of the kernels (including autograd kernels) would be thrown in `XLANativeFunctions.h`.\r\n\r\ncc @JackCaoG \r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90226\n\r\n", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90221, "title": "Fix torch.distributed.run init connect timeout by comparing `host` with the current IP list", "time": "2022-12-05T21:37:37Z", "body": "Summary:\nPull Request: https://github.com/pytorch/pytorch/issues/79388\n\nFix torch.distributed.run init connect timeout by comparing `host` with the current IP list.\n\nTest Plan: unit tests\n\nDifferential Revision: D41373962\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}]}
{"number": 90220, "title": "Update patch release cherry pick condition", "time": "2022-12-05T21:33:04Z", "body": null, "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90217, "title": "Reland: [Autograd] Use in-place input accumulation fast path for dense Tensors.", "time": "2022-12-05T21:10:58Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90217\n\nIdentical to https://github.com/pytorch/pytorch/pull/88339 except with a `.has_storage()` check before `.storage()`.\n\nDifferential Revision: [D41737935](https://our.internmc.facebook.com/intern/diff/D41737935/)\n\ncc @ezyang @albanD @zou3519 @gqchen @pearu @nikitaved @soulitzer @Lezcano @Varal7", "label": [{"id": 1076922545, "node_id": "MDU6TGFiZWwxMDc2OTIyNTQ1", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20autograd", "name": "module: autograd", "color": "f7e101", "default": false, "description": "Related to torch.autograd, and the autograd engine in general"}, {"id": 3769211609, "node_id": "LA_kwDOA-j9z87gqZrZ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20autograd", "name": "release notes: autograd", "color": "9433A1", "default": false, "description": "release notes category"}, {"id": 3773062847, "node_id": "LA_kwDOA-j9z87g5F6_", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20performance", "name": "topic: performance", "color": "AE9D13", "default": false, "description": "topic category"}]}
{"number": 90211, "title": "Align max_pool1d Error Checking between CPU and CUDA/CPU requires_grad", "time": "2022-12-05T20:26:19Z", "body": "Fixes https://github.com/pytorch/pytorch/issues/85712\r\n\r\nStandardizes error checking for max_pool1d between CPU and CPU requires_grad/CUDA.\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 90207, "title": "Enable proxy tensor tests for ops that don't support floating point types", "time": "2022-12-05T19:55:26Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90207\n\r\nFixes https://github.com/pytorch/pytorch/issues/89060", "label": [{"id": 3769203231, "node_id": "LA_kwDOA-j9z87gqXof", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20composability", "name": "release notes: composability", "color": "A5282C", "default": false, "description": "release notes category"}, {"id": 4347564645, "node_id": "LA_kwDOA-j9z88AAAABAyKSZQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20ProxyTensor", "name": "module: ProxyTensor", "color": "306932", "default": false, "description": ""}]}
{"number": 90206, "title": "Integrate `ncclCommInitRankConfig`", "time": "2022-12-05T19:52:59Z", "body": "Use `ncclCommInitRankConfig` instead of `ncclCommInitRank` if it's not ROCm and the available NCCL >= 2.14.3.\r\n\r\nRef:\r\n- https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/comms.html#c.ncclCommInitRankConfig\r\n- https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/communicators.html#creating-a-communication-with-options\r\n\r\nfuture work\r\n- take and reflect `blocking` parameter\r\n\r\nSigned-off-by: Masaki Kozuki <mkozuki@nvidia.com>", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3769218061, "node_id": "LA_kwDOA-j9z87gqbQN", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(c10d)", "name": "release notes: distributed (c10d)", "color": "EE86D4", "default": false, "description": "release notes category"}]}
{"number": 90198, "title": "naive attempt", "time": "2022-12-05T18:20:26Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 3769207236, "node_id": "LA_kwDOA-j9z87gqYnE", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20nn", "name": "release notes: nn", "color": "29E07F", "default": false, "description": "release notes category"}, {"id": 3773062847, "node_id": "LA_kwDOA-j9z87g5F6_", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20performance", "name": "topic: performance", "color": "AE9D13", "default": false, "description": "topic category"}]}
{"number": 90188, "title": "[inductor] Update TIMM skip list", "time": "2022-12-05T16:39:41Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90188\n\n", "label": [{"id": 3769204372, "node_id": "LA_kwDOA-j9z87gqX6U", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20releng", "name": "release notes: releng", "color": "EED552", "default": false, "description": "release notes category"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90177, "title": "Implement hybrid compressed sparse to dense conversions.", "time": "2022-12-05T13:05:29Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90177\n\n\n\ncc @nikitaved @pearu @cpuhrsch @amjames @bhosmer", "label": [{"id": 679954154, "node_id": "MDU6TGFiZWw2Nzk5NTQxNTQ=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20sparse", "name": "module: sparse", "color": "f7e101", "default": false, "description": "Related to torch.sparse"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90176, "title": "enable -Wdeprecated-declarations", "time": "2022-12-05T12:40:13Z", "body": "Stack created with [Sapling](https://sapling-scm.com). Best reviewed with [ReviewStack](https://reviewstack.dev/pytorch/pytorch/pull/90176).\n* __->__ #90176\n\nenable -Wdeprecated-declarations\n\nTest Plan: Rely on CI.\n\n", "label": [{"id": 1303456553, "node_id": "MDU6TGFiZWwxMzAzNDU2NTUz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20mkldnn", "name": "module: mkldnn", "color": "f7e101", "default": false, "description": "Related to Intel IDEEP/MKL-DNN (mkldnn) integration"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90173, "title": "remove torch.equal usages in test suite", "time": "2022-12-05T12:31:37Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90173\n* #90172\n* #90005\n* #90004\n* #90003\n\r\nRedo of #89527\n\ncc @mcarilli @ptrblck @leslie-fang-intel @jgong5", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1972560798, "node_id": "MDU6TGFiZWwxOTcyNTYwNzk4", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20amp%20(automated%20mixed%20precision)", "name": "module: amp (automated mixed precision)", "color": "f7e101", "default": false, "description": "autocast"}, {"id": 2484356711, "node_id": "MDU6TGFiZWwyNDg0MzU2NzEx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20testing", "name": "module: testing", "color": "fbca04", "default": false, "description": "Issues related to the torch.testing module (not tests)"}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}, {"id": 4417837777, "node_id": "LA_kwDOA-j9z88AAAABB1La0Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/mps", "name": "ciflow/mps", "color": "02EDA7", "default": false, "description": "Run MPS tests (subset of trunk)"}]}
{"number": 90172, "title": "improve memory footprint of assert_close", "time": "2022-12-05T12:31:33Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #90173\n* __->__ #90172\n* #90005\n* #90004\n* #90003\n\r\nPer title. This PR does two things in response to #84944, that I'll highlight inline.\r\n", "label": [{"id": 1300909511, "node_id": "MDU6TGFiZWwxMzAwOTA5NTEx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20memory%20usage", "name": "module: memory usage", "color": "f7e101", "default": false, "description": "PyTorch is using more memory than it should, or it is leaking memory"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2484356711, "node_id": "MDU6TGFiZWwyNDg0MzU2NzEx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20testing", "name": "module: testing", "color": "fbca04", "default": false, "description": "Issues related to the torch.testing module (not tests)"}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}]}
{"number": 90162, "title": "[Draft] Test ideep commit", "time": "2022-12-05T07:53:11Z", "body": "Only for test purpose.\r\nTest PyTorch CI with new ideep commit.\n\ncc @gujinghui @PenghuiCheng @XiaobingSuper @jianyuh @VitalyFedyunin @jgong5 @mingfeima @sanchitintel @ashokei @jingxu10 @min-jean-cho @yanbing-j @Guobing-Chen @Xia-Weiwen", "label": [{"id": 1303456553, "node_id": "MDU6TGFiZWwxMzAzNDU2NTUz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20mkldnn", "name": "module: mkldnn", "color": "f7e101", "default": false, "description": "Related to Intel IDEEP/MKL-DNN (mkldnn) integration"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90161, "title": "[xla hash update] update the pinned xla hash", "time": "2022-12-05T07:38:43Z", "body": "This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/master/.github/workflows/_update-commit-hash.yml).\nUpdate the pinned xla hash.", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90159, "title": "[1/N] Adapt DTensor's test_pointwise_ops To Using MultiThreadedTestCase", "time": "2022-12-05T07:00:24Z", "body": "This series of PRs aim to adapt existing DTensor pointwise ops unit test from using `MultiProcessTestCase` to `MultiThreadedTestCase`. Currently we found a set of issues in `MultiThreadedTestCase` (#89941) which restrains it due to some potential negative consequences. In this case, there're mainly two issues:\r\n\r\n1. Inconsistent random seed set up among ranks. This is because of the blank implementation of `setUp()` within `MultiThreadedTestCase`.\r\n2. A single failure case causes the whole test hang until timeout. This is because the `join()` function of multi-threaded ProcessGroup's `Collective` class uses a blocking `wait()` without passing a `timeout` argument. \r\n\r\nThis PR is the first in stack of PRs and focuses on adapting DTensor pointwise ops unit test to using `MultiThreadedTestCase` and solving the hanging issue, to unblock fixing other issues that prevents us to adapt `MultiThreadedTestCase`.", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90157, "title": "[Quant] Remove all the dequant nodes when the ref module has multi input args", "time": "2022-12-05T06:40:11Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #90364\n* #90262\n* __->__ #90157\n\r\n**Summary**:\r\nWhen converting a ref module into a quant module, `_lower_static_weighted_ref_module` pass assumes the `ref_node` only has 1 input node, and only remove the first `dequant` node. However, when we enable the `conv add` fusion, there will be a extra input node from `add` node besides the original input node from `conv`. Similar as did in the `_lower_quantized_binary_op` pass https://github.com/pytorch/pytorch/blob/41c3b41b92f5019f8d5e2f2846a06b87db01ca4e/torch/ao/quantization/fx/_lower_to_native_backend.py#L766-L775, We should remove all the `dequant` nodes in the  `_lower_static_weighted_ref_module` pass.\r\n\r\n**Test Plan**:\r\nIt's a bug fix instead of a new feature. When we enable the `conv add` fusion PR later, we will add test cases accordingly.\r\n\n\ncc @jerryzh168 @jianyuh @raghuramank100 @jamesr66a @vkuzo @jgong5 @Xia-Weiwen @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10", "label": [{"id": 1031340614, "node_id": "MDU6TGFiZWwxMDMxMzQwNjE0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20quantization", "name": "oncall: quantization", "color": "f7e101", "default": false, "description": "Quantization support in PyTorch"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}, {"id": 4726472156, "node_id": "LA_kwDOA-j9z88AAAABGbg93A", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20AO%20frontend", "name": "release notes: AO frontend", "color": "ededed", "default": false, "description": null}]}
{"number": 90150, "title": "[inductor] fix crash issue when input is a view tensor", "time": "2022-12-05T03:25:50Z", "body": "Fix the crash failure mentioned in https://github.com/pytorch/torchdynamo/issues/1946\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90149, "title": "Fix a static initialization order fiasco in c10d", "time": "2022-12-05T03:25:38Z", "body": "The `TORCH_LIBRARY_IMPL` registrations in `OpsImpl.cpp` needs to happen after `ProcessGroup` is registered as a torch class -- which happens in `Ops.cpp`. However, the order of the registrations is undefined between the two files.\r\n\r\nIf the registration in `OpsImpl.cpp` runs before `Ops.cpp`, we get a crash at program launch similar to #83255 . This happens in our internal build.\r\n\r\nThis PR moves `OpsImpl.cpp` to the end of `Oops.cpp`. Because according to the omniscient lord of chatGPT:\r\n<img width=\"600\" alt=\"2022-12-04_19-25\" src=\"https://user-images.githubusercontent.com/1381301/205542847-3535b319-3c2a-4e8e-bc11-27913f6afb39.png\">\r\n\r\n\r\n ", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769218061, "node_id": "LA_kwDOA-j9z87gqbQN", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(c10d)", "name": "release notes: distributed (c10d)", "color": "EE86D4", "default": false, "description": "release notes category"}]}
{"number": 90148, "title": "Some memory saving in large unittests", "time": "2022-12-05T02:31:34Z", "body": "Two tests test_large_cumsum, test_large_cumprod use a lot of memory. This PR:\r\n* Reduces their memory usage by: avoid `self.assertEqual` and avoid a temporary python variable\r\n* Mark their memory requirement by decorator.\r\n\r\n\r\nrelated to https://github.com/pytorch/pytorch/issues/84944", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90145, "title": "refactor show_traces in memory_tracker", "time": "2022-12-04T22:25:07Z", "body": "refactor show_tracers in memory_tracker to make it plot multiple figures and also can load serialized stats and then plot figures", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90140, "title": "[WIP] [Do not review] use shape_env for source of truth for inductor sizevars", "time": "2022-12-04T19:20:26Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90140\n* #90139\n\n\n\ncc @mlazos @soumith @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90130, "title": "Add tests to aid in debugging dynamo + aot_autograd dedup", "time": "2022-12-03T22:13:04Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #90370\n* __->__ #90130\n* #90129\n\r\nThis adds a pair of equivalent tests, both of which invoke aot_autograd with the same inputs, more or less. One through dynamo, one directly. Where the direct call fails, dynamo passes due to guards. This pair of tests protects that behavior and sets us up for future refactors. \r\n\r\ncc @mlazos @soumith @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}]}
{"number": 90127, "title": "wip", "time": "2022-12-03T22:06:28Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* #90128\r\n* #90123\r\n* __->__ #90124\r\n\r\nwip\r\n\r\nwip\r\n\r\nWIP\r\n\r\nUseful tests\r\n\r\nlint\r\n\r\ncc @mlazos @soumith @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90121, "title": "[Inductor] make `sym_float` and `sym_floor` work with symbolic shapes", "time": "2022-12-03T19:23:11Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* __->__ #90121\n* #90120\n\n\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90120, "title": "[Inductor] make sure the type signature always matches", "time": "2022-12-03T19:23:05Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #90121\n* __->__ #90120\n\n\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90119, "title": "[WIP] CUTLASS 2:4 sparsity integration", "time": "2022-12-03T19:15:16Z", "body": "Based on https://github.com/NVIDIA/cutlass/blob/master/examples/15_ampere_sparse_tensorop_gemm/ampere_sparse_tensorop_gemm.cu\r\n\r\nWork in progress", "label": [{"id": 3769217376, "node_id": "LA_kwDOA-j9z87gqbFg", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20sparse", "name": "release notes: sparse", "color": "2A7626", "default": false, "description": "release notes category"}]}
{"number": 90112, "title": "Don't overwrite torch::nn::Sequential module names when cloning", "time": "2022-12-03T09:31:17Z", "body": "__What?__\r\nFixes #71069\r\n\r\nFurther changes:\r\n1. Changed the parameters of the private `torch::nn::Sequential::push_back()` method used to unpack the parameters from the variadic constructor\r\n2. Added documentation to said constructor\r\n\r\n__Why?__\r\n1. I initially changed the private `push_back()` method to solve what turned out to be a non-issue. Afterwards, I found these changes made it easier to understand. Also, now we don't need to rely on any public `push_back()` methods in unpacking: originally, the base case of 1 parameter in the variadic template was covered by the public `push_back(M&& module)`.\r\n\r\n__Further Development?__\r\nPossible changes with C++17 (lmk if there's a wish list for that):\r\n* Replace `c10::optional` with `std::optional`\r\n* Add fine-grained type-checking with type traits in `class Sequential`; will save following the dependency chain\r\n\r\n__P.S.__\r\nI think you guys squash commits; if not, I can smash them.", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3769208226, "node_id": "LA_kwDOA-j9z87gqY2i", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20cpp", "name": "release notes: cpp", "color": "8250CF", "default": false, "description": "release notes category"}]}
{"number": 90105, "title": "[WIP] Checkpoint that can be nested and contain grad", "time": "2022-12-03T03:08:24Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90105\n* #85849\n* #88357\n\r\nBasically works in all situations I've tested, as in it correctly recomputes all the values when using nested checkpointing and even if you mix that with higher order autograd. However, I don't know how much memory it actually saves; it could be leaking everything which may defeat the purpose of checkpointing at all.\r\n\r\n```\r\nimport torch\r\nfrom torch.autograd.graph import _checkpoint as checkpoint\r\n\r\nx = torch.ones(1, requires_grad=True)\r\ndef fn(x):\r\n  return x.sin().exp().sin()\r\n\r\ndef c(fn):\r\n  def wrapped(*args, **kwargs):\r\n    return checkpoint(fn, *args, **kwargs)\r\n  return wrapped\r\n\r\ndef g(fn):\r\n    def wrapper(x):\r\n        with torch.enable_grad():\r\n            out = fn(x)\r\n            grad_input = torch.autograd.grad(out, inputs=(x,), create_graph=True)[0]\r\n        return grad_input\r\n    return wrapper\r\n\r\ndef sum(fn):\r\n    def wrapped(x):\r\n        return fn(x).sum()\r\n    return wrapped\r\n\r\ngrad = g\r\n\r\nout = g(c(g(c(fn))))(x)\r\nprint(\"out:\", out)\r\nout = g(g(fn))(x)\r\nprint(\"out:\", out)\r\n```", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90103, "title": "Nuttall window", "time": "2022-12-03T02:56:28Z", "body": "Relates #85366", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 90101, "title": "Call profiler step via optimizer post hook", "time": "2022-12-03T01:43:40Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90099, "title": "[torchgen] Move Executorch custom ops logic into torchgen", "time": "2022-12-03T00:47:47Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89596\n* __->__ #90099\n* #90098\n* #89595\n* #89594\n* #89487\n\r\n## Logic to handle custom ops\r\n\r\nWe generate files for custom ops, so that they can be registered into PyTorch.\r\n\r\nGenerated file is `Register{dispatch_key}CustomOps.cpp` (dispatch_key = CPU), it's basically the same as vanilla PyTorch `RegisterCPU.cpp`. The only difference is that we bind to native functions directly.\r\n\r\nAs an example: \r\n\r\n```cpp\r\nnamespace {\r\n\r\nat::Tensor & wrapper_out_unsqueeze_out(const at::Tensor & self, int64_t dim, at::Tensor & out) {\r\n    // No device check\r\n\r\n\r\n  // DeviceGuard omitted\r\n  return torch::executor::native::unsqueeze_out(self, dim, out);\r\n}\r\n} // anonymous namespace\r\n\r\nTORCH_LIBRARY_IMPL(aten, CPU, m) {\r\n\r\nm.impl(\"unsqueeze.out\",\r\nTORCH_FN(wrapper_out_unsqueeze_out));\r\n}\r\n```\r\n\r\nDifferential Revision: [D41597382](https://our.internmc.facebook.com/intern/diff/D41597382/)\r\n\r\n**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D41597382/)!", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90098, "title": "[torchgen] Move Executorch unboxing logic into torchgen", "time": "2022-12-03T00:47:42Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89596\n* #90099\n* __->__ #90098\n* #89595\n* #89594\n* #89487\n\r\n## Adding the \"unboxing\" logic for Executorch\r\n\r\nThis PR adds `unboxing.py` which converts a `EValue` (similar to `IValue`) to its corresponding C++ type, based on the `CppSignature`.\r\n\r\nAdded unit tests to it in `test_executorch_unboxing.py`. Notice that this unboxing logic should work for both ATen types and Executorch types, hence the unit tests are parametrized.\r\n\r\nDifferential Revision: [D41597383](https://our.internmc.facebook.com/intern/diff/D41597383/)\r\n\r\n**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D41597383/)!", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90096, "title": "Update to sdp benchmark to take into account pt2.0 stack", "time": "2022-12-03T00:19:43Z", "body": "Updates to sdp benchmark to fix failures due to sdp being included into nn.f.mha. As well compare against compiled version.\r\n", "label": []}
{"number": 90089, "title": "Purge MINIFIER_SPAWNED", "time": "2022-12-02T21:54:21Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90089\n* #90039\n\nPurge MINIFIER_SPAWNED\n\ncc @mlazos @soumith @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90083, "title": "[Vulkan] Enable including GLSL files from custom locations in gen_vulkan_spv", "time": "2022-12-02T20:43:24Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90083\n* #90082\n* #90081\n* #90080\n\nTo include custom locations when building with buck, use a ```-c gen_vulkan_spv.additional_glsl_paths=\"...\"``` flag where ... is a list of filegroups and source directory paths separated by spaces,\n\nex. to include the sources added in D41413913, you would use\n\n```\nbuck build ... -c gen_vulkan_spv.additional_glsl_paths=\"//xplat/caffe2:test_glsl_src_path_a test_src/a //xplat/caffe2:test_glsl_src_path_b test_src/b\"\n```\n\n(as shown in the test plan)\n\nDifferential Revision: [D41413914](https://our.internmc.facebook.com/intern/diff/D41413914/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D41413914/)!", "label": [{"id": 3769206412, "node_id": "LA_kwDOA-j9z87gqYaM", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20vulkan", "name": "release notes: vulkan", "color": "23DD30", "default": false, "description": "release notes category"}]}
{"number": 90082, "title": "[Vulkan] Remove GLSL Code Gen", "time": "2022-12-02T20:43:19Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #90083\n* __->__ #90082\n* #90081\n* #90080\n\nGLSL Code Gen is not used, so this diff removes\n- GLSL parts of ShaderSource\n- Anything enclosed by USE_VULKAN_SHADERC_RUNTIME, as well as the flag itself\n- gen_vulkan_glsl script\n\nPlus some additional refactoring\n\nDifferential Revision: [D41358861](https://our.internmc.facebook.com/intern/diff/D41358861/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D41358861/)!", "label": [{"id": 3769206412, "node_id": "LA_kwDOA-j9z87gqYaM", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20vulkan", "name": "release notes: vulkan", "color": "23DD30", "default": false, "description": "release notes category"}]}
{"number": 90081, "title": "[Vulkan] Generate ShaderInfos Directly via Codegen in gen_vulkan_spv", "time": "2022-12-02T20:43:13Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #90083\n* #90082\n* __->__ #90081\n* #90080\n\nBefore this change, we have the data members which make up a ```ShaderInfo``` sitting in ```spv.h/.cpp``` in an unorganized manner. This diff makes the change such that the ```ShaderInfo```s are initialized directly in spv.h/.cpp\n\nNow spv.h looks like\n```\n#pragma once\n#include <stdint.h>\n#include <vector>\n#include <string>\n#include <ATen/native/vulkan/api/Types.h>\n#include <ATen/native/vulkan/api/vk_api.h>\nnamespace at {\nnamespace native {\nnamespace vulkan {\nnamespace api {\nstruct ShaderInfo;\n} // namespace api\nextern const api::ShaderInfo adaptive_avg_pool2d_spv;\n...\nextern const api::ShaderInfo conv2d_pw_2x2_spv;\n} // namespace vulkan\n} // namespace native\n} // namespace at\n```\n(Full File: https://www.internalfb.com/phabricator/paste/view/P557399150)\nand spv.cpp looks like\n```\n#include <ATen/native/vulkan/spv.h>\n#include <ATen/native/vulkan/api/Shader.h>\nnamespace at {\nnamespace native {\nnamespace vulkan {\nnamespace {\nconst uint32_t adaptive_avg_pool2d_spv_bin[] = {\n  119734787,\n  ...\n};\n...\nconst uint32_t conv2d_pw_2x2_spv_bin[] = {\n  119734787,\n  ...\n};\n} // namespace\nconst api::ShaderInfo adaptive_avg_pool2d_spv(\n  \"adaptive_avg_pool2d_spv\",\n  adaptive_avg_pool2d_spv_bin,\n  3204,\n  {VK_DESCRIPTOR_TYPE_STORAGE_IMAGE, VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER}\n);\n...\nconst api::ShaderInfo conv2d_pw_2x2_spv(\n  \"conv2d_pw_2x2_spv\",\n  conv2d_pw_2x2_spv_bin,\n  7736,\n  {VK_DESCRIPTOR_TYPE_STORAGE_IMAGE, VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER},\n  {2, 2, 1},\n  api::StorageType::TEXTURE_2D,\n  api::StorageType::TEXTURE_2D\n);\n} // namespace vulkan\n} // namespace native\n} // namespace at\n```\n(Full File: https://www.internalfb.com/intern/everpaste/?handle=GNHk2xKUX6ivv-UCAMLwrYibYmkLbsIXAAAz)\n\nDifferential Revision: [D41354313](https://our.internmc.facebook.com/intern/diff/D41354313/)", "label": [{"id": 3769206412, "node_id": "LA_kwDOA-j9z87gqYaM", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20vulkan", "name": "release notes: vulkan", "color": "23DD30", "default": false, "description": "release notes category"}]}
{"number": 90080, "title": "[Vulkan] Merge ShaderSource into ShaderInfo", "time": "2022-12-02T20:43:08Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #90083\n* #90082\n* #90081\n* __->__ #90080\n\n```ShaderInfo``` was added by Kimish in D40280338 to be an extension of ```ShaderSource``` with extra fields. In this diff, I merge the two into one struct, using the combined struct in place of wherever either of the two was used before\n\nDifferential Revision: [D41197273](https://our.internmc.facebook.com/intern/diff/D41197273/)", "label": [{"id": 3769206412, "node_id": "LA_kwDOA-j9z87gqYaM", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20vulkan", "name": "release notes: vulkan", "color": "23DD30", "default": false, "description": "release notes category"}]}
{"number": 90077, "title": "functorch.jvp support for autograd.Function", "time": "2022-12-02T19:46:07Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #90418\n* #90415\n* __->__ #90077\n* #90240\n* #90037\n* #89860\n* #89859\n* #89858\n\nThis PR adds functorch.jvp support for autograd.Function. It does so by\nadding a jvp rule for custom_function_call.\n\nFor a regular PyTorch operation (like at::sin), the VariableType kernel:\n- re-dispatches to at::sin\n- calls the jvp rule for at::sin\n\nThe jvp rule for custom_function_call does just that. It constructs a\nnew autograd.Function (because the above logic already exists). Inside\nthe forward, it re-dispatches to custom_function_call. In the jvp rule,\nit just calls whatever the jvp rule is supposed to be.\n\nSince this logic is really close to the custom_function_call_grad, I\njust put them together.\n\nTest Plan:\n- added jvp rules to the autograd.Function in autograd_function_db", "label": [{"id": 4395386783, "node_id": "LA_kwDOA-j9z88AAAABBfxHnw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20functorch", "name": "release notes: functorch", "color": "957DAE", "default": false, "description": "release notes category"}]}
{"number": 90056, "title": "[WIP] Use Blackhex/pytorch-builder for testing purposes.", "time": "2022-12-02T10:28:37Z", "body": "This PR only tests https://github.com/pytorch/builder/pull/1217\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3618084153, "node_id": "LA_kwDOA-j9z87Xp5U5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/binaries", "name": "ciflow/binaries", "color": "245567", "default": false, "description": "Trigger all binary build and upload jobs on the PR"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90053, "title": "[WIP][fix] integral support for round.decimals", "time": "2022-12-02T08:20:15Z", "body": "Fixes #ISSUE_NUMBER\r\n\n\ncc @VitalyFedyunin @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 90049, "title": "[WIP] Test invariant", "time": "2022-12-02T07:35:23Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90049\n* #90039\n\n\n\ncc @mlazos @soumith @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90037, "title": "autograd.Function supports vmap staticmethod", "time": "2022-12-02T00:59:39Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #90418\n* #90415\n* #90077\n* #90240\n* __->__ #90037\n* #89860\n* #89859\n* #89858\n\nThis PR adds a `vmap` staticmethod to autograd.Function and a\ncorresponding vmap kernel for custom_function_call. These two items mean\nthat autograd.Function with a vmap staticmethod can be used with vmap.\n\n```py\nclass NumpyMul(torch.autograd.Function)\n    staticmethod\n    def forward(x, y):\n        return torch.tensor(to_numpy(x) * to_numpy(y), device=x.device)\n\n    staticmethod\n    def setup_context(ctx, outputs, x, y):\n        ctx.save_for_backward(x, y)\n\n    staticmethod\n    def backward(ctx, grad_output):\n        x, y = ctx.saved_tensors\n        gx = None\n        if isinstance(x, torch.Tensor) and x.requires_grad:\n            gx = NumpyMul.apply(grad_output, y)\n        gy = None\n        if isinstance(y, torch.Tensor) and y.requires_grad:\n            gy = NumpyMul.apply(grad_output, x)\n        return gx, gy\n\n    staticmethod\n    def vmap(info, in_dims, x, y):\n        x_bdim, y_bdim = in_dims\n        x = x.movedim(x_bdim, -1) if x_bdim else x.unsqueeze(-1)\n        y = y.movedim(y_bdim, -1) if y_bdim else y.unsqueeze(-1)\n        result = NumpyMul.apply(x, y)\n        result = result.movedim(-1, 0)\n        return result, 0\n```\n\nAPI Spec\n- the staticmethod takes two arguments (info, in_dims) as well as the\nunexpanded inputs (x, y).\n- If we think about it as `vmap(info, in_dims, *args)`, `in_dims` is a\npytree with the same tree structure as args. It has None if the arg is\nnot being vmapped over and an integer vmapped dimension index if it is.\n- `info` is an object with metadata about the vmap. It currently has one\nfield, `info.batch_size`. In the future we can extend this by adding\nthings like the randomness information.\n- If there is a single vmap going on, (x, y) are NOT BatchedTensors,\nthey've already been unpacked.\n- We expect the user to return a `(outputs, out_dims)` tuple. `out_dims`\nmust \"broadcast\" to the same pytree structure as `outputs`.\n\nSemantics\n- vmap(NumpyMul.apply)(x) will apply the vmap staticmethod if there is\none and will never actually run NumpyMul.forward.\n- In order for the autograd.Function to support nested vmap (e.g.,\n`vmap(vmap(NumpyMul.apply))(x)`, then the vmap staticmethod must call\ninto operations that vmap understands (i.e. PyTorch operators or more\nautograd.Function).\n\nAt a high level, this PR:\n- adds a vmap rule for custom_function_call\n\nTesting\n- Added some tests for in_dims and info\n- Added vmap staticmethod to most of the autograd.Function in\nautograd_function_db and sent them through functorch's vmap-related\nOpInfo tests\n\nFuture\n- Better error messages if the user gets the return contract wrong. I\ndidn't include them in this PR because it might involve a refactor of\nsome of the existing code in functorch/_src/vmap.py that will add\n~200LOC to the PR, but LMK if you'd prefer it here.", "label": [{"id": 4395386783, "node_id": "LA_kwDOA-j9z88AAAABBfxHnw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20functorch", "name": "release notes: functorch", "color": "957DAE", "default": false, "description": "release notes category"}]}
{"number": 90032, "title": "For test_transformer_FuseConvBNNoConvBias set deadline to None", "time": "2022-12-01T23:52:15Z", "body": "Summary: I am on the current oncall  for aml_ai_platform and this is a fix for one of the failing tests.\n\nTest Plan: The function fails the deadline on the first run but passes on subsequent. Implemented the suggested solution of setting deadline=None.\n\nDifferential Revision: D41669143\n\n", "label": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false, "description": ""}, {"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}]}
{"number": 90024, "title": "Meta kernel for allreduce_", "time": "2022-12-01T22:03:26Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #90349\n* __->__ #90024\n* #89984\n\n", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 90010, "title": "one_hot type hint", "time": "2022-12-01T15:29:54Z", "body": "Fixes #89834 \r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 90005, "title": "add torch.testing.assert_not_close", "time": "2022-12-01T12:06:36Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #90173\n* #90172\n* __->__ #90005\n* #90004\n* #90003\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2484356711, "node_id": "MDU6TGFiZWwyNDg0MzU2NzEx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20testing", "name": "module: testing", "color": "fbca04", "default": false, "description": "Issues related to the torch.testing module (not tests)"}, {"id": 3769210091, "node_id": "LA_kwDOA-j9z87gqZTr", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20python_frontend", "name": "release notes: python_frontend", "color": "87E01C", "default": false, "description": "release notes category"}, {"id": 3773060564, "node_id": "LA_kwDOA-j9z87g5FXU", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20new%20feature", "name": "topic: new feature", "color": "C32883", "default": false, "description": "topic category"}]}
{"number": 90004, "title": "add not_close_error_metas for internal comparison machinery", "time": "2022-12-01T12:06:32Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #90173\n* #90172\n* #90005\n* __->__ #90004\n* #90003\n\r\nWhile discussing a possible addition of `assert_not_close` to the API (See #90005 later in the stack), it became clear that we should have an intermediate function that returns a bool-ish value that one can assert on. This PR introduces this function as `are_equal` as replacement for `assert_equal`. Interface is the same, but instead of raising in case a comparison failed, we return the `ErrorMeta`'s of all failures and leave it to the caller to handle. Note that this only applies to errors raised during the comparison stage. Everything else, e.g. only setting `atol` *or* `rtol`, will raise just as before.\r\n\r\nWe decided to keep this private for now unless there is user demand. The largest issue that needs to be solved before this can become public is the return type: if we have something like `torch.testing.are_close` we are targeting two uses cases:\r\n\r\n1. Using it to branch inside code like `if are_close(...):`\r\n2. Using it to assert closeness inside a test like `assert are_close(...)`. This is the default way to assert something with `pytest`\r\n\r\nTo do that, the return type has to be bool-ish, i.e. being an instance of `bool` or implementing `__bool__`. Plus, `bool(are_close()) is True` needs to be the if the inputs are close and `False` otherwise. The current logic of `are_close` satisfies the former, but violates the latter. In case everything is close, we return an empty list, but `bool([]) is False`.\r\n\r\nDirectly using an instance of `bool` would work for the requirements above, but then we would have no option to add diagnositics to the error. Meaning `assert are_close()` would work, but would be non-descriptive. \r\n\r\nUsing `Tuple[bool, str]` would work in general, but is quite dangerous and unexpected: since all non-empty tuples evaluate to `True`, this can easily hide bugs if the user is not super careful:\r\n\r\n```pycon\r\n>>> close = (False, \"error message with diagnostics\")\r\n>>> assert close[0]\r\nAssertionError: error message with diagnostics\r\n>>> assert close\r\n```\r\n\r\nOne possible solution here would be a thin custom object:\r\n\r\n```py\r\nclass Close:\r\n    def __init__(self, flag:bool, msg: str = \"\") -> None:\r\n        self._flag = flag\r\n        self._msg = msg\r\n        \r\n    def __bool__(self):\r\n        return self._flag\r\n    \r\n    def __str__(self):\r\n        return self._msg\r\n```\r\n\r\nNow we can do something like\r\n\r\n```pycon\r\nclose = Close(False, \"error message with diagnostics\")  # coming from are_close\r\n>>> if not close:\r\n...     print(\"It works!\")\r\nIt works!\r\n>>> assert close\r\nAssertionError\r\n>>> assert close, close  # This looks weird, but does its job\r\nAssertionError: error message with diagnostics\r\n```\r\n\r\nBut this means we introduce another abstraction that the user has to deal with.\r\n\r\nTo reiterate, we are not going to make `are_close` public until there is user demand, since none of the options above is without flaws.", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2484356711, "node_id": "MDU6TGFiZWwyNDg0MzU2NzEx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20testing", "name": "module: testing", "color": "fbca04", "default": false, "description": "Issues related to the torch.testing module (not tests)"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90003, "title": "minor internal cleanup in assert_close", "time": "2022-12-01T12:06:28Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #90173\n* #90172\n* #90005\n* #90004\n* __->__ #90003\n\r\nPer title. I'm going to highlight them with inline comments.", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2484356711, "node_id": "MDU6TGFiZWwyNDg0MzU2NzEx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20testing", "name": "module: testing", "color": "fbca04", "default": false, "description": "Issues related to the torch.testing module (not tests)"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 90002, "title": "Add shape function for prims::squeeze op", "time": "2022-12-01T12:00:33Z", "body": null, "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 90001, "title": "[FX][Quant] Enable FX quant for patterns like x.view(x.size(...), ...)", "time": "2022-12-01T10:11:20Z", "body": "**Summary**\r\nThis work continues with https://github.com/pytorch/pytorch/pull/83784 by @vkuzo and includes all the changes in that PR.\r\nQuote from https://github.com/pytorch/pytorch/pull/83784:\r\n> Issue #83658 reports that ops followed by a certain pattern of `view` and `size` ops were not quantized correctly by FX graph mode quantization.\r\nBefore this PR, the \"size\" op was in the \"op shares qparams with input\" category, and the code assumed that the input of this op has the same dtype as its output. This led to incorrectly propagating the `int` dtype as the output of whichever op was preceding the `view` op, which in turn made that op blocklisted from quantization.\r\n\r\n> The fix is to create a new category of ops which work on different dtypes of tensors but are not observed. This PR does so for `size`, and also for `shape` since it works the same way.\r\n\r\n**Test plan**\r\n```\r\npython test/test_quantization.py -k test_linear_size_view\r\npython test/test_quantization.py -k test_linear_shape_view\r\n```\n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 89990, "title": "Add bits tensor types", "time": "2022-12-01T05:56:45Z", "body": "Test Plan: CI\n\nDifferential Revision: D41638739\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}]}
{"number": 89984, "title": "Dynamo traces distributed.allreduce", "time": "2022-12-01T01:22:10Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #90349\n* #90024\n* __->__ #89984\n\n- a new guard type (PROCESS_GROUP_MATCH) enforces critical properties\n  of the PG match at runtime, while not exhaustively safeguarding\n  other transient state vars of c++ PG impl\n- allreduce is inplace-only, and stream is ignored\n\nBased on work from Yifu Wang\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89981, "title": "[FSDP][optim_state_dict][5/N] Remove optim_inputs for sharded state_dict.", "time": "2022-12-01T00:16:09Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89981\n* #89980\n* #89900\n* #89899\n* #89898\n\r\nThe argument, `optim_inputs`, is being deprecated. Sharded optimizer state_dict APIs are not be used. It is safe to remove them.\r\n", "label": [{"id": 3769221312, "node_id": "LA_kwDOA-j9z87gqcDA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(fsdp)", "name": "release notes: distributed (fsdp)", "color": "C1F551", "default": false, "description": "release notes category"}]}
{"number": 89980, "title": "[FSDP][optim_state_dict][4/N] Remove the unused _get_flat_param_to_fsdp_module API", "time": "2022-12-01T00:15:46Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89981\n* __->__ #89980\n* #89900\n* #89899\n* #89898\n\r\nThis is an easy PR, just remove an unused internal API.\r\n", "label": [{"id": 3769221312, "node_id": "LA_kwDOA-j9z87gqcDA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(fsdp)", "name": "release notes: distributed (fsdp)", "color": "C1F551", "default": false, "description": "release notes category"}]}
{"number": 89976, "title": "[ONNX] Remove all requantize_bias for quantized ops", "time": "2022-11-30T23:42:50Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #89976\r\n\r\nWe discovered that re-quantizing the bias will cause the output to not match torch eager.\r\n\r\ncc @yufenglee", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3768821874, "node_id": "LA_kwDOA-j9z87go6hy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20onnx", "name": "release notes: onnx", "color": "B4EB93", "default": false, "description": "torch.onnx related changes that should show up in the release notes"}]}
{"number": 89975, "title": "fix aot autograd for None fw inputs", "time": "2022-11-30T23:18:54Z", "body": "hot fix: Confirmed this fixes an internal model that had None as one if its inputs\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #89975\r\n\r\n", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 4395386783, "node_id": "LA_kwDOA-j9z88AAAABBfxHnw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20functorch", "name": "release notes: functorch", "color": "957DAE", "default": false, "description": "release notes category"}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89972, "title": "[TEST] Does inductor ever create fresh sizevars", "time": "2022-11-30T22:53:52Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89972\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89966, "title": "[WIP] support functionalization on torch.cond", "time": "2022-11-30T22:22:21Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89966\n\n", "label": []}
{"number": 89963, "title": "Use single stream for apply optimizer in backward", "time": "2022-11-30T21:33:15Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89963\n* #89194\n\nPer title\n\nDifferential Revision: [D41622106](https://our.internmc.facebook.com/intern/diff/D41622106/)", "label": [{"id": 3769219717, "node_id": "LA_kwDOA-j9z87gqbqF", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(ddp)", "name": "release notes: distributed (ddp)", "color": "C64122", "default": false, "description": "release notes category"}]}
{"number": 89960, "title": "[quantized] [executorch] quantized_embbeding_byte in python", "time": "2022-11-30T21:03:43Z", "body": "Summary: Inefficient impl in python\n\nTest Plan: buck2 test mode/dev //caffe2/test/quantization:test_quantization -- --exact 'caffe2/test/quantization:test_quantization - test_quantized_embedding_byte (caffe2.test.quantization.core.test_quantized_tensor.TestQuantizedTensor)'\n\nDifferential Revision: D41627744\n\n\n\ncc @jerryzh168 @jianyuh @raghuramank100 @jamesr66a @vkuzo @jgong5 @Xia-Weiwen @leslie-fang-intel", "label": [{"id": 1031340614, "node_id": "MDU6TGFiZWwxMDMxMzQwNjE0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20quantization", "name": "oncall: quantization", "color": "f7e101", "default": false, "description": "Quantization support in PyTorch"}, {"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}]}
{"number": 89956, "title": "[Testing CI] Disable implicit fallbacks in benchmarking", "time": "2022-11-30T20:49:04Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89956\n* #89961\n\n\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89947, "title": "[ONNX] Add repro export from `GraphInfo`", "time": "2022-11-30T19:40:26Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89947\n* #89946\n* #89808\n* #89807\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3768821874, "node_id": "LA_kwDOA-j9z87go6hy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20onnx", "name": "release notes: onnx", "color": "B4EB93", "default": false, "description": "torch.onnx related changes that should show up in the release notes"}, {"id": 3773060564, "node_id": "LA_kwDOA-j9z87g5FXU", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20new%20feature", "name": "topic: new feature", "color": "C32883", "default": false, "description": "topic category"}, {"id": 3773061335, "node_id": "LA_kwDOA-j9z87g5FjX", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20improvements", "name": "topic: improvements", "color": "22499E", "default": false, "description": "topic category"}]}
{"number": 89946, "title": "[ONNX] Verification tool to find mismatch in model export", "time": "2022-11-30T19:40:22Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89947\n* __->__ #89946\n* #89808\n* #89807\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3768821874, "node_id": "LA_kwDOA-j9z87go6hy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20onnx", "name": "release notes: onnx", "color": "B4EB93", "default": false, "description": "torch.onnx related changes that should show up in the release notes"}, {"id": 3773060564, "node_id": "LA_kwDOA-j9z87g5FXU", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20new%20feature", "name": "topic: new feature", "color": "C32883", "default": false, "description": "topic category"}, {"id": 3773061335, "node_id": "LA_kwDOA-j9z87g5FjX", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20improvements", "name": "topic: improvements", "color": "22499E", "default": false, "description": "topic category"}]}
{"number": 89944, "title": "Adds more nvidia pypi dependencies", "time": "2022-11-30T19:15:25Z", "body": "This PR adds more nvidia pypi dependencies for cuda 11.7 wheel. Additionally, it pins cufft version to 10.9.0.58 to resolve https://github.com/pytorch/pytorch/issues/88038\r\n\r\nDepends on: https://github.com/pytorch/builder/pull/1196\r\n\r\ncc: @malfet @atalman @ptrblck ", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3618084153, "node_id": "LA_kwDOA-j9z87Xp5U5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/binaries", "name": "ciflow/binaries", "color": "245567", "default": false, "description": "Trigger all binary build and upload jobs on the PR"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4077622962, "node_id": "LA_kwDOA-j9z87zC5ay", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/binaries_conda", "name": "ciflow/binaries_conda", "color": "3FC2AC", "default": false, "description": "Trigger binary build and upload jobs for conda on the PR"}, {"id": 4077623621, "node_id": "LA_kwDOA-j9z87zC5lF", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/binaries_wheel", "name": "ciflow/binaries_wheel", "color": "AD47B4", "default": false, "description": "Trigger binary build and upload jobs for wheel on the PR"}]}
{"number": 89918, "title": "If a torch.* returns non-Tensor, make this unimplemented rather than assert.", "time": "2022-11-30T16:23:18Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89919\n* __->__ #89918\n* #89879\n* #89871\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 3773061952, "node_id": "LA_kwDOA-j9z87g5FtA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20bug%20fixes", "name": "topic: bug fixes", "color": "A44870", "default": false, "description": "topic category"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}, {"id": 4705034454, "node_id": "LA_kwDOA-j9z88AAAABGHEg1g", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20dynamo", "name": "release notes: dynamo", "color": "b60205", "default": false, "description": ""}]}
{"number": 89911, "title": "Run meta tests for tensor constructors", "time": "2022-11-30T12:53:12Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89911\n\n", "label": [{"id": 3769216677, "node_id": "LA_kwDOA-j9z87gqa6l", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20Meta%20API", "name": "release notes: Meta API", "color": "C36F2F", "default": false, "description": "release notes category"}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89909, "title": "Remove Windows compilation dependencies installation from CI/CD scripts", "time": "2022-11-30T12:19:08Z", "body": "They should be already installed in the runner VM.\r\n\n\ncc @seemethere @malfet @pytorch/pytorch-dev-infra", "label": [{"id": 1300896147, "node_id": "MDU6TGFiZWwxMzAwODk2MTQ3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20ci", "name": "module: ci", "color": "f7e101", "default": false, "description": "Related to continuous integration"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3618084153, "node_id": "LA_kwDOA-j9z87Xp5U5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/binaries", "name": "ciflow/binaries", "color": "245567", "default": false, "description": "Trigger all binary build and upload jobs on the PR"}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}]}
{"number": 89901, "title": "Add fused lstm_cell on CPU", "time": "2022-11-30T07:30:25Z", "body": null, "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1972560798, "node_id": "MDU6TGFiZWwxOTcyNTYwNzk4", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20amp%20(automated%20mixed%20precision)", "name": "module: amp (automated mixed precision)", "color": "f7e101", "default": false, "description": "autocast"}]}
{"number": 89900, "title": "[FSDP][optim_state_dict][3/N] Support use_orig_param optim_state_dict (non-broadcast version)", "time": "2022-11-30T07:24:04Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89981\n* #89980\n* __->__ #89900\n* #89899\n* #89898\n\r\n**What:**\r\nThis PR add the optim state_dict support of `use_orig_params` with rank0_only is False. rank0_only support will be added in a following PR. The design of this PR focus on the simplicity and may not have good performance, especially for optim state_dict loading. Since optim state_dict loading is only called once in the beginning of the training, performance is not the major concern. ", "label": [{"id": 3769221312, "node_id": "LA_kwDOA-j9z87gqcDA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(fsdp)", "name": "release notes: distributed (fsdp)", "color": "C1F551", "default": false, "description": "release notes category"}]}
{"number": 89894, "title": "Move old fastpath into a conditional that's enabled via an attribute, go to sdpa path by default", "time": "2022-11-30T03:35:38Z", "body": "Summary: Move old fastpath into a conditional that's enabled via an attribute, go to sdpa path by default\n\nTest Plan: sandcastle\n\nDifferential Revision: D41590030\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}]}
{"number": 89892, "title": "[ao][ns] PNP demo for exposing arbitrary model transforms", "time": "2022-11-30T03:00:35Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89892\n\nAdding way to use arbitrary prepare and convert functions with PNP.\n\nDifferential Revision: [D41167110](https://our.internmc.facebook.com/intern/diff/D41167110/)", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}]}
{"number": 89886, "title": "Run Mobile Upgrader", "time": "2022-11-30T00:58:18Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89886\n\n", "label": [{"id": 3768809762, "node_id": "LA_kwDOA-j9z87go3ki", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20mobile", "name": "release notes: mobile", "color": "bfdadc", "default": false, "description": "release notes category"}]}
{"number": 89885, "title": "Test Conda+Triton builds", "time": "2022-11-30T00:32:04Z", "body": "Fixes #ISSUE_NUMBER\n", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4077622962, "node_id": "LA_kwDOA-j9z87zC5ay", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/binaries_conda", "name": "ciflow/binaries_conda", "color": "3FC2AC", "default": false, "description": "Trigger binary build and upload jobs for conda on the PR"}, {"id": 4077623621, "node_id": "LA_kwDOA-j9z87zC5lF", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/binaries_wheel", "name": "ciflow/binaries_wheel", "color": "AD47B4", "default": false, "description": "Trigger binary build and upload jobs for wheel on the PR"}]}
{"number": 89880, "title": "[NCCL] (re-open) Optionally avoid `recordStream` calls in `ProcessGroupNCCL`", "time": "2022-11-29T23:34:12Z", "body": "Rebased version of @mcarilli's #76861\r\n\r\nCC @ptrblck ", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1301450383, "node_id": "MDU6TGFiZWwxMzAxNDUwMzgz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20nccl", "name": "module: nccl", "color": "f7e101", "default": false, "description": "Problems related to nccl support"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769218061, "node_id": "LA_kwDOA-j9z87gqbQN", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(c10d)", "name": "release notes: distributed (c10d)", "color": "EE86D4", "default": false, "description": "release notes category"}]}
{"number": 89877, "title": "[inductor] WIP: Allow scheduler to reorder loops when it allow more fusion", "time": "2022-11-29T22:48:39Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89877\n* #90434\n\nThis allows the iteration order of pointwise loops to be changed, if it allows\nfor more fusions. This allows softmax on the outer dimension to be fused into a\nsingle kernel.", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89870, "title": "Add Tests for FX-to-ONNX exporter", "time": "2022-11-29T22:02:26Z", "body": "Run some tests from OpInfo to test FX-to-ONNX exporter.", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3768821874, "node_id": "LA_kwDOA-j9z87go6hy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20onnx", "name": "release notes: onnx", "color": "B4EB93", "default": false, "description": "torch.onnx related changes that should show up in the release notes"}]}
{"number": 89866, "title": "Enable more torchinductor tests in fbcode", "time": "2022-11-29T21:03:41Z", "body": "Test Plan: `buck2 test //caffe2/test/inductor:test_torchinductor`\n\nDifferential Revision: D41474517\n\n\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89865, "title": "set -Werror for builds", "time": "2022-11-29T20:33:07Z", "body": "Stack created with [Sapling](https://sapling-scm.com). Best reviewed with [ReviewStack](https://reviewstack.dev/pytorch/pytorch/pull/89865).\n* __->__ #89865\n* #89852\n* #89851\n\nset -Werror for builds\n\nTest Plan: Rely on CI.\n\n", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 2510754463, "node_id": "MDU6TGFiZWwyNTEwNzU0NDYz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/NNC", "name": "NNC", "color": "e5678d", "default": false, "description": ""}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 89864, "title": "Fix mps constant pad", "time": "2022-11-29T20:12:24Z", "body": "Support arbitrary dimensions for constant padding on MPS\r\n\r\nFixes #89624\r\nFixes #87277\r\n\r\n\r\ncc @kulinseth @albanD @malfet @DenisVieriu97 @razarmehr @abhudev", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 4121057428, "node_id": "LA_kwDOA-j9z871oliU", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20mps", "name": "module: mps", "color": "f7e101", "default": false, "description": "Related to Apple Metal Performance Shaders framework"}, {"id": 4164781683, "node_id": "LA_kwDOA-j9z874PYZz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20mps", "name": "release notes: mps", "color": "1d76db", "default": false, "description": "Release notes category"}, {"id": 4417837777, "node_id": "LA_kwDOA-j9z88AAAABB1La0Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/mps", "name": "ciflow/mps", "color": "02EDA7", "default": false, "description": "Run MPS tests (subset of trunk)"}]}
{"number": 89863, "title": "Set default values of class variable in PassManager", "time": "2022-11-29T19:55:49Z", "body": "Test Plan: CI\n\nDifferential Revision: D41564742\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}]}
{"number": 89861, "title": "[replacing D41588356] Trying to fix issues in TP serialization (#89765)", "time": "2022-11-29T19:11:19Z", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/89765\n\nDifferential Revision:\nD41588356\n\nLaMa Project: L1142320\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 3769202179, "node_id": "LA_kwDOA-j9z87gqXYD", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20package/deploy", "name": "release notes: package/deploy", "color": "EC030C", "default": false, "description": "release notes category"}]}
{"number": 89852, "title": "set -Wsuggest-override for builds", "time": "2022-11-29T18:19:37Z", "body": "Stack created with [Sapling](https://sapling-scm.com). Best reviewed with [ReviewStack](https://reviewstack.dev/pytorch/pytorch/pull/89852).\n* #89865\n* __->__ #89852\n* #89851\n\nset -Wsuggest-override for builds\n\nSummary: This was flagged by a Meta internal build.\n\nTest Plan: Rely on CI.\n\n", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 2510754463, "node_id": "MDU6TGFiZWwyNTEwNzU0NDYz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/NNC", "name": "NNC", "color": "e5678d", "default": false, "description": ""}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 89851, "title": "set -Winconsistent-missing-override for builds", "time": "2022-11-29T18:10:11Z", "body": "Stack created with [Sapling](https://sapling-scm.com). Best reviewed with [ReviewStack](https://reviewstack.dev/pytorch/pytorch/pull/89851).\n* #89865\n* #89852\n* __->__ #89851\n\nset -Winconsistent-missing-override for builds\n\nSummary: This has triggered internally on some PyTorch code.\n\nTest Plan: Rely on CI.\n\n", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 89846, "title": "Automated submodule update: kineto", "time": "2022-11-29T17:15:27Z", "body": "This is an automated pull request to update the first-party submodule for [pytorch/kineto](https://github.com/pytorch/kineto).\n\nNew submodule commit: https://github.com/pytorch/kineto/commit/72fa713ba67fed547d7df5d09bdf869cae40f588\n\nTest Plan: Ensure that CI jobs succeed on GitHub before landing.", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}]}
{"number": 89831, "title": "Caching OneDNN param for conv in TorchInductor", "time": "2022-11-29T09:11:36Z", "body": "Add mechanism in TorchInductor to cache oneDNN related OP param which can be reused for these OPs in later invoking. Conv related OPs are updated to support this feature.\r\n\r\nTwo oneDNN param generation functions are added in pytorch to provide conv param generation based on input tensors and other parameters. They will return a param handler which been stored into a param cache sitting in TorchInductor generated sub-graph code. Conv related OPs in the generated sub-graph code will then query the cache to get the param and use it directly instead of initializing a conv param everytime been invoked.\r\n\r\nTorchDynamo will guard the input shape change and invoke TorchInductor to re-generate new sub-graph code which will then include a new param cache for the new shapes.\r\n\r\ncc @VitalyFedyunin @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @chunyuan-w @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89830, "title": "update transformer init function", "time": "2022-11-29T09:00:33Z", "body": "Fixes #89829\r\n\n\ncc @jbschlosser @bhosmer @cpuhrsch @erichan1", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1900929949, "node_id": "MDU6TGFiZWwxOTAwOTI5OTQ5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20transformer/mha", "name": "oncall: transformer/mha", "color": "f7e101", "default": false, "description": "Issues related to Transformers and MultiheadAttention"}]}
{"number": 89826, "title": "use proper temp directories in test_tensorboard.py", "time": "2022-11-29T08:19:06Z", "body": "The old `temp_dir` is created under `PWD`. But `PWD` may not be writable and in general is not a good place to create temporary directories. Use the standard `tempfile` instead.", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 89814, "title": "[WIP] Generic input shape checking for SDPA", "time": "2022-11-29T01:52:00Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": []}
{"number": 89809, "title": "Fix prelu ref when a.ndim < 2", "time": "2022-11-29T00:30:48Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89809\n\r\nFixes https://github.com/pytorch/pytorch/issues/89560\r\n\r\nPreviously the test case for \"input is 1-D or scalar + weight is not scalar\" did not exist; adding it introduced some failures:\r\n- forward AD (fixed in this PR)\r\n- vmap (filed https://github.com/pytorch/pytorch/issues/89895)\r\n- ref/meta (fixed this PR, though this also regresses nvFuser support)", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769216677, "node_id": "LA_kwDOA-j9z87gqa6l", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20Meta%20API", "name": "release notes: Meta API", "color": "C36F2F", "default": false, "description": "release notes category"}, {"id": 3773061952, "node_id": "LA_kwDOA-j9z87g5FtA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20bug%20fixes", "name": "topic: bug fixes", "color": "A44870", "default": false, "description": "topic category"}]}
{"number": 89808, "title": "[ONNX] Introduce ONNX reference evaluator for verification", "time": "2022-11-29T00:01:39Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89947\n* #89946\n* __->__ #89808\n* #89807\n\r\nReference evaluator requires ONNX >= 1.13. Running in CI is blocked by unable to bump onnx submodule version, like in #83201. Local tests pass.\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3768821874, "node_id": "LA_kwDOA-j9z87go6hy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20onnx", "name": "release notes: onnx", "color": "B4EB93", "default": false, "description": "torch.onnx related changes that should show up in the release notes"}, {"id": 3773060564, "node_id": "LA_kwDOA-j9z87g5FXU", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20new%20feature", "name": "topic: new feature", "color": "C32883", "default": false, "description": "topic category"}, {"id": 3773061335, "node_id": "LA_kwDOA-j9z87g5FjX", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20improvements", "name": "topic: improvements", "color": "22499E", "default": false, "description": "topic category"}]}
{"number": 89807, "title": "[ONNX] Use `VerificationOptions` to wrap option arguments", "time": "2022-11-29T00:01:35Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89947\n* #89946\n* #89808\n* __->__ #89807\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3768821874, "node_id": "LA_kwDOA-j9z87go6hy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20onnx", "name": "release notes: onnx", "color": "B4EB93", "default": false, "description": "torch.onnx related changes that should show up in the release notes"}, {"id": 3773061335, "node_id": "LA_kwDOA-j9z87g5FjX", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20improvements", "name": "topic: improvements", "color": "22499E", "default": false, "description": "topic category"}]}
{"number": 89806, "title": "Added per stream stats for CUDA Caching Allocator", "time": "2022-11-28T23:49:52Z", "body": "Test Plan: Added test to test_cuda.py\n\nDifferential Revision: D41309246\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}]}
{"number": 89802, "title": "Add debugability comments to DDPOptimizer", "time": "2022-11-28T23:34:24Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89457\n* __->__ #89802\n* #89520\n\n\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89797, "title": "update aten op overload to not use `from` to avoid compile errors", "time": "2022-11-28T23:04:22Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89797\n* #89796\n\r\nFix for https://github.com/pytorch/torchdynamo/issues/1795 by changing `random_.from` to `random_.from_int`.\r\n\r\nThe previous signature would fail when printed in an fx graph, because `from` is a reserved python keyword. This change affects serialization but I have added an adapter.\r\n ", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4164781683, "node_id": "LA_kwDOA-j9z874PYZz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20mps", "name": "release notes: mps", "color": "1d76db", "default": false, "description": "Release notes category"}, {"id": 4417837777, "node_id": "LA_kwDOA-j9z88AAAABB1La0Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/mps", "name": "ciflow/mps", "color": "02EDA7", "default": false, "description": "Run MPS tests (subset of trunk)"}]}
{"number": 89796, "title": "update test fixture", "time": "2022-11-28T23:04:19Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89797\n* __->__ #89796\n\n", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 89790, "title": "[Dynamo] Support torch.get_default_dtype", "time": "2022-11-28T21:51:37Z", "body": "Fixes https://github.com/pytorch/torchdynamo/issues/1930\r\n\n\ncc @mlazos @soumith @voznesenskym @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 3212089274, "node_id": "MDU6TGFiZWwzMjEyMDg5Mjc0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/with-ssh", "name": "with-ssh", "color": "127AF1", "default": false, "description": ""}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89773, "title": "Get CI passing", "time": "2022-11-28T18:35:00Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89773\n* #89672\n\n\n\ncc @mlazos @soumith @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89759, "title": "Workaround for NumPy builds that ship with a broken Dlpack deleter", "time": "2022-11-28T16:24:28Z", "body": "NumPy versions 1.22 and 1.23 (and their respective bugfix releases included) have a buggy implementation of the Dlpack deleter that doesn't account for no-GIL contexts. Since we now release the GIL when deallocating tensors in `THPVariable_clear`, this leads to a failure of internal consistency checks when freeing a Dlpack-backed tensor from NumPy.\r\n\r\nThis PR adds a check for the buggy NumPy versions and overrides the `DlManagedTensor` deleter to reacquire the GIL before deallocation.\r\n\r\n### Rationale for this implementation\r\nThe version check was added to `tensor_numpy.h/cpp` as it seemed like a more logical location for it than creating a new translation unit. The overriding of the deleter was originally attempted by directly modifying `at::fromDlpack`, but the lack of a build dependency on the Python C API in A10 prevented that. So, I extended the A10 Dlpack API instead to additionally accept a custom deleter functor.\r\n\r\nFixes #88082\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 89754, "title": "[bfloat16] adpative_{max, avg}_pool3d", "time": "2022-11-28T13:01:19Z", "body": "Add bfloat16 support in `adaptive_{max, avg}_pool3d` as discussed in https://github.com/pytorch/pytorch/pull/88906#discussion_r1033466164.", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 89751, "title": "An example of showing how onnxscript Graph works.", "time": "2022-11-28T10:03:34Z", "body": "Check the difference to see how the design works.", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3768821874, "node_id": "LA_kwDOA-j9z87go6hy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20onnx", "name": "release notes: onnx", "color": "B4EB93", "default": false, "description": "torch.onnx related changes that should show up in the release notes"}]}
{"number": 89750, "title": "Fix MultiProcess failure on nodes with 1 GPU", "time": "2022-11-28T09:21:45Z", "body": "The decorator(s) is written to `sys.exit` when the test function is called which is AFTER the `setup` call which forks the processes and uses (potentially) a GPU/NCCL based barrier which requires \"n GPUs\" to be present befor checking if \"n GPUs\" are available.\r\n\r\nRewrite those decorators to use `unittest.skipIf` which will not even enter the `setup` function.\r\nThis also exposed that `require_n_gpus_for_nccl_backend` is the same as `nccl_skip_if_lt_x_gpu` but the former has a better name so I removed the latter.\r\n\r\nFixes #89686\r\n\r\nNote that the `torch.cuda.is_available()` check is redundant as `torch.cuda.device_count()` will return a value of at most zero and the former might already use the latter, i.e. `is_available == device_count() > 0`", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 89747, "title": "Fix test large cumprod cuda", "time": "2022-11-28T07:21:32Z", "body": "Test to see if test_large_cumprod_cuda_float16 and test_large_cumsum_cuda_float16 fail on CI\r\n\r\nThis is a draft in order to see in CI if the issue still fails in order to see if a fix will work.\r\n#75731\r\n#75726", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 89744, "title": "inductor: sort the reads buf by name", "time": "2022-11-28T06:16:21Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89744\n* #89743\n* #89742\n* #88667\n* #88666\n* #88561\n* #88560\n\n\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89743, "title": "Inductor cpp wrapper: cache the wrapper", "time": "2022-11-28T06:12:49Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89744\n* __->__ #89743\n* #89742\n* #88667\n* #88666\n* #88561\n* #88560\n\n\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89742, "title": "Inductor cpp wrapper: cache the loading of the kernel", "time": "2022-11-28T05:49:47Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89744\n* #89743\n* __->__ #89742\n* #88667\n* #88666\n* #88561\n* #88560\n\n\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89735, "title": "[Hacky] Port from symbolic-shapes - Record TensorReference for ._base tensors, but don't install guards on them  (#89250)", "time": "2022-11-28T03:21:13Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89735\n\nlint\n\nlint\n\ncc @mlazos @soumith @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89732, "title": "Print partial fx graph on dispatch_trace failures", "time": "2022-11-28T01:45:30Z", "body": "Add support for setting torch.fx.experimental.proxy_tensor.DISPATCH_TRACE_ERROR_PRINT_GRAPH to enable users to print partial fx graphs when their dispatch_trace fails. Also wrap the error to make it more clear that it is coming from dispatch trace.\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89732\n\r\n", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}]}
{"number": 89726, "title": "[WIP] Debug util, assert no fake mode on stack when we create one", "time": "2022-11-27T19:52:18Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89726\n\n", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89717, "title": "Implement generic batch normalization layer and add most (if not all) tests.", "time": "2022-11-27T09:17:27Z", "body": "Fixes #89116 by implementing a generic batch normalization layer and lazy version.\r\nQuantized batch normalization has not been implemented.\r\nAll tests for lazy batch normalization have been implemented but not for batchnorm because of tight pairing with dimension number in many tests.\r\nOpen to feedback concerning tests.\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3769221312, "node_id": "LA_kwDOA-j9z87gqcDA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(fsdp)", "name": "release notes: distributed (fsdp)", "color": "C1F551", "default": false, "description": "release notes category"}, {"id": 3773060564, "node_id": "LA_kwDOA-j9z87g5FXU", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20new%20feature", "name": "topic: new feature", "color": "C32883", "default": false, "description": "topic category"}]}
{"number": 89709, "title": "Store graphargs on dynamo produced GraphModule", "time": "2022-11-26T18:51:29Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89709\n\nWhen debugging, I found having access to graphargs useful.  I propose\nwe always propagate them.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89707, "title": "[WIP] JIT torchdynamo dynamic shapes guards", "time": "2022-11-26T11:39:36Z", "body": "Currently, I am using `numba` for JIT-ing the guards.\r\nAlternatively, we could use LLVM ORC JIT for building our own JIT for solely this purpose.\r\n\r\n#### Summary of the changes\r\n- Modified SymPy code generation for tensor references\r\n    - `tensor.size()[idx]` turns into `tensor_size[idx]`\r\n    - i.e. access of an array element\r\n- Generating actual comparison expressions instead of function calls\r\n    - `Eq(lhs, rhs)` turns into `(lhs) == (rhs)`\r\n- `_parse_symbolic_shape_expressions` returns a (named) tuple of 3 things:\r\n    1. `expr`: the JIT'd function (for reporting reasons)\r\n    2. `wrapper_expr`: the actual expression that should be called (call to the JIT'd function)\r\n    3. `fn`: the JIT'd function reference\r\n\r\n#### Observations\r\nI have been profiling a function that consists in a series of 2-dimensional convolutions + reduction in some dimension (the actual script is in this thread). After running it for a few times, I realized there were no significant speedups.\r\n\r\n\r\n(base)\r\n```\r\nRan 20 times a loop of 10 iterations.\r\n[3.188875675201416, 3.189727544784546, 3.19596529006958, 3.1962969303131104, 3.19639253616333, 3.2035884857177734, 3.205667495727539, 3.207493782043457, 3.209066390991211, 3.2138023376464844, 3.2142698764801025, 3.218745708465576, 3.2189385890960693, 3.2743585109710693, 3.281322479248047, 3.281557559967041, 3.2845256328582764, 3.285775661468506, 3.297717809677124, 3.305040121078491]\r\n           Average: 3.2334563732147217 seconds\r\nStandard deviation: 0.04170801490545273\r\n```\r\n\r\n(JIT)\r\n```\r\nRan 20 times a loop of 10 iterations.\r\n[3.20041823387146, 3.2077603340148926, 3.2175168991088867, 3.223036527633667, 3.2248380184173584, 3.2248899936676025, 3.22586727142334, 3.228421211242676, 3.2328808307647705, 3.235170364379883, 3.236780881881714, 3.2392501831054688, 3.2441165447235107, 3.2497799396514893, 3.25089430809021, 3.275514602661133, 3.2784759998321533, 3.278923988342285, 3.284738063812256, 3.305278778076172]\r\n           Average: 3.243227481842041 seconds\r\nStandard deviation: 0.027927348390221596\r\n```\r\n\r\n----- \r\n\r\nAdmittedly, this may not be the best model for showing the advantages of JIT-ing the guards, though. \r\n\r\n<details>\r\n<summary>Profiling Script</summary>\r\n\r\n```python\r\nimport argparse\r\nimport timeit\r\nimport torch\r\nimport torch._dynamo\r\nimport torch.nn.functional as F\r\nimport warnings\r\n\r\n@torch._dynamo.optimize(\"aot_eager\")\r\ndef model(input: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor):\r\n    conv = F.conv2d(input, weight, padding=\"same\", groups=3)\r\n    mean = conv.mean(dim=0)\r\n    w_mean = weight.mean(dim=1, keepdim=True)\r\n    conv2 = F.conv2d(mean, w_mean, padding=\"same\", groups=3)\r\n    mean2 = conv2.mean(dim=0, keepdim=True)\r\n    conv3 = F.conv2d(mean2, w_mean, padding=\"same\")\r\n    mean3 = mean.mean(dim=0)\r\n    lhs = mean3.sum(dim=0) + mean2 + conv3\r\n    lhs = lhs / (mean3.shape[0] + mean2.shape[0] + conv3.shape[0])\r\n    return bias + lhs\r\n\r\ndef t(shape):\r\n    return torch.rand(shape, requires_grad=True)\r\n\r\ndef run():\r\n    i_shape, w_shape, b_shape = (1, 6, 16, 20), (3, 2, 3, 3), (1, 16, 1)\r\n    with warnings.catch_warnings():\r\n        warnings.simplefilter(\"ignore\")\r\n        model(torch.rand(i_shape, requires_grad=True), torch.rand(w_shape, requires_grad=True), torch.rand(b_shape, requires_grad=True))\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\"-n\", \"--number\", default=1, type=int)\r\nparser.add_argument(\"-v\", \"--verbose\", action=\"store_true\")\r\nargs = parser.parse_args()\r\n\r\nif args.verbose:\r\n    torch._dynamo.config.verbose = True\r\n    torch._dynamo.config.log_level = logging.CODE\r\n\r\nWARMUP = 5\r\nREPEAT = 20\r\nNUMBER = args.number\r\n\r\nfor _i in range(WARMUP):\r\n    run()\r\n\r\ntimings = timeit.repeat(\r\n    stmt=\"run()\",\r\n    globals={ \"model\": model, \"run\": run },\r\n    repeat=REPEAT,\r\n    number=NUMBER\r\n)\r\ntimings = torch.tensor(timings)\r\n\r\nprint(f\"Ran {REPEAT} times a loop of {NUMBER} iterations.\")\r\nprint(timings.msort().tolist())\r\nprint(f\"           Average: {timings.mean()} seconds\")\r\nprint(f\"Standard deviation: {timings.std()}\")\r\n```\r\n</details>\r\n\r\n-----\r\n \r\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire @ezyang ", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89703, "title": "Unit test fixes", "time": "2022-11-26T05:40:12Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89703\n* #89672\n* #89671\n* #89670\n* #89669\n* #89656\n* #89666\n* #89664\n* #89662\n\nSmall test fix\n\nfix\n\ncc @mlazos @soumith @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89700, "title": "Voz/89695 test fixes", "time": "2022-11-25T23:32:13Z", "body": "Fixes #ISSUE_NUMBER\r\n\n\ncc @mlazos @soumith @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 3769215850, "node_id": "LA_kwDOA-j9z87gqatq", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20linalg_frontend", "name": "release notes: linalg_frontend", "color": "B7C4BC", "default": false, "description": "release notes category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89696, "title": "[WIP] Test smoke tests", "time": "2022-11-25T21:17:02Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 3618084153, "node_id": "LA_kwDOA-j9z87Xp5U5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/binaries", "name": "ciflow/binaries", "color": "245567", "default": false, "description": "Trigger all binary build and upload jobs on the PR"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4077622962, "node_id": "LA_kwDOA-j9z87zC5ay", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/binaries_conda", "name": "ciflow/binaries_conda", "color": "3FC2AC", "default": false, "description": "Trigger binary build and upload jobs for conda on the PR"}, {"id": 4077623621, "node_id": "LA_kwDOA-j9z87zC5lF", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/binaries_wheel", "name": "ciflow/binaries_wheel", "color": "AD47B4", "default": false, "description": "Trigger binary build and upload jobs for wheel on the PR"}, {"id": 4077624848, "node_id": "LA_kwDOA-j9z87zC54Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/binaries_libtorch", "name": "ciflow/binaries_libtorch", "color": "D22ACF", "default": false, "description": "Trigger binary build and upload jobs for libtorch on the PR"}]}
{"number": 89695, "title": "Total revamp of how ShapeEnv symbol allocation works", "time": "2022-11-25T18:48:18Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89695\n\r\nThis subsumes the stack at https://github.com/pytorch/pytorch/pull/89604 but you may find it easier to review those PRs, as they build the feature progressively (even if CI isn't all passing each step of the way).\r\n\r\nThe order you should read the changes in this patch:\r\n\r\n* torch/fx/experimental/symbolic_shapes.py - the ShapeEnv changes\r\n\r\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\r\n\r\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89685, "title": "Create README-PtBr.md", "time": "2022-11-25T14:46:10Z", "body": "translation of the README into Brazilian Portuguese\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 89682, "title": "Make libtorch buildable with buck1 & buck2", "time": "2022-11-25T12:07:52Z", "body": "This change makes a few changes to the buck setup script, and to the build definitions to allow building the project with buck1 _or_ two. Because of the prelude, the buckconfig files are incompatible so we instead have two. Note that this is building with the oss-ready buck2 build which is not currently freely available but we expect it to be ready before the end of the year. \r\n\r\nTo make this work, we update some imports, and have a minimal prelude shim which buck1 can use to expose the appropriate methods in the correct places. These imports are bundled in the buck2 prelude.\r\n\r\nTo set up the project using buck1:\r\n\r\n```\r\ngit clone --recursive https://github.com/pytorch/pytorch.git\r\ncd pytorch\r\nbash ./scripts/buck_setup.sh\r\nbuck build :torch\r\n```\r\n\r\nTo set up the project for buck2:\r\n\r\n```\r\ngit clone --recursive https://github.com/pytorch/pytorch.git\r\ncd pytorch\r\nbash ./scripts/buck_setup.sh 2\r\nbuck2 build :torch\r\n```\r\n\r\nThe buck2 setup command uses `buck2 init`, so you will need a copy of buck2 to run it. It also relies on a few patches that still need to land in the prelude, but this will happen before buck2 oss is released anyways. Note that, until the prelude repo is made public, the buck2 setup script will fail, since it needs to clone the prelude as a submodule.\r\n\r\nThe next step is to get pytorch bindings building with buck2. I have a patch in progress for this but it is not yet finished. It does not aim to also be compatible with buck1 (however it is reasonable to expect we could make it compatible with some tweaks)\r\n\r\nSome notes:\r\n\r\n- I was getting build errors on a recent version of clang, because `__has_trivial_copy` has gone from a deprecation warning to a hard error. I naively followed the suggestion of the warning and put the code change in its own commit.\r\n- buck2 doesn't (currently) support the .buckconfig `includes` directive in `.bzl` files. this is going to be fixed at some point but in the mean time we just import the 'patch_select.bzl' file to fix those cases.\r\n- prelude-shim is included by default in the root. we could perhaps move it if needed\r\n- the 'devserver' param for the buck2_setup command has been moved to the second parameter, and is only respected when generating for buck1\r\n- a simple toolchains folder has been added. we are currently in the process of moving some basic system toolchains into the prelude however, so `toolchains/toolchain.bzl` will be much smaller. I originally looked at using our hermetic cpp toolchain, but it does not currently support asm, so the toolchain is just using whatever is on your system\r\n\r\ncc @jerryzh168 @jianyuh @raghuramank100 @jamesr66a @vkuzo @jgong5 @Xia-Weiwen @leslie-fang-intel @VitalyFedyunin @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}]}
{"number": 89672, "title": "[UPDATED PROTOTYPE] Use dynamo fake tensor mode in aot_autograd, move aot_autograd compilation to lowering time", "time": "2022-11-25T01:36:20Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89773\n* __->__ #89672\n\nAfter all of the preparatory commits, this is a subset of the\nchanges in https://github.com/pytorch/pytorch/pull/89392 that actually\nchange us to propagating fake tensors to backends.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89663, "title": "Switch use_fake_tensor to True by default", "time": "2022-11-24T23:13:38Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89663\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>", "label": []}
{"number": 89660, "title": "Use `@pytorch//` in bazel build files", "time": "2022-11-24T22:10:05Z", "body": "This change aims to make bazel build more embeeding-friendly.\r\nNamely, when PyTorch is included as an external repo in another project, it is usually included like this\r\n```\r\n        native.local_repository(\r\n            name = \"pytorch\",\r\n            path = ...,\r\n            repo_mapping = repo_mapping,\r\n        )\r\n```\r\nOr\r\n```\r\n        http_archive(\r\n            name = \"pytorch\",\r\n            urls = ...\r\n            repo_mapping = repo_mapping,\r\n        )\r\n```\r\nIn this case, references to `@//` would resolve to the top-level WORKSPACE that includes PyTorch.\r\nThat makes upgrades harder because we need to carry around this patch.\r\nNote that under some edge-case circumstances even `//` resolves to the top-level `WORKSPACE`.\r\n\r\nThis change makes the embedding of the bazel build easier without compromising anything for the main repo, since the `@pytorch//` still refers to the same thing.\n\ncc @gujinghui @PenghuiCheng @XiaobingSuper @jianyuh @VitalyFedyunin @jgong5 @mingfeima @sanchitintel @ashokei @jingxu10 @min-jean-cho @yanbing-j @Guobing-Chen @Xia-Weiwen", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1303456553, "node_id": "MDU6TGFiZWwxMzAzNDU2NTUz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20mkldnn", "name": "module: mkldnn", "color": "f7e101", "default": false, "description": "Related to Intel IDEEP/MKL-DNN (mkldnn) integration"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 89654, "title": "Suppress self attention check for MHA native fastpath", "time": "2022-11-24T20:38:49Z", "body": "Summary: Suppress self attention check for MHA native fastpath\n\nTest Plan: sandcastle\n\nDifferential Revision: D41526485\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}]}
{"number": 89649, "title": "Add support for complex numbers to CUDA nn.silu()", "time": "2022-11-24T17:36:45Z", "body": "Signed-off-by: Shogo Hida <shogo.hida@gmail.com>\r\n\r\nFixes #89382 \r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3768824578, "node_id": "LA_kwDOA-j9z87go7MC", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20cuda", "name": "release notes: cuda", "color": "91275E", "default": false, "description": "release notes category"}]}
{"number": 89636, "title": "[ezyang] Use dynamo fake tensor mode in aot_autograd, move aot_autograd compilation to lowering time", "time": "2022-11-24T14:41:16Z", "body": "This is https://github.com/pytorch/pytorch/pull/89392 plus fixes from me\n\ncc @jerryzh168 @jianyuh @raghuramank100 @jamesr66a @vkuzo @jgong5 @Xia-Weiwen @leslie-fang-intel @VitalyFedyunin @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @gujinghui @PenghuiCheng @min-jean-cho @yanbing-j @Guobing-Chen @mcarilli @ptrblck @EikanWang @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @chunyuan-w @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1303456553, "node_id": "MDU6TGFiZWwxMzAzNDU2NTUz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20mkldnn", "name": "module: mkldnn", "color": "f7e101", "default": false, "description": "Related to Intel IDEEP/MKL-DNN (mkldnn) integration"}, {"id": 1972560798, "node_id": "MDU6TGFiZWwxOTcyNTYwNzk4", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20amp%20(automated%20mixed%20precision)", "name": "module: amp (automated mixed precision)", "color": "f7e101", "default": false, "description": "autocast"}, {"id": 2510754463, "node_id": "MDU6TGFiZWwyNTEwNzU0NDYz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/NNC", "name": "NNC", "color": "e5678d", "default": false, "description": ""}, {"id": 3769204372, "node_id": "LA_kwDOA-j9z87gqX6U", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20releng", "name": "release notes: releng", "color": "EED552", "default": false, "description": "release notes category"}, {"id": 4417837777, "node_id": "LA_kwDOA-j9z88AAAABB1La0Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/mps", "name": "ciflow/mps", "color": "02EDA7", "default": false, "description": "Run MPS tests (subset of trunk)"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89635, "title": "xfail test_rewrite_assert tests on Python 3.9", "time": "2022-11-24T14:37:59Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89642\n* #89641\n* #89640\n* #89639\n* #89638\n* #89637\n* __->__ #89635\n* #89631\n\nSee also https://github.com/pytorch/torchdynamo/issues/1922\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4705034454, "node_id": "LA_kwDOA-j9z88AAAABGHEg1g", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20dynamo", "name": "release notes: dynamo", "color": "b60205", "default": false, "description": ""}]}
{"number": 89621, "title": "[NVFUSER] refactor nvfuser build", "time": "2022-11-24T09:21:55Z", "body": "This PR is the first step towards refactors the build for nvfuser in order to have the coegen being a standalone library.\r\n\r\nContents inside this PR:\r\n1. splits the build system so nvfuser is generating its own `.so` files. Currently there are:\r\n    - `libnvfuser_codegen.so`, which contains the integration, codegen and runtime system of nvfuser\r\n    - `libnvfuser_python.so`, which is nvfuser's python API via pybind\r\n2. nvfuser cpp tests is currently being compiled into `nvfuser_tests`\r\n3. cmake is refactored so that:\r\n    - nvfuser now has its own `CMakeLists.txt`, which is under `torch/csrc/jit/codegen/cuda/`.\r\n    - nvfuser backend code is not compiled inside `libtorch_cuda_xxx` any more\r\n    - nvfuser is added as a subdirectory under `./CMakeLists.txt` at the very end after torch is built.\r\n    - since nvfuser has dependency on torch, the registration of nvfuser at runtime is done via dlopen (`at::DynamicLibrary`). This avoids circular dependency in cmake, which will be a nightmare to handle. For details, look at `torch/csrc/jit/codegen/cuda/interface.cpp::LoadingNvfuserLibrary`\r\n\r\nFuture work that's scoped in following PR:\r\n- Currently since nvfuser codegen has dependency on torch, we need to refactor that out so we can move nvfuser into a submodule and not rely on dlopen to load the library. @malfet \r\n- Since we moved nvfuser into a cmake build, we effectively disabled bazel build for nvfuser. This could impact internal workload at Meta, so we need to put support back. cc'ing @vors\r\n\r\n\r\ncc @kevinstephano @EikanWang @jgong5", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2510754463, "node_id": "MDU6TGFiZWwyNTEwNzU0NDYz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/NNC", "name": "NNC", "color": "e5678d", "default": false, "description": ""}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}, {"id": 3997471357, "node_id": "LA_kwDOA-j9z87uRJJ9", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20nvfuser", "name": "module: nvfuser", "color": "9CA380", "default": false, "description": ""}, {"id": 4443556792, "node_id": "LA_kwDOA-j9z88AAAABCNtLuA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/skip-pr-sanity-checks", "name": "skip-pr-sanity-checks", "color": "BED7D5", "default": false, "description": ""}]}
{"number": 89618, "title": "Enable access to \"sbgemm_\" routine with user-defined OpenBLAS paths", "time": "2022-11-24T06:22:34Z", "body": "Fixes #89617 \r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 89616, "title": "Small fixes for better channels last performance", "time": "2022-11-24T05:58:44Z", "body": "1) don't codegen maxpool backward, it's exceedingly slow\r\n2) better determine reduction variables for more accurate hints\r\n3) deterministic iteration order for reduction arguments, take into account all full size reduction argument, for hints break ties to outer reduction\r\n\r\nfixes #1653\r\n\r\n\r\n\r\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89596, "title": "[PR][torchgen] Move Executorch codegen logic into torchgen", "time": "2022-11-23T21:35:12Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #89596\r\n* #90099\r\n* #90098\r\n* #89595\r\n* #89594\r\n* #89487\r\n\r\n## Codegen Entry:\r\n\r\nMain logic and Executorch codegen entry: `gen_executorch.py`.\r\n\r\n## Generated code:\r\n\r\n`RegisterCodegenUnboxedKernels.cpp`:\r\n```cpp\r\nregister_operators({\r\n\tOperator(\r\n\t\t\"aten::add.out\",\r\n\t\t[](EValue** stack) {\r\n\t\t\tEValue& self = *stack[0];\r\n\t\t\tEValue& other = *stack[1];\r\n\t\t\tEValue& alpha = *stack[2];\r\n\t\t\tEValue& out = *stack[3];\r\n\t\t\t\r\n\t\t\tconst at::Tensor & self_base = self.to<at::Tensor>();\r\n\t\t\tconst at::Tensor & other_base = other.to<at::Tensor>();\r\n\t\t\tconst at::Scalar & alpha_base = alpha.to<at::Scalar>();     \t\r\n\t\t\tat::Tensor & out_base = out.to<at::Tensor>();                  \r\n\t\t\t\r\n\t\t\tEXECUTORCH_SCOPE_PROF(\"native_call_add.out\");      \r\n\t\t\ttorch::executor::aten::add_outf(self_base, other_base, alpha_base, out_base);\r\n\t})\r\n);\r\n```\r\n\r\n`Functions.h`:\r\n```cpp\r\n#include <ATen/Functions.h>\r\n\r\nnamespace torch {\r\nnamespace executor {\r\n\r\nnamespace aten {\r\n\r\n// aten::add_outf(Tensor self, Tensor other, Scalar alpha, *, Tensor(a!) out) -> Tensor(a!)\r\nTORCH_API inline at::Tensor & add_outf(const at::Tensor & self, const at::Tensor & other, at::Scalar alpha, at::Tensor & out) {\r\n    return at::add_outf(self, other, alpha, out);\r\n}\r\n\r\n} // namespace aten\r\n\r\n} // namespace executor\r\n} // namespace torch\r\n```\r\n## Testing:\r\n\r\n* Unit tests\r\n* CI job `linux-focal-py3_7-gcc7-edge-executorch-build`\r\n    * In order to test, have to copy some of the Executorch source code into test directory.\r\n    * `Evalue.h` and `operator_registry.h/cpp` are the ones.\r\n    * Testing logic is to build test binary with generated code and `Evalue.h`, `operator_registry.h/cpp` so that we register a few operators. The operators are listed in `selected_operators.yaml`.\r\n    *  Currently only testing `aten::add` as an example. In future PRs will add more test.\r\n\r\nDifferential Revision: [D41506912](https://our.internmc.facebook.com/intern/diff/D41506912/)", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 89595, "title": "[torchgen] Introduce Executorch types and signatures", "time": "2022-11-23T21:35:07Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89596\n* #90099\n* #90098\n* __->__ #89595\n* #89594\n* #89487\n\r\n## Forked `BaseCppType`\r\n\r\nCreated a module for Executorch: `torchgen.executorch`.\r\n\r\nIn `torchgen.executorch.api.types.types`:\r\n* Define `BaseCppType` with `torch::executor` namespace.\r\n\r\nIn `torchgen.executorch.api.cpp`:\r\n* Help generate `NamedCType` for `CppSignature` arguments.\r\n\r\nIn `torchgen.executorch.api.types.signatures`:\r\n* Define the signature using these types. (`CppSignature`)\r\n\r\nIn `torchgen.executorch.api.types.__init__`:\r\n* List out all exported types from `signatures` and `types`. (to make `mypy` happy)\r\n\r\nDifferential Revision: [D41501836](https://our.internmc.facebook.com/intern/diff/D41501836/)", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 89594, "title": "[torchgen] Let native function declaration generation logic take a callable", "time": "2022-11-23T21:35:01Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89596\n* #90099\n* #90098\n* #89595\n* __->__ #89594\n* #89487\n\r\nThis PR allows `get_native_function_declarations` API to take a function as argument. This function should take `NativeFunction` as input and emit code for native function declaration. By default it is `dest.compute_native_function_declaration`.\r\n\r\nDifferential Revision: [D41501838](https://our.internmc.facebook.com/intern/diff/D41501838/)", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 89593, "title": "[inductor] Simplified TritonKernel.indexing", "time": "2022-11-23T21:34:19Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89593\n* #89566\n\r\nFollowing up on #89566, I tried removing some of the logic in `TritonKernel.indexing` that seems that is no longer necessary. Probably not everything removed in first commit _should_ be removed, as I am not sure what was the intent of some parts, but at least _some_ is no longer necessary with new logic. FWIW, tests seem to pass after removal.\r\n\r\n\r\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}, {"id": 4705034454, "node_id": "LA_kwDOA-j9z88AAAABGHEg1g", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20dynamo", "name": "release notes: dynamo", "color": "b60205", "default": false, "description": ""}]}
{"number": 89591, "title": "[mta] Backward of unary foreach functions", "time": "2022-11-23T21:11:33Z", "body": "as per title, this PR defines backward of those.\r\n\r\nThis doesn't implement forward-mode automatic differentiation as [the current codegen](https://github.com/pytorch/pytorch/blob/a747326423ed4731996769e3b8eb73eecbdee2d4/tools/autograd/gen_variable_type.py#L1513) doesn't seem to handle `ArrayRef<Tensor>`.\r\n\r\nRel:\r\n- https://github.com/pytorch/pytorch/issues/53796\r\n- https://github.com/pytorch/pytorch/issues/58833\n\ncc @mcarilli @ngimel", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3027440319, "node_id": "MDU6TGFiZWwzMDI3NDQwMzE5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20mta", "name": "module: mta", "color": "FBCA04", "default": false, "description": "Issues related to multi-tensor apply kernels and foreach functions"}, {"id": 3769214458, "node_id": "LA_kwDOA-j9z87gqaX6", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20foreach_frontend", "name": "release notes: foreach_frontend", "color": "96F26C", "default": false, "description": "release notes category"}]}
{"number": 89587, "title": "Compiler warning/error on the const qualifier of the map function parameter", "time": "2022-11-23T20:35:25Z", "body": "Currently, Intel compiler reports the following errors:\r\n```\r\nerror: argument of type \"__m256 (*)(__m256)\" is incompatible with parameter of type \"const __m256 (*)(__m256)\"\r\nerror: argument of type \"__m512 (*)(__m512)\" is incompatible with parameter of type \"const __m512 (*)(__m512)\"\r\n```\r\n\r\nThis PR\r\n- Revise `map` parameters to make all compilers (gcc, clang, icc) happy.\r\n- Also suppress clang warning in vec256. Ref: https://github.com/pytorch/pytorch/commit/3219f6a4873f74b2f1a8af9fc81eedac764370ff\r\n\r\ncc @VitalyFedyunin @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 89582, "title": "[WIP][CUDA] Drop CUDA 10 support", "time": "2022-11-23T19:50:46Z", "body": "CC @ptrblck @ngimel @malfet ", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769217376, "node_id": "LA_kwDOA-j9z87gqbFg", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20sparse", "name": "release notes: sparse", "color": "2A7626", "default": false, "description": "release notes category"}]}
{"number": 89564, "title": "jane fixes", "time": "2022-11-23T13:58:58Z", "body": "Not actually landing this - just for hacking / debugging the batch norm issues\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #89564\r\n* #89563\r\n* #89562\r\n\r\n\r\n\r\ncc @VitalyFedyunin @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @Guobing-Chen @chunyuan-w @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 4164781683, "node_id": "LA_kwDOA-j9z874PYZz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20mps", "name": "release notes: mps", "color": "1d76db", "default": false, "description": "Release notes category"}, {"id": 4417837777, "node_id": "LA_kwDOA-j9z88AAAABB1La0Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/mps", "name": "ciflow/mps", "color": "02EDA7", "default": false, "description": "Run MPS tests (subset of trunk)"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89563, "title": "tmp hack", "time": "2022-11-23T13:58:55Z", "body": "Not actually landing this - just for hacking / debugging the batch norm issues\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* #89564\r\n* __->__ #89563\r\n* #89562\r\n\r\n\r\n\r\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89562, "title": "tmp copy of input mutation PR", "time": "2022-11-23T13:58:51Z", "body": "Not actually landing this - just for hacking / debugging the batch norm issues\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* #89564\r\n* #89563\r\n* __->__ #89562\r\n\r\n\r\n\r\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89559, "title": "deprecate torch.equal", "time": "2022-11-23T11:48:13Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89559\n* #89527\n* #89620\n* #89526\n* #89525\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2484356711, "node_id": "MDU6TGFiZWwyNDg0MzU2NzEx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20testing", "name": "module: testing", "color": "fbca04", "default": false, "description": "Issues related to the torch.testing module (not tests)"}, {"id": 3769210091, "node_id": "LA_kwDOA-j9z87gqZTr", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20python_frontend", "name": "release notes: python_frontend", "color": "87E01C", "default": false, "description": "release notes category"}, {"id": 3773059130, "node_id": "LA_kwDOA-j9z87g5FA6", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20deprecation", "name": "topic: deprecation", "color": "2824A8", "default": false, "description": "topic category"}]}
{"number": 89557, "title": "move TypedStorage handling to assertEqual", "time": "2022-11-23T10:36:25Z", "body": "#85303 added a patch to `torch.testing.assert_close` to handle `torch.storage.TypedStorage`'s. This change is not reflected in the docs and is not intended for the public API. This PR removes the patch ones again and moves the behavior to `TestCase.assertEqual` instead. Meaning, `TypedStorage`'s are again not supported by the public API, but the behavior is the same for all internal use cases. ", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2484356711, "node_id": "MDU6TGFiZWwyNDg0MzU2NzEx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20testing", "name": "module: testing", "color": "fbca04", "default": false, "description": "Issues related to the torch.testing module (not tests)"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 89532, "title": "AOT Autograd refactor + cleanup, handle intermediate views of bases, use view replay, fix non-tensor input handling", "time": "2022-11-22T23:12:27Z", "body": "This PR is a pretty large refactor of the AOT Autograd logic, to clean things up + fix a few more broken edge cases. The changes are roughly:  \r\n\r\n(1) (largest change) - for outputs of the fw that alias in some way, we used to *not* return them in the fw graph, and instead return a long tuple of ints, corresponding to the metadata of the outputs’ sizes/strides/storage_offset. The wrapper around the CompiledFunction.forward() would then figure out how to regenerate the alias with a big .as_strided call, indexing into the giant tuple of ints from the fwd graph to get the size/stride metadata.\r\n\r\nNow instead, the compiled forward graph returns the actual aliased tensor outputs along with every other output, and the wrapper uses that output to regenerate the alias. \r\nEven though the aliased outputs are now returned in the compiled graph (and in the `CompiledFunction.forward()`), I explicitly removed them from the backward graph. That felt like the right call (the aliases shouldn’t participate in the compiled backward, because we don’t actually care about them - we just use them to regenerate the “real” aliases in the forward, but I’m open to other ideas. Doing this required the following:  (a) I updated the `CompiledFunction.forward()` to wrap all aliased outputs into an opaque `TensorAlias` wrapper object, so that the `autograd.Function` will know to not assign gradients to them (b) I updated `CompiledFunction.backward()` to filter out the grad_outputs that correspond to aliased outputs (which I assert are all None)\r\n(c) Before tracing the `joint_forward_backward()`, I update `tangents` to filter out tangents corresponding to aliased outputs\r\n\r\n\r\n(3) Cleaned up the metadata and removed some redundant info. Take a look at the `ViewAndMutationMetadata` class\r\n\r\n(4) Precompute more things so that the hot path code should be faster. For example, when applying mutations back to mutated inputs, we used to loop through all inputs. Now, we precompute the indices of inputs that need to be mutated and only loop through those. This should be a meaningful speedup, since many models get graphs with 200+ inputs, and only a handful need mutations\r\n\r\n(5) Added support for graphs with outputs that alias intermediates. This should fix a bug that has shown up on multiple models in the benchmark suite, where a graph returns an output that aliases an intermediate, and later tries to mutate that output (there’s are a few gh issues for this that I tried to find but couldn’t)\r\n\r\nThe way I handled this is that I check the `._base` attribute of every output of the forward. Any `._base`’s that don’t already exist as other outputs are then added as extra outputs to the graph They also get their own metadata slots in `ViewAndMutationMetadata.output_info` (which is not strictly necessary, but made handling them easier). I then also tag every output with a `._base` as having `OutputType.alias_of_intermediate`. In the wrapper around the `CompiledFunction.forward()`, for every output that is an alias of an intermediate, I discard that output and regenerate it off its intermediate.\r\n\r\n\r\n(6) We now use Alban’s view-replay logic, instead of always doing `.as_strided()`. This is notably best effort, and still falls back to as_strided() in many cases. In particular: in the synthetic base tests, the aliased inputs are created in eager mode, so they are forced to always replay with .as_strided().\r\n\r\n(7) Fixed non-tensor input handling. This was also breaking an internal test (cc @aazzolini). I confirmed the invariant we have inside of `aot_dispatch_deduplicated_autograd` is that we are given a flattened list of inputs (flattened by pytrees), but we are *not* guaranteed that the inputs are tensor-only.\r\n\r\n(8) I think I responded to and fixed any other relevant PR feedback from the original PR.\r\n\r\n\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89532\n\r\n\r\n\r\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}, {"id": 4726472156, "node_id": "LA_kwDOA-j9z88AAAABGbg93A", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20AO%20frontend", "name": "release notes: AO frontend", "color": "ededed", "default": false, "description": null}]}
{"number": 89520, "title": "Fix AutogradMonkeyPatch typo, enable replacements", "time": "2022-11-22T20:54:40Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89457\n* #89802\n* __->__ #89520\n\n- seems like this codepath has not been exercised, and possibly is not needed\n- running this through CI as a data point, but maybe we should delete this instead?\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89506, "title": "Add decomp for max_pool1d", "time": "2022-11-22T17:32:27Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89506\n* #89434\n\n", "label": []}
{"number": 89495, "title": "Fix JIT test failure on different CUDA toolkit and driver/GPU", "time": "2022-11-22T14:04:10Z", "body": "Continuation and extension of #80329\r\n\r\nThere are 2 ways this test can fail:\r\n\r\n1. CUDA toolkit/nvcc/CUDA compiler is newer than the GPU and/or GPU driver: Although this combination works in general the test may fail because it creates a GPU binary with incompatible PTX code\r\n2. CUDA compiler is older than the GPU: Compiling for the \"native\" compute capability may fail as the compiler may not support it yet\r\n\r\n1. is caused by the hard-coded architectures and 2. by the hardcoded \"use-the-device-compute-capability\"\r\n\r\nThe 2 commits address each issue individually.\r\n\r\nFixes #51950\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 89487, "title": "[torchgen] Refactor types", "time": "2022-11-22T07:16:27Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89596\n* #90099\n* #90098\n* #89595\n* #89594\n* __->__ #89487\n\r\n## Split `torchgen.api.types` into `types_base`, `types` and `signatures`.\r\n\r\nIn `types_base`:\r\n* Created base class `CType`. `BaseCType` and `ConstRefCType` etc are inheriting `CType`.\r\n* Only keep abstract type model definitions, such as `BaseCppType`.\r\n\r\nIn `types`:\r\n* Define `BaseCppType` with `at` and `c10` namespaces.\r\n* All the signatures using these types.\r\n\r\nIn `signatures`:\r\n* Define all the signatures.\r\n\r\nIn `__init__`:\r\n* Explicitly list out all exporting types, to make `mypy` happy.\r\n\r\nDifferential Revision: [D41455634](https://our.internmc.facebook.com/intern/diff/D41455634/)\r\n\r\n**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D41455634/)!", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 89485, "title": "add channels last with mixed data type support for GroupNorm backward", "time": "2022-11-22T05:44:08Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89485\n* #88663\n* #88662\n\r\n\r\n### Motivation\r\n1. Add channels last support for GroupNorm backward to make sure GroupNorm fully support channels last.\r\n2. Same as #88663, mixed data type support is also needed for channels last implementation of GroupNorm backward.\r\n\r\n### Testing\r\nSingle socket (28cores):\r\n\r\n* Contiguous:\r\n\r\nshape | forward / s | forward / s | backward / s | backward / s\r\n-- | -- | -- | -- | --\r\n  | fp32 | mixed fp32 bf16 | fp32 | mixed fp32 bf16\r\n[10, 128, 20, 20] | 3.20E-05 | 3.60E-05 | 8.31E-05 | 8.13E-05\r\n[10, 128, 50, 50] | 0.000126 | 0.000115 | 0.000356 | 0.000257\r\n\r\n\r\n* Channels Last:\r\n\r\nshape | forward / s | forward / s | backward / s | backward / s\r\n-- | -- | -- | -- | --\r\n  | fp32 | mixed fp32 bf16 | fp32 | mixed fp32 bf16\r\n[10, 128, 20, 20] | 4.11E-05 | 4.12E-05 | 9.74E-05 | 9.66E-05\r\n[10, 128, 50, 50] | 0.000179 | 0.000178 | 0.000393 | 0.000317\r\n\r\n\r\nSingle core:\r\n\r\n* Contiguous:\r\n\r\nshape | forward / s | forward / s | backward / s | backward / s\r\n-- | -- | -- | -- | --\r\n  | fp32 | mixed fp32 bf16 | fp32 | mixed fp32 bf16\r\n[10, 128, 20, 20] | 2.47E-04 | 2.53E-04 | 5.92E-04 | 4.50E-04\r\n[10, 128, 50, 50] | 0.001559 | 0.001384 | 0.004343 | 0.002436\r\n\r\n* Channels Last:\r\n\r\nshape | forward / s | forward / s | backward / s | backward / s\r\n-- | -- | -- | -- | --\r\n  | fp32 | mixed fp32 bf16 | fp32 | mixed fp32 bf16\r\n[10, 128, 20, 20] | 2.27E-04 | 3.24E-04 | 0.0006224 | 0.000459\r\n[10, 128, 50, 50] | 0.00167 | 0.001278 | 0.0041858 | 0.003027\r\n\r\n\r\n\r\n\n\ncc @VitalyFedyunin @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3769207236, "node_id": "LA_kwDOA-j9z87gqYnE", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20nn", "name": "release notes: nn", "color": "29E07F", "default": false, "description": "release notes category"}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 89477, "title": "TorchDynamo: set output stride using eager output for cat", "time": "2022-11-22T02:45:20Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89477\n\r\nFor squeezenet1_1 and densenet121 model, the cat's post op is always convolution, for channels last path, the currently cat path always set the output format as contiguous format, but convolution's input requires channels last, there always has a memory copy before convolution. This PR use eaged model's output format to set the format to reduce the memory copy.\r\n\r\nBefore:\r\n```\r\nfrom ctypes import c_void_p, c_long\r\nimport torch\r\nimport random\r\nfrom torch import empty_strided, as_strided, device\r\nfrom torch._inductor.codecache import AsyncCompile\r\n\r\naten = torch.ops.aten\r\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\r\nasync_compile = AsyncCompile()\r\n\r\n\r\nkernel_cpp_0 = async_compile.cpp('''\r\n#include \"/tmp/torchinductor_xiaobing/ik/cikrybpw4xhois4wll6h5afsswjrhpsb6gslcxrntzqtlyw2btey.h\"\r\nextern \"C\" void kernel(const float* __restrict__ in_ptr0,\r\n                       const float* __restrict__ in_ptr1,\r\n                       const float* __restrict__ in_ptr2,\r\n                       float* __restrict__ out_ptr0,\r\n                       float* __restrict__ out_ptr1,\r\n                       float* __restrict__ out_ptr2)\r\n{\r\n    #pragma GCC ivdep\r\n    for(long i0=0; i0<3; i0+=1)\r\n    {\r\n        #pragma GCC ivdep\r\n        for(long i1=0; i1<256; i1+=1)\r\n        {\r\n            {\r\n                {\r\n                    auto tmp0 = in_ptr0[i0 + (3*i1)];\r\n                    out_ptr0[i1 + (256*i0)] = tmp0;\r\n                }\r\n            }\r\n        }\r\n    }\r\n    #pragma GCC ivdep\r\n    for(long i0=0; i0<3; i0+=1)\r\n    {\r\n        #pragma GCC ivdep\r\n        for(long i1=0; i1<256; i1+=1)\r\n        {\r\n            {\r\n                {\r\n                    auto tmp0 = in_ptr1[i0 + (3*i1)];\r\n                    out_ptr1[i1 + (256*i0)] = tmp0;\r\n                }\r\n            }\r\n        }\r\n    }\r\n    #pragma GCC ivdep\r\n    for(long i0=0; i0<6; i0+=1)\r\n    {\r\n        #pragma GCC ivdep\r\n        for(long i1=0; i1<256; i1+=1)\r\n        {\r\n            {\r\n                {\r\n                    auto tmp0 = in_ptr2[i1 + (256*i0)];\r\n                    out_ptr2[i0 + (6*i1)] = tmp0;\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n''')\r\n\r\n\r\nasync_compile.wait(globals())\r\ndel async_compile\r\n\r\ndef call(args):\r\n    arg0_1, arg1_1, arg2_1, arg3_1 = args\r\n    args.clear()\r\n    buf2 = empty_strided((1, 6, 16, 16), (1536, 256, 16, 1), device='cpu', dtype=torch.float32)\r\n    buf0 = as_strided(buf2, (1, 3, 16, 16), (1536, 256, 16, 1))  # alias\r\n    buf1 = as_strided(buf2, (1, 3, 16, 16), (1536, 256, 16, 1), 768)  # alias\r\n    buf3 = empty_strided((1, 6, 16, 16), (1536, 1, 96, 6), device='cpu', dtype=torch.float32)\r\n    kernel_cpp_0(c_void_p(arg2_1.data_ptr()), c_void_p(arg3_1.data_ptr()), c_void_p(buf2.data_ptr()), c_void_p(buf0.data_ptr()), c_void_p(buf1.data_ptr()), c_void_p(buf3.data_ptr()))\r\n    del arg2_1\r\n    del arg3_1\r\n    del buf0\r\n    del buf1\r\n    del buf2\r\n    buf4 = aten.convolution(buf3, arg0_1, arg1_1, (1, 1), (0, 0), (1, 1), False, (0, 0), 1)\r\n    assert_size_stride(buf4, (1, 3, 16, 16), (768, 1, 48, 3))\r\n    del arg0_1\r\n    del arg1_1\r\n    return (buf4, )\r\n\r\n```\r\n\r\n\r\nafter:\r\n```\r\nfrom ctypes import c_void_p, c_long\r\nimport torch\r\nimport random\r\nfrom torch import empty_strided, as_strided, device\r\nfrom torch._inductor.codecache import AsyncCompile\r\n\r\naten = torch.ops.aten\r\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\r\nasync_compile = AsyncCompile()\r\n\r\n\r\nkernel_cpp_0 = async_compile.cpp('''\r\n#include \"/tmp/torchinductor_xiaobing/ik/cikrybpw4xhois4wll6h5afsswjrhpsb6gslcxrntzqtlyw2btey.h\"\r\nextern \"C\" void kernel(const float* __restrict__ in_ptr0,\r\n                       const float* __restrict__ in_ptr1,\r\n                       float* __restrict__ out_ptr0,\r\n                       float* __restrict__ out_ptr1)\r\n{\r\n    #pragma GCC ivdep\r\n    for(long i0=0; i0<256; i0+=1)\r\n    {\r\n        #pragma GCC ivdep\r\n        for(long i1=0; i1<3; i1+=1)\r\n        {\r\n            {\r\n                {\r\n                    auto tmp0 = in_ptr0[i1 + (3*i0)];\r\n                    out_ptr0[i1 + (6*i0)] = tmp0;\r\n                }\r\n            }\r\n        }\r\n    }\r\n    #pragma GCC ivdep\r\n    for(long i0=0; i0<256; i0+=1)\r\n    {\r\n        #pragma GCC ivdep\r\n        for(long i1=0; i1<3; i1+=1)\r\n        {\r\n            {\r\n                {\r\n                    auto tmp0 = in_ptr1[i1 + (3*i0)];\r\n                    out_ptr1[i1 + (6*i0)] = tmp0;\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n''')\r\n\r\n\r\nasync_compile.wait(globals())\r\ndel async_compile\r\n\r\ndef call(args):\r\n    arg0_1, arg1_1, arg2_1, arg3_1 = args\r\n    args.clear()\r\n    buf2 = empty_strided((1, 6, 16, 16), (1536, 1, 96, 6), device='cpu', dtype=torch.float32)\r\n    buf0 = as_strided(buf2, (1, 3, 16, 16), (1536, 1, 96, 6))  # alias\r\n    buf1 = as_strided(buf2, (1, 3, 16, 16), (1536, 1, 96, 6), 3)  # alias\r\n    kernel_cpp_0(c_void_p(arg2_1.data_ptr()), c_void_p(arg3_1.data_ptr()), c_void_p(buf0.data_ptr()), c_void_p(buf1.data_ptr()))\r\n    del arg2_1\r\n    del arg3_1\r\n    del buf0\r\n    del buf1\r\n    buf3 = aten.convolution(buf2, arg0_1, arg1_1, (1, 1), (0, 0), (1, 1), False, (0, 0), 1)\r\n    assert_size_stride(buf3, (1, 3, 16, 16), (768, 1, 48, 3))\r\n    del arg0_1\r\n    del arg1_1\r\n    return (buf3, )\r\n```\r\n\r\n\r\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}, {"id": 4705034454, "node_id": "LA_kwDOA-j9z88AAAABGHEg1g", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20dynamo", "name": "release notes: dynamo", "color": "b60205", "default": false, "description": ""}]}
{"number": 89474, "title": "Preserve strides when scattering", "time": "2022-11-22T01:48:23Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89474\n\nThis is a carve out of https://github.com/pytorch/pytorch/pull/88198 to\ndiagnose CI failures\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>", "label": [{"id": 3769203231, "node_id": "LA_kwDOA-j9z87gqXof", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20composability", "name": "release notes: composability", "color": "A5282C", "default": false, "description": "release notes category"}, {"id": 3773061952, "node_id": "LA_kwDOA-j9z87g5FtA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20bug%20fixes", "name": "topic: bug fixes", "color": "A44870", "default": false, "description": "topic category"}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89468, "title": "Enabled --rerun-disabled-tests mode for distributed tests", "time": "2022-11-21T23:47:35Z", "body": "Distributed tests are running differently than the rest and haven't been included in --rerun-disabled-tests mode.  None of them shows up on Rocket `commons.rerun_disabled_tests` yet\r\n\r\nNow this explains why distributed tests still take a while to run even with `--rerun-disabled-tests`\r\n\r\n### Testing\r\n* pull https://github.com/pytorch/pytorch/actions/runs/3519061125\r\n* trunk https://github.com/pytorch/pytorch/actions/runs/3519061607", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769204372, "node_id": "LA_kwDOA-j9z87gqX6U", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20releng", "name": "release notes: releng", "color": "EED552", "default": false, "description": "release notes category"}, {"id": 4457707019, "node_id": "LA_kwDOA-j9z88AAAABCbM2Cw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/test-config/distributed", "name": "test-config/distributed", "color": "9A2FDF", "default": false, "description": ""}]}
{"number": 89465, "title": "Adding test when registering a batching rule for a CompositeImplicitAutograd operation", "time": "2022-11-21T23:10:58Z", "body": "This is a Follow on from https://github.com/pytorch/pytorch/pull/88771 which should close out https://github.com/pytorch/functorch/issues/1009 I've got another PR where I'm moving some operators over https://github.com/pytorch/pytorch/pull/89762\r\n\r\nyou can see that the new test file is being picked [run here](https://github.com/pytorch/pytorch/actions/runs/3617298059/jobs/6096218583#step:10:472)\r\n\r\n ", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 89460, "title": "[NVFUSER] Upstream merge 1121", "time": "2022-11-21T21:55:50Z", "body": null, "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 89458, "title": "WIP debug fsdp inductor backward hook issue", "time": "2022-11-21T21:37:39Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89458\n* #89457\n* #89330\n* #89325\n* #89096\n* #88523\n* #89324\n\n", "label": [{"id": 3769221312, "node_id": "LA_kwDOA-j9z87gqcDA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(fsdp)", "name": "release notes: distributed (fsdp)", "color": "C1F551", "default": false, "description": "release notes category"}]}
{"number": 89457, "title": "Add gpu memory profiler script for FSDP bench", "time": "2022-11-21T21:37:36Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89457\n* #89802\n* #89520\n\r\n- Script copied from https://github.com/lessw2020/transformer_framework/blob/main/performance/gpu_memory.py\r\n\r\n###Results for hf_T5 using default batch size on 2 gpu\r\n\r\n**eager**\r\n\r\n`torchrun --nproc_per_node 2 benchmarks/dynamo/distributed.py --torchbench_model hf_T5 --fsdp --fsdp_wrap --gpu_memory_stats`\r\n```\r\n--> cuda max reserved memory = 18.7695\r\n--> max reserved percentage = 47.63 %\r\n\r\n--> cuda max memory allocated = 17.8112\r\n--> max allocated percentage = 45.2 %\r\n\r\n--> peak active memory = 17.8738\r\n--> peak active memory 45.36 %\r\n\r\ncudaMalloc retries = 0\r\ncuda OOM = 0\r\n\r\nmean latency 0.191880607791245 across 10 runs\r\n```\r\n\r\n**inductor**\r\n\r\n`torchrun --nproc_per_node 2 benchmarks/dynamo/distributed.py --torchbench_model hf_T5 --fsdp --fsdp_wrap --gpu_memory_stats --dynamo inductor`\r\n\r\n```\r\n--> cuda max reserved memory = 17.0293\r\n--> max reserved percentage = 43.21 %\r\n\r\n--> cuda max memory allocated = 14.7215\r\n--> max allocated percentage = 37.36 %\r\n\r\n--> peak active memory = 15.4266\r\n--> peak active memory 39.15 %\r\n\r\ncudaMalloc retries = 0\r\ncuda OOM = 0\r\n\r\nmean latency 0.13600909896194935 across 10 runs\r\n```\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89444, "title": "Update README.md", "time": "2022-11-21T18:14:05Z", "body": "Dummy pull request for technical reasons\r\n\r\nFixes #ISSUE_NUMBER\r\n", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 89443, "title": "[WIP] make op db tests to use multithreaded pg", "time": "2022-11-21T17:58:32Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89443\n* #89442\n* #89441\n* #89440\n* #89439\n\n", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 89436, "title": "[MPS] Remap the view ops to exisiting graph APIs.", "time": "2022-11-21T17:29:34Z", "body": "This helps in performance by avoiding the generic gather/scatter graph.", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 4164781683, "node_id": "LA_kwDOA-j9z874PYZz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20mps", "name": "release notes: mps", "color": "1d76db", "default": false, "description": "Release notes category"}, {"id": 4417837777, "node_id": "LA_kwDOA-j9z88AAAABB1La0Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/mps", "name": "ciflow/mps", "color": "02EDA7", "default": false, "description": "Run MPS tests (subset of trunk)"}]}
{"number": 89434, "title": "Add meta impl for cholesky_solve", "time": "2022-11-21T17:13:40Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89506\n* __->__ #89434\n\n", "label": [{"id": 3769203231, "node_id": "LA_kwDOA-j9z87gqXof", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20composability", "name": "release notes: composability", "color": "A5282C", "default": false, "description": "release notes category"}]}
{"number": 89433, "title": "[Not for Land] Check for reverse post-forward != pre-backward", "time": "2022-11-21T17:04:28Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#89433 [Not for Land] Check for reverse post-forward != pre-backward**\n* #89432 [FSDP] Track pre-backward order for profiling\n\n", "label": [{"id": 3769221312, "node_id": "LA_kwDOA-j9z87gqcDA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(fsdp)", "name": "release notes: distributed (fsdp)", "color": "C1F551", "default": false, "description": "release notes category"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 89432, "title": "[FSDP] Track pre-backward order for profiling", "time": "2022-11-21T17:04:20Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #89433 [Not for Land] Check for reverse post-forward != pre-backward\n* **#89432 [FSDP] Track pre-backward order for profiling**\n\n", "label": [{"id": 3769221312, "node_id": "LA_kwDOA-j9z87gqcDA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(fsdp)", "name": "release notes: distributed (fsdp)", "color": "C1F551", "default": false, "description": "release notes category"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 89429, "title": "Tracking block padding when doing allocation tracing", "time": "2022-11-21T15:14:39Z", "body": "Summary:\nCurrently CUDACachingAllocator tracing records logical sizes for allocations(i.e. the amount of bytes that are being requested by the application). We would like record however the block size as well to show padding from the allocator.\n\nFor example the following can be sources of padding:\n1) Rounding\nhttps://github.com/pytorch/pytorch/blob/6796979ee1063890fd04bbf21f298f669129df8f/c10/cuda/CUDACachingAllocator.cpp#LL1293-L1304C4\n\n2) As well as further padding when doing new allocations(rounding to kSmallBuffer and kRoundLarge):\nhttps://github.com/pytorch/pytorch/blob/6796979ee1063890fd04bbf21f298f669129df8f/c10/cuda/CUDACachingAllocator.cpp#LL1538-L1546C3\n\n3) When blocks are not being split:\nhttps://github.com/pytorch/pytorch/blob/6796979ee1063890fd04bbf21f298f669129df8f/c10/cuda/CUDACachingAllocator.cpp#LL1528-L1536C4\n\nTest Plan: Added test in test_cuda.py\n\nDifferential Revision: D41374700\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}]}
{"number": 89428, "title": "[Vulkan + Profiler] Add Timestamp Readjustment Algorithm", "time": "2022-11-21T15:14:11Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89428\n* #89427\n* #89426\n* #89425\n* #89424\n* #89423\n\nThis change ensures that vulkan event start/end times are correctly synced with their parent CPU times.\n\nDifferential Revision: [D39893109](https://our.internmc.facebook.com/intern/diff/D39893109/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D39893109/)!", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 89427, "title": "[Vulkan + Profiler] Use 0 as Vulkan Event Durations During Tree Building", "time": "2022-11-21T15:14:06Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89428\n* __->__ #89427\n* #89426\n* #89425\n* #89424\n* #89423\n\nThis change ensures that parent/child relationships between vulkan events and their corresponding CPU events are established correctly. (Previously, if a vulkan event's duration was too long, it would not be made a child correctly).\n\nThis could be merged in with the preceding diff, but I wanted to separate it for now because I'm not sure what the most appropriate way to pass through the events and adjust the in_tree_building_ flag (the way I have it now seems a bit awkward), so keeping it separate for now makes it easier to understand/fix. Taylor if you have feedback on this let me know.\n\nDifferential Revision: [D40084788](https://our.internmc.facebook.com/intern/diff/D40084788/)", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 89426, "title": "[Vulkan + Profiler] Report Vulkan Events to Profiler in QueryPool", "time": "2022-11-21T15:14:01Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89428\n* #89427\n* __->__ #89426\n* #89425\n* #89424\n* #89423\n\nWith this change, we see Vulkan events reported on the generated chrometrace with proper names and durations.\n\nHowever, their start/end times are not yet synced with the cpu event timeline, and their parent/child relationships are not established properly. These concerns will be addressed in future diffs\n\nDifferential Revision: [D39834807](https://our.internmc.facebook.com/intern/diff/D39834807/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D39834807/)!", "label": [{"id": 3769206412, "node_id": "LA_kwDOA-j9z87gqYaM", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20vulkan", "name": "release notes: vulkan", "color": "23DD30", "default": false, "description": "release notes category"}]}
{"number": 89425, "title": "[Vulkan + Profiler] Enable Processing Vulkan Events in Profiler", "time": "2022-11-21T15:13:57Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89428\n* #89427\n* #89426\n* __->__ #89425\n* #89424\n* #89423\n\nThis diff enables passing processing events in the profiler. Passing the events from QueryPool, and making sure vulkan events align with parent CPU events correctly will be handled later in this diff stack.\n\nThis diff was made by forking Taylor's scaffolding diff, D39779878, with a few changes:\n- Rebasing + resolving merge conflicts\n- Fixing (i.e. removing) auto import of profiler/containers.h\n- Changing the activity type to CPU_OP which makes the vulkan events appear on chrometrace\n- Setting adjust_timestamps to true makes adjust_timestamps(...) actually happen\n- moving addKinteoEvents after building the tree to make chrometrace show changes based on timestamp readjustment, and then adding an extra call to tree building.\n\nDifferential Revision: [D39834805](https://our.internmc.facebook.com/intern/diff/D39834805/)", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 89424, "title": "[Vulkan] Remove vulkan_profiling_observer (from lite_predictor_benchmark)", "time": "2022-11-21T15:13:52Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89428\n* #89427\n* #89426\n* #89425\n* __->__ #89424\n* #89423\n\nWith this diff stack, we will be directly integrating the profiler with QueryPool to establish proper vulkan event timestamps and parent/child relationships, so we no longer need Vulkan Profiling Observer\n\nThis is a revert of D38280587, except for ```aibench_pytorch_android```\n\nDifferential Revision: [D40292269](https://our.internmc.facebook.com/intern/diff/D40292269/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D40292269/)!", "label": [{"id": 3769206412, "node_id": "LA_kwDOA-j9z87gqYaM", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20vulkan", "name": "release notes: vulkan", "color": "23DD30", "default": false, "description": "release notes category"}]}
{"number": 89423, "title": "[Vulkan] Store entries in a separate queue after resetting query pool", "time": "2022-11-21T15:13:47Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89428\n* #89427\n* #89426\n* #89425\n* #89424\n* __->__ #89423\n\nWe want to avoid tossing shader log entries when we reset the query pool so that the old entires can be used by the profiler after gathering all profiling data is done.\n\n```get_shader_name_and_execution_duration_ns``` is used for accessing shader names/durations after they are flushed. It will be used with the torch profiler.\n\nDifferential Revision: [D40119621](https://our.internmc.facebook.com/intern/diff/D40119621/)", "label": [{"id": 3769206412, "node_id": "LA_kwDOA-j9z87gqYaM", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20vulkan", "name": "release notes: vulkan", "color": "23DD30", "default": false, "description": "release notes category"}]}
{"number": 89414, "title": "Optimized vertical flip using memcpy", "time": "2022-11-21T10:55:14Z", "body": "## Description\r\n\r\n- Use memcpy for vertical flip\r\n- Added bool type support for horizontal flip\r\n  - channels last input with horizontal flip goes also into cpu_vflip_memcpy and has a speed-up\r\n\r\nPrevious PRs:\r\n- https://github.com/pytorch/pytorch/pull/90013\r\n- https://github.com/pytorch/pytorch/pull/88989\r\n\r\n## Results\r\n\r\n### Horizontal flip\r\n\r\n- AVX2 (only cases with speed-up or same perfs for channels last input)\r\n```\r\n[------------------------------------------------------------------------- Horizontal flip -------------------------------------------------------------------------]\r\n                                                                      |  torch (1.14.0a0+giteb3e189) PR  |    Pillow (9.3.0)   |  torch (1.14.0a0+gitb0bd5c4) nightly\r\n1 threads: ----------------------------------------------------------------------------------------------------------------------------------------------------------\r\n      channels=3, size=256, dtype=torch.int64, mf=channels_last       |        204.813 (+-1.018)         |                     |           308.070 (+-1.573)         \r\n      channels=3, size=520, dtype=torch.int64, mf=channels_last       |        844.523 (+-2.302)         |                     |           1226.801 (+-5.069)        \r\n      channels=3, size=712, dtype=torch.int64, mf=channels_last       |        2246.512 (+-8.935)        |                     |          2689.692 (+-22.654)        \r\n\r\n      channels=1, size=256, dtype=torch.int32, mf=channels_last       |         21.024 (+-0.083)         |   44.196 (+-0.131)  |            22.564 (+-0.066)         \r\n      channels=1, size=520, dtype=torch.int32, mf=channels_last       |         71.806 (+-0.150)         |  166.653 (+-0.789)  |            72.660 (+-0.160)         \r\n      channels=1, size=712, dtype=torch.int32, mf=channels_last       |        129.354 (+-0.385)         |  306.998 (+-0.819)  |           130.094 (+-0.274)         \r\n\r\n      channels=3, size=256, dtype=torch.uint8, mf=channels_last       |        177.250 (+-0.485)         |   44.232 (+-0.465)  |           289.201 (+-2.837)         \r\n      channels=3, size=520, dtype=torch.uint8, mf=channels_last       |        699.055 (+-1.940)         |  166.540 (+-0.903)  |           1172.747 (+-3.645)        \r\n      channels=3, size=712, dtype=torch.uint8, mf=channels_last       |        1302.968 (+-5.390)        |  307.210 (+-0.852)  |          2149.396 (+-23.570)        \r\n\r\n      channels=1, size=256, dtype=torch.int16, mf=channels_last       |         11.943 (+-0.079)         |                     |            12.451 (+-0.033)         \r\n      channels=1, size=520, dtype=torch.int16, mf=channels_last       |         39.830 (+-0.093)         |                     |            40.583 (+-0.070)         \r\n      channels=1, size=712, dtype=torch.int16, mf=channels_last       |         69.001 (+-0.078)         |                     |            69.590 (+-0.162)         \r\n\r\n      channels=3, size=256, dtype=torch.int8, mf=channels_last        |        177.378 (+-0.507)         |                     |           283.461 (+-2.957)         \r\n      channels=3, size=520, dtype=torch.int8, mf=channels_last        |        698.915 (+-1.840)         |                     |          1061.208 (+-10.449)        \r\n      channels=3, size=712, dtype=torch.int8, mf=channels_last        |        1299.365 (+-3.919)        |                     |          1957.424 (+-13.149)        \r\n\r\n      channels=3, size=256, dtype=torch.int8, mf=channels_first       |         17.955 (+-0.077)         |                     |            89.456 (+-0.285)         \r\n      channels=3, size=520, dtype=torch.int8, mf=channels_first       |         56.901 (+-0.081)         |                     |           339.802 (+-0.879)         \r\n      channels=3, size=712, dtype=torch.int8, mf=channels_first       |        103.629 (+-0.256)         |                     |           627.845 (+-1.185)         \r\n\r\n      channels=1, size=256, dtype=torch.float32, mf=channels_last     |         21.179 (+-0.077)         |   44.146 (+-0.260)  |            22.957 (+-0.138)         \r\n      channels=1, size=520, dtype=torch.float32, mf=channels_last     |         71.685 (+-0.155)         |  166.666 (+-0.730)  |            72.606 (+-0.124)         \r\n      channels=1, size=712, dtype=torch.float32, mf=channels_last     |        129.168 (+-0.288)         |  307.094 (+-1.571)  |           130.156 (+-0.453)         \r\n\r\n      channels=1, size=256, dtype=torch.float16, mf=channels_last     |         33.049 (+-0.089)         |                     |            33.056 (+-0.477)         \r\n      channels=1, size=520, dtype=torch.float16, mf=channels_last     |        116.635 (+-0.299)         |                     |           113.433 (+-0.891)         \r\n      channels=1, size=712, dtype=torch.float16, mf=channels_last     |        212.134 (+-0.413)         |                     |           204.394 (+-0.822)         \r\n\r\n      channels=3, size=256, dtype=torch.float64, mf=channels_last     |        207.214 (+-0.586)         |                     |           302.370 (+-0.670)         \r\n      channels=3, size=520, dtype=torch.float64, mf=channels_last     |        846.553 (+-2.301)         |                     |           1223.851 (+-5.280)        \r\n      channels=3, size=712, dtype=torch.float64, mf=channels_last     |        2251.687 (+-6.513)        |                     |          2711.557 (+-14.011)        \r\n\r\n      channels=1, size=256, dtype=torch.bfloat16, mf=channels_last    |         33.237 (+-0.072)         |                     |            33.101 (+-0.070)         \r\n      channels=1, size=520, dtype=torch.bfloat16, mf=channels_last    |        113.605 (+-0.337)         |                     |           117.067 (+-0.547)         \r\n      channels=1, size=712, dtype=torch.bfloat16, mf=channels_last    |        204.632 (+-0.487)         |                     |           212.590 (+-0.848)         \r\n\r\n      channels=1, size=256, dtype=torch.bool, mf=channels_last        |         7.950 (+-0.030)          |                     |            37.757 (+-0.080)         \r\n      channels=1, size=520, dtype=torch.bool, mf=channels_last        |         23.799 (+-0.080)         |                     |           136.571 (+-0.441)         \r\n      channels=1, size=712, dtype=torch.bool, mf=channels_last        |         37.970 (+-0.075)         |                     |           246.894 (+-0.926)         \r\n\r\n      channels=1, size=256, dtype=torch.bool, mf=channels_first       |         8.009 (+-0.077)          |                     |            37.800 (+-0.100)         \r\n      channels=1, size=520, dtype=torch.bool, mf=channels_first       |         23.861 (+-0.099)         |                     |           136.553 (+-0.519)         \r\n      channels=1, size=712, dtype=torch.bool, mf=channels_first       |         38.211 (+-0.104)         |                     |           246.939 (+-0.692)         \r\n\r\nTimes are in microseconds (us).\r\n```\r\n[Source](https://gist.github.com/vfdev-5/c2ca615b522aeb1c4636dc8d948fec74#file-20221209-100405-pr_vs_nightly-md)\r\n\r\n- AVX512 (only cases with speed-up or same perfs for channels last input)\r\n```\r\n[---------------------------------------------------------------------------- Horizontal flip ----------------------------------------------------------------------------]\r\n                                                                      |  torch (1.14.0a0+giteb3e189) PR  |    Pillow (9.3.0)    |  torch (1.14.0.dev20221208+cu116) nightly\r\n1 threads: ----------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n      channels=3, size=256, dtype=torch.int64, mf=channels_last       |        194.708 (+-9.566)         |                      |             372.067 (+-12.430)           \r\n      channels=3, size=520, dtype=torch.int64, mf=channels_last       |        765.151 (+-10.098)        |                      |            1524.231 (+-111.283)          \r\n      channels=3, size=712, dtype=torch.int64, mf=channels_last       |       1587.229 (+-88.117)        |                      |            2950.081 (+-92.322)           \r\n\r\n      channels=1, size=256, dtype=torch.int32, mf=channels_last       |         13.328 (+-0.375)         |   49.693 (+-1.193)   |              10.323 (+-0.333)            \r\n      channels=1, size=520, dtype=torch.int32, mf=channels_last       |         90.580 (+-0.812)         |  191.936 (+-4.369)   |              92.269 (+-0.980)            \r\n      channels=1, size=712, dtype=torch.int32, mf=channels_last       |        163.821 (+-3.174)         |  352.053 (+-10.909)  |             165.661 (+-4.436)            \r\n\r\n      channels=3, size=256, dtype=torch.uint8, mf=channels_last       |        206.862 (+-4.417)         |   49.336 (+-1.492)   |             287.373 (+-7.266)            \r\n      channels=3, size=520, dtype=torch.uint8, mf=channels_last       |        829.736 (+-15.857)        |  191.489 (+-5.645)   |            1166.126 (+-45.667)           \r\n      channels=3, size=712, dtype=torch.uint8, mf=channels_last       |       1540.953 (+-28.269)        |  352.171 (+-8.784)   |            2171.570 (+-82.740)           \r\n\r\n      channels=1, size=256, dtype=torch.int16, mf=channels_last       |         7.856 (+-0.131)          |                      |              7.943 (+-0.148)             \r\n      channels=1, size=520, dtype=torch.int16, mf=channels_last       |         34.750 (+-1.195)         |                      |              36.309 (+-0.716)            \r\n      channels=1, size=712, dtype=torch.int16, mf=channels_last       |         85.858 (+-0.729)         |                      |              87.306 (+-0.981)            \r\n\r\n      channels=3, size=256, dtype=torch.int8, mf=channels_last        |        206.896 (+-5.716)         |                      |             262.551 (+-6.598)            \r\n      channels=3, size=520, dtype=torch.int8, mf=channels_last        |        828.212 (+-13.441)        |                      |            1077.916 (+-28.810)           \r\n      channels=3, size=712, dtype=torch.int8, mf=channels_last        |       1542.748 (+-31.379)        |                      |            2003.661 (+-71.614)           \r\n\r\n      channels=3, size=256, dtype=torch.int8, mf=channels_first       |         11.038 (+-0.271)         |                      |             126.867 (+-5.590)            \r\n      channels=3, size=520, dtype=torch.int8, mf=channels_first       |         90.190 (+-1.185)         |                      |             501.446 (+-13.498)           \r\n      channels=3, size=712, dtype=torch.int8, mf=channels_first       |        165.797 (+-3.016)         |                      |             921.131 (+-20.500)           \r\n\r\n      channels=1, size=256, dtype=torch.float32, mf=channels_last     |         13.516 (+-0.578)         |   49.678 (+-1.966)   |              10.360 (+-0.256)            \r\n      channels=1, size=520, dtype=torch.float32, mf=channels_last     |         91.195 (+-0.830)         |  191.778 (+-4.742)   |              91.117 (+-0.855)            \r\n      channels=1, size=712, dtype=torch.float32, mf=channels_last     |        168.551 (+-3.352)         |  351.585 (+-8.230)   |             164.199 (+-3.725)            \r\n\r\n      channels=1, size=256, dtype=torch.float16, mf=channels_last     |         35.832 (+-0.840)         |                      |              35.087 (+-0.972)            \r\n      channels=1, size=520, dtype=torch.float16, mf=channels_last     |        133.624 (+-5.293)         |                      |             131.423 (+-6.002)            \r\n      channels=1, size=712, dtype=torch.float16, mf=channels_last     |        240.702 (+-5.213)         |                      |             236.876 (+-7.867)            \r\n\r\n      channels=3, size=256, dtype=torch.float64, mf=channels_last     |        192.351 (+-6.740)         |                      |             313.999 (+-12.141)           \r\n      channels=3, size=520, dtype=torch.float64, mf=channels_last     |        766.553 (+-16.669)        |                      |            1270.797 (+-49.828)           \r\n      channels=3, size=712, dtype=torch.float64, mf=channels_last     |       1501.700 (+-69.499)        |                      |            2427.303 (+-126.694)          \r\n\r\n      channels=1, size=256, dtype=torch.bfloat16, mf=channels_last    |         35.386 (+-0.801)         |                      |              34.539 (+-0.844)            \r\n      channels=1, size=520, dtype=torch.bfloat16, mf=channels_last    |        132.369 (+-4.107)         |                      |             130.926 (+-3.597)            \r\n      channels=1, size=712, dtype=torch.bfloat16, mf=channels_last    |        237.722 (+-6.680)         |                      |             237.072 (+-5.027)            \r\n\r\n      channels=1, size=256, dtype=torch.bool, mf=channels_last        |         6.796 (+-0.132)          |                      |              44.727 (+-0.905)            \r\n      channels=1, size=520, dtype=torch.bool, mf=channels_last        |         24.827 (+-0.669)         |                      |             166.758 (+-5.141)            \r\n      channels=1, size=712, dtype=torch.bool, mf=channels_last        |         42.392 (+-0.980)         |                      |             310.830 (+-6.130)            \r\n\r\n      channels=1, size=256, dtype=torch.bool, mf=channels_first       |         8.114 (+-0.141)          |                      |              44.776 (+-0.707)            \r\n      channels=1, size=520, dtype=torch.bool, mf=channels_first       |         24.787 (+-0.787)         |                      |             167.766 (+-5.004)            \r\n      channels=1, size=712, dtype=torch.bool, mf=channels_first       |         42.545 (+-0.636)         |                      |             313.715 (+-7.603)            \r\n\r\nTimes are in microseconds (us).\r\n```\r\n[Source](https://gist.github.com/vfdev-5/c2ca615b522aeb1c4636dc8d948fec74#file-20221209-105633-pr_vs_nightly-avx512-md)\r\n\r\n### Vertical flip\r\n\r\n- AVX2 (all tested cases showing speed-up or same perfs)\r\n```\r\n[-------------------------------------------------------------------------- Vertical flip --------------------------------------------------------------------------]\r\n                                                                      |  torch (1.14.0a0+giteb3e189) PR  |    Pillow (9.3.0)   |  torch (1.14.0a0+gitb0bd5c4) nightly\r\n1 threads: ----------------------------------------------------------------------------------------------------------------------------------------------------------\r\n      channels=3, size=256, dtype=torch.int64, mf=channels_last       |         93.125 (+-3.022)         |                     |           101.064 (+-0.436)         \r\n      channels=3, size=520, dtype=torch.int64, mf=channels_last       |        412.942 (+-57.066)        |                     |           461.463 (+-2.098)         \r\n      channels=3, size=712, dtype=torch.int64, mf=channels_last       |        1533.265 (+-4.071)        |                     |          1829.713 (+-14.311)        \r\n\r\n      channels=3, size=256, dtype=torch.int64, mf=channels_first      |        101.134 (+-0.924)         |                     |           102.858 (+-0.319)         \r\n      channels=3, size=520, dtype=torch.int64, mf=channels_first      |        421.679 (+-1.101)         |                     |           477.413 (+-1.809)         \r\n      channels=3, size=712, dtype=torch.int64, mf=channels_first      |        1550.418 (+-3.647)        |                     |           1877.143 (+-6.622)        \r\n\r\n      channels=1, size=256, dtype=torch.int32, mf=channels_last       |         20.961 (+-0.063)         |   19.515 (+-0.302)  |            21.980 (+-0.070)         \r\n      channels=1, size=520, dtype=torch.int32, mf=channels_last       |         71.199 (+-0.173)         |   70.199 (+-0.332)  |            95.262 (+-0.109)         \r\n      channels=1, size=712, dtype=torch.int32, mf=channels_last       |        128.532 (+-0.318)         |  127.325 (+-0.328)  |           167.190 (+-0.370)         \r\n\r\n      channels=1, size=256, dtype=torch.int32, mf=channels_first      |         21.206 (+-0.059)         |   19.471 (+-0.128)  |            21.469 (+-0.064)         \r\n      channels=1, size=520, dtype=torch.int32, mf=channels_first      |         71.284 (+-0.163)         |   70.124 (+-0.388)  |            94.988 (+-0.239)         \r\n      channels=1, size=712, dtype=torch.int32, mf=channels_first      |        129.017 (+-0.286)         |  128.088 (+-0.461)  |           167.115 (+-1.075)         \r\n\r\n      channels=3, size=256, dtype=torch.uint8, mf=channels_last       |         16.909 (+-0.057)         |   19.570 (+-0.353)  |            17.981 (+-0.072)         \r\n      channels=3, size=520, dtype=torch.uint8, mf=channels_last       |         55.163 (+-0.138)         |   70.218 (+-0.275)  |           107.938 (+-0.620)         \r\n      channels=3, size=712, dtype=torch.uint8, mf=channels_last       |         98.518 (+-0.121)         |  127.737 (+-0.486)  |           170.965 (+-0.436)         \r\n\r\n      channels=3, size=256, dtype=torch.uint8, mf=channels_first      |         18.150 (+-0.084)         |   19.758 (+-0.221)  |            18.122 (+-0.088)         \r\n      channels=3, size=520, dtype=torch.uint8, mf=channels_first      |         56.693 (+-0.200)         |   70.278 (+-0.386)  |            89.018 (+-0.206)         \r\n      channels=3, size=712, dtype=torch.uint8, mf=channels_first      |        100.409 (+-0.235)         |  127.772 (+-0.457)  |           168.072 (+-0.436)         \r\n\r\n      channels=1, size=256, dtype=torch.int16, mf=channels_last       |         12.817 (+-0.041)         |                     |            12.818 (+-0.049)         \r\n      channels=1, size=520, dtype=torch.int16, mf=channels_last       |         38.359 (+-0.081)         |                     |            63.378 (+-0.165)         \r\n      channels=1, size=712, dtype=torch.int16, mf=channels_last       |         68.246 (+-0.090)         |                     |           116.637 (+-0.583)         \r\n\r\n      channels=1, size=256, dtype=torch.int16, mf=channels_first      |         12.899 (+-0.054)         |                     |            12.649 (+-0.060)         \r\n      channels=1, size=520, dtype=torch.int16, mf=channels_first      |         38.404 (+-0.069)         |                     |            63.448 (+-0.108)         \r\n      channels=1, size=712, dtype=torch.int16, mf=channels_first      |         68.378 (+-0.104)         |                     |           116.415 (+-0.332)         \r\n\r\n      channels=3, size=256, dtype=torch.int8, mf=channels_last        |         17.071 (+-0.044)         |                     |            17.792 (+-0.050)         \r\n      channels=3, size=520, dtype=torch.int8, mf=channels_last        |         55.163 (+-0.100)         |                     |           108.539 (+-0.466)         \r\n      channels=3, size=712, dtype=torch.int8, mf=channels_last        |         98.537 (+-0.091)         |                     |           171.675 (+-0.553)         \r\n\r\n      channels=3, size=256, dtype=torch.int8, mf=channels_first       |         17.837 (+-0.071)         |                     |            18.355 (+-0.067)         \r\n      channels=3, size=520, dtype=torch.int8, mf=channels_first       |         56.051 (+-0.087)         |                     |            88.261 (+-0.129)         \r\n      channels=3, size=712, dtype=torch.int8, mf=channels_first       |        100.603 (+-0.245)         |                     |           169.067 (+-0.430)         \r\n\r\n      channels=1, size=256, dtype=torch.float32, mf=channels_last     |         21.204 (+-0.063)         |   19.607 (+-0.140)  |            22.202 (+-0.094)         \r\n      channels=1, size=520, dtype=torch.float32, mf=channels_last     |         71.356 (+-0.211)         |   69.844 (+-0.343)  |            94.614 (+-0.167)         \r\n      channels=1, size=712, dtype=torch.float32, mf=channels_last     |        129.087 (+-0.290)         |  127.065 (+-0.319)  |           166.513 (+-0.444)         \r\n\r\n      channels=1, size=256, dtype=torch.float32, mf=channels_first    |         21.196 (+-0.065)         |   19.156 (+-0.132)  |            21.516 (+-0.073)         \r\n      channels=1, size=520, dtype=torch.float32, mf=channels_first    |         71.422 (+-0.180)         |   70.296 (+-0.136)  |            94.913 (+-0.095)         \r\n      channels=1, size=712, dtype=torch.float32, mf=channels_first    |        129.045 (+-0.312)         |  128.023 (+-0.585)  |           166.089 (+-0.409)         \r\n\r\n      channels=1, size=256, dtype=torch.float16, mf=channels_last     |         12.770 (+-0.045)         |                     |            34.853 (+-0.089)         \r\n      channels=1, size=520, dtype=torch.float16, mf=channels_last     |         38.363 (+-0.064)         |                     |           131.969 (+-0.577)         \r\n      channels=1, size=712, dtype=torch.float16, mf=channels_last     |         67.954 (+-0.107)         |                     |           239.507 (+-0.835)         \r\n\r\n      channels=1, size=256, dtype=torch.float16, mf=channels_first    |         12.855 (+-0.067)         |                     |            35.124 (+-0.109)         \r\n      channels=1, size=520, dtype=torch.float16, mf=channels_first    |         38.725 (+-0.079)         |                     |           131.708 (+-0.586)         \r\n      channels=1, size=712, dtype=torch.float16, mf=channels_first    |         68.931 (+-0.086)         |                     |           239.022 (+-0.914)         \r\n\r\n      channels=3, size=256, dtype=torch.float64, mf=channels_last     |         90.277 (+-0.083)         |                     |           101.512 (+-0.285)         \r\n      channels=3, size=520, dtype=torch.float64, mf=channels_last     |        421.277 (+-1.030)         |                     |           471.913 (+-3.654)         \r\n      channels=3, size=712, dtype=torch.float64, mf=channels_last     |        1534.394 (+-7.572)        |                     |          1833.262 (+-12.185)        \r\n\r\n      channels=3, size=256, dtype=torch.float64, mf=channels_first    |        100.809 (+-0.328)         |                     |           103.166 (+-0.335)         \r\n      channels=3, size=520, dtype=torch.float64, mf=channels_first    |        425.535 (+-0.926)         |                     |           482.606 (+-1.450)         \r\n      channels=3, size=712, dtype=torch.float64, mf=channels_first    |        1550.832 (+-3.547)        |                     |           1859.098 (+-6.517)        \r\n\r\n      channels=1, size=256, dtype=torch.bfloat16, mf=channels_last    |         12.954 (+-0.051)         |                     |            12.744 (+-0.046)         \r\n      channels=1, size=520, dtype=torch.bfloat16, mf=channels_last    |         41.180 (+-0.064)         |                     |            63.362 (+-0.139)         \r\n      channels=1, size=712, dtype=torch.bfloat16, mf=channels_last    |         68.136 (+-0.142)         |                     |           117.009 (+-0.292)         \r\n\r\n      channels=1, size=256, dtype=torch.bfloat16, mf=channels_first   |         13.049 (+-0.052)         |                     |            12.792 (+-0.076)         \r\n      channels=1, size=520, dtype=torch.bfloat16, mf=channels_first   |         38.488 (+-0.092)         |                     |            63.451 (+-0.096)         \r\n      channels=1, size=712, dtype=torch.bfloat16, mf=channels_first   |         68.103 (+-0.091)         |                     |           116.693 (+-0.290)         \r\n\r\n      channels=1, size=256, dtype=torch.bool, mf=channels_last        |         7.572 (+-0.029)          |                     |            8.017 (+-0.071)          \r\n      channels=1, size=520, dtype=torch.bool, mf=channels_last        |         22.121 (+-0.061)         |                     |            23.614 (+-0.074)         \r\n      channels=1, size=712, dtype=torch.bool, mf=channels_last        |         36.896 (+-0.094)         |                     |            39.460 (+-0.084)         \r\n\r\n      channels=1, size=256, dtype=torch.bool, mf=channels_first       |         7.671 (+-0.028)          |                     |            8.034 (+-0.058)          \r\n      channels=1, size=520, dtype=torch.bool, mf=channels_first       |         21.989 (+-0.053)         |                     |            23.645 (+-0.063)         \r\n      channels=1, size=712, dtype=torch.bool, mf=channels_first       |         37.252 (+-0.072)         |                     |            39.477 (+-0.100)         \r\n\r\n      channels=1, size=256, dtype=torch.complex64, mf=channels_last   |         37.129 (+-0.052)         |                     |            37.801 (+-0.101)         \r\n      channels=1, size=520, dtype=torch.complex64, mf=channels_last   |        122.646 (+-0.230)         |                     |           139.074 (+-0.467)         \r\n      channels=1, size=712, dtype=torch.complex64, mf=channels_last   |        228.946 (+-0.736)         |                     |           257.589 (+-0.545)         \r\n\r\n      channels=1, size=256, dtype=torch.complex64, mf=channels_first  |         37.088 (+-0.070)         |                     |            37.894 (+-0.078)         \r\n      channels=1, size=520, dtype=torch.complex64, mf=channels_first  |        122.695 (+-0.268)         |                     |           138.933 (+-0.336)         \r\n      channels=1, size=712, dtype=torch.complex64, mf=channels_first  |        234.655 (+-0.454)         |                     |           255.787 (+-0.530)         \r\n\r\nTimes are in microseconds (us).\r\n```\r\n[Source](https://gist.github.com/vfdev-5/c2ca615b522aeb1c4636dc8d948fec74#file-20221209-100440-pr_vs_nightly-md)\r\n\r\n- AVX512 (all tested cases showing speed-up or same perfs)\r\n\r\n```\r\n[---------------------------------------------------------------------------- Vertical flip -----------------------------------------------------------------------------]\r\n                                                                      |  torch (1.14.0a0+giteb3e189) PR  |    Pillow (9.3.0)   |  torch (1.14.0.dev20221208+cu116) nightly\r\n1 threads: ---------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n      channels=3, size=256, dtype=torch.int64, mf=channels_last       |        122.544 (+-1.962)         |                     |             129.161 (+-1.809)            \r\n      channels=3, size=520, dtype=torch.int64, mf=channels_last       |        508.274 (+-4.790)         |                     |             533.872 (+-7.457)            \r\n      channels=3, size=712, dtype=torch.int64, mf=channels_last       |        951.176 (+-29.534)        |                     |            1073.603 (+-44.676) \r\n          \r\n      channels=3, size=256, dtype=torch.int64, mf=channels_first      |        127.872 (+-2.700)         |                     |             127.326 (+-2.666)            \r\n      channels=3, size=520, dtype=torch.int64, mf=channels_first      |        518.019 (+-4.157)         |                     |             538.094 (+-6.600)            \r\n      channels=3, size=712, dtype=torch.int64, mf=channels_first      |       1002.176 (+-42.545)        |                     |            1033.989 (+-42.137)           \r\n\r\n      channels=1, size=256, dtype=torch.int32, mf=channels_last       |         10.025 (+-0.135)         |   10.054 (+-0.369)  |              10.155 (+-0.285)            \r\n      channels=1, size=520, dtype=torch.int32, mf=channels_last       |         89.867 (+-0.994)         |   88.712 (+-0.622)  |             103.029 (+-2.254)            \r\n      channels=1, size=712, dtype=torch.int32, mf=channels_last       |        161.787 (+-2.080)         |  161.370 (+-1.801)  |             182.608 (+-7.031)            \r\n\r\n      channels=1, size=256, dtype=torch.int32, mf=channels_first      |         10.005 (+-0.277)         |   9.965 (+-0.338)   |              10.604 (+-0.334)            \r\n      channels=1, size=520, dtype=torch.int32, mf=channels_first      |         89.116 (+-0.996)         |   88.840 (+-0.608)  |             102.103 (+-2.111)            \r\n      channels=1, size=712, dtype=torch.int32, mf=channels_first      |        164.328 (+-3.284)         |  161.538 (+-2.739)  |             181.702 (+-3.770)            \r\n\r\n      channels=3, size=256, dtype=torch.uint8, mf=channels_last       |         8.853 (+-0.148)          |   10.292 (+-0.494)  |              8.961 (+-0.190)             \r\n      channels=3, size=520, dtype=torch.uint8, mf=channels_last       |         68.368 (+-1.158)         |   90.068 (+-1.780)  |              81.155 (+-0.945)            \r\n      channels=3, size=712, dtype=torch.uint8, mf=channels_last       |        125.458 (+-2.511)         |  163.150 (+-2.532)  |             147.039 (+-4.264)            \r\n\r\n      channels=3, size=256, dtype=torch.uint8, mf=channels_first      |         10.409 (+-0.435)         |   10.406 (+-0.351)  |              10.263 (+-0.252)            \r\n      channels=3, size=520, dtype=torch.uint8, mf=channels_first      |         69.077 (+-1.062)         |   90.057 (+-0.992)  |              79.910 (+-0.884)            \r\n      channels=3, size=712, dtype=torch.uint8, mf=channels_first      |        127.286 (+-2.789)         |  162.862 (+-2.953)  |             142.821 (+-2.119)            \r\n\r\n      channels=1, size=256, dtype=torch.int16, mf=channels_last       |         7.513 (+-0.143)          |                     |              7.364 (+-0.154)             \r\n      channels=1, size=520, dtype=torch.int16, mf=channels_last       |         33.140 (+-0.779)         |                     |              42.141 (+-0.820)            \r\n      channels=1, size=712, dtype=torch.int16, mf=channels_last       |         86.235 (+-1.187)         |                     |             104.205 (+-2.205)            \r\n\r\n      channels=1, size=256, dtype=torch.int16, mf=channels_first      |         7.410 (+-0.162)          |                     |              7.075 (+-0.126)             \r\n      channels=1, size=520, dtype=torch.int16, mf=channels_first      |         33.656 (+-0.914)         |                     |              40.991 (+-0.893)            \r\n      channels=1, size=712, dtype=torch.int16, mf=channels_first      |         86.087 (+-1.191)         |                     |             105.419 (+-1.801)            \r\n\r\n      channels=3, size=256, dtype=torch.int8, mf=channels_last        |         8.802 (+-0.196)          |                     |              8.627 (+-0.202)             \r\n      channels=3, size=520, dtype=torch.int8, mf=channels_last        |         66.348 (+-0.775)         |                     |              80.631 (+-1.832)            \r\n      channels=3, size=712, dtype=torch.int8, mf=channels_last        |        126.275 (+-2.318)         |                     |             144.597 (+-4.242)            \r\n\r\n      channels=3, size=256, dtype=torch.int8, mf=channels_first       |         10.255 (+-0.383)         |                     |              10.101 (+-0.335)            \r\n      channels=3, size=520, dtype=torch.int8, mf=channels_first       |         68.124 (+-0.849)         |                     |              79.286 (+-0.748)            \r\n      channels=3, size=712, dtype=torch.int8, mf=channels_first       |        127.118 (+-2.225)         |                     |             142.029 (+-2.507)            \r\n\r\n      channels=1, size=256, dtype=torch.float32, mf=channels_last     |         9.850 (+-0.453)          |   9.299 (+-0.253)   |              10.030 (+-0.234)            \r\n      channels=1, size=520, dtype=torch.float32, mf=channels_last     |         91.506 (+-1.319)         |   90.265 (+-0.824)  |             107.570 (+-2.093)            \r\n      channels=1, size=712, dtype=torch.float32, mf=channels_last     |        167.820 (+-3.883)         |  162.871 (+-2.397)  |             180.046 (+-8.952)            \r\n\r\n      channels=1, size=256, dtype=torch.float32, mf=channels_first    |         10.118 (+-0.359)         |   10.433 (+-0.479)  |              10.204 (+-0.344)            \r\n      channels=1, size=520, dtype=torch.float32, mf=channels_first    |         90.862 (+-1.486)         |   90.138 (+-0.969)  |             107.011 (+-1.801)            \r\n      channels=1, size=712, dtype=torch.float32, mf=channels_first    |        163.931 (+-3.653)         |  163.155 (+-2.673)  |             186.707 (+-2.248)            \r\n\r\n      channels=1, size=256, dtype=torch.float16, mf=channels_last     |         7.304 (+-0.134)          |                     |              24.141 (+-0.444)            \r\n      channels=1, size=520, dtype=torch.float16, mf=channels_last     |         35.186 (+-0.656)         |                     |             101.523 (+-1.465)            \r\n      channels=1, size=712, dtype=torch.float16, mf=channels_last     |         85.707 (+-0.841)         |                     |             192.640 (+-4.942)            \r\n\r\n      channels=1, size=256, dtype=torch.float16, mf=channels_first    |         7.286 (+-0.142)          |                     |              24.155 (+-0.555)            \r\n      channels=1, size=520, dtype=torch.float16, mf=channels_first    |         33.819 (+-1.009)         |                     |             101.620 (+-3.034)            \r\n      channels=1, size=712, dtype=torch.float16, mf=channels_first    |         84.811 (+-0.993)         |                     |             192.286 (+-4.707)            \r\n\r\n      channels=3, size=256, dtype=torch.float64, mf=channels_last     |        126.273 (+-2.519)         |                     |             128.831 (+-1.975)            \r\n      channels=3, size=520, dtype=torch.float64, mf=channels_last     |        551.861 (+-4.159)         |                     |             517.343 (+-4.501)            \r\n      channels=3, size=712, dtype=torch.float64, mf=channels_last     |       1102.465 (+-66.427)        |                     |            1224.532 (+-55.656)           \r\n\r\n      channels=3, size=256, dtype=torch.float64, mf=channels_first    |        129.965 (+-2.083)         |                     |             130.709 (+-2.261)            \r\n      channels=3, size=520, dtype=torch.float64, mf=channels_first    |        526.332 (+-5.354)         |                     |             515.399 (+-4.320)            \r\n      channels=3, size=712, dtype=torch.float64, mf=channels_first    |       1169.215 (+-78.889)        |                     |            1102.536 (+-51.178)           \r\n\r\n      channels=1, size=256, dtype=torch.bfloat16, mf=channels_last    |         7.478 (+-0.147)          |                     |              7.154 (+-0.162)             \r\n      channels=1, size=520, dtype=torch.bfloat16, mf=channels_last    |         33.836 (+-1.022)         |                     |              38.854 (+-0.648)            \r\n      channels=1, size=712, dtype=torch.bfloat16, mf=channels_last    |         85.483 (+-0.582)         |                     |              99.190 (+-2.202)            \r\n\r\n      channels=1, size=256, dtype=torch.bfloat16, mf=channels_first   |         7.416 (+-0.125)          |                     |              7.169 (+-0.121)             \r\n      channels=1, size=520, dtype=torch.bfloat16, mf=channels_first   |         34.958 (+-0.717)         |                     |              40.136 (+-0.784)            \r\n      channels=1, size=712, dtype=torch.bfloat16, mf=channels_first   |         85.505 (+-1.207)         |                     |              99.793 (+-2.065)            \r\n\r\n      channels=1, size=256, dtype=torch.bool, mf=channels_last        |         5.856 (+-0.178)          |                     |              5.824 (+-0.118)             \r\n      channels=1, size=520, dtype=torch.bool, mf=channels_last        |         12.030 (+-0.330)         |                     |              14.478 (+-0.554)            \r\n      channels=1, size=712, dtype=torch.bool, mf=channels_last        |         30.116 (+-0.639)         |                     |              31.163 (+-0.873)            \r\n\r\n      channels=1, size=256, dtype=torch.bool, mf=channels_first       |         5.804 (+-0.113)          |                     |              5.825 (+-0.102)             \r\n      channels=1, size=520, dtype=torch.bool, mf=channels_first       |         12.043 (+-0.363)         |                     |              14.240 (+-0.341)            \r\n      channels=1, size=712, dtype=torch.bool, mf=channels_first       |         30.001 (+-1.001)         |                     |              33.199 (+-0.430)            \r\n\r\n      channels=1, size=256, dtype=torch.complex64, mf=channels_last   |         29.941 (+-0.861)         |                     |              28.229 (+-0.904)            \r\n      channels=1, size=520, dtype=torch.complex64, mf=channels_last   |        173.244 (+-2.577)         |                     |             173.173 (+-2.260)            \r\n      channels=1, size=712, dtype=torch.complex64, mf=channels_last   |        323.548 (+-3.338)         |                     |             318.318 (+-2.764)            \r\n\r\n      channels=1, size=256, dtype=torch.complex64, mf=channels_first  |         29.001 (+-1.029)         |                     |              28.565 (+-2.074)            \r\n      channels=1, size=520, dtype=torch.complex64, mf=channels_first  |        173.078 (+-1.993)         |                     |             170.664 (+-1.722)            \r\n      channels=1, size=712, dtype=torch.complex64, mf=channels_first  |        324.782 (+-3.759)         |                     |             315.745 (+-2.600)            \r\n\r\nTimes are in microseconds (us).\r\n\r\n```\r\n\r\n[Source](https://gist.github.com/vfdev-5/c2ca615b522aeb1c4636dc8d948fec74#file-20221209-105707-pr_vs_nightly-avx512-md)\r\n\r\n\r\ncc @VitalyFedyunin @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 89413, "title": "Add _thnn_fused_lstm_cell meta impl to master", "time": "2022-11-21T10:53:15Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #89413\r\n\r\nThis is a merge of _thnn_fused_lstm_cell_meta from symbolic shapes branch to master.\r\n\r\nTODO: remove xfails and add tests if needed", "label": [{"id": 3769203231, "node_id": "LA_kwDOA-j9z87gqXof", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20composability", "name": "release notes: composability", "color": "A5282C", "default": false, "description": "release notes category"}]}
{"number": 89396, "title": "[WIP] change test_c10d_error_logger to use MultiThreadedTestCase", "time": "2022-11-21T03:53:59Z", "body": "Update test_c10d_error_logger.py to use MultiThreadedTestCase.", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 89376, "title": "[WIP] jacrev : for loop approach", "time": "2022-11-20T09:49:12Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 89365, "title": "Directly link against MKL::MKL target, instead of doing it from scratch", "time": "2022-11-20T05:41:53Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89365\n\nSee also https://github.com/pytorch/pytorch/issues/73008\n\nAlso fixes https://github.com/pytorch/audio/issues/2784\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>", "label": []}
{"number": 89325, "title": "[don't land] - debug fsdp stuff", "time": "2022-11-18T23:28:19Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89458\n* #89457\n* #89330\n* __->__ #89325\n* #89096\n* #88523\n* #89324\n\nMainly makes the toymodel smaller/simpler,\n\nbut also modifies the torchvis output to include 'view' tensors created by fsdp\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 3769221312, "node_id": "LA_kwDOA-j9z87gqcDA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(fsdp)", "name": "release notes: distributed (fsdp)", "color": "C1F551", "default": false, "description": "release notes category"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89323, "title": "Initialize Kineto at startup when running in a daemon mode", "time": "2022-11-18T23:25:14Z", "body": "To the best of my understanding, when running with `KINETO_USE_DAEMON=1` the flow is as follows:\r\n1. Static initializer registers profiler with kineto\r\n2. Kineto installs CuPTI callback\r\n3. Any CUDA operation triggers CuPTI callback\r\n4. The callback initializes the rest of Kineto, including config update thread.\r\n\r\nThis doesn't work for CPU only workloads and in fact daemon mode seems to be broken there.\r\n\r\nI'm not sure what's the right place to put the initialization is. This PR puts it in a static initializer that seems workable but potentially dangerous (creating background threads there). Open to suggestions there (wire to ATen::Context?).\r\n\r\nTesting: it's non-trivial to run a unittest (forking and such), so tested manually with CPU-only build of PyTorch.", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 89313, "title": "End-to-end BERT_pytorch working with dynamic shapes and inductor", "time": "2022-11-18T21:43:42Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89313\n\r\nThis contains https://github.com/pytorch/pytorch/pull/88546 plus a big pile of inductor hacks.\r\n\r\n```\r\n$ TORCHDYNAMO_DYNAMIC_SHAPES=1 AOT_DYNAMIC_SHAPES=1 python benchmarks/dynamo/torchbench.py --accuracy --backend inductor --training --only BERT_pytorch\r\ncuda train BERT_pytorch                       PASS\r\n```\r\n\r\nI don't know if we're actually generating generic kernels though. Maybe @Chillee can check.\r\n\r\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\r\n\r\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89312, "title": "[WIP] Test MPS on Ventura", "time": "2022-11-18T21:19:56Z", "body": null, "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4417837777, "node_id": "LA_kwDOA-j9z88AAAABB1La0Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/mps", "name": "ciflow/mps", "color": "02EDA7", "default": false, "description": "Run MPS tests (subset of trunk)"}]}
{"number": 89311, "title": "[WIP] [Draft] Implement sparse all reduce for NCCL", "time": "2022-11-18T21:15:49Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89311\n\nDifferential Revision: [D41413399](https://our.internmc.facebook.com/intern/diff/D41413399/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D41413399/)!", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769218061, "node_id": "LA_kwDOA-j9z87gqbQN", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(c10d)", "name": "release notes: distributed (c10d)", "color": "EE86D4", "default": false, "description": "release notes category"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}]}
{"number": 89307, "title": "Add cleaned up TorchDynamo docs", "time": "2022-11-18T20:04:09Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": []}
{"number": 89305, "title": "[inductor] Check memory compression ratio in model tests", "time": "2022-11-18T19:56:42Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89305\n\n\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89292, "title": "Downgrade NCCL to v2.11.4", "time": "2022-11-18T17:39:40Z", "body": "https://github.com/NVIDIA/nccl/issues/749", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4077623621, "node_id": "LA_kwDOA-j9z87zC5lF", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/binaries_wheel", "name": "ciflow/binaries_wheel", "color": "AD47B4", "default": false, "description": "Trigger binary build and upload jobs for wheel on the PR"}]}
{"number": 89286, "title": "Delete .pyi files in torch/nn/parallel folder to enable mypy type checking", "time": "2022-11-18T14:55:14Z", "body": "Summary:\nAs title.\n\nContext in https://fburl.com/4irjskbe\n\nTest Plan: CI\n\nDifferential Revision: D41148641\n\n", "label": [{"id": 1447309924, "node_id": "MDU6TGFiZWwxNDQ3MzA5OTI0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/better-engineering", "name": "better-engineering", "color": "94f76a", "default": false, "description": "Relatively self-contained tasks for better engineering contributors"}, {"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 89280, "title": "Stft add named windows", "time": "2022-11-18T11:59:34Z", "body": "Fixes #88919\r\n\r\nAs discussed in issue #88919, here is the pull request that adds the option to use a named window for the STFT. This PR\r\n\r\n* adds the functionality directly to the Python code on the `functional.py` module. the `bartlett`, `blackman`, `hamming`, `hann`, and `kaiser` windows are supported.\r\n* modifies the documentation to describe this new option and to mention that not specifying a window with throw an error in version 1.15.0 (is this the correct version?).\r\n* adds a warning if a window is not specified\r\n* adds a unit test in `test/test_spectral_ops.py`. The test verifies that using a window string produces the same result as providing a precomputed Tensor window of the same type.\r\n\r\nI did my best to follow the contribution guidelines, but as this is my first PR, thank you for pointing out any mistakes I might have made.\r\n\r\n@mruberry, should I create these two new issues:\r\n* the first to be solved before 1.15.0, which throws an error if the window is not specified\r\n* the second to be solved before 1.16.0, which changes the default window to `hann`\r\n\r\nBest wishes,\r\n\r\nEric\r\n\r\n@mruberry @peterbell10", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 89262, "title": "[Inductor] Add option to build without OpenMP", "time": "2022-11-18T03:06:27Z", "body": "Move compiler standard/optimization flags to `config.cpp` class\r\nAdd option to disable OpenMP integration if `config.cpp.threads` is set to 1\r\n\r\nGuard `#include <omp.h>` with `#ifdef _OPENMP`, which according to [OpenMP Spec](https://www.openmp.org/spec-html/5.0/openmpse10.html) should be defined if compiler supports OpenMP pragmas\r\n\r\nThis unblocks testing Inductor on M1 machines by making following tweaks to config:\r\n```\r\nimport torch._inductor.config as config\r\nconfig.cpp.threads=1\r\nconfig.cpp.cxx_opt_flags = config.cpp.cxx_opt_flags.replace(\"-march=native\",\"\")\r\nconfig.cpp.cxx=(\"clang++\")\r\n```\r\n\r\nOn MacOS, compiler does not support `-fopenmp` by default\r\n\r\nFixes #ISSUE_NUMBER\r\n", "label": [{"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89259, "title": "[ao] maintain BC for is_activation_post_process", "time": "2022-11-18T02:14:39Z", "body": "Summary:\ntests https://www.internalfb.com/mlhub/pipelines/runs/fblearner/388380008 and\nand https://www.internalfb.com/tasks?t=135543814 are failing due to code packaged with trained models calling now defunct function names (is_activation_post_process).\n\nthis diff maintains BC temporarily until the cached code can be refreshed\n\n#cogwheel cogwheel_trainer_gmpp_ig_test\n\nTest Plan: buck2 run //aiplatform/modelstore/model_generation/flow/tests:cogwheel_trainer_gmpp_ig_test-launcher -- --build-fbpkg --run-disabled\n\nDifferential Revision: D41394240\n\n\n\ncc @jerryzh168 @jianyuh @raghuramank100 @jamesr66a @vkuzo @jgong5 @Xia-Weiwen @leslie-fang-intel", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 4726472156, "node_id": "LA_kwDOA-j9z88AAAABGbg93A", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20AO%20frontend", "name": "release notes: AO frontend", "color": "ededed", "default": false, "description": null}]}
{"number": 89221, "title": "[reland] rename DisableTorchFunction to DisableTorchFunctionSubclass (#88218)", "time": "2022-11-17T17:52:29Z", "body": "Summary: First half of #87990. This doesn't change any of the behavior and is just a rename\r\n\r\n#88218 got reverted for internal breakages. This is the reland of started from internal\r\n\r\nDifferential Revision:\r\nD41268423\r\n\r\nLaMa Project: L1098534\r\n\r\n\r\n\r\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 3769222160, "node_id": "LA_kwDOA-j9z87gqcQQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(sharded)", "name": "release notes: distributed (sharded)", "color": "746D79", "default": false, "description": "release notes category"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89220, "title": "[WIP] Migrate Windows CI to Python", "time": "2022-11-17T17:45:58Z", "body": "Fixes #65718\r\n\r\n", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 89215, "title": "[FSDP][4/N] Add `_ExecOrderBasePolicy`", "time": "2022-11-17T15:51:38Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #89229 [FSDP][Easy] Remove unused methods\n* #89227 [FSDP][Easy] Remove internal default arg\n* #89217 [FSDP][Easy] Remove outdated TODO\n* **#89215 [FSDP][4/N] Add `_ExecOrderBasePolicy`**\n* #89182 [FSDP][3/N] Integrate LCA into `fully_shard`\n* #89181 [FSDP][2/N] Add util for computing LCAs for shared params\n* #89180 [FSDP][1/N] Refactor module materialization\n\r\n**Overview**\r\nThis PR adds `_ExecOrderBasePolicy: _FSDPPolicy` that wraps `_exec_order_base_policy` and represents the 1st iteration base policy for the upcoming `ExecOrderPolicy`. This policy is like `always_wrap_policy` with the exclude and force-leaf ability from `size_based_auto_wrap_policy`.\r\n\r\nNotably, we must force `nn.MultiheadAttention` to be a leaf because it has _external parameters_. This refers to the case when there is a `module` whose `forward()` uses parameters from its `submodule` without calling `submodule.forward()`. In that case, we must not wrap `submodule` separately from `module`, or else `submodule`'s parameters will not be unsharded for `module.forward()`.\r\n\r\nBy adding this policy, I can simplify a unit test from the previous PR to use `TransformerWithSharedParams` from `common_fsdp.py` instead of writing a separate model with similar structure except does not use `nn.MultiheadAttention`.\r\n\r\n**Discussion**\r\nI keep this policy and its exclude/force-leaf lists distinct from those of `size_based_auto_wrap_policy` since this policy is experimental and may change. We should also not change `always_wrap_policy` since it may be used for other recursive wrapping (like `apply_activation_checkpoint`), and I would like to preserve BC with this PR.", "label": [{"id": 3769221312, "node_id": "LA_kwDOA-j9z87gqcDA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(fsdp)", "name": "release notes: distributed (fsdp)", "color": "C1F551", "default": false, "description": "release notes category"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 89213, "title": "[functorch] update make_functional tests to also test using functional_call", "time": "2022-11-17T14:58:27Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89213\n* #88851\n* #88850\n\n", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 89211, "title": "Update upsample_nearest2d meta impl", "time": "2022-11-17T14:29:23Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89211\n\n", "label": []}
{"number": 89199, "title": "enable bf16 embeddingbag in aten", "time": "2022-11-17T07:18:41Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89199\n\r\nSummary:\r\n\r\nThis PR adds BF16 support For embedding_bag(CPU) op\r\n - Add bf16 in fp32 out embedding_lookup_idx kernel in Caffe2\r\n - Use Caffe2 kernel for BF16 fast path. Use FP32 buffer for accumulation (same as fp16 https://github.com/pytorch/pytorch/pull/74844)\r\n - Add UT coverage \r\n \r\nBenchmark result on CPX 28 core/socket BS = 28 * 128:\r\n```python\r\nimport torch\r\n\r\na = torch.ones(256 * 1024 * 1024 // 4, dtype=torch.float)\r\nb = torch.ones(256 * 1024 * 1024 // 4, dtype=torch.float)\r\n\r\ndef cache_flush():\r\n    # We assume the cache size is <= 512MB here.\r\n    # a = torch.ones(256 * 1024 * 1024 // 4, dtype=torch.float)\r\n    # b = torch.ones(256 * 1024 * 1024 // 4, dtype=torch.float)\r\n    # a, b are initialized out of this function to avoid allocate memory every time\r\n    global a, b\r\n    a += b\r\n\r\nBATCHSIZE = 128 * 28\r\n\r\nE = torch.nn.EmbeddingBag(100000, 512)\r\n\r\nimport time\r\n\r\nfor BAGSIZE in [20, 10, 1]:\r\n    input = torch.randint(0, 100000, (BATCHSIZE * BAGSIZE, ))\r\n    offset = torch.arange(0, BATCHSIZE * BAGSIZE, BAGSIZE)\r\n    t = 0\r\n    for i in range(1000):\r\n        cache_flush()\r\n        start = time.time()\r\n        y = E(input, offset)\r\n        t +=time.time() - start\r\n    print(\"fp32:\", t)\r\n\r\n    bf16 = E.bfloat16()\r\n    t = 0\r\n    for i in range(1000):\r\n        cache_flush()\r\n        start = time.time()\r\n        y = bf16(input, offset)\r\n        t +=time.time() - start\r\n    print(\"bf16:\",t)\r\n```\r\n\r\n\r\n| BAG SIZE | FP32               | BF16                | ratio | flush cache |\r\n|----------|--------------------|---------------------|-------|-------------|\r\n| 1        | 0.6721155643463135 | 0.6748666763305664  | 1.00  | TRUE        |\r\n| 10       | 1.2210030555725098 | 1.211266279220581   | 0.99  | TRUE        |\r\n| 20       | 2.381131649017334  | 1.7345335483551025  | 1.37  | TRUE        |\r\n| 1        | 0.2621328830718994 | 0.31323862075805664 | 0.83  | FALSE       |\r\n| 10       | 0.6128485202789307 | 0.6218626499176025  | 0.98  | FALSE       |\r\n| 20       | 1.7913429737091064 | 1.2499897480010986  | 1.43  | FALSE       |", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769207236, "node_id": "LA_kwDOA-j9z87gqYnE", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20nn", "name": "release notes: nn", "color": "29E07F", "default": false, "description": "release notes category"}]}
{"number": 89195, "title": "Handle empty xml file exception when parsing test report XML stats", "time": "2022-11-17T06:50:56Z", "body": "In one of my manual test, I see a case when the XML file in test report zip is empty https://gha-artifacts.s3.amazonaws.com/pytorch/pytorch/3484930319/1/artifact/test-reports-test-default-5-5-linux.2xlarge_9541151567.zip (`test/test-reports/python-pytest/test_ops/test_ops-3105761a9348c555.xml`). The test stats scripts fails with a `ET.ParseError: no element found: line 1, column 0` exception.  It's better to be handle the exception gracefully here without crashing the job", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4542782543, "node_id": "LA_kwDOA-j9z88AAAABDsVcTw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/test-config/default", "name": "test-config/default", "color": "3AF818", "default": false, "description": ""}]}
{"number": 89194, "title": "Integrate apply_optim_in_backward with DDP", "time": "2022-11-17T06:38:22Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89963\n* __->__ #89194\n\nAllow _apply_optim_in_backward to work with DDP.\n\nExample:\n\n```\ndist.init_process_group(\"nccl\", rank=rank, world_size=2)\n    torch.cuda.set_device(rank)\n    e = enc().cuda(rank)\n    _apply_optimizer_in_backward(\n        optimizer_class=torch.optim.SGD,\n        params=e.parameters(),\n        optimizer_kwargs={\"lr\": 0.03},\n    )\n    e = DDP(e, device_ids=[rank])\n    inp = torch.randn(1, 10, device=rank)\n    e(inp).sum().backward()\n```\n\nConstraints:\n\n1. Custom communication hook not yet supported\n2. _apply_optim_in_backward needs to be called _before_ wrapping model in DDP.\n3. DDP will remove the gradient hooks _apply_optim_in_backward registers, so these gradient hooks will not be fired and cannot be used.\n4. All DDP managed parameters have grads set to None by default once optimizer is applied. There is no support for setting only some parameter grads to None, this must be done manually by user (and DDP_OVERLAPPED_OPTIM_SET_GRADS_TO_NONE=0 needs to be set.)\n\nDifferential Revision: [D41329694](https://our.internmc.facebook.com/intern/diff/D41329694/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D41329694/)!", "label": [{"id": 3769218061, "node_id": "LA_kwDOA-j9z87gqbQN", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(c10d)", "name": "release notes: distributed (c10d)", "color": "EE86D4", "default": false, "description": "release notes category"}]}
{"number": 89188, "title": "[Quant] lower fused LinearTanh for onednn backend", "time": "2022-11-17T05:44:26Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89188\n* #88923\n* #88879\n* #88668\n* #88665\n* #88661\n* #88478\n\r\n\r\n**Summary**\r\nAdd fuser method and quantization mappings for `QLinearLeakyReLU` for int8 inference for onednn backend. The fusion and lowering are supported only in FX mode.\r\n\r\n**Test plan**\r\npython test_quantization.py TestFuseFx TestQuantizeFx\r\n\r\n\r\ncc @jerryzh168 @jianyuh @raghuramank100 @jamesr66a @vkuzo @jgong5 @leslie-fang-intel @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10", "label": [{"id": 1031340614, "node_id": "MDU6TGFiZWwxMDMxMzQwNjE0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20quantization", "name": "oncall: quantization", "color": "f7e101", "default": false, "description": "Quantization support in PyTorch"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 89182, "title": "[FSDP][3/N] Integrate LCA into `fully_shard`", "time": "2022-11-17T03:39:18Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #89229 [FSDP][Easy] Remove unused methods\n* #89227 [FSDP][Easy] Remove internal default arg\n* #89217 [FSDP][Easy] Remove outdated TODO\n* #89215 [FSDP][4/N] Add `_ExecOrderBasePolicy`\n* **#89182 [FSDP][3/N] Integrate LCA into `fully_shard`**\n* #89181 [FSDP][2/N] Add util for computing LCAs for shared params\n* #89180 [FSDP][1/N] Refactor module materialization\n\r\n**Overview**\r\nThis PR adds support for explicit \"LCA assignment\" for `fully_shard()` (and later `ExecOrderPolicy`). This refers to assigning a shared parameter to the lowest module that is a parent of the shared parameter's lowest common ancestor (LCA) module (where the lowest module may be the LCA module itself).\r\n\r\n**Details**\r\nSupporting this behavior introduces some complexity to the runtime logic that is also needed for `ExecOrderPolicy`. Consider an example:\r\n```\r\nModule(\r\n    Submodule1,\r\n    Submodule2,\r\n)\r\n# Assume `Module` -> `Submodule1` -> `Submodule2` forward order\r\n```\r\nIf `Submodule1` and `Submodule2` share a parameter (as siblings), then we should assign the parameter to `Module`'s `FlatParameter`. However, we must ensure that we do not reshard `Module`'s `FlatParameter` until after `Module`'s forward. (Even though it may theoretically be freed after `Submodule2`'s forward, we do not have a design to achieve that.) \r\n\r\nIn general, we must ensure that the module that actually unshards a `FlatParameter` is the one to reshard it. This is different from the existing design in that there can now be multiple wrapped modules that are consumers of a `FlatParameter`.\r\n\r\nFor this LCA assignment, the multiple consumers always include a definitive root. In the future, for `ExecOrderPolicy`, the multiple consumers may simply be siblings (i.e. without a definitive root).\r\n\r\n**Addendum**\r\nThe change to `_exec_order_utils.py` fixes a previously unexercised code path.\r\n", "label": [{"id": 3769221312, "node_id": "LA_kwDOA-j9z87gqcDA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(fsdp)", "name": "release notes: distributed (fsdp)", "color": "C1F551", "default": false, "description": "release notes category"}, {"id": 3773061335, "node_id": "LA_kwDOA-j9z87g5FjX", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20improvements", "name": "topic: improvements", "color": "22499E", "default": false, "description": "topic category"}]}
{"number": 89181, "title": "[FSDP][2/N] Add util for computing LCAs for shared params", "time": "2022-11-17T03:39:11Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* #89229 [FSDP][Easy] Remove unused methods\r\n* #89227 [FSDP][Easy] Remove internal default arg\r\n* #89217 [FSDP][Easy] Remove outdated TODO\r\n* #89215 [FSDP][4/N] Add `_ExecOrderBasePolicy`\r\n* #89182 [FSDP][3/N] Integrate LCA into `fully_shard`\r\n* **#89181 [FSDP][2/N] Add util for computing LCAs for shared params**\r\n* #89180 [FSDP][1/N] Refactor module materialization\r\n\r\n**Overview**\r\n- This PR implements a utility function `get_shared_param_info_to_lca()` that returns a `Dict[SharedParamInfo, nn.Module]` mapping `SharedParamInfo` (representing a shared parameter) to its lowest common ancestor (LCA) module.\r\n- This function can be used as a subroutine for assigning shared parameters to their LCA modules during FSDP initialization (for the composable code path in the short term).\r\n\r\n**Details**\r\nThe implementation follows a simple version of [Tarjan's offline LCA algorithm](https://en.wikipedia.org/wiki/Tarjan%27s_off-line_lowest_common_ancestors_algorithm) that is based on a union-find data structure. We can use this algorithm because the set of LCA queries is fixed a priori (i.e. this is offline).\r\n\r\nEach module represents a vertex in the module tree, where there is a directed edge from parent module to child module (i.e. `p` is a parent of `c` if `c` is returned from `p.children()`). The LCA module `lca` of two modules `a` and `b` is the lowest (i.e. greatest depth) module that includes both `a` and `b` in its subtree.\r\n\r\n\r\n**Addendum**\r\nThe change to `flat_param.py` is purely formatting (from running `ufmt`).\r\n\r\nFor the unit test, here is a visualization of the module tree:\r\n![tree](https://user-images.githubusercontent.com/31054793/202576843-688694dc-ccbd-4d98-9cf8-b82d51d05e8e.png)\r\n\r\n", "label": [{"id": 3769221312, "node_id": "LA_kwDOA-j9z87gqcDA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(fsdp)", "name": "release notes: distributed (fsdp)", "color": "C1F551", "default": false, "description": "release notes category"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 89180, "title": "[FSDP][1/N] Refactor module materialization", "time": "2022-11-17T03:39:02Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #89229 [FSDP][Easy] Remove unused methods\n* #89227 [FSDP][Easy] Remove internal default arg\n* #89217 [FSDP][Easy] Remove outdated TODO\n* #89215 [FSDP][4/N] Add `_ExecOrderBasePolicy`\n* #89182 [FSDP][3/N] Integrate LCA into `fully_shard`\n* #89181 [FSDP][2/N] Add util for computing LCAs for shared params\n* **#89180 [FSDP][1/N] Refactor module materialization**\n\r\n**Overview**\r\nThis refactors module materialization (i.e. meta device or `torchdistX` deferred initialization) to compute the parameter and buffer names as needed instead of pre-computing them. These are needed to reacquire references to the states (e.g. `module.get_parameter(param_name)`) after materialization since the materialization may create new variables.\r\n\r\nThis refactor simplifies `_get_submodule_to_states()` (the core function for \"pseudo auto wrapping\") to better enable lowest common ancestor (LCA) module computation for shared parameters, for which tracking parameter and buffer names may complicate the already non-obvious implementation.\r\n\r\n**Discussion**\r\nThe tradeoff is a worst case quadratic traversal over modules if materializing all of them. However, since (1) the number of modules is relatively small, (2) the computation per module in the quadratic traversal is negligible, (3) this runs only once per training session, and (4) module materialization targets truly large models, I think this tradeoff is tolerable.\r\n\r\n**For Reviewers**\r\n- `_init_param_handle_from_module()` initializes _one_ `FlatParamHandle` from a _subroot_ module and represents the module wrapper code path. For this code path, there is no need to reacquire references to the parameters/buffers for now since the managed parameters are only computed after materialization. This works because the managed parameters have a simple definition: any parameter in the local root module's tree excluding those already marked as flattened by FSDP. Similarly, FSDP marks buffers to indicate that they have already been processed (synced if `sync_module_states`).\r\n- `_init_param_handles_from_module()` initializes _all_ `FlatParamHandle`s from a _local root_ module and represents the composable code path (and later non-recursive wrapping code path as well). For this code path, we must reacquire references to parameters/buffers because each logical wrapping is specified as a list of parameters/buffers to group together by those variables and because materialization may create new variables.\r\n\r\nExample to clarify terminology:\r\n```\r\nModule(\r\n    Submodule(\r\n        Subsubmodule,\r\n        Subsubmodule,\r\n    ),\r\n    Submodule(\r\n        Subsubmodule,\r\n        Subsubmodule,\r\n    ),\r\n)\r\n```\r\nSuppose we apply FSDP to `Module` and each `Submodule`. \r\n- I am calling each `Submodule` a _subroot_ because when applying FSDP to it, it is the root of the subtree in consideration -- there are still child `Subsubmodule`s in the subtree.\r\n- I am calling `Module` the _local root_ because it is the module whose FSDP instance has `_is_root = True` but, in general, `Module` may be part of a larger module tree, in which case it may not be the global root.", "label": [{"id": 3769221312, "node_id": "LA_kwDOA-j9z87gqcDA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(fsdp)", "name": "release notes: distributed (fsdp)", "color": "C1F551", "default": false, "description": "release notes category"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 89172, "title": "[cuBLAS] Add an option to disable reduced precision reductions for BF16 GEMM", "time": "2022-11-17T01:01:45Z", "body": "Essentially the same change as #67946, except that the default is to disallow reduced precision reductions in `BFloat16` GEMMs (for now). If performance is severely regressed, we can change the default, but this option appears to be necessary to pass some `addmm` `BFloat16` tests on H100.\r\n\r\nCC @ptrblck @ngimel ", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}]}
{"number": 89169, "title": "Add Hook to store arbitrary python objects that are copied over in tls", "time": "2022-11-16T23:48:11Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89169\n* #89146\n\n\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89166, "title": "Update save_on_cpu and checkpointing to work with functorch wrapped tensors", "time": "2022-11-16T22:39:55Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89166\n* #85849\n* #88357\n\r\nDesign doc: https://docs.google.com/document/d/1OX5__xKsZP-natgEnsRrK4gfD0wSN9WS6j4kekfn-IA/edit\r\n\r\nThis approach saves the inner-most tensor. As we unwrap, we also append to a list of rewrap functions that capture the necessary information to restore the metadata on the original wrappers. This PR tries to do most things in Python, but there are probably some APIs that could exist (or maybe already exist) that could simplify this PR.\r\n\r\n- This PR does very weird things to stash autograd metadata:\r\n  - The rewrap function needs to capture autograd metadata so that the higher order graphs don't get disconnected, we reuse TensorWrapper to do this, but in a way that is careful not to save the original TensorWrapper's data\r\n  - During packing, we do a clone on the original TensorWrapper, then replace the value_ with an empty tensor, so this new dataless TensorWrapper gets captured instead by rewrap fn\r\n  - During unpacking, when we run the rewrap fn, we just replace the value_ with the value we desire (this could either be the \r\n    recomputed value or value that was previously offloaded)\r\n  - The API exposed to replace value_ is set_data!\r\n- There doesn't seem to be a reliable way to uniquely identify a tensor since id() gets reused, using data_ptr helps but it is \r\n  also not enough sometimes. In this PR, I'm also using the first element of the Tensor to get a test to pass.\r\n\r\nUnanswered questions:\r\n- Why did we need to enable grad mode while packing (where was it disabled)\r\n\r\nOther prototypes:\r\n- https://github.com/pytorch/pytorch/pull/89159 (alternate approach that saves the outer-most tensor instead and unwraps the necessary number of layers during unpack - the issue is that we cannot tell when we are saving the outer-most tensor)\r\n- https://github.com/pytorch/pytorch/pull/88976 (same approach as this PR, but in cpp, unfinished)\r\n\r\nTODO:\r\n- verify in tests that we are actually saving the correct amount of tensors\r\n- try a non-zero bdim\r\n- make that assert more precise\r\n", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 89163, "title": "Faster import distributed.elastic", "time": "2022-11-16T22:32:17Z", "body": "inspect.stack() is expensive. Similar change was done in https://github.com/pytorch/pytorch/pull/12839.\r\n\r\nE.g. `import accelerate`, which uses elastic, currently takes ~1130 ms but ~830 ms after this change.\r\n\r\n\r\n\r\nReplacing `inspect.getmodule(frame)`  with `frame.f_locals.get('__name__')` might also be an idea, gets ~790 ms  ", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 89159, "title": "[doesn't work] Update save_on_cpu to handle saving the same possibly wrapped tensor multiple times", "time": "2022-11-16T21:15:42Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #89159\r\n* #85849\r\n* #88357\r\n\r\nI wanted to see if I could get something to work without having to stash and restore autograd metadata.\r\nIt does this by saving the outer most wrapping, and then during unpacking, unwrap to the desired level.\r\nThe issue is that this doesn't completely work - when outputs are saved for backward, we save the inner-most\r\ntensors first, and we never know if the current layer of unwrapping is the last layer or not (because there may be a vmap somewhere that changes what the grad layers above end up saving)", "label": []}
{"number": 89151, "title": "Expose retrieveDesyncReport() C++ API as internal method to Python", "time": "2022-11-16T19:05:09Z", "body": "Summary: This diff exposes desync information from `retrieveDesyncReport()` which is normally used in `ProcessGroupNCCL` implementation, but we are exposing it as an internal API. The API is surfaced from pybind using a newly created pybind module.\n\nTest Plan:\nAdded a new file and test:\n\n`buck run caffe2/test/distributed/fb:test_c10d_distributed`\n\nDifferential Revision: D41237217\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 3769218061, "node_id": "LA_kwDOA-j9z87gqbQN", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(c10d)", "name": "release notes: distributed (c10d)", "color": "EE86D4", "default": false, "description": "release notes category"}]}
{"number": 89147, "title": "[Composable API]Common _State parent class for composable and wrapper FSDP", "time": "2022-11-16T18:16:23Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89147\n\r\n**Why this PR?**\r\n\r\nFor the composable APIs implementation, sometimes the internal APIs may not have the application (FSDP, DDP) root module but only the local module. One example is the state_dict/optimizer_state_dict implementation of FSDP. These APIs  are designed to start with the root module of the model. It is tricky for these APIs to tell whether a random submodule is managed by either DDP or FSDP. \r\n\r\nIt will be useful to have APIs like:\r\n`_get_module_state(module)`: return the composable state if this module is managed by composable API.\r\n`_get_module_fsdp_state(module)`: return the FSDP state if this module is managed by FSDP.\r\n\r\n**What does this PR propose?**\r\n1. Make `_State` out of `_composable` module so that `FullyShardedDataParallel` can inherit from it.\r\n2. A global `_module_state_mapping: Dict[nn.Module, _State]` that keeps the mapping of all submodules (not just root module) to the state.\r\n3. Create `_get_module_state(module)` to look up `_module_state_mapping`.\r\n4. Create `_get_module_fsdp_state(module)` that uses `_get_module_state(module)` to get the state then verifies if the state is `_FSDPState`.\r\n\r\n", "label": [{"id": 3769221312, "node_id": "LA_kwDOA-j9z87gqcDA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(fsdp)", "name": "release notes: distributed (fsdp)", "color": "C1F551", "default": false, "description": "release notes category"}]}
{"number": 89146, "title": "cudagraph tapes'", "time": "2022-11-16T17:49:51Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89169\n* __->__ #89146\n\r\n\r\n\r\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire\r\n\r\nThis is a first PR implementing the following plan of POR:\r\n\r\n- We will use a single Memory Pool across all cudagraphs. The memory pool will contain inputs (?), intermediates, and outputs. CUDAGraphs memory pool ensures that memory allocated within graph capture is not allocated outside of the memory pool, so that memory addresses remain static during graph replay. However, within the memory pool, normal storage allocation occurs, so there is no memory overhead. So long as we invoke separate CUDAGraphs in a consistent order as they are recorded, and do not retain extraneous references to Tensors, there should be no overhead. We will keep a global tape of the initial graph recordings and subsequent usages. Enforcing consistent CUDAGraph usage will also allow us to avoid copying over inputs to a CUDAGraph that are the outputs of a prior one. \r\n- To avoid “static outputs” being permanently allocated, which interferes with both memory being reclaimed within CUDAGraphs and [gradient stealing](https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/functions/accumulate_grad.h#L52), we will reconstruct output tensors Tensors with the appropriate fixed storage data pointers and other metadata on each cudagraph invocation. \r\n- When a CUDAGraph usage diverges from the existing tape we can 1. free the memory pool to the rest of the cuda allocator, invalidating the CUDAGraphs of our tape but keeping the existing Tensors alive or 2. Run torch inductor without cudagraphs for the remainder of the tape. The latter would have some memory penalty because it would not use memory from the memory pool \r\n- Because cudagraphs freeze memory addresses, the invocation of a cudagraph will replace whatever values the memory had previously. To avoid overwriting the live output values of a previous invocation, we can either check that the backward has been run for a cudagraph forward (not clear to me this is sufficient ?). We potentially could also do weak-ref ref counting.\r\n\r\nNot everything is implemented yet, and I will not land this pr without subsequent follow ups. \r\n\r\nFollows ups, for correctness/minimal viable product in terms of priority include :\r\n- Registering the cudagraphs as thread local state that needs to be copied over in autograd\r\n- Ensuring clean up works successfully\r\n- Adding tests for all the edge cases provided reviewers are okay with approach\r\n- Handling subsequent usage when a graph invariant is broken, i.e. moving cuda graph allocations from the private memory pool to eager\r\n\r\nAnd then further work would be:\r\n- Allowing multiple, separate cudagraph paths by making the cuda caching allocator resumable and doing new recordings. \r\n- Adding configs to weaken some of the correctness conditions (for power user who can verify correctness)\r\n\r\n", "label": [{"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89144, "title": "Fix resource consumption in reductions", "time": "2022-11-16T17:10:53Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89144\n\nReductions along a (large enough) contiguous dimension vectorise the\nloading of the inputs. This vectorisation was not taken into account\nwhen computing the necessary resources for the kernel.", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2406316143, "node_id": "MDU6TGFiZWwyNDA2MzE2MTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20reductions", "name": "module: reductions", "color": "f7e101", "default": false, "description": ""}, {"id": 3768824578, "node_id": "LA_kwDOA-j9z87go7MC", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20cuda", "name": "release notes: cuda", "color": "91275E", "default": false, "description": "release notes category"}, {"id": 3769201219, "node_id": "LA_kwDOA-j9z87gqXJD", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20performance_as_product", "name": "release notes: performance_as_product", "color": "006b75", "default": false, "description": "release notes category"}, {"id": 3773062847, "node_id": "LA_kwDOA-j9z87g5F6_", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20performance", "name": "topic: performance", "color": "AE9D13", "default": false, "description": "topic category"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}]}
{"number": 89135, "title": "Make ModuleList derive from Sequence[T] and type it appropriately", "time": "2022-11-16T14:52:19Z", "body": "I see https://github.com/pytorch/pytorch/issues/53103 says this might be problematic, but I'm a bit confused at this point, because it looks like ModuleList does in fact already adhere to the Sequence API\r\n\r\nThe big win here is that for homogenous ModuleLists, you now get typing for individual members, e.g.\r\n`ModuleList([Linear(), Linear(), Linear()])[1]` properly has type `Linear`\r\n\r\nIf this looks good, I can do a followup PR to do similarly for `ModuleDict` and `Parameter[List,Dict]`", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 89134, "title": "ROCMLoops and elementwise optimization patch", "time": "2022-11-16T14:48:12Z", "body": "Fixes #32383\r\nBased on [PR](https://github.com/pytorch/pytorch/pull/77779/)\r\n\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport", "label": [{"id": 1078897659, "node_id": "MDU6TGFiZWwxMDc4ODk3NjU5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20rocm", "name": "module: rocm", "color": "f7e101", "default": false, "description": "AMD GPU support for Pytorch"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}, {"id": 4804263862, "node_id": "LA_kwDOA-j9z88AAAABHls_tg", "url": "https://api.github.com/repos/pytorch/pytorch/labels/rocm", "name": "rocm", "color": "B3FC7B", "default": false, "description": "This tag is for PRs from ROCm team"}]}
{"number": 89123, "title": "Re-enable CPU tests", "time": "2022-11-16T08:35:05Z", "body": "Summary: re-enable CPU tests. Previously had issues on macos tests.\r\n\r\nTest Plan: CI\r\n\r\nDifferential Revision: D41333423\r\n\r\n\r\n\r\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89107, "title": "[ONNX] Add symbolic for _convolution_mode", "time": "2022-11-16T01:11:26Z", "body": "As per #68880\r\nimplement the operator _convolution_mode in the ONNX exporter. This will allow user to leverage the padding 'str' mode where it can be set to 'valid' or 'same'.", "label": [{"id": 693805995, "node_id": "MDU6TGFiZWw2OTM4MDU5OTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20onnx", "name": "module: onnx", "color": "f7e101", "default": false, "description": "Related to torch.onnx"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3768821874, "node_id": "LA_kwDOA-j9z87go6hy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20onnx", "name": "release notes: onnx", "color": "B4EB93", "default": false, "description": "torch.onnx related changes that should show up in the release notes"}, {"id": 3773060564, "node_id": "LA_kwDOA-j9z87g5FXU", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20new%20feature", "name": "topic: new feature", "color": "C32883", "default": false, "description": "topic category"}]}
{"number": 89098, "title": "[Dynamo] Inline class method", "time": "2022-11-15T22:56:31Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89098\n\r\nFixes: https://github.com/pytorch/torchdynamo/issues/1858\r\n\r\nClass will be treated as UserDefinedObject which will treat forward as UserFunction. Then, it will just inline the function body into the top level module\r\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89097, "title": "Add TORCH_CHECK_TENSOR", "time": "2022-11-15T22:49:33Z", "body": "`TORCH_CHECK_TENSOR` is a wrapper around `TORCH_CHECK` which allows the condition argument to be a tensor, batched or unbatched\r\n\r\nPart of #72948", "label": [{"id": 1300932205, "node_id": "MDU6TGFiZWwxMzAwOTMyMjA1", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20logging", "name": "module: logging", "color": "f7e101", "default": false, "description": "Features which make it easier to tell what PyTorch is doing under the hood"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 89094, "title": "[onnx] Add atan2 symbolic", "time": "2022-11-15T22:25:29Z", "body": "Fixes #51334\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3768821874, "node_id": "LA_kwDOA-j9z87go6hy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20onnx", "name": "release notes: onnx", "color": "B4EB93", "default": false, "description": "torch.onnx related changes that should show up in the release notes"}, {"id": 3773060564, "node_id": "LA_kwDOA-j9z87g5FXU", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20new%20feature", "name": "topic: new feature", "color": "C32883", "default": false, "description": "topic category"}]}
{"number": 89085, "title": "Revert D40424319: Multisect successfully blamed D40424319 for test or build failures", "time": "2022-11-15T20:54:29Z", "body": "Summary:\nThis diff is reverting D40424319\nD40424319 has been identified to be causing the following test or build failures:\nTests affected:\n- https://www.internalfb.com/intern/test/562950028663203/\n\nHere's the Multisect link:\nhttps://www.internalfb.com/intern/testinfra/multisect/1356750\nHere are the tasks that are relevant to this breakage:\nT93240038: 7 tests started failing for oncall speech_model_infra in the last 2 weeks\nWe're generating a revert to back out the changes in this diff, please note the backout may land if someone accepts it.\n\nTest Plan: NA\n\nDifferential Revision: D40498925\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}]}
{"number": 89075, "title": "Add meta support for polar and update error check in complex", "time": "2022-11-15T17:10:36Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89075\n\n", "label": [{"id": 3769203231, "node_id": "LA_kwDOA-j9z87gqXof", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20composability", "name": "release notes: composability", "color": "A5282C", "default": false, "description": "release notes category"}]}
{"number": 89067, "title": "Avoid dereferencing element [0] if the vector is empty", "time": "2022-11-15T15:46:30Z", "body": "Summary: Avoid dereferencing element [0] if the vector is empty\n\nTest Plan: Run tests\n\nReviewed By: salilsdesai\n\nDifferential Revision: D41296037\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 3769206412, "node_id": "LA_kwDOA-j9z87gqYaM", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20vulkan", "name": "release notes: vulkan", "color": "23DD30", "default": false, "description": "release notes category"}]}
{"number": 89048, "title": "Use scalar implementation to keep the precision in linspace of integral types", "time": "2022-11-15T08:29:02Z", "body": "Fixes #88652\r\n\r\nIn the CPU implementation of linspace of integral types, `base` type in vectorized implementation is `int64_t`, which will drop the precision when `base` comes from a floating number. Meanwhile, its vectorized implementation tends to suffer from the catastrophic cancellation of floating point arithemtic since both the `base (start + step * idx)` and the `step` are not exact. Its scalar implementation is fine since start is always an integer and the result would be truncated to integer as well.\r\n\r\nTherefore, in this PR , we will skip the vectorized implementation since the vec doesn't contribute to performance anyway. And now the behaviors between CPU and GPU are the same. In some cases, the results are the same as numpy's. In some other cases, the results are different from numpy's, but it is not related to the devices (CPU and GPU). https://github.com/pytorch/pytorch/issues/81996#issuecomment-1192980485\r\n\r\ncc @VitalyFedyunin @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 89047, "title": "add vmap test with noncontiguous inputs", "time": "2022-11-15T08:16:36Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 89035, "title": "[WIP] [CI] graph break on  c-tor functions", "time": "2022-11-15T03:58:48Z", "body": "Fixes #ISSUE_NUMBER\r\n\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89027, "title": "[cuBLAS] Fix default cuBLAS workspace size and parsing for multiple workspaces", "time": "2022-11-15T00:33:52Z", "body": "Follow-up of #86167 ; The number of pools was mistakenly ignored and the default workspace size appears to be too small to match selected cuBLAS kernels before the explicit allocation change.\r\n\r\nCC @ptrblck @ngimel \r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}]}
{"number": 89022, "title": "[cuDNN][cuDNN V8 API] (re-re-open) cuDNN V8 API on by default", "time": "2022-11-14T23:22:58Z", "body": "Testing V8 on by default again after fixes have been merged for e.g., https://github.com/pytorch/torchdynamo/issues/1833\r\n\r\nOne new failure that seems to be surfaced with V8 on appears in halonext + amp\r\n```\r\nRuntimeError: Internal Triton PTX codegen error:\r\nSegmentation fault (core dumped)\r\n```\r\nBut I'm not sure if this points to a V8 issue or a Triton issue CC @ngimel @ptrblck \r\n\r\nCurrent dynamo benchmarks on A100: \r\nv7 vs. v8\r\n|dev |name                           |batch_size|abs_latency_v7|abs_latency_v8|\r\n|----|-------------------------------|----------|--------------|--------------|\r\n|cuda|adv_inception_v3               |128       |166.0240      |165.5798      |\r\n|cuda|beit_base_patch16_224          |64        |123.5912      |123.0797      |\r\n|cuda|botnet26t_256                  |128       |107.7343      |107.5948      |\r\n|cuda|cait_m36_384                   |4         |184.5038      |184.0271      |\r\n|cuda|coat_lite_mini                 |128       |142.3061      |140.5814      |\r\n|cuda|convit_base                    |64        |165.2499      |161.0743      |\r\n|cuda|convmixer_768_32               |32        |325.6984      |325.7094      |\r\n|cuda|convnext_base                  |64        |237.4632      |238.0142      |\r\n|cuda|crossvit_9_240                 |128       |72.2980       |72.4367       |\r\n|cuda|cspdarknet53                   |64        |96.6862       |96.8308       |\r\n|cuda|deit_base_distilled_patch16_224|64        |117.6045      |117.9616      |\r\n|cuda|dla102                         |128       |182.3073      |182.2304      |\r\n|cuda|dm_nfnet_f0                    |128       |133.6011      |133.6298      |\r\n|cuda|dpn107                         |32        |148.5080      |148.5885      |\r\n|cuda|eca_botnext26ts_256            |128       |113.8676      |113.1514      |\r\n|cuda|eca_halonext26ts               |128       |119.2242      |119.1845      |\r\n|cuda|ese_vovnet19b_dw               |128       |80.0217       |79.9438       |\r\n|cuda|fbnetc_100                     |128       |91.4548       |91.4009       |\r\n|cuda|fbnetv3_b                      |128       |115.4496      |115.5058      |\r\n|cuda|gernet_l                       |128       |114.8365      |114.7870      |\r\n|cuda|ghostnet_100                   |128       |58.5766       |58.5766       |\r\n|cuda|gluon_inception_v3             |128       |165.5222      |165.7167      |\r\n|cuda|gluon_xception65               |32        |165.8779      |165.7818      |\r\n|cuda|gmixer_24_224                  |128       |116.3611      |113.4925      |\r\n|cuda|gmlp_s16_224                   |128       |121.2607      |121.2534      |\r\n|cuda|hrnet_w18                      |128       |246.5706      |246.7599      |\r\n|cuda|inception_v3                   |128       |166.1096      |166.2034      |\r\n|cuda|jx_nest_base                   |32        |93.6064       |93.4088       |\r\n|cuda|lcnet_050                      |128       |21.4156       |21.4207       |\r\n|cuda|levit_128                      |128       |27.2901       |27.2543       |\r\n|cuda|mixer_b16_224                  |128       |157.8992      |158.2878      |\r\n|cuda|mixnet_l                       |128       |197.3443      |197.2125      |\r\n|cuda|mnasnet_100                    |128       |71.4604       |71.2997       |\r\n|cuda|mobilenetv2_100                |128       |67.6080       |67.7515       |\r\n|cuda|mobilenetv3_large_100          |128       |57.7224       |57.6591       |\r\n|cuda|mobilevit_s                    |64        |93.0372       |93.0530       |\r\n|cuda|nfnet_l0                       |128       |113.1664      |113.2853      |\r\n|cuda|pit_b_224                      |64        |133.3333      |133.4153      |\r\n|cuda|pnasnet5large                  |16        |238.9545      |238.8122      |\r\n|cuda|poolformer_m36                 |64        |144.2353      |144.2375      |\r\n|cuda|regnety_002                    |128       |32.8534       |32.9069       |\r\n|cuda|repvgg_a2                      |128       |102.4150      |102.3827      |\r\n|cuda|res2net101_26w_4s              |64        |120.8127      |120.8322      |\r\n|cuda|res2net50_14w_8s               |128       |149.7052      |149.8969      |\r\n|cuda|res2next50                     |128       |153.7439      |153.8215      |\r\n|cuda|resmlp_12_224                  |128       |89.1918       |86.9226       |\r\n|cuda|resnest101e                    |64        |159.4706      |159.3133      |\r\n|cuda|rexnet_100                     |128       |88.0032       |88.0397       |\r\n|cuda|sebotnet33ts_256               |64        |80.4635       |80.0120       |\r\n|cuda|selecsls42b                    |128       |70.4430       |70.3663       |\r\n|cuda|spnasnet_100                   |128       |78.0537       |78.1991       |\r\n|cuda|swin_base_patch4_window7_224   |64        |212.9073      |213.0824      |\r\n|cuda|swsl_resnext101_32x16d         |32        |193.0229      |193.0404      |\r\n|cuda|tf_efficientnet_b0             |128       |97.1316       |97.0410       |\r\n|cuda|tf_mixnet_l                    |128       |203.4956      |203.5340      |\r\n|cuda|tinynet_a                      |128       |82.4038       |82.8733       |\r\n|cuda|tnt_s_patch16_224              |128       |284.8576      |284.8867      |\r\n|cuda|twins_pcpvt_base               |64        |118.3893      |119.2329      |\r\n|cuda|visformer_small                |128       |126.0533      |126.0390      |\r\n|cuda|vit_base_patch16_224           |64        |118.2873      |118.0573      |\r\n|cuda|volo_d1_224                    |64        |108.7764      |108.2063      |\r\n|cuda|xcit_large_24_p8_224           |5         |100.4656      |100.5209      |\r\n\r\nv7 vs. v8 amp\r\n\r\n|dev |name                           |batch_size|abs_latency_v7|abs_latency_v8|\r\n|----|-------------------------------|----------|--------------|--------------|\r\n|cuda|adv_inception_v3               |128       |104.9729      |105.1237      |\r\n|cuda|beit_base_patch16_224          |64        |75.4330       |75.2039       |\r\n|cuda|botnet26t_256                  |128       |74.5149       |74.8071       |\r\n|cuda|cait_m36_384                   |4         |110.9788      |111.5170      |\r\n|cuda|coat_lite_mini                 |128       |62.3618       |64.4965       |\r\n|cuda|convit_base                    |64        |116.4054      |117.9129      |\r\n|cuda|convmixer_768_32               |32        |264.4401      |264.4491      |\r\n|cuda|convnext_base                  |64        |182.9009      |179.2136      |\r\n|cuda|crossvit_9_240                 |128       |48.8586       |48.8359       |\r\n|cuda|cspdarknet53                   |64        |80.0245       |80.0160       |\r\n|cuda|deit_base_distilled_patch16_224|64        |66.5921       |66.7448       |\r\n|cuda|dla102                         |128       |116.7780      |117.1683      |\r\n|cuda|dm_nfnet_f0                    |128       |78.9322       |79.1135       |\r\n|cuda|dpn107                         |32        |85.5206       |85.7514       |\r\n|cuda|eca_botnext26ts_256            |128       |76.3672       |77.0050       |\r\n|cuda|eca_halonext26ts               |128       |86.2458       |              |\r\n|cuda|ese_vovnet19b_dw               |128       |43.2943       |43.3379       |\r\n|cuda|fbnetc_100                     |128       |54.8479       |54.9251       |\r\n|cuda|fbnetv3_b                      |128       |70.7504       |71.0188       |\r\n|cuda|gernet_l                       |128       |66.1607       |66.0379       |\r\n|cuda|ghostnet_100                   |128       |43.8882       |43.9336       |\r\n|cuda|gluon_inception_v3             |128       |104.9297      |105.0204      |\r\n|cuda|gluon_xception65               |32        |85.7118       |85.8370       |\r\n|cuda|gmixer_24_224                  |128       |75.1214       |76.1170       |\r\n|cuda|gmlp_s16_224                   |128       |76.4207       |76.6641       |\r\n|cuda|hrnet_w18                      |128       |186.1326      |186.2435      |\r\n|cuda|inception_v3                   |128       |105.0561      |105.0783      |\r\n|cuda|jx_nest_base                   |32        |65.3066       |65.3245       |\r\n|cuda|lcnet_050                      |128       |14.7991       |14.8687       |\r\n|cuda|levit_128                      |128       |19.2893       |19.4772       |\r\n|cuda|mixer_b16_224                  |128       |93.9826       |94.2056       |\r\n|cuda|mixnet_l                       |128       |147.1245      |147.0435      |\r\n|cuda|mnasnet_100                    |128       |39.1781       |39.2565       |\r\n|cuda|mobilenetv2_100                |128       |42.3704       |42.3114       |\r\n|cuda|mobilenetv3_large_100          |128       |37.2946       |37.2816       |\r\n|cuda|mobilevit_s                    |64        |55.8930       |55.8934       |\r\n|cuda|nfnet_l0                       |128       |64.0448       |64.4438       |\r\n|cuda|pit_b_224                      |64        |80.6342       |80.2933       |\r\n|cuda|pnasnet5large                  |16        |154.9611      |154.8654      |\r\n|cuda|poolformer_m36                 |64        |101.7489      |101.8138      |\r\n|cuda|regnety_002                    |128       |27.0939       |27.0309       |\r\n|cuda|repvgg_a2                      |128       |60.9651       |61.2533       |\r\n|cuda|res2net101_26w_4s              |64        |77.3291       |77.4739       |\r\n|cuda|res2net50_14w_8s               |128       |93.6572       |93.7221       |\r\n|cuda|res2next50                     |128       |112.4975      |112.3248      |\r\n|cuda|resmlp_12_224                  |128       |59.5422       |60.7644       |\r\n|cuda|resnest101e                    |64        |97.9894       |98.3358       |\r\n|cuda|rexnet_100                     |128       |55.2218       |55.0718       |\r\n|cuda|sebotnet33ts_256               |64        |60.4880       |60.8113       |\r\n|cuda|selecsls42b                    |128       |41.4294       |41.5341       |\r\n|cuda|spnasnet_100                   |128       |45.0037       |45.0304       |\r\n|cuda|swin_base_patch4_window7_224   |64        |98.2561       |98.6925       |\r\n|cuda|swsl_resnext101_32x16d         |32        |100.6179      |100.9195      |\r\n|cuda|tf_efficientnet_b0             |128       |56.5344       |56.4591       |\r\n|cuda|tf_mixnet_l                    |128       |153.0318      |152.9367      |\r\n|cuda|tinynet_a                      |128       |54.1307       |53.9298       |\r\n|cuda|tnt_s_patch16_224              |128       |142.4801      |142.6589      |\r\n|cuda|twins_pcpvt_base               |64        |67.9027       |67.8325       |\r\n|cuda|visformer_small                |128       |72.5589       |72.9427       |\r\n|cuda|vit_base_patch16_224           |64        |71.4885       |71.7342       |\r\n|cuda|volo_d1_224                    |64        |69.3539       |69.5910       |\r\n|cuda|xcit_large_24_p8_224           |5         |59.9000       |59.9699       |\r\n\r\nv7 vs. v8 float16\r\n|dev |name                           |batch_size|abs_latency|abs_latency|\r\n|----|-------------------------------|----------|-----------|-----------|\r\n|cuda|adv_inception_v3               |128       |104.2544   |104.2677   |\r\n|cuda|beit_base_patch16_224          |64        |85.3601    |85.3786    |\r\n|cuda|botnet26t_256                  |128       |72.1476    |71.8277    |\r\n|cuda|cait_m36_384                   |4         |108.3075   |108.5941   |\r\n|cuda|coat_lite_mini                 |128       |61.2382    |61.6049    |\r\n|cuda|convmixer_768_32               |32        |263.3818   |263.3598   |\r\n|cuda|convnext_base                  |64        |172.6821   |173.8520   |\r\n|cuda|crossvit_9_240                 |128       |44.6321    |44.6340    |\r\n|cuda|cspdarknet53                   |64        |79.3165    |79.2964    |\r\n|cuda|deit_base_distilled_patch16_224|64        |61.9816    |62.2109    |\r\n|cuda|dla102                         |128       |115.7403   |115.9928   |\r\n|cuda|dm_nfnet_f0                    |128       |77.5434    |77.7440    |\r\n|cuda|dpn107                         |32        |83.6489    |83.5605    |\r\n|cuda|eca_botnext26ts_256            |128       |73.9953    |74.1031    |\r\n|cuda|eca_halonext26ts               |128       |81.7951    |81.7103    |\r\n|cuda|ese_vovnet19b_dw               |128       |42.9618    |42.8853    |\r\n|cuda|fbnetc_100                     |128       |54.3590    |54.3575    |\r\n|cuda|fbnetv3_b                      |128       |69.7977    |70.1696    |\r\n|cuda|gernet_l                       |128       |64.8684    |65.1726    |\r\n|cuda|ghostnet_100                   |128       |43.2054    |43.1319    |\r\n|cuda|gluon_inception_v3             |128       |104.1988   |104.3030   |\r\n|cuda|gluon_xception65               |32        |84.2245    |84.5085    |\r\n|cuda|gmixer_24_224                  |128       |82.0418    |82.7252    |\r\n|cuda|gmlp_s16_224                   |128       |75.4792    |75.8374    |\r\n|cuda|hrnet_w18                      |128       |184.1450   |184.1848   |\r\n|cuda|inception_v3                   |128       |104.1203   |104.2536   |\r\n|cuda|jx_nest_base                   |32        |58.2386    |58.4901    |\r\n|cuda|lcnet_050                      |128       |14.6409    |14.5616    |\r\n|cuda|levit_128                      |128       |22.3875    |22.4680    |\r\n|cuda|mixer_b16_224                  |128       |98.9534    |98.4730    |\r\n|cuda|mixnet_l                       |128       |146.1623   |146.1947   |\r\n|cuda|mnasnet_100                    |128       |38.9208    |39.3463    |\r\n|cuda|mobilenetv2_100                |128       |41.8946    |41.9847    |\r\n|cuda|mobilenetv3_large_100          |128       |36.7810    |36.8264    |\r\n|cuda|mobilevit_s                    |64        |55.3211    |55.3186    |\r\n|cuda|nfnet_l0                       |128       |63.1302    |63.5544    |\r\n|cuda|pit_b_224                      |64        |73.8752    |73.4602    |\r\n|cuda|pnasnet5large                  |16        |151.6806   |151.6111   |\r\n|cuda|poolformer_m36                 |64        |86.8341    |86.8021    |\r\n|cuda|regnety_002                    |128       |26.6798    |26.5295    |\r\n|cuda|repvgg_a2                      |128       |61.6652    |62.1482    |\r\n|cuda|res2net101_26w_4s              |64        |75.8037    |75.7739    |\r\n|cuda|res2net50_14w_8s               |128       |92.6362    |92.4338    |\r\n|cuda|res2next50                     |128       |111.5371   |111.5832   |\r\n|cuda|resmlp_12_224                  |128       |58.2349    |57.9807    |\r\n|cuda|resnest101e                    |64        |96.1114    |96.2742    |\r\n|cuda|rexnet_100                     |128       |54.8138    |54.7643    |\r\n|cuda|sebotnet33ts_256               |64        |53.1524    |53.3823    |\r\n|cuda|selecsls42b                    |128       |40.6070    |40.7104    |\r\n|cuda|spnasnet_100                   |128       |44.5732    |44.4318    |\r\n|cuda|swin_base_patch4_window7_224   |64        |98.6447    |98.8445    |\r\n|cuda|swsl_resnext101_32x16d         |32        |97.0195    |97.2968    |\r\n|cuda|tf_efficientnet_b0             |128       |56.0640    |56.0278    |\r\n|cuda|tf_mixnet_l                    |128       |152.0958   |152.0874   |\r\n|cuda|tinynet_a                      |128       |53.3694    |53.3762    |\r\n|cuda|tnt_s_patch16_224              |128       |130.2981   |130.3726   |\r\n|cuda|twins_pcpvt_base               |64        |62.5459    |62.6416    |\r\n|cuda|visformer_small                |128       |68.8502    |69.1756    |\r\n|cuda|vit_base_patch16_224           |64        |65.8587    |66.0285    |\r\n|cuda|volo_d1_224                    |64        |64.5348    |64.6057    |\r\n\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}]}
{"number": 89017, "title": "squeeze: allow squeezing multiple dimensions at once", "time": "2022-11-14T21:49:08Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89017\n\nRef #70924\n\nThis addresses part 1 of the issue, allowing `torch.squeeze` to be\npassed a tuple of dimensions. e.g.\n```python\nx.squeeze(0).squeeze(0)\n```\ncan now be written\n```python\nx.squeeze((0, 1))\n```\n(assuming x has at least 2 dimensions)\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3769210091, "node_id": "LA_kwDOA-j9z87gqZTr", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20python_frontend", "name": "release notes: python_frontend", "color": "87E01C", "default": false, "description": "release notes category"}, {"id": 3773061335, "node_id": "LA_kwDOA-j9z87g5FjX", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20improvements", "name": "topic: improvements", "color": "22499E", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89015, "title": "[dynamo] NNModuleVariable traces into call_function", "time": "2022-11-14T21:22:14Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #89015\n* #89096\n* #88523\n* #89167\n* #89149\n\n- Tracing into NNModuleVariable should reduce the special-case\n  nature of nn.modules in dynamo and make graphs with nn.Module\n  vs user code more consistent\n- We still specialize on NNModuleVariable, and fall back to\n  UnspecializedNNModuleVaraible upon mutation.  This specialization\n  is thought to be important for performance (reduced guards,\n  ability of backends to specialize on parameters, etc)\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 89009, "title": "clarify doc for torch.unique sort=False", "time": "2022-11-14T19:30:44Z", "body": "Fixes #88447\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 89000, "title": "Fix: [ATen] add more missing moves - part 2", "time": "2022-11-14T16:36:11Z", "body": "Applies some more missing std::move found by static analysis. This should improve performance and reduce unnecessary copies. This PR only targets ATen for now.\r\n\r\nAnd before you ask about the edits, std::move is optimal in a ternary operator as copy ellision cannot happen one. The best thing is probably rewriting it as an if else, but ultimately this should be performant enough.\r\nFollowup to #88512 and #88514 \r\n\r\ncc @jerryzh168 @jianyuh @raghuramank100 @jamesr66a @vkuzo @jgong5 @Xia-Weiwen @leslie-fang-intel @VitalyFedyunin @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10", "label": [{"id": 1031340614, "node_id": "MDU6TGFiZWwxMDMxMzQwNjE0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20quantization", "name": "oncall: quantization", "color": "f7e101", "default": false, "description": "Quantization support in PyTorch"}, {"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}, {"id": 3769215850, "node_id": "LA_kwDOA-j9z87gqatq", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20linalg_frontend", "name": "release notes: linalg_frontend", "color": "B7C4BC", "default": false, "description": "release notes category"}]}
{"number": 88998, "title": "Fix fake tensor propagation for nvprims", "time": "2022-11-14T15:26:43Z", "body": "Fake tensors were not working with `torch.ops.nvprims` functions because the fake tensors code expects to see the \"prim_meta_impl\" attribute that was missing.\r\n\r\nAlso fixes https://github.com/pytorch/pytorch/issues/87236\r\n\r\ncc @kevinstephano @jjsjann123", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3997471357, "node_id": "LA_kwDOA-j9z87uRJJ9", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20nvfuser", "name": "module: nvfuser", "color": "9CA380", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 88997, "title": "Use common binary build matrix for binary workflows, Nova and validation workflows", "time": "2022-11-14T15:24:20Z", "body": "Use common binary build matrix for binary workflows, Nova and validation workflows.\r\n \r\nTest Plan: Run ./regenerate.sh and observe no changes in generated-* workflow files\r\n\r\nIssues with current workflows are found and corrected by this refactoring:\r\n1 .github/workflows/generated-linux-binary-libtorch-pre-cxx11-master.yml\r\n2 .github/workflows/generated-macos-arm64-binary-wheel-nightly.yml\r\n\r\nThis PR just consolidates the matrix, does not change the binary workflow generation procedure. ", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 88987, "title": "Add logging for aot autograd and unified debug flag", "time": "2022-11-14T11:15:05Z", "body": "- Adds `log_level` to aot's config\r\n- Outputs log to `<graph_name>_<log_level>.log` in aot_torchinductor subfolder of the debug directory\r\n- Modifies the Inductor debug context to use the graph name when naming the folder instead of the os pid\r\n- Adds `TORCH_COMPILE_DEBUG` flag to enable it, (as well as separate dynamo and inductor logs)\r\n\r\ncc @gujinghui @PenghuiCheng @XiaobingSuper @jianyuh @VitalyFedyunin @jgong5 @mingfeima @sanchitintel @ashokei @jingxu10 @min-jean-cho @yanbing-j @Guobing-Chen @Xia-Weiwen @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @chunyuan-w @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}, {"id": 4705034454, "node_id": "LA_kwDOA-j9z88AAAABGHEg1g", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20dynamo", "name": "release notes: dynamo", "color": "b60205", "default": false, "description": ""}]}
{"number": 88976, "title": "[WIP] dedupe tensors saved by save_on_cpu when running  grad(grad", "time": "2022-11-13T22:39:01Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #88976\r\n* #85849\r\n* #88357\r\n\r\nTODO: still need to stash and replay autograd metadata! \r\n\r\nI don't think I want to continue with this exact approach, I'd rather do more things in python like in https://github.com/pytorch/pytorch/pull/89159, but we still need some API in python to stash and restore autograd information. This feels like .data again... because what we really want to do here is replace the value_ tensor from underneath the TensorWrapper.\r\n\r\n```\r\nimport torch\r\nfrom functorch import grad\r\nfrom torch.autograd.graph import save_on_cpu\r\n\r\nx = torch.tensor(3., requires_grad=True)\r\n\r\ndef fn(x):\r\n    return x.sin().sin()\r\n\r\nwith save_on_cpu():\r\n    grad(fn)(x)\r\n```", "label": []}
{"number": 88938, "title": "WIP: Support for integer pow", "time": "2022-11-12T15:08:44Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #88938\n* #88936\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 88933, "title": "[inductor] Prevent blowup in inner_fn_str and extract_read_writes", "time": "2022-11-12T14:21:24Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #88933\n\nCurrently the default `ops` handler expects strings as arguments and\njust formats them into a function call template string. For complex\nexpressions, this can lead to exponential growth in terms. Say for\nexample you have:\n\n```python\ndef fn(a):\n   for _ in range(3)\n       a = ops.mul(a, a)\n   return a\n```\n\nYou might expect `inner_fn_str` to contain 1 load and 3 multiplies,\nbut instead you find 8 loads and 7 multiplies:\n```python\nload(arg_0, i0) * load(arg_0, i0) * load(arg_0, i0) * load(arg_0, i0) * load(arg_0, i0) * load(arg_0, i0) * load(arg_0, i0) * load(arg_0, i0)\n```\n\nThis type of blowup is present in the lowering for\n`max_pool2d_with_indices_backward` which in #pytorch/torchdynamo#1352\nwas reported to have caused the entire compilation to hang.\n\nThis PR fixes the issue by formatting the string as a series of assignments to\nvariables, so for the example above, we now get:\n```\ntmp0 = load(arg_0, i0)\ntmp1 = tmp0 * tmp0\ntmp2 = tmp1 * tmp1\ntmp3 = tmp2 * tmp2\nreturn tmp3\n```\n\nWhich corresponds to sequence of `ops` calls made.", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 88923, "title": "[Quant] Add fused LinearTanh module for onednn backend", "time": "2022-11-12T02:01:32Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89188\n* __->__ #88923\n* #88879\n* #88668\n* #88665\n* #88661\n* #88478\n\r\n\r\n**Summary**\r\nThis PR adds fused `QLinearTanh` module for onednn backend, which will be used for int8 inference with onednn backend. Cannot call this module with other quantization backends otherwise an error is thrown.\r\n\r\n**Test plan**\r\npython test_quantization.py TestStaticQuantizedModule\r\n\r\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 88922, "title": "Update dynamic renderzvous nodes to use rendezvous hostname if provided", "time": "2022-11-12T01:11:33Z", "body": "Summary:\r\nUpdate dynamic renderzvous nodes to use rendezvous hostname if provided.\r\nFor PR: https://github.com/pytorch/pytorch/issues/85300\r\n\r\nBefore:\r\nFor dynamic renderzvous, it always grab the `fqdn` from socket for each node even if user specified the address.\r\nFor example,\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/distributed/elastic/rendezvous/dynamic_rendezvous.py#L248-L256\r\n```\r\nreturn _NodeDesc(socket.getfqdn(), os.getpid(), local_id)\r\n```\r\n\r\nNow:\r\nIf user specifies the hostname, each node will respect the given hostname.\r\nFor example, `socket.getfqdn(<hostname>) `\r\n\r\nTest Plan: Unit tests.\r\n\r\nDifferential Revision: D41204028\r\n\r\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}]}
{"number": 88913, "title": "[follow-up] Python Attr Serialization", "time": "2022-11-11T22:33:45Z", "body": "Ref: https://github.com/pytorch/pytorch/pull/81616#issuecomment-1307595402", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2404913419, "node_id": "MDU6TGFiZWwyNDA0OTEzNDE5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Merged", "name": "Merged", "color": "ededed", "default": false, "description": null}, {"id": 2510927053, "node_id": "MDU6TGFiZWwyNTEwOTI3MDUz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Reverted", "name": "Reverted", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}]}
{"number": 88912, "title": "[AI Accelerators] Update torch.nn.functional multi_head_attention_forward(). Add pastpath", "time": "2022-11-11T22:15:26Z", "body": "Summary: It presents as a fastpath where common execution paths are executed natively in Pytorch using torch._native_multi_head_attention. At present, the fast path is implemented in [torch.nn.MultiHeadAttention](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/activation.py).\r\nChange replicates fastpath implementation in torch.nn.functional multi_head_attention_forward.\r\nWith some refactoring done to avoid duplication.\r\n\r\nTest Plan: buck2 test @//mode/dev-nosan //caffe2/test:jit -- 'jit_multihead_attn_forward (test_jit.TestJit)' test_functional_multi_head_attn_fast_path --print-passing-details\r\n", "label": []}
{"number": 88911, "title": "Add prebuilt libtorch builds for macOS arm64 to CI.", "time": "2022-11-11T21:37:06Z", "body": "Fixes #62371.\r\n\r\nI wasn't sure if I was suppose to commit the generated scripts or not. Let me know if I should remove them from the PR.\n\ncc @ezyang @seemethere @malfet", "label": [{"id": 679952949, "node_id": "MDU6TGFiZWw2Nzk5NTI5NDk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20binaries", "name": "oncall: binaries", "color": "f7e101", "default": false, "description": "Anything related to official binaries that we release to users"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 88906, "title": "[fix] `adaptive_{avg, max}_pool` variants : cuda & cpu", "time": "2022-11-11T20:34:39Z", "body": "Fixes #78868\r\n\r\n#### TODO\r\n- [x] add tests\r\n- [x] adaptive_avg_pool2d\r\n- [x] adaptive_avg_pool3d\r\n- [x] adaptive_max_pool2d\r\n- [x] fix adaptive_max_pool3d_cuda\r\n\r\ncc: @kshitij12345 @ngimel ", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 88901, "title": "Move xnnpack taget to fb code base", "time": "2022-11-11T19:20:49Z", "body": null, "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 88899, "title": "Add support for fmod on MPS", "time": "2022-11-11T19:10:14Z", "body": "Fixes [#86810](https://github.com/pytorch/pytorch/issues/86810)\r\n`fmod` now supported on MPS", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 4164781683, "node_id": "LA_kwDOA-j9z874PYZz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20mps", "name": "release notes: mps", "color": "1d76db", "default": false, "description": "Release notes category"}, {"id": 4417837777, "node_id": "LA_kwDOA-j9z88AAAABB1La0Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/mps", "name": "ciflow/mps", "color": "02EDA7", "default": false, "description": "Run MPS tests (subset of trunk)"}]}
{"number": 88892, "title": "Separate clang-tidy config files for internal and OSS", "time": "2022-11-11T17:34:51Z", "body": " # Summary\r\n \r\nAs a follow up to the [internal discussion](https://fburl.com/xyexl7nz), we want to turn off `WarningsAsErrors` for internal clang-tidy checks. This is because internal linter is now throwing errors for unchanged code in the diffs. However, we still want to keep `WarningsAsErrors` turned on for OSS. \r\n\r\nThe solution is to separate out `.clang-tidy` config for internal (meta) and OSS. \r\n\r\n# Test plan\r\n\r\n`\r\nlintrunner init\r\n`\r\n`\r\nlintrunner -a\r\n`\r\n\r\n# Followup\r\n\r\nWe want to ensure that these two files do not diverge in future. We want to add a lint action in `lint.yml` to catch any further divergence between the two files\r\n\r\n\r\n", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 88891, "title": "Permute weight in cpu conv within kernel", "time": "2022-11-11T16:47:16Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #88891\n\n\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 88888, "title": "Enable unit testing test_matmul_cuda for ROCm", "time": "2022-11-11T15:09:50Z", "body": "test_file | test_name | test_class\r\n-- | -- | --\r\ntest_matmul_cuda | test_cublas_addmm_size_10000_cuda_bfloat16 | (__main__.TestMatmulCudaCUDA)\r\ntest_matmul_cuda | test_cublas_addmm_size_10000_cuda_float16 | (__main__.TestMatmulCudaCUDA)\r\ntest_matmul_cuda | test_cublas_addmm_size_10000_cuda_float32 | (__main__.TestMatmulCudaCUDA)\r\ntest_matmul_cuda | test_cublas_addmm_size_1000_cuda_bfloat16 | (__main__.TestMatmulCudaCUDA)\r\ntest_matmul_cuda | test_cublas_addmm_size_1000_cuda_float16 | (__main__.TestMatmulCudaCUDA)\r\ntest_matmul_cuda | test_cublas_addmm_size_1000_cuda_float32 | (__main__.TestMatmulCudaCUDA)\r\ntest_matmul_cuda | test_cublas_addmm_size_100_cuda_bfloat16 | (__main__.TestMatmulCudaCUDA)\r\ntest_matmul_cuda | test_cublas_addmm_size_100_cuda_float16 | (__main__.TestMatmulCudaCUDA)\r\ntest_matmul_cuda | test_cublas_addmm_size_100_cuda_float32 | (__main__.TestMatmulCudaCUDA)\r\n\r\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport", "label": [{"id": 1078897659, "node_id": "MDU6TGFiZWwxMDc4ODk3NjU5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20rocm", "name": "module: rocm", "color": "f7e101", "default": false, "description": "AMD GPU support for Pytorch"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}, {"id": 4804263862, "node_id": "LA_kwDOA-j9z88AAAABHls_tg", "url": "https://api.github.com/repos/pytorch/pytorch/labels/rocm", "name": "rocm", "color": "B3FC7B", "default": false, "description": "This tag is for PRs from ROCm team"}]}
{"number": 88879, "title": "[Quant] Add fused linear-tanh op for onednn backend", "time": "2022-11-11T09:18:40Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89188\n* #88923\n* __->__ #88879\n* #88668\n* #88665\n* #88661\n* #88478\n\r\n\r\n**Summary**\r\nPost op fusion can reduce data movement overhead and improve inference performance. This PR adds fused `linear-tanh` op for `onednn` backend, which will be used for int8 inference with `onednn` backend. Linear-tanh is found in models like CGAN.\r\nCannot call this op with other quantization backends otherwise an error is thrown.\r\n\r\n**Test Plan**\r\npython test_quantization.py TestQuantizedLinear\r\n\r\ncc @jerryzh168 @jianyuh @raghuramank100 @jamesr66a @vkuzo @jgong5 @leslie-fang-intel @VitalyFedyunin @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10", "label": [{"id": 1031340614, "node_id": "MDU6TGFiZWwxMDMxMzQwNjE0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20quantization", "name": "oncall: quantization", "color": "f7e101", "default": false, "description": "Quantization support in PyTorch"}, {"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 88872, "title": "WIP debug checkpointing", "time": "2022-11-11T06:03:18Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #88872\n* #88781\n* #88523\n\n\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 88862, "title": "Cleanup Windows pip dependencies", "time": "2022-11-11T01:48:46Z", "body": "The new Windows AMI from https://github.com/pytorch/test-infra/pull/1065 is now ready. All Windows pip dependencies are now part of the Windows AMI and can be cleaned up from the CI", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4440784484, "node_id": "LA_kwDOA-j9z88AAAABCLD-ZA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/test-config/functorch", "name": "test-config/functorch", "color": "85192B", "default": false, "description": "Use this label to run only functorch tests"}, {"id": 4542782543, "node_id": "LA_kwDOA-j9z88AAAABDsVcTw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/test-config/default", "name": "test-config/default", "color": "3AF818", "default": false, "description": ""}]}
{"number": 88851, "title": "[functorch] rewrite examples that use make_functional to use functional_call", "time": "2022-11-10T23:14:10Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89213\n* __->__ #88851\n* #88850\n\n", "label": [{"id": 3773063595, "node_id": "LA_kwDOA-j9z87g5GGr", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20documentation", "name": "topic: documentation", "color": "B2E89B", "default": false, "description": "topic category"}, {"id": 4395386783, "node_id": "LA_kwDOA-j9z88AAAABBfxHnw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20functorch", "name": "release notes: functorch", "color": "957DAE", "default": false, "description": "release notes category"}]}
{"number": 88850, "title": "[functorch] add new ensembling api, demonstrate in example", "time": "2022-11-10T23:13:58Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89213\n* #88851\n* __->__ #88850\n\n", "label": [{"id": 3773063595, "node_id": "LA_kwDOA-j9z87g5GGr", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20documentation", "name": "topic: documentation", "color": "B2E89B", "default": false, "description": "topic category"}, {"id": 4395386783, "node_id": "LA_kwDOA-j9z88AAAABBfxHnw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20functorch", "name": "release notes: functorch", "color": "957DAE", "default": false, "description": "release notes category"}]}
{"number": 88830, "title": "Support custom args matching in SubgraphMatcher", "time": "2022-11-10T18:28:45Z", "body": "Summary:\nCurrently for non-Node args the matching logic performs exact comparisons. However it's convenient to be able to override this to be more flexible, so that for example one doesn't need to generate patterns for every combination of kernel sizes, strides, paddings, etc. if one just wants to match all convolutions.\n\nAfter this change the custom matching will only be called if both args are not Nodes. Otherwise the old comparison is performed.\n\nTest Plan: test_fx_passes.py\n\nDifferential Revision: D41173205\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}]}
{"number": 88828, "title": "Skip `test_multiple_devices_randint_cuda_(float64 | int64)`", "time": "2022-11-10T18:11:49Z", "body": "Since the first sample of `randint`'s sample_inputs_func is a Python scalar, `randint` would return a CPU `Tensor`.\r\n\r\nRel: https://github.com/pytorch/pytorch/pull/87231", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769210091, "node_id": "LA_kwDOA-j9z87gqZTr", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20python_frontend", "name": "release notes: python_frontend", "color": "87E01C", "default": false, "description": "release notes category"}]}
{"number": 88816, "title": "aot_autograd: add assert for functional-only graph", "time": "2022-11-10T15:51:09Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #88816\n* #88817\n\n", "label": []}
{"number": 88797, "title": "[ONNX] Add test case for ListType infer shape/type for input in Scripting", "time": "2022-11-10T04:19:39Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #88797\n* #88796\n\n", "label": [{"id": 693805995, "node_id": "MDU6TGFiZWw2OTM4MDU5OTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20onnx", "name": "module: onnx", "color": "f7e101", "default": false, "description": "Related to torch.onnx"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3768821874, "node_id": "LA_kwDOA-j9z87go6hy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20onnx", "name": "release notes: onnx", "color": "B4EB93", "default": false, "description": "torch.onnx related changes that should show up in the release notes"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 88796, "title": "[ONNX] Fix ListType infer shape and type for input in Scripting", "time": "2022-11-10T04:19:36Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #88797\n* __->__ #88796\n\r\nWith the previous logic, ListType seems to be capturing the first element in input list, leading to a wrong type of input.\r\n\r\nFix #81388 \r\n\r\n", "label": [{"id": 693805995, "node_id": "MDU6TGFiZWw2OTM4MDU5OTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20onnx", "name": "module: onnx", "color": "f7e101", "default": false, "description": "Related to torch.onnx"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}, {"id": 3768821874, "node_id": "LA_kwDOA-j9z87go6hy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20onnx", "name": "release notes: onnx", "color": "B4EB93", "default": false, "description": "torch.onnx related changes that should show up in the release notes"}, {"id": 3773061952, "node_id": "LA_kwDOA-j9z87g5FtA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20bug%20fixes", "name": "topic: bug fixes", "color": "A44870", "default": false, "description": "topic category"}, {"id": 3837551288, "node_id": "LA_kwDOA-j9z87kvGK4", "url": "https://api.github.com/repos/pytorch/pytorch/labels/onnx-needs-import", "name": "onnx-needs-import", "color": "547451", "default": false, "description": "This PR is related to ONNX, but touches files outside of merge rule patterns, and hence needs import"}]}
{"number": 88787, "title": "Fixes for PyTorch/XLA functionalization integration", "time": "2022-11-10T00:41:03Z", "body": "Picking up https://github.com/pytorch/pytorch/pull/88506", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 88774, "title": "Copy on write reshape", "time": "2022-11-09T23:02:01Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #88774\n\r\nDesign doc at https://docs.google.com/document/d/1kWIRUeixlgnOk1eXJSsbxOAKUKQFpmz0yzwLZUjWotg/edit#\r\n\r\nThis PR adds a copy kwarg to reshape, which triggers a copy-on-write reshape. The intention is to eventually make this default. It also adds a warning for improper use of mutation before/after reshape call.\r\n\r\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\r\n\r\ncc @EikanWang @jgong5", "label": [{"id": 2510754463, "node_id": "MDU6TGFiZWwyNTEwNzU0NDYz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/NNC", "name": "NNC", "color": "e5678d", "default": false, "description": ""}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769210091, "node_id": "LA_kwDOA-j9z87gqZTr", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20python_frontend", "name": "release notes: python_frontend", "color": "87E01C", "default": false, "description": "release notes category"}, {"id": 3773060564, "node_id": "LA_kwDOA-j9z87g5FXU", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20new%20feature", "name": "topic: new feature", "color": "C32883", "default": false, "description": "topic category"}]}
{"number": 88764, "title": "[WIP] draft", "time": "2022-11-09T20:24:26Z", "body": null, "label": []}
{"number": 88755, "title": "Use cstdint in CUDAMiscFunctions; fix spacing", "time": "2022-11-09T18:59:00Z", "body": "Fixes:\r\n```\r\ninclusion of deprecated C++ header 'stdlib.h'; consider using 'cstdlib' instead\r\n```\r\nAlso fixes bad grammar:\r\n```\r\ncall,so the\r\n```", "label": [{"id": 1304454372, "node_id": "MDU6TGFiZWwxMzA0NDU0Mzcy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20build%20warnings", "name": "module: build warnings", "color": "f7e101", "default": false, "description": "Related to warnings during build process"}, {"id": 3769208226, "node_id": "LA_kwDOA-j9z87gqY2i", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20cpp", "name": "release notes: cpp", "color": "8250CF", "default": false, "description": "release notes category"}, {"id": 3773061335, "node_id": "LA_kwDOA-j9z87g5FjX", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20improvements", "name": "topic: improvements", "color": "22499E", "default": false, "description": "topic category"}, {"id": 4183081985, "node_id": "MDU6TGFiZWw0MTgzMDgxOTg1", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20build", "name": "topic: build", "color": "0e8a16", "default": false, "description": null}]}
{"number": 88752, "title": "Fix inline ignored warning in layer_norm_kernel.cu", "time": "2022-11-09T18:02:36Z", "body": "Fixes\r\n```\r\n[1852/2212] Building CUDA object caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/layer_norm_kernel.cu.o\r\n/home/rbarnes/pytorch/aten/src/ATen/native/cuda/layer_norm_kernel.cu(286): warning: inline qualifier ignored for \"__global__\" function\r\n```", "label": [{"id": 1304454372, "node_id": "MDU6TGFiZWwxMzA0NDU0Mzcy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20build%20warnings", "name": "module: build warnings", "color": "f7e101", "default": false, "description": "Related to warnings during build process"}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768824578, "node_id": "LA_kwDOA-j9z87go7MC", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20cuda", "name": "release notes: cuda", "color": "91275E", "default": false, "description": "release notes category"}, {"id": 3769212502, "node_id": "LA_kwDOA-j9z87gqZ5W", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20build", "name": "release notes: build", "color": "F57FFB", "default": false, "description": "release notes category"}, {"id": 3773061335, "node_id": "LA_kwDOA-j9z87g5FjX", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20improvements", "name": "topic: improvements", "color": "22499E", "default": false, "description": "topic category"}, {"id": 3773064149, "node_id": "LA_kwDOA-j9z87g5GPV", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20developer%20feature", "name": "topic: developer feature", "color": "A78C18", "default": false, "description": "Not end-user facing but still impact people that compile from source, develop into pytorch, etc."}]}
{"number": 88747, "title": "[NOT FOR LAND] Clang Tidy Experiment 1: Add -facebook-hte-BadMemberName", "time": "2022-11-09T15:28:18Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #88747\n\nControl: D40922188\nExperiment 2: D40951277\nExperiment 3: D40958205\n\n___\n\nOther file changes copied from D39779878\n\nDifferential Revision: [D40922187](https://our.internmc.facebook.com/intern/diff/D40922187/)", "label": []}
{"number": 88729, "title": "use scatter_add for index_add when dim is the most inner dim", "time": "2022-11-09T07:33:06Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #88729\n\r\n### Motivation\r\nWhen dim is -1 and the slice of source or result is noncontiguous, original `index_add` is slow as it uses add for the sliced tensor, which is serial on index and parallel on sliced tensor to avoid write conflict. Doing parallel on the sliced tensor is not optimal as the size of sliced tensor may be not big enough to parallel and also causes multiple parallelizations.\r\n\r\n`scatter_add ` is used to speedup for this case as `scatter_add ` parallels on the outer dimension of input and is serial on the inner dimension to avoid write conflict. `scatter_add ` only need one parallel and the size of outer dimensions is bigger to do parallel.\r\n\r\n\r\n### Testing\r\n\r\n- Single core:\r\n\r\nBefore:\r\n\r\nshape | fp32 / s | bf16 / s\r\n-- | -- | --\r\n[10, 128, 20, 20] | 2.82E-03 | 2.11E-03\r\n[10, 128, 50, 50] | 0.023604 | 0.023794\r\n\r\n\r\nAfter:\r\n\r\nshape | fp32 / s | bf16 / s\r\n-- | -- | --\r\n[10, 128, 20, 20] | 9.30E-04 | 1.66E-03\r\n[10, 128, 50, 50] | 0.005995 | 0.010003\r\n\r\n- Single socket (28 cores):\r\n\r\nBefore:\r\n\r\nshape | fp32 / s | bf16 / s\r\n-- | -- | --\r\n[10, 128, 20, 20] | 2.96E-03 | 2.52E-03\r\n[10, 128, 50, 50] | 0.012208 | 0.012568\r\n\r\nAfter:\r\n\r\nshape | fp32 / s | bf16 / s\r\n-- | -- | --\r\n[10, 128, 20, 20] | 7.44E-05 | 1.33E-04\r\n[10, 128, 50, 50] | 0.000333 | 0.000469\r\n\r\n\r\n\r\n\r\n", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 88725, "title": "Add `torch._check` functions analogous to C++ `TORCH_CHECK`", "time": "2022-11-09T05:15:21Z", "body": "Part of #72948", "label": [{"id": 1300932205, "node_id": "MDU6TGFiZWwxMzAwOTMyMjA1", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20logging", "name": "module: logging", "color": "f7e101", "default": false, "description": "Features which make it easier to tell what PyTorch is doing under the hood"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 88717, "title": "add check for invalid maxunpoolkd", "time": "2022-11-09T02:36:04Z", "body": "Fixes #73154\r\n\r\nAs found by #73154, `MaxUnpoolKd` returns empty tensor with zero/negative dimension for invalid `max_unpoolKd` usage with negative stride, kernel, padding sizes. \r\n\r\nThe stride, kernel, padding sizes of `MaxUnpoolKd` should be the sizes that `MaxPoolKd` was performed with, and negative stride, kernel, padding sizes are invalid for `MaxPoolKd`; hence, invalid for `MaxUnpoolKd` as well. \r\n\r\nThis PR adds check for invalid `MaxUnpoolKd` parameters. \r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\n\r\n# maxunpool2d\r\noutput = torch.rand([1, 1, 2, 2], dtype=torch.float32)                        \r\nindices = torch.randint(0,17,[1, 1, 2, 2], dtype=torch.int64)\r\n\r\n# negative kernel\r\n# outputs invalid zero dim \r\ntorch.nn.functional.max_unpool2d(output, indices, -1, 1) #tensor([], size=(1, 1, 0, 0))\r\n# outputs invalid negative dim\r\ntorch.nn.functional.max_unpool2d(output, indices, (-2,-1), (1,1)) #tensor([], size=(1, 1, -1, 0))\r\ntorch.nn.functional.max_unpool2d(output, indices, (-1,-2), (1,1)) #tensor([], size=(1, 1, 0, -1))\r\n\r\n# negative stride\r\n# outputs invalid zero dim\r\ntorch.nn.functional.max_unpool2d(output, indices, 1, -1) #tensor([], size=(1, 1, 0, 0))\r\n# outputs invalid negative dim \r\ntorch.nn.functional.max_unpool2d(output, indices, (1,1), (-2,-1)) #tensor([], size=(1, 1, -1, 0))\r\ntorch.nn.functional.max_unpool2d(output, indices, (1,1), (-1,-2)) #tensor([], size=(1, 1, 0, -1))\r\n\r\n#negative padding\r\n# should fail, but passes \r\ntorch.nn.functional.max_unpool2d(output, indices, 1, 1, -1)\r\n\r\n# maxunpool1d \r\noutput = torch.rand([1, 2, 2], dtype=torch.float32)                        \r\nindices = torch.randint(0,4,[1, 2, 2], dtype=torch.int64)\r\n\r\n# negative kernel\r\n# outputs invalid zero dim \r\ntorch.nn.functional.max_unpool1d(output, indices, -1, 1) #tensor([], size=(1, 2, 0))\r\n\r\n# negative stride\r\n# outputs invalid zero dim\r\ntorch.nn.functional.max_unpool1d(output, indices, 1, -1) #tensor([], size=(1, 2, 0))\r\n\r\n#negative padding\r\n# should fail, but passes\r\ntorch.nn.functional.max_unpool1d(output, indices, 1, 1, -1)\r\n\r\n# maxunpool3d\r\noutput = torch.rand([1, 1, 1, 2, 2], dtype=torch.float32)                        \r\nindices = torch.randint(0,17,[1, 1, 1, 2, 2], dtype=torch.int64)\r\n\r\n# negative kernel \r\ntorch.nn.functional.max_unpool3d(output, indices, -1, 1) #tensor([], size=(1, 1, -1, 0, 0))\r\n\r\n# negative stride checked \r\nhttps://github.com/pytorch/pytorch/blob/release/1.13/aten/src/ATen/native/MaxUnpooling.cpp#L107-L110\r\n\r\n# negative padding \r\n# should fail, but passes\r\ntorch.nn.functional.max_unpool3d(output, indices, 1, 1, -1)\r\n```", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3769208226, "node_id": "LA_kwDOA-j9z87gqY2i", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20cpp", "name": "release notes: cpp", "color": "8250CF", "default": false, "description": "release notes category"}]}
{"number": 88702, "title": "support single-arg min/max", "time": "2022-11-08T22:25:55Z", "body": "Fixes #1685 from TorchDynamo\r\nhttps://github.com/pytorch/torchdynamo/issues/1685\r\n\r\n\r\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 88696, "title": "Minor fix to not ignore fmt/include/core.h", "time": "2022-11-08T20:56:55Z", "body": null, "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 88688, "title": "Fix bug in unsqueeze_nested stride calculation", "time": "2022-11-08T19:08:57Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* __->__ #88688\n\n", "label": [{"id": 3773061952, "node_id": "LA_kwDOA-j9z87g5FtA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20bug%20fixes", "name": "topic: bug fixes", "color": "A44870", "default": false, "description": "topic category"}, {"id": 4251729749, "node_id": "LA_kwDOA-j9z879bD9V", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20nested%20tensor", "name": "release notes: nested tensor", "color": "13B071", "default": false, "description": "Changes that have a direct impact on nested tensors"}]}
{"number": 88668, "title": "[Quant][FX] Lower QLinearLeakyReLU for onednn backend", "time": "2022-11-08T10:22:07Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89188\n* #88923\n* #88879\n* __->__ #88668\n* #88665\n* #88661\n* #88478\n\r\n\r\n**Summary**\r\nAdd quantization mappings for `QLinearLeakyReLU` for int8 inference for onednn backend. The fusion and lowering is supported only in FX mode.\r\n\r\n**Test plan**\r\npython test_quantization.py TestQuantizeFx\r\n\r\ncc @jerryzh168 @jianyuh @raghuramank100 @jamesr66a @vkuzo @jgong5 @leslie-fang-intel @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10", "label": [{"id": 1031340614, "node_id": "MDU6TGFiZWwxMDMxMzQwNjE0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20quantization", "name": "oncall: quantization", "color": "f7e101", "default": false, "description": "Quantization support in PyTorch"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 88667, "title": "Inductor cpp wrapper: support bmm, mm, addmm extern call", "time": "2022-11-08T10:02:55Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89744\n* #89743\n* #89742\n* __->__ #88667\n* #88666\n* #88561\n* #88560\n\n\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 88666, "title": "Inductor cpp wrapper: support more dtypes of input", "time": "2022-11-08T09:58:57Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* #89744\r\n* #89743\r\n* #89742\r\n* #88667\r\n* __->__ #88666\r\n* #88561\r\n* #88560\r\n\r\n\r\nPreviously only float32 is supported as input types for the cpp wrapper.\r\nThis PR extends the cpp wrapper to support the built-in types: float32, float64, int64, int32, int16, int8, uint8, bool.\r\nBfloat16 and Float16 will be covered later.\r\n\r\n\r\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 88665, "title": "[Quant][FX] Add backend config for onednn backend and fuse Linear-LeakyReLU", "time": "2022-11-08T09:58:52Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89188\n* #88923\n* #88879\n* #88668\n* __->__ #88665\n* #88661\n* #88478\n\r\n**Summary**\r\nAdd backend config for onednn backend so that it can support more post op fusion for int8 inference. First `Linear - LeakyReLU` fusion is implemented based on previous PRs.\r\n\r\n**Test plan**\r\npython test_quantization.py TestFuseFx\r\n\r\ncc @jerryzh168 @jianyuh @raghuramank100 @jamesr66a @vkuzo @jgong5 @leslie-fang-intel @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10", "label": [{"id": 1031340614, "node_id": "MDU6TGFiZWwxMDMxMzQwNjE0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20quantization", "name": "oncall: quantization", "color": "f7e101", "default": false, "description": "Quantization support in PyTorch"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}, {"id": 4726472156, "node_id": "LA_kwDOA-j9z88AAAABGbg93A", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20AO%20frontend", "name": "release notes: AO frontend", "color": "ededed", "default": false, "description": null}]}
{"number": 88663, "title": "add mixed data type support for GroupNorm backward on CPU", "time": "2022-11-08T09:33:56Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89485\n* __->__ #88663\n* #88662\n\r\n### Motivation\r\nAmp provides convenience methods for mixed precision. If users use amp to run bfloat16 models, torch.autocast will keep module parameters in acc dtype which will leave gamma and beta in float while input/output will be in bfloat16. The same goes for backward: parameters are in float, and X & dX & dY are in bfloat16.\r\nMixed data type support for GroupNorm backward is also needed for model training with GroupNorm.\r\n\r\n### Testing\r\n\r\nSingle socket (28cores):\r\n* Contiguous:\r\n\r\nshape | forward / s | forward / s | backward / s | backward / s\r\n-- | -- | -- | -- | --\r\n  | fp32 | mixed fp32 bf16 | fp32 | mixed fp32 bf16\r\n[10, 128, 20, 20] | 3.08E-05 | 3.50E-05 | 8.06E-05 | 7.69E-05\r\n[10, 128, 50, 50] | 0.000121 | 0.000114 | 0.000358 | 0.000203\r\n\r\n\r\n* Channels Last (inputs and outputs will be converted to contiguous):\r\n\r\n\r\nshape | forward / s | forward / s | backward / s | backward / s\r\n-- | -- | -- | -- | --\r\n  | fp32 | mixed fp32 bf16 | fp32 | mixed fp32 bf16\r\n[10, 128, 20, 20] | 4.04E-05 | 4.41E-05 | 0.000226 | 0.000305\r\n[10, 128, 50, 50] | 0.000169 | 0.000166 | 0.001628 | 0.001169\r\n\r\n\r\nSingle core:\r\n\r\n* Contiguous:\r\n\r\nshape | forward / s | forward / s | backward / s | backward / s\r\n-- | -- | -- | -- | --\r\n  | fp32 | mixed fp32 bf16 | fp32 | mixed fp32 bf16\r\n[10, 128, 20, 20] | 2.38E-04 | 2.51E-04 | 5.94E-04 | 4.50E-04\r\n[10, 128, 50, 50] | 0.00171 | 0.001395 | 0.0044455 | 0.00243\r\n\r\n\r\n* Channels Last (inputs and outputs will be converted to contiguous):\r\n\r\nshape | forward / s | forward / s | backward / s | backward / s\r\n-- | -- | -- | -- | --\r\n  | fp32 | mixed fp32 bf16 | fp32 | mixed fp32 bf16\r\n[10, 128, 20, 20] | 2.28E-04 | 3.26E-04 | 0.0016528 | 0.003165\r\n[10, 128, 50, 50] | 0.001788 | 0.001302 | 0.0276621 | 0.019447\r\n\r\n\r\n", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3769207236, "node_id": "LA_kwDOA-j9z87gqYnE", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20nn", "name": "release notes: nn", "color": "29E07F", "default": false, "description": "release notes category"}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 88662, "title": "add mixed data type support for LayerNorm and GroupNorm", "time": "2022-11-08T09:27:21Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89485\n* #88663\n* __->__ #88662\n\r\n\r\n\r\nThis PR is cherry-picked from https://github.com/pytorch/pytorch/pull/84404 ~ https://github.com/pytorch/pytorch/pull/81852.", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3769207236, "node_id": "LA_kwDOA-j9z87gqYnE", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20nn", "name": "release notes: nn", "color": "29E07F", "default": false, "description": "release notes category"}]}
{"number": 88661, "title": "[Quant] Add fused LinearLeakyReLU module for onednn backend", "time": "2022-11-08T08:56:07Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89188\n* #88923\n* #88879\n* #88668\n* #88665\n* __->__ #88661\n* #88478\n\r\n**Summary**\r\nPost op fusion can reduce data movement overhead and improve inference performance. This PR adds fused `QLinearLeakyReLU` module for onednn backend, which will be used for int8 inference with onednn backend. Cannot call this module with other quantization backends otherwise an error is thrown.\r\n\r\n**Test plan**\r\npython test_quantization.py TestStaticQuantizedModule\r\n\r\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}, {"id": 4726472156, "node_id": "LA_kwDOA-j9z88AAAABGbg93A", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20AO%20frontend", "name": "release notes: AO frontend", "color": "ededed", "default": false, "description": null}]}
{"number": 88660, "title": "Issue when info save to tensorboard", "time": "2022-11-08T08:15:40Z", "body": "Working with this [model](https://pytorch-forecasting.readthedocs.io/en/stable/tutorials/stallion.html), when I do the .fit and it finishes, it tries to save the metrics in the folders for tensorboard use. It gives an error in the tf.io package being used, as it corresponds to the previous version of tensorflow.\r\n\r\n\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1616460679, "node_id": "MDU6TGFiZWwxNjE2NDYwNjc5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20visualization", "name": "oncall: visualization", "color": "e99695", "default": false, "description": "Related to visualization in PyTorch, e.g., tensorboard"}]}
{"number": 88649, "title": "[MPS] Add repeat_interleave to MPS", "time": "2022-11-08T03:34:29Z", "body": "Fixes #87219\r\n\r\nImplements new ``repeat_interleave`` function into ``aten/src/ATen/native/mps/operations/Repeat.mm``\r\nAdds it to ``aten/src/ATen/native/native_functions.yaml``\r\nAdds new test ``test_repeat_interleave`` to ``test/test_mps/py``", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 4164781683, "node_id": "LA_kwDOA-j9z874PYZz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20mps", "name": "release notes: mps", "color": "1d76db", "default": false, "description": "Release notes category"}, {"id": 4417837777, "node_id": "LA_kwDOA-j9z88AAAABB1La0Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/mps", "name": "ciflow/mps", "color": "02EDA7", "default": false, "description": "Run MPS tests (subset of trunk)"}]}
{"number": 88619, "title": "Stable topk for CPU ", "time": "2022-11-07T23:39:17Z", "body": "Fixes #88227 #27542\r\n \r\nAdds `stable` flag to `torch.topk`.\r\nExpected behavior for duplicates is defined as lower indices have higher priority.\r\n\r\ncc @jerryzh168 @jianyuh @raghuramank100 @jamesr66a @vkuzo @jgong5 @Xia-Weiwen @leslie-fang-intel @VitalyFedyunin @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10", "label": [{"id": 1031340614, "node_id": "MDU6TGFiZWwxMDMxMzQwNjE0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20quantization", "name": "oncall: quantization", "color": "f7e101", "default": false, "description": "Quantization support in PyTorch"}, {"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}]}
{"number": 88608, "title": "Copy get attr nodes to submodules during partition fusion", "time": "2022-11-07T21:50:38Z", "body": "Summary: Prior to this change, partition attributes were copied as input(placeholder) nodes into the subgraph. This is not ideal for lowering - copy them as get_attr nodes instead.\n\nTest Plan: test_fx_passes.py\n\nDifferential Revision: D40950356\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}]}
{"number": 88606, "title": "[WIP] rename random.from to random._from", "time": "2022-11-07T21:37:27Z", "body": "flushing out issues due to the rename", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 4417837777, "node_id": "LA_kwDOA-j9z88AAAABB1La0Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/mps", "name": "ciflow/mps", "color": "02EDA7", "default": false, "description": "Run MPS tests (subset of trunk)"}]}
{"number": 88600, "title": "[caffe2] Fix pybind11 native python link error", "time": "2022-11-07T19:29:10Z", "body": "Summary:\nCurrently, we define some C++ functions in one C++ Python extension\nwhich are used by another.  This happens to work, but isn't guaranteed to.\nThis diff moves these functions to a separate C++ library rule to fix this.\n\nFollowup to D40657708 (https://github.com/pytorch/pytorch/commit/1dad051b05f896a5958e33423ccd3baa10ad1072).\n\nTest Plan: CI\n\nReviewed By: BrandonTheBuilder\n\nDifferential Revision: D40854142\n\n", "label": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false, "description": ""}, {"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}]}
{"number": 88597, "title": "[C10D][BE] Add device info to c10d scuba logging", "time": "2022-11-07T19:24:18Z", "body": "Summary: As title.\n\nTest Plan: CI.\n\nDifferential Revision: D41090731\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 3769218061, "node_id": "LA_kwDOA-j9z87gqbQN", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(c10d)", "name": "release notes: distributed (c10d)", "color": "EE86D4", "default": false, "description": "release notes category"}]}
{"number": 88596, "title": "[dims] Fix large array inputs", "time": "2022-11-07T19:22:09Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #88596\n\r\nVariable length arguments can overflow the arena being used to keep overhead\r\nlow for torch dims. If we hit this case, we know the amount of work being done\r\nis already relatively big, so we just fallback to standard memory allocation.\r\n\r\nFixes #88586 ", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 88594, "title": "Add bits tensor types", "time": "2022-11-07T18:42:44Z", "body": "TODO (in later PRs)\r\n- [ ] the other bits8, 4x2, 2x4, 1x8\r\n- [ ] bits printer function", "label": [{"id": 2404913419, "node_id": "MDU6TGFiZWwyNDA0OTEzNDE5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Merged", "name": "Merged", "color": "ededed", "default": false, "description": null}, {"id": 2510927053, "node_id": "MDU6TGFiZWwyNTEwOTI3MDUz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Reverted", "name": "Reverted", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 88588, "title": "[inductor] Allow view to take dtype as its input", "time": "2022-11-07T17:20:45Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #88588\n\nSummary: as title\n\nTest Plan:\n```\nPYTORCH_TEST_WITH_INDUCTOR=1 python test/test_ops.py -k test_non_standard_bool_values\n```", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 88583, "title": "[FSDP][Perf] Avoid post-bwd pad", "time": "2022-11-07T15:44:55Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* **#88583 [FSDP][Perf] Avoid post-bwd pad**\r\n\r\n**Overview**\r\nThis PR adds an additional code path for when the `FlatParameter` must be padded to achieve an unsharded size divisible by the world size to enable the performant `_all_gather_base()` and `_reduce_scatter_base()`.\r\n- In the padded case, we register a new pre-backward hook on the `FlatParameter` itself that _pre-allocates_ the padded unsharded gradient as a zero tensor to avoid doing so in the post-backward hook, which would (1) allocate unsharded memory in the post-backward stream and (2) require a D2D copy, which may incur nontrivial overhead and block the subsequent reduce-scatter.\r\n    - The newly-computed unsharded gradient is added to the pre-allocated zero gradient. From a running time perspective, we tradeoff a GPU `memcpy` kernel with a GPU `add` kernel, so the running time may not decrease. From a memory perspective, we still have the computed gradient from autograd and an existing unsharded gradient in memory.\r\n    - The `handle.prepare_gradient_for_backward()` call is refactored to be in this new pre-backward hook `_flat_param_pre_backward_hook()`. This is to delay the pre-allocation as much as possible. The new hook is guarded by `handle._ran_flat_param_pre_backward_hook` because otherwise the hook may run multiple times per backward. It is reset to `False` in the post-backward hook.\r\n    - ~~This approach leverages the private API `torch._C._will_engine_execute_node(flat_param_acc_grad)` to check if a `FlatParameter` will compute a gradient in the current `backward()`. As a part of this, `FlatParameter`s that are not involved in the current `backward()` are not all-gathered.~~ See Notes section.\r\n- We do not pre-allocate for the non-padded case because that would allocate an extra tensor compared to the existing behavior.\r\n- For both the padded and non-padded cases, this PR changes the sharded gradient allocation to use `torch.empty()` instead of `torch.zeros()` to avoid a `fill` kernel.\r\n- This PR renames `param` to `flat_param` in the post-backward hook for clarity.\r\n\r\n**Example: RegNet-3B**\r\nFor RegNet-3B using an auto wrap policy that wraps `RegNetYLayer` and world size of 8, there are both `FlatParameter`s that need padding and those that do not.\r\n\r\nWith this PR (using a batch size of 48, `BACKWARD_PRE` prefetching, FP16 mixed precision, and activation checkpointing):\r\n- The number of `memcpy` kernels per training iteration decreases from 234 to 6.\r\n- [`use_orig_params=False`] The peak reserved memory decreases from 16.625 GB to 16.551 GB.\r\n- [`use_orig_params=False`] The peak active memory decreases from 10.500 GB to 10.363 GB.\r\n- [`use_orig_params=True`] The peak reserved memory decreases from 14.051 GB to 13.977 GB.\r\n- [`use_orig_params=True`] The peak active memory decreases from 10.501 GB to 10.363 GB.\r\n\r\nThis reduction in peak active memory comes from saving the extra unsharded gradient allocation in the post-backward. (This suggests that the peak memory happens in the backward pass for this workload.)\r\n\r\n<details>\r\n    <summary>Example: 2nd ReLU Backward</summary>\r\n\r\nAs an example, we examine the 2nd ReLU backward. First, we look at the trace for the existing code:\r\n![Screen Shot 2022-11-07 at 9 23 31 AM](https://user-images.githubusercontent.com/31054793/200333826-cde7ba91-32ef-4c22-8f33-df8d26c208a7.png)\r\n![Screen Shot 2022-11-07 at 9 23 22 AM](https://user-images.githubusercontent.com/31054793/200333905-500995bc-6a1f-45e4-b339-778583283392.png)\r\nWe see above the `memcpy` kernel. Now, we look at the trace for the new code:\r\n![Screen Shot 2022-11-07 at 9 23 38 AM](https://user-images.githubusercontent.com/31054793/200334043-08cab1b4-7ac5-42cd-a141-3098b4db2f1a.png)\r\n![Screen Shot 2022-11-07 at 9 25 39 AM](https://user-images.githubusercontent.com/31054793/200334476-c53e768b-4b62-4238-851f-a88477cd16c6.png)\r\nWe see an `add` kernel above but no more `memcpy` kernel. I am not sure why exactly, but this somehow allows the reduce-scatter to run earlier. (The preceding `BUnaryFunctor` kernel right before the reduce-scatter somehow runs earlier. This behavior is consistent across multiple runs. The `BUnaryFunctor` should be the gradient pre-divide.)\r\n\r\n</details>\r\n\r\n\r\n<details>\r\n<summary> _flat_param_pre_backward_hook() </summary>\r\n\r\nThis allocates a zero tensor for the padded unsharded gradient, and it runs after the normal pre-backward hook.\r\n![Screen Shot 2022-11-07 at 8 30 24 AM](https://user-images.githubusercontent.com/31054793/200340101-b1b84091-8ff6-4961-96cc-66ad1bbd2a07.png)\r\n\r\n</details>\r\n\r\n**Notes**\r\nOriginally, I had the PR such that we only unshard in the pre-backward if `torch._C._will_engine_execute_node(flat_param_acc_grad)`. However, this breaks with reentrant activation checkpointing when applied as `FSDP(CheckpointWrapper(module))`. This is because when the module-level pre-backward hook runs, the `bool` will return `False`, and instead, the `FlatParameter` gradient is computed later by the explicit `torch.autograd.backward()` call:\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/utils/checkpoint.py#L157\r\n\r\nThis is not a major issue, but the consequence of this is that we may unnecessarily unshard `FlatParameter`s (e.g. for mult-modal models), where the pre-backward hook may run but gradients are not computed for the `FlatParameter`.\r\n\r\nDifferential Revision: [D41137855](https://our.internmc.facebook.com/intern/diff/D41137855)", "label": [{"id": 3769221312, "node_id": "LA_kwDOA-j9z87gqcDA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(fsdp)", "name": "release notes: distributed (fsdp)", "color": "C1F551", "default": false, "description": "release notes category"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 88575, "title": "Added requested_bytes to CUDA Caching Allocator Stats", "time": "2022-11-07T14:30:38Z", "body": "Summary:\nThe caching allocator can be configured to round memory allocations in order to reduce fragmentation. Sometimes however, the overhead from rounding can be higher than the fragmentation it helps reduce.\n\nWe have added a new stat to CUDA caching allocator stats to help track if rounding is adding too much overhead and help tune the roundup_power2_divisions flag:\n    - \"requested_bytes.{current,peak,allocated,freed}\": memory requested by client code, compare this with allocated_bytes to check if allocation rounding adds too much overhead\n\nTest Plan: Added test case in caffe2/test/test_cuda.py\n\nDifferential Revision: D40810674\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}]}
{"number": 88561, "title": "Inductor cpp wrapper: support Reduction", "time": "2022-11-07T03:08:56Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89744\n* #89743\n* #89742\n* #88667\n* #88666\n* __->__ #88561\n* #88560\n\r\nFor reductions, the code string in the codegen stage and the execution stage are different due to `\\`. \r\n\r\n- The code string getting from `code.getvalue()` (`code` is an `IndentedBuffer`) in codegen stage:\r\n  ```\r\n  #pragma omp declare reduction(argmax : struct IndexValue_1 :\\\r\n                  omp_out.value = omp_in.value < omp_out.value ? omp_out.value : omp_in.value,\\\r\n                  omp_out.index = omp_in.value < omp_out.value ? omp_out.index : omp_in.index)\\\r\n                  initializer(omp_priv = {0, -std::numeric_limits<float>::infinity()})\r\n  ```\r\n\r\n- The code string loaded during the execution (`\\` will be escaped):\r\n  ```\r\n  #pragma omp declare reduction(argmax : struct IndexValue_1 :                omp_out.value = omp_in.value < omp_out.value ? omp_out.value : omp_in.value,                omp_out.index = omp_in.value < omp_out.value ? omp_out.index : omp_in.index)                  initializer(omp_priv = {0, -std::numeric_limits<float>::infinity()})\r\n  ```\r\n\r\nThus we can't get the same hash value for these two pieces of code.\r\nThis PR adds a function to make the transformation escape the backslash in the codegen stage.\r\n\r\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 88560, "title": "Inductor cpp wrapper: support None as output", "time": "2022-11-07T03:01:01Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #89744\n* #89743\n* #89742\n* #88667\n* #88666\n* #88561\n* __->__ #88560\n\r\nMap `None` to `at::Tensor()` in the cpp wrapper\r\n\r\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 88552, "title": "median test build to see if any test fails on pytorch testing cycle ", "time": "2022-11-06T06:23:32Z", "body": "This is copy of PyTorch master brach. To see if everything is okay.  \r\nmedian op implentaion is [here \r\npytorch/pull/88554](https://github.com/pytorch/pytorch/pull/88554)\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 4164781683, "node_id": "LA_kwDOA-j9z874PYZz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20mps", "name": "release notes: mps", "color": "1d76db", "default": false, "description": "Release notes category"}, {"id": 4417837777, "node_id": "LA_kwDOA-j9z88AAAABB1La0Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/mps", "name": "ciflow/mps", "color": "02EDA7", "default": false, "description": "Run MPS tests (subset of trunk)"}]}
{"number": 88542, "title": "[MPS] Fix index_add with non-f32 inputs", "time": "2022-11-05T09:03:33Z", "body": "The `multiplicationWithPrimaryTensor` and/or `scatterWithDataTensor` api has issues with handling two f16 tensor inputs, resulting in zeros outputs. With int16 or int64 inputs, there are issues as well.\r\n\r\nThis PR conditionally casts inputs to f32 if they're not and then casts the output back to the source's datatype.  \r\n\r\nFixes #82645.\r\n\r\n\r\ncc @kulinseth @albanD @malfet @DenisVieriu97 @razarmehr @abhudev", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 4121057428, "node_id": "LA_kwDOA-j9z871oliU", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20mps", "name": "module: mps", "color": "f7e101", "default": false, "description": "Related to Apple Metal Performance Shaders framework"}, {"id": 4164781683, "node_id": "LA_kwDOA-j9z874PYZz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20mps", "name": "release notes: mps", "color": "1d76db", "default": false, "description": "Release notes category"}, {"id": 4417837777, "node_id": "LA_kwDOA-j9z88AAAABB1La0Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/mps", "name": "ciflow/mps", "color": "02EDA7", "default": false, "description": "Run MPS tests (subset of trunk)"}]}
{"number": 88540, "title": "Move generic FSDP helpers to torch.distributed.utils", "time": "2022-11-05T08:42:18Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #88540\n* #88539\n\n", "label": [{"id": 3769221312, "node_id": "LA_kwDOA-j9z87gqcDA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(fsdp)", "name": "release notes: distributed (fsdp)", "color": "C1F551", "default": false, "description": "release notes category"}]}
{"number": 88538, "title": "[WIP] option to set grads to none at end of backward", "time": "2022-11-05T08:18:18Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #88538\n\nPer title\n\nDifferential Revision: [D40990976](https://our.internmc.facebook.com/intern/diff/D40990976/)", "label": [{"id": 3769218061, "node_id": "LA_kwDOA-j9z87gqbQN", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(c10d)", "name": "release notes: distributed (c10d)", "color": "EE86D4", "default": false, "description": "release notes category"}]}
{"number": 88532, "title": "[MPS] Add Unique and unique_consecutive ops.", "time": "2022-11-05T00:15:22Z", "body": "Add check for macos 13.0\r\n\r\nFixes #88487 \r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 4164781683, "node_id": "LA_kwDOA-j9z874PYZz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20mps", "name": "release notes: mps", "color": "1d76db", "default": false, "description": "Release notes category"}, {"id": 4417837777, "node_id": "LA_kwDOA-j9z88AAAABB1La0Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/mps", "name": "ciflow/mps", "color": "02EDA7", "default": false, "description": "Run MPS tests (subset of trunk)"}]}
{"number": 88526, "title": "[WIP] handling zero dims in reindexer", "time": "2022-11-04T22:44:13Z", "body": "Addresses https://github.com/pytorch/torchdynamo/issues/1787 by adding some extra logic to check for 0 dims\r\n\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 88524, "title": "Fix distutils.LooseVersion DeprecationWarning", "time": "2022-11-04T22:35:26Z", "body": "Fixes #84712", "label": []}
{"number": 88506, "title": "various fixes for functionalization <> XLA integration", "time": "2022-11-04T17:35:37Z", "body": "The fixes in this PR are mostly around:\r\n- More asserts in `FunctionalTensorWrapper`, so bugs show up more cleanly in cases where we e.g. forget to wrap an output\r\n- make the *_scatter ops `CompositeExplicitAutogradNonFunctional`, so we get a better error message and XLA doesn't accidentally try to us them\r\n- Fix LTC/XLA codegen in core to handle multi-tensor out= ops with no returns\r\n- Better erroring: Allow XLA to use the CPU fallback from core in a way so that it always errors on view ops, which XLA should no longer see.\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #88506\n\r\n", "label": []}
{"number": 88490, "title": "Convert tag to string before saving FileWriterTag", "time": "2022-11-04T13:26:58Z", "body": "Case in point, I wanted to log my classifier accuracy per class. Each class is defined as an integer, so I log the following:\r\n```\r\n{0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}\r\n```\r\nEvery step, it logs successfully - however at the end of the epoch add_scalars is called and it fails to save because the dict keys are not strings. The keys of the dict can be converted as strings as you can see in this commit.\r\n\r\nFixes #ISSUE_NUMBER\r\n", "label": []}
{"number": 88477, "title": "Dynamo benchmark: add CPU specific changes", "time": "2022-11-04T04:26:11Z", "body": "This pr adds some CPU specific changes:\r\n\r\n- Add support for IPEX backend\r\n- https://github.com/pytorch/torchdynamo/issues/1618\r\n- https://github.com/pytorch/torchdynamo/issues/1534\r\n- Add numactl settings in runner.py\r\n- Fix the issue that some environment variables are not support on CPU\r\n\r\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @Xia-Weiwen @wenzhe-nrv @jiayisunx", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 88454, "title": "disable test_AdaptiveLogSoftmax_cuda_fp32 as TI doesn't support out= ops", "time": "2022-11-03T21:23:51Z", "body": "Disabling the test as supporting out= ops isn't a priority for TI . This fixes https://github.com/pytorch/torchdynamo/issues/1786\r\n\r\n\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769207236, "node_id": "LA_kwDOA-j9z87gqYnE", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20nn", "name": "release notes: nn", "color": "29E07F", "default": false, "description": "release notes category"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 88449, "title": "training support for dynamo+torchxla integration", "time": "2022-11-03T21:01:50Z", "body": "We've already shown some promising perf result by integrating dynamo with torchxla for inference. To provide consistent UX for training and for inference, in this PR we try to enable training for dynamo/torchxla.\r\n\r\nTraining is trickier than inference and we may not expect much perf gains since\r\n1. in training case, torchxla only generate a single combined graph for fwd/bwd/optimizer while in `torchxla_trace_once` bridge we added in dynamo, due to how AOT_Autograd works, we will generate 3 graphs: one for forward, one for backward and one for the optimizer. XLA favors larger graph to do more optimizations.\r\n2. in training case, tracing overhead can be overlapped with computation. Tracing overhead is not as a big deal for training as for inference. After all training cares more about throughput while inference cares more about latency.\r\n3. in training case, people can increase batch size to 'mitigate' the tracing overhead. Increase batch size does not change tracing overhead, thus it shows like the tracing overhead 'per example' reduces.\r\n\r\nBut we still want to add training support to dynamo/torchxla to make the work complete.\r\n\r\nWe added '--iterations-per-run' argument to control how may iterations we do per measure/device sync. This is to understand the impact of item 2 above.\r\n\r\nResults: (NOTE: need add more results).\r\n\r\nWith '--iterations-per-run' equals to 1, here are the perf numbers:\r\n```\r\nresnet18 0.868x\r\nresnet50 0.922x\r\nresnext50_32x4d 0.918x\r\nalexnet 0.981x\r\nmobilenet_v2 0.909x\r\nmnasnet1_0 0.898x\r\nvgg16 0.880x\r\nBERT_pytorch 1.181x\r\ntimm_vision_transformer 1.218x\r\n```\r\n\r\nOverall it looks like graph break indeed cause perf loss. But for BERT_pytorch and timm_vision_transformer we still see perf gain. We need do more experiments with larger '--iterations-per-run'\r\n\r\n\r\n\r\nNOTE:\r\nIn torchbench.py I added the following code to do a few workaround:\r\n```\r\nfrom myscripts import workaround # TODO will remove this line before landing\r\n```\r\n\r\nHere are the content of workaround.py:\r\n```\r\nimport torch\r\nfrom torch import nn\r\n\r\n# override max_pool2d with avg_pool2d\r\ntorch.nn.MaxPool2d = torch.nn.AvgPool2d\r\n\r\n# remove the registration of native_batch_norm_decomposition\r\nbatch_norm_op = torch.ops.aten.native_batch_norm.default\r\ndel batch_norm_op.py_kernels[torch._C.DispatchKey.Autograd]\r\nbatch_norm_op._dispatch_cache.clear()\r\n\r\n# override dropout to be a Nop\r\nclass NopModule(nn.Module):\r\n    def __init__(self, p=0.5, inplace=False):\r\n        super().__init__()\r\n\r\n    def forward(self, x):\r\n        return x\r\n\r\ntorch.nn.Dropout = NopModule\r\n```\r\n\r\nIt work around a few issues we found\r\n1. MaxPool2d does not work for training in dynamo/torchxla: https://github.com/pytorch/torchdynamo/issues/1837 . WIP fix from Brian in https://github.com/pytorch/pytorch/pull/90226 , https://github.com/pytorch/xla/pull/4276/files (WIP)\r\n2. recent change ( this PR https://github.com/pytorch/pytorch/pull/88697 ) in op decomposition cause batch_norm ops to fallback in torchxla. Fix from jack in https://github.com/pytorch/xla/pull/4282#event-7969608134 . (confirmed the fix after adding Deduper to handle duplicated return from fx graph generated by AOTAutograd)\r\n3. we have issue to handle dropout because of random seed out of sync issue. Here is the fix: https://github.com/pytorch/xla/pull/4293 (confirmed the fix)\r\n\r\n\r\n", "label": [{"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 88439, "title": "Reenable optimizer overlap tests", "time": "2022-11-03T18:33:14Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #88439\r\n\r\nCloses https://github.com/pytorch/pytorch/issues/73259. Not sure the root cause but CI seems fine with these tests.", "label": [{"id": 2404913419, "node_id": "MDU6TGFiZWwyNDA0OTEzNDE5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Merged", "name": "Merged", "color": "ededed", "default": false, "description": null}, {"id": 2510927053, "node_id": "MDU6TGFiZWwyNTEwOTI3MDUz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Reverted", "name": "Reverted", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 88434, "title": "Use int64_t in size_to_dim_", "time": "2022-11-03T16:43:23Z", "body": "`size_to_dim_` unnecessarily uses `int32_t` as an argument type. `int64_t` would work just as well and not cause a narrowing conversion warning.\r\n\r\nTest Plan: Sandcastle\r\n\r\nDifferential Revision: D40967382\r\n\r\n\n\ncc @jbschlosser", "label": [{"id": 917150434, "node_id": "MDU6TGFiZWw5MTcxNTA0MzQ=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpp", "name": "module: cpp", "color": "f7e101", "default": false, "description": "Related to C++ API"}, {"id": 1304454372, "node_id": "MDU6TGFiZWwxMzA0NDU0Mzcy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20build%20warnings", "name": "module: build warnings", "color": "f7e101", "default": false, "description": "Related to warnings during build process"}, {"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 3769208226, "node_id": "LA_kwDOA-j9z87gqY2i", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20cpp", "name": "release notes: cpp", "color": "8250CF", "default": false, "description": "release notes category"}, {"id": 4183081985, "node_id": "MDU6TGFiZWw0MTgzMDgxOTg1", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20build", "name": "topic: build", "color": "0e8a16", "default": false, "description": null}]}
{"number": 88432, "title": "[FSDP] Default to `limit_all_gathers=True`", "time": "2022-11-03T16:27:05Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#88432 [FSDP] Default to `limit_all_gathers=True`**\n* #88431 [FSDP][Docs] Reword `sharding_strategy` docs and other minor doc changes\n* #88430 [FSDP][Docs] Simplify CPU offload docs\n* #88429 [FSDP][Docs] Simplify `mixed_precision` ctor docs\n* #88428 [FSDP] Default to `BACKWARD_PRE`\n* #88260 [FSDP()][Easy] Make `fully_shard()` only `FULL_SHARD`\n* #88235 [FSDP()] Have `fully_shard()` abide by `@contract`!\n* #88234 [FSDP()][Easy] Rename `_State` to `_FSDPState`\n* #88233 [FSDP()] Rename to `fully_shard()` and move to `_composable/`\n* #88232 [FSDP][Easy] Remove unneeded `TrainingState` transition\n* #88123 [FSDP] Rename `unflat_param_name` -> `fqn` for consistency\n* #88122 [FSDP] Simplify `_get_buffer_names()`\n* #88121 [FSDP] Remove unneeded `torch.no_grad()` context when offloading to CPU\n* #88120 [FSDP][Docs] Add note mentioning rate limiter for backward prefetch\n* #88040 [FSDP()][27/N] Add forward hook registration\n* #87941 [FSDP()][26/N] Move `_lazy_init()` into `_fsdp_root_pre_forward()`\n* #87940 [FSDP()][25/N] Add `_post_forward_reshard()`\n* #87939 [FSDP()][24/N] Refactor `_lazy_init()`\n* #87938 [FSDP()][23/N] Refactor handle attr initialization\n* #87935 [FSDP()][21/N] Refactor and fix `_cast_buffers()`\n* #87934 [FSDP] Rename `dtype` to `buffer_name_to_dtype`\n* #87933 [FSDP] Remove `device` arg from `_cast_buffers()`\n* #87932 [FSDP()][20/N][Easy] Move functions in file\n* #87931 [FSDP()][18/N] Refactor `pre_forward_unshard()`\n* #87930 [FSDP()][17/N] Refactor `_fsdp_root_pre_forward()`\n* #87929 [FSDP()][16/N] Refactor post-forward/pre-backward\n* #87928 [FSDP()][15/N] Refactor `_init_streams()`\n* #87927 [FSDP()][14/N] Refactor pre-forward/post-backward\n\n", "label": [{"id": 3769221312, "node_id": "LA_kwDOA-j9z87gqcDA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(fsdp)", "name": "release notes: distributed (fsdp)", "color": "C1F551", "default": false, "description": "release notes category"}, {"id": 3773061335, "node_id": "LA_kwDOA-j9z87g5FjX", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20improvements", "name": "topic: improvements", "color": "22499E", "default": false, "description": "topic category"}]}
{"number": 88427, "title": "Enable tracing of Vulkan models with the model tracer", "time": "2022-11-03T16:25:40Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #88427\n\nAdds `aten_vulkan` to the `torch_model_tracer` which allows Vulkan models to be traced.\n\nNote that there are some caveats, the\n\n```\nLD_LIBRARY_PATH=~/fbsource/third-party/swiftshader/lib/linux-x64/\n```\n\nenv var has to be set whenever a Vulkan model is traced. This is to ensure that the Vulkan drivers can be loaded.\n\nDifferential Revision: [D40775049](https://our.internmc.facebook.com/intern/diff/D40775049/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D40775049/)!", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 88421, "title": "Make avgpool2d output shape consistent", "time": "2022-11-03T14:09:06Z", "body": "Fixes #88144\r\n\r\nThe root-cause is that the original output shape calculation formula is mixed-up. Fundamentally, dilation is not relevant when in ceiling mode ---- no matter dilation param is 1 or more, the starting point of each pooling window is not impacted. The updated formula for ceiling mode only includes input_size, padding and stride, and then do the ceiling.\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 88418, "title": "Implement reshape_copy without is_contiguous test.", "time": "2022-11-03T12:57:31Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #88418\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>", "label": [{"id": 3769203231, "node_id": "LA_kwDOA-j9z87gqXof", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20composability", "name": "release notes: composability", "color": "A5282C", "default": false, "description": "release notes category"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 88411, "title": "fix order accumulation for parallel sum", "time": "2022-11-03T06:22:03Z", "body": "Fixes [#1774](https://github.com/pytorch/torchdynamo/issues/1774).\r\n\r\nSince `#pragma omp for reduction(+:var)` will not guarantee a fixed reduce order for accumulation. and different reduce order for accumulation will leads to different results. We can choose to achieve fixed order reduce sum without OMP native support.\r\n\r\nFor `test_dense_mask_index`, generated CPP codes with OMP interface:\r\n```cpp\r\nextern \"C\" void kernel(const float* __restrict__ in_ptr0,\r\n                       const float* __restrict__ in_ptr1,\r\n                       float* __restrict__ out_ptr0)\r\n{\r\n    {\r\n        {\r\n            float tmp3 = 0;\r\n            #pragma omp parallel num_threads(64)\r\n            {\r\n                #pragma omp for reduction(+:tmp3) \r\n                for(long i0=0; i0<102400; ++i0)\r\n                {\r\n                    {\r\n                        auto tmp0 = in_ptr0[i0];\r\n                        auto tmp1 = in_ptr1[2];\r\n                        auto tmp2 = tmp0 * tmp1;\r\n                        tmp3 += tmp2;\r\n                    }\r\n                }\r\n            }\r\n            out_ptr0[0] = tmp3;\r\n        }\r\n    }\r\n}\r\n```\r\nand without using OMP interface\r\n```cpp\r\nextern \"C\" void kernel(const float* __restrict__ in_ptr0,\r\n                       const float* __restrict__ in_ptr1,\r\n                       float* __restrict__ out_ptr0)\r\n{\r\n    {\r\n        {\r\n            float tmp3 = {0};\r\n            float tmp3_to_accumulate[64] = {0};\r\n            #pragma omp parallel num_threads(64)\r\n            {\r\n                #pragma omp for  \r\n                for(long i0=0; i0<102400; ++i0)\r\n                {\r\n                    {\r\n                        auto tmp0 = in_ptr0[i0];\r\n                        auto tmp1 = in_ptr1[2];\r\n                        auto tmp2 = tmp0 * tmp1;\r\n                        int tid = omp_get_thread_num();\r\n                        tmp3_to_accumulate[tid] += tmp2;\r\n                    }\r\n                }\r\n            }\r\n            for (int i = 0; i < 64; i++)\r\n                tmp3 += tmp3_to_accumulate[i];\r\n            out_ptr0[0] = tmp3;\r\n        }\r\n    }\r\n}\r\n```\r\n\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 88405, "title": "[ao] fixing private in quantization-support.rst", "time": "2022-11-03T04:56:05Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #88405\n* #88404\n* #88400\n* #88399\n* #88398\n* #88397\n* #88396\n* #88395\n* #88394\n* #88393\n* #88392\n* #88391\n* #87885\n* #87883\n* #87521\n* #87520\n* #87519\n* #87518\n* #87517\n* #87516\n* #87515\n\nSummary: some functions have been made private which wasn't reflexted in\nthe docs\n\nTest Plan: see CI\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\nDifferential Revision: [D41015546](https://our.internmc.facebook.com/intern/diff/D41015546)", "label": []}
{"number": 88404, "title": "[ao] removing private from allowlist", "time": "2022-11-03T04:51:52Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #88404\n* #88400\n* #88399\n* #88398\n* #88397\n* #88396\n* #88395\n* #88394\n* #88393\n* #88392\n* #88391\n* #87885\n* #87883\n* #87521\n\nSummary: removed functions like _add_observer_ from allowlist since\nthey've been made private\n\nTest Plan: python test/test_public_bindings.py\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\nDifferential Revision: [D41015541](https://our.internmc.facebook.com/intern/diff/D41015541)", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 88400, "title": "[ao][fx] fixing public v private utils.py", "time": "2022-11-03T04:29:59Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #88400\n* #88399\n* #88398\n* #88397\n* #88396\n* #88395\n* #88394\n* #88393\n* #88392\n* #88391\n* #87885\n* #87883\n* #87521\n\nSummary: made _all_node_args_except_first,\n_all_node_args_have_no_tensors,\n_assert_and_get_unique_device,\n_collect_producer_nodes,\n_create_getattr_from_value,\n_create_node_from_old_node_preserve_meta,\n_create_qparam_nodes,\n_EMPTY_ARG_DICT,\n_get_custom_module_class_keys,\n_get_linear_prepack_op_for_dtype,\n_get_new_attr_name_with_prefix,\n_get_non_observable_arg_indexes_and_types,\n_get_per_tensor_qparams,\n_get_qconv_op,\n_get_qconv_prepack_op,\n_get_quantize_node_info,\n_get_skipped_module_name_and_classes,\n_graph_module_from_producer_nodes,\n_is_get_tensor_info_node,\n_maybe_get_next_module,\n_node_return_type_is_int,\n_node_arg_is_bias,\n_node_arg_is_weight,\n_NON_OBSERVABLE_ARG_DICT,\n_NON_QUANTIZABLE_WEIGHT_OPS,\n_return_arg_list private\n\nTest Plan: python test/test_public_bindings.py\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\nDifferential Revision: [D41015539](https://our.internmc.facebook.com/intern/diff/D41015539)", "label": [{"id": 1031340614, "node_id": "MDU6TGFiZWwxMDMxMzQwNjE0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20quantization", "name": "oncall: quantization", "color": "f7e101", "default": false, "description": "Quantization support in PyTorch"}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}]}
{"number": 88399, "title": "[ao][fx] fixing public v private qconfig_mapping_utils.py", "time": "2022-11-03T04:29:55Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #88400\n* __->__ #88399\n* #88398\n* #88397\n* #88396\n* #88395\n* #88394\n* #88393\n* #88392\n* #88391\n* #87885\n* #87883\n* #87521\n\nSummary: made _check_is_valid_config_dict,\n_compare_prepare_convert_qconfig_mappings,\n_generate_node_name_to_qconfig,\n_is_qconfig_supported_by_dtype_configs,\n_maybe_adjust_qconfig_for_module_name_object_type_order,\n_update_qconfig_for_fusion private\n\nTest Plan: python test/test_public_bindings.py\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\nDifferential Revision: [D41015544](https://our.internmc.facebook.com/intern/diff/D41015544)", "label": [{"id": 1031340614, "node_id": "MDU6TGFiZWwxMDMxMzQwNjE0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20quantization", "name": "oncall: quantization", "color": "f7e101", "default": false, "description": "Quantization support in PyTorch"}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}]}
{"number": 88398, "title": "[ao][fx] fixing public v private prepare.py", "time": "2022-11-03T04:29:50Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #88400\n* #88399\n* __->__ #88398\n* #88397\n* #88396\n* #88395\n* #88394\n* #88393\n* #88392\n* #88391\n* #87885\n* #87883\n* #87521\n\nSummary: made _DO_NOT_OBS_DTYPE_LIST, _add_matched_node_name_to_set,\n_get_arg_target_is_dynamic_as_input_to_node, _get_arg_target_is_dynamic_as_input_to_node,\n_get_arg_target_dtype_as_input_to_node,\n_get_arg_target_dtype_as_output,\n_get_target_activation_dtype_for_node,\n_get_standalone_module_configs,\n_insert_observer,\n_is_activation_post_process_node,\n_is_input_arg_dtype_supported_by_backend,\n_is_observer_in_same_graph,\n_is_output_dtype_supported_by_backend,\n_maybe_insert_input_equalization_observers_for_node,\n_maybe_insert_input_observer_for_arg_or_kwarg,\n_maybe_insert_input_observers_for_node,\n_maybe_insert_observers_before_graph_output,\n_maybe_insert_output_observer_for_node,\n_maybe_make_input_output_share_observers,\n_maybe_propagate_dtype_for_node,\n_qat_swap_modules,\n_remove_output_observer,\n_run_prepare_fx_on_standalone_modules,\n_save_state,\n_swap_custom_module_to_observed private\n\nTest Plan: python test/test_public_bindings.py\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\nDifferential Revision: [D41015542](https://our.internmc.facebook.com/intern/diff/D41015542)\n\ncc @jerryzh168 @jianyuh @raghuramank100 @jamesr66a @vkuzo @jgong5 @Xia-Weiwen @leslie-fang-intel", "label": [{"id": 1031340614, "node_id": "MDU6TGFiZWwxMDMxMzQwNjE0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20quantization", "name": "oncall: quantization", "color": "f7e101", "default": false, "description": "Quantization support in PyTorch"}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}, {"id": 4726472156, "node_id": "LA_kwDOA-j9z88AAAABGbg93A", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20AO%20frontend", "name": "release notes: AO frontend", "color": "ededed", "default": false, "description": null}]}
{"number": 88397, "title": "[ao][fx] fixing public v private for pattern_utils.py", "time": "2022-11-03T04:29:45Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #88400\n* #88399\n* #88398\n* __->__ #88397\n* #88396\n* #88395\n* #88394\n* #88393\n* #88392\n* #88391\n* #87885\n* #87883\n* #87521\n\nSummary: made _DEFAULT_FUSION_PATTERNS,\n_register_fusion_pattern,\n_DEFAULT_QUANTIZATION_PATTERNS,\n_DEFAULT_OUTPUT_FAKE_QUANTIZE_MAP,\n_DEFAULT_OUTPUT_OBSERVER_MAP,\n_register_quant_pattern,\n_sorted_patterns_dict private\n\nTest Plan: python test/test_public_bindings.py\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\nDifferential Revision: [D41015537](https://our.internmc.facebook.com/intern/diff/D41015537)", "label": [{"id": 1031340614, "node_id": "MDU6TGFiZWwxMDMxMzQwNjE0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20quantization", "name": "oncall: quantization", "color": "f7e101", "default": false, "description": "Quantization support in PyTorch"}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}]}
{"number": 88396, "title": "[ao][fx] fixing public v private match_utils.py", "time": "2022-11-03T04:29:41Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #88400\n* #88399\n* #88398\n* #88397\n* __->__ #88396\n* #88395\n* #88394\n* #88393\n* #88392\n* #88391\n* #87885\n* #87883\n* #87521\n\nSummary: made _is_match, _find_matches, _MatchResult private also added\n__all__ to lower_to_qnnpack.py\n\nTest Plan: python test/test_public_bindings.py\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\nDifferential Revision: [D41015540](https://our.internmc.facebook.com/intern/diff/D41015540)", "label": [{"id": 1031340614, "node_id": "MDU6TGFiZWwxMDMxMzQwNjE0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20quantization", "name": "oncall: quantization", "color": "f7e101", "default": false, "description": "Quantization support in PyTorch"}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}]}
{"number": 88395, "title": "[ao][fx] fixing public v private graph_module.py", "time": "2022-11-03T04:29:36Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #88395\n* #88394\n* #88392\n* #88391\n\nSummary: made _is_observed_module, _is_observed_standalone_module\nprivate\n\nTest Plan: python test/test_public_bindings.py\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\nDifferential Revision: [D41015545](https://our.internmc.facebook.com/intern/diff/D41015545)", "label": [{"id": 1031340614, "node_id": "MDU6TGFiZWwxMDMxMzQwNjE0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20quantization", "name": "oncall: quantization", "color": "f7e101", "default": false, "description": "Quantization support in PyTorch"}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}]}
{"number": 88394, "title": "[ao][fx] public v private convert.py", "time": "2022-11-03T04:29:31Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #88395\n* __->__ #88394\n* #88392\n* #88391\n\nSummary: made _restore_state,\n_has_none_qconfig,\n_run_weight_observers,\n_maybe_recursive_remove_dequantize,\n_get_module_path_and_prefix,\n_insert_dequantize_node,\n_maybe_get_observer_for_node,\n_remove_previous_dequantize_in_custom_module private\n\nTest Plan: python test/test_public_bindings.py\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\nDifferential Revision: [D41015547](https://our.internmc.facebook.com/intern/diff/D41015547)\n\ncc @jerryzh168 @jianyuh @raghuramank100 @jamesr66a @vkuzo @jgong5 @Xia-Weiwen @leslie-fang-intel", "label": [{"id": 1031340614, "node_id": "MDU6TGFiZWwxMDMxMzQwNjE0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20quantization", "name": "oncall: quantization", "color": "f7e101", "default": false, "description": "Quantization support in PyTorch"}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}, {"id": 4726472156, "node_id": "LA_kwDOA-j9z88AAAABGbg93A", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20AO%20frontend", "name": "release notes: AO frontend", "color": "ededed", "default": false, "description": null}]}
{"number": 88393, "title": "[ao][fx] public v private for backend_config_utils.py", "time": "2022-11-03T04:29:27Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #88400\n* #88399\n* #88398\n* #88397\n* #88396\n* #88395\n* #88394\n* __->__ #88393\n* #88392\n* #88391\n* #87885\n* #87883\n* #87521\n\nSummary: made _get_quantize_handler_cls,\n_get_pattern_to_quantize_handlers,\n_get_fusion_pattern_to_fuse_handler_cls,\n_get_native_quant_patterns, private\n\nTest Plan: python test/test_public_bindings.py\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\nDifferential Revision: [D41015548](https://our.internmc.facebook.com/intern/diff/D41015548)", "label": [{"id": 1031340614, "node_id": "MDU6TGFiZWwxMDMxMzQwNjE0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20quantization", "name": "oncall: quantization", "color": "f7e101", "default": false, "description": "Quantization support in PyTorch"}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}]}
{"number": 88381, "title": "Dynamo traces through nn.modules (WIP)", "time": "2022-11-03T00:22:05Z", "body": "WIP just trying to get this to work.. don't freak out that i disabled some tests along the way, i'll come back for those\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 88366, "title": "zero dim support for any and all", "time": "2022-11-02T21:19:47Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #88366\n* #88280\n* #88279\n\n\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 88361, "title": "Preserve reshapes in AOTAutograd", "time": "2022-11-02T20:40:23Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #88361\n* #88063\n\nThis commit adds a configuration knob to AOTAutograd to make it\ntransform reshape calls into reshape_copy by default, making the\nreshape work even if the striding of tensors changes on subsequent\nruns.  This is not sound if the input/output get modified after this\nchange, so for safety, we use the new functionalization \"freeze storage\"\nfeature to detect if this case happened.\n\nTODO:\n- Plumb this as a configuration option so backends can pick what they\n  want\n- Figure out what the fallback strategy should be if the user actually\n  did mutate the input/output of reshape.  One possibility is to try\n  tracing again but this time without preserving reshapes.\n- Teach backends how to compile _reshape_copy\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>", "label": [{"id": 3773060564, "node_id": "LA_kwDOA-j9z87g5FXU", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20new%20feature", "name": "topic: new feature", "color": "C32883", "default": false, "description": "topic category"}, {"id": 4395386783, "node_id": "LA_kwDOA-j9z88AAAABBfxHnw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20functorch", "name": "release notes: functorch", "color": "957DAE", "default": false, "description": "release notes category"}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 88358, "title": "Test opinfo's aten op coverage", "time": "2022-11-02T20:33:16Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #88358\n\n", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 88330, "title": "Allow Process Group to support multiple backends and move PG specfic implementations to backend class", "time": "2022-11-02T16:50:41Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#88330 Allow Process Group to support multiple backends and move PG specfic implementations to backend class**\n* #89813 [21/N] Add alltoall_base custom op with CPU/CUDA implementations\n* #89505 [20/N] Add recv_any_source custom op with CPU/CUDA implementations\n\r\n# DRAFT PR\r\n\r\n### Changes\r\n\r\n#### c++ changes (ProcessGroup files, `Ops.cpp`, `init.cpp`)\r\n- Update pybind definitions for new process group base class and new backend class\r\n- Update pybinded backend class with collective definitions to keep BC with Python PG instances (e.g. `dist.ProcessGroupGloo`, `dist.ProcessGroupNCCL`) which are used in tests\r\n- Switch `ProcessGroupGloo`, `ProcessGroupNCCL`, `ProcessGroupMPI`, `ProcessGroupUCC` to derive from the `Backend` class.\r\n- Update CPU/CUDA `Ops.cpp` and `OpsImpl.cpp` to perform this dispatching by querying the backend using the device type\r\n- Update internal dispatched implementation of `barrier` to use a tensor which allows operation to be dispatched.\r\n- Update `allgather` collective to use `TensorList`. For some reason it was using the default implementation of `allgather` rather than dispatching it correctly. I still don't understand why and had originally filed an issue in 85122.\r\n\r\n#### python changes (`distributed_c10d.py`, test files)\r\n- Add BackendConfig class to specify the default configurations of backends and `get_backend_config()` API\r\n- `get_backend()` deprecation warning\r\n- `init_process_group` how returns a generic `ProcessGroup` object, it contains a list of backends (the ones stated above) which it will dispatch operations to.\r\n- `new_group` updated to return the same as above\r\n- Update `test_c10d_gloo.py`, Update `DistributedDataParallelTest` to use `init_process_group`, Update `ReducerTest`, update `test_broadcast_coalesced_gloo` to move from PG instance and gloo options\r\n- Update `test_c10d_nccl.py`, Update `DistributedDataParallelTest` to use `init_process_group`\r\n- Specific tests updated: `test_Backend_enum_class`\r\n\r\n### Changes missing\r\n- lazy initialization of backends\r\n- PG wrapper gaps\r\n- support parsing of BackendConfig\r\n\r\n### open questions\r\n- Pure Python PG extensions (https://github.com/pytorch/pytorch/pull/66338)\r\n- dealing with empty tensor list arguments (these are not dispatched)\r\n- confusion around `std::vector<at::Tensor>` and` std::vector<std::vector<at::Tensor>>` and why the arguments are not dispatched.\r\n- `recv` can be called with NoneType tensors\r\n\r\nThis basic script (using 2 backends within a process group) is working, running CI to understand gaps\r\n\r\n```python\r\n# python -m torch.distributed.run --nnodes=1 --nproc_per_node=2 basic_scenario.py\r\nimport torch.distributed as dist\r\nimport torch\r\nimport os\r\n\r\nif __name__ == \"__main__\":\r\n    rank = os.environ.get(\"RANK\")\r\n    # initialize with both gloo and nccl\r\n    dist.init_process_group()\r\n    # with gloo\r\n    dist.all_reduce(torch.tensor([1.0]))\r\n    print(f\"Rank {rank} finished\")\r\n    # with nccl\r\n    dist.all_reduce(torch.tensor([1.0], device=f\"cuda:{rank}\"))\r\n```\n\nDifferential Revision: [D41812672](https://our.internmc.facebook.com/intern/diff/D41812672)", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769218061, "node_id": "LA_kwDOA-j9z87gqbQN", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(c10d)", "name": "release notes: distributed (c10d)", "color": "EE86D4", "default": false, "description": "release notes category"}]}
{"number": 88303, "title": "functorch/README.md formatting fixup", "time": "2022-11-02T12:00:04Z", "body": null, "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 88302, "title": "Fix: Make `__len__` of datapipes dynamic", "time": "2022-11-02T10:15:01Z", "body": "Fixes #88074\r\n\r\nSeveral datapipes have their lengths cached on being executed for the first time. However, source datapipes might change in length (most prominently, whenever `apply_sharding` is called). The behaviour is counter-intuitive because we do not expect `__len__` to have side-effects.\r\n\r\nThis PR makes `__len__` dynamically computed.\r\n\r\nChanges:\r\n- Add note to the `datapipes` README that `__len__` should be dynamic and why.\r\n- Remove caching of length computations in `ConcaterIterDataPipe`, `MultiplexerIterDataPipe`, `ZipperIterDataPipe`, `BatcherIterDataPipe`, `ConcaterMapDataPipe`, and `BatcherMapDataPipe`.\r\n- This required removal of the `length` attribute in setstate/getstate of `MultiplexerIterDataPipe`. I am unsure whether to remove this completely and risk breaking saved checkpoints (as I did) or whether to just ignore the `length` of the loaded `state`.\r\n- This also means the classes above no longer have a `length` attribute. I have found no uses of this, though.\n\ncc @SsnL @VitalyFedyunin @ejguan @NivekT", "label": [{"id": 1076923878, "node_id": "MDU6TGFiZWwxMDc2OTIzODc4", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dataloader", "name": "module: dataloader", "color": "f7e101", "default": false, "description": "Related to torch.utils.data.DataLoader and Sampler"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3769215019, "node_id": "LA_kwDOA-j9z87gqagr", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20dataloader", "name": "release notes: dataloader", "color": "23169D", "default": false, "description": "release notes category"}, {"id": 3773061952, "node_id": "LA_kwDOA-j9z87g5FtA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20bug%20fixes", "name": "topic: bug fixes", "color": "A44870", "default": false, "description": "topic category"}]}
{"number": 88297, "title": "Upgrade CI to ROCm5.3", "time": "2022-11-02T06:56:19Z", "body": "cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport", "label": [{"id": 1078897659, "node_id": "MDU6TGFiZWwxMDc4ODk3NjU5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20rocm", "name": "module: rocm", "color": "f7e101", "default": false, "description": "AMD GPU support for Pytorch"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}, {"id": 4804263862, "node_id": "LA_kwDOA-j9z88AAAABHls_tg", "url": "https://api.github.com/repos/pytorch/pytorch/labels/rocm", "name": "rocm", "color": "B3FC7B", "default": false, "description": "This tag is for PRs from ROCm team"}, {"id": 4811640592, "node_id": "LA_kwDOA-j9z88AAAABHsvPEA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/rocm%20priority", "name": "rocm priority", "color": "bfdadc", "default": false, "description": "high priority ROCm PRs from performance or other aspects"}]}
{"number": 88287, "title": "[Caffe2] Suppress goto lints in clang-tidy", "time": "2022-11-02T03:38:00Z", "body": "Summary:\nIt seems that the gtest macro `EXPECT_THROW` uses `goto` internally, and clang-tidy isn't happy about that. This is quite noisy since we can't do much about gtest and some tests use `EXPECT_THROW` frequently.\n\nSo we just suppress these lints instead. It should be fine since people probably would not use `goto` anyway...\n\nTest Plan:\n```\narc lint -e extra --take CLANGTIDY caffe2/caffe2/fb/predictor/PyTorchPredictorContainerTest.cpp\n```\n\nnow no longer reports issues about `goto` usage in `EXPECT_THROW`.\n\nReviewed By: yinghai\n\nDifferential Revision: D40917713\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 88279, "title": "generate literals of the right type for triton", "time": "2022-11-02T02:33:31Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #88280\n* __->__ #88279\n\r\n\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 88262, "title": "Upgrade oneTBB to 2021.7.0", "time": "2022-11-01T23:02:44Z", "body": "Currently third_party/tbb uses an old `tbb_2018` branch by #20454. \r\n\r\nFixes https://github.com/pytorch/pytorch/issues/88265\r\n\r\ncc @VitalyFedyunin @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 88221, "title": "Add torch.tensor replacement and int_tensor prim", "time": "2022-11-01T18:22:59Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #88221\n\n", "label": []}
{"number": 88217, "title": "[WIP] Rand recomputation", "time": "2022-11-01T18:00:39Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #88217\r\n* #87641\r\n\r\n\r\n\r\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx\r\n\r\nStill need to add test. Fix for https://github.com/pytorch/torchdynamo/issues/1766\r\n\r\nNo cudagraphs, resnet18, before this pr:\r\n\r\ncuda train resnet18                           memory: eager: 0.55 GB, dynamo: 0.79 GB, ratio: 0.70\r\n1.082x p=0.00\r\n\r\nNo cudagraphs, resnet18, with this pr.\r\ncuda train resnet18                           memory: eager: 0.55 GB, dynamo: 0.71 GB, ratio: 0.78\r\n1.075x p=0.00\r\n\r\n\r\nThe memory savings are significant, but it does also introduce a slowdown. Thoughts ?\r\n\r\n\r\n\r\n", "label": [{"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 88212, "title": "Add API logging for interpolate()", "time": "2022-11-01T16:48:37Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #88212\n\r\n\r\nThis PR calls `torch._C._log_api_usage_once()` for `torch.nn.functional.interpolate()`. See [this internal post](https://fb.workplace.com/groups/1144215345733672/permalink/2382133531941841/) for more context about the level of details needed in the logger\r\n\n\ncc @albanD @mruberry @jbschlosser @walterddr @kshitij12345 @saketh-are", "label": [{"id": 1076922764, "node_id": "MDU6TGFiZWwxMDc2OTIyNzY0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20nn", "name": "module: nn", "color": "f7e101", "default": false, "description": "Related to torch.nn"}, {"id": 1300932205, "node_id": "MDU6TGFiZWwxMzAwOTMyMjA1", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20logging", "name": "module: logging", "color": "f7e101", "default": false, "description": "Features which make it easier to tell what PyTorch is doing under the hood"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 88203, "title": "Reserve space for std::vector output in extract_tensors for nccl python bindings", "time": "2022-11-01T16:00:52Z", "body": "Optimizes the nccl python bindings to reserve space when converting PythonObject* into Tensors. This should reduce the number of unnecessary allocations in the nccl bindings as the std::vector grows.\n\ncc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu", "label": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20distributed", "name": "oncall: distributed", "color": "f7e101", "default": false, "description": "Add this issue/PR to distributed oncall triage queue"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 88202, "title": "[ROCm] hipGraph support for pytorch mainline", "time": "2022-11-01T15:46:15Z", "body": "With the release of ROCm 5.3 hip now supports a hipGraph implementation.\r\n\r\nAll necessary backend work and hipification is done to support the same functionality as cudaGraph.\r\n\r\nUnit tests are modified to support a new TEST_GRAPH feature which allows us to create a single check for graph support instead of attempted to gather the CUDA level in annotations for every graph test\r\n\r\n\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport", "label": [{"id": 1078897659, "node_id": "MDU6TGFiZWwxMDc4ODk3NjU5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20rocm", "name": "module: rocm", "color": "f7e101", "default": false, "description": "AMD GPU support for Pytorch"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}, {"id": 4804263862, "node_id": "LA_kwDOA-j9z88AAAABHls_tg", "url": "https://api.github.com/repos/pytorch/pytorch/labels/rocm", "name": "rocm", "color": "B3FC7B", "default": false, "description": "This tag is for PRs from ROCm team"}, {"id": 4811640592, "node_id": "LA_kwDOA-j9z88AAAABHsvPEA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/rocm%20priority", "name": "rocm priority", "color": "bfdadc", "default": false, "description": "high priority ROCm PRs from performance or other aspects"}]}
{"number": 88197, "title": "misc symintifying fixes", "time": "2022-11-01T15:30:00Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #88198\n* __->__ #88197\n* #88230\n\n", "label": [{"id": 3769203231, "node_id": "LA_kwDOA-j9z87gqXof", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20composability", "name": "release notes: composability", "color": "A5282C", "default": false, "description": "release notes category"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 88196, "title": "cudagraphify Dynamo's nvFuser backend", "time": "2022-11-01T15:17:27Z", "body": "This PR rewrites the \"nvprims_nvfuser\" code to reuse `cudagraphify` function.\r\n\n\ncc @kevinstephano @jjsjann123 @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3997471357, "node_id": "LA_kwDOA-j9z87uRJJ9", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20nvfuser", "name": "module: nvfuser", "color": "9CA380", "default": false, "description": ""}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 88174, "title": "[WIP] Manual derivative for scaled_dot_product_attention", "time": "2022-11-01T04:28:36Z", "body": "Credit for this goes to @ngoyal2707!", "label": []}
{"number": 88173, "title": "[optim] remove .item calls in all optimizers", "time": "2022-11-01T04:25:40Z", "body": "Removes all `.item()` calls in all of `torch.optim`, but does not attempt to remove or optimize list comprehensions and other redundant-looking operations with `tensor` operations.\r\n\r\nFixes https://github.com/pytorch/torchdynamo/issues/1083\r\n\r\nWill update this PR after https://github.com/pytorch/pytorch/pull/88157 lands to remove the workarounds that account for not having that `addcdiv` overload.\n\ncc @mlazos @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 3769207236, "node_id": "LA_kwDOA-j9z87gqYnE", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20nn", "name": "release notes: nn", "color": "29E07F", "default": false, "description": "release notes category"}, {"id": 3773061335, "node_id": "LA_kwDOA-j9z87g5FjX", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20improvements", "name": "topic: improvements", "color": "22499E", "default": false, "description": "topic category"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}]}
{"number": 88170, "title": "[WIP] Remove some dead code", "time": "2022-11-01T03:50:53Z", "body": null, "label": []}
{"number": 88169, "title": "[WIP] Unifysdpnativemha1", "time": "2022-11-01T03:35:02Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": []}
{"number": 88146, "title": "[DO NOT REVIEW] ci test on build change", "time": "2022-10-31T22:20:00Z", "body": "Nothing in this PR Is final. This is merely a build refactor experiment.\n\ncc @EikanWang @jgong5", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2510754463, "node_id": "MDU6TGFiZWwyNTEwNzU0NDYz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/NNC", "name": "NNC", "color": "e5678d", "default": false, "description": ""}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 88124, "title": "[ao][docs] update qat snippet", "time": "2022-10-31T19:37:37Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #88124\r\n\r\nSummary: used fuse_modules_qat instead of fuse_modules\r\n\r\nsee https://github.com/pytorch/pytorch/issues/87679\r\n\r\nTest Plan: python test/test_quantization.py TestQuantizationDocs\r\n\r\nReviewers:\r\n\r\nSubscribers:\r\n\r\nTasks:\r\n\r\nTags:", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}, {"id": 3773063595, "node_id": "LA_kwDOA-j9z87g5GGr", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20documentation", "name": "topic: documentation", "color": "B2E89B", "default": false, "description": "topic category"}]}
{"number": 88110, "title": "Add test c10d ucc tests", "time": "2022-10-31T18:28:53Z", "body": "Creates the equivalent c10d test for ucc for https://github.com/pytorch/pytorch/blob/master/test/distributed/test_c10d_gloo.py and https://github.com/pytorch/pytorch/blob/master/test/distributed/test_c10d_nccl.py. Uses test_c10d_gloo.py as the reference and adds all the common ops. More detailed comparison of available ops here: https://docs.google.com/document/d/1yPsa_X9EiEiqo-j2Yn7ierhccBtEjwoqC-B7-amI0MI/edit?usp=sharing\r\n\r\nAlso removes extra line for ProcessGroupUCC.cpp barrier blocking wait that got duplicated from merging https://github.com/pytorch/pytorch/pull/85047.\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3769218061, "node_id": "LA_kwDOA-j9z87gqbQN", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(c10d)", "name": "release notes: distributed (c10d)", "color": "EE86D4", "default": false, "description": "release notes category"}]}
{"number": 88106, "title": "WIP: feat: LARS optimizer", "time": "2022-10-31T18:13:38Z", "body": "Followup to [#6323](https://github.com/pytorch/vision/issues/6323).\r\n\r\nAddition of LARS optimizer.\r\n\r\n- [ ] LARS optimizer\r\n- [ ] Tests\r\n- [ ] Documentation\r\n- [ ] Multi-Tensor support\r\n- [ ] Extra params (e.g., ~~maximize~~, ~~differentiable~~, foreach)\r\n- [ ] .pyi\r\n\r\nReference implementations: [[1](https://lightning-flash.readthedocs.io/en/0.5.0/_modules/flash/core/optimizers/lars.html#LARS)]", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 88078, "title": "Improve `bsr @ strided` performance in `baddmm` for `bfloat16/half` with Triton kernels.", "time": "2022-10-31T11:32:16Z", "body": "As per title.\r\n\r\nAdditionally we also introduce support for:\r\n- Rectangular block sizes which are powers of 2 and at least 16 (triton's `dot` limitation).\r\n- Batch support with broadcasting for either of the arguments.\r\n\r\n\r\ncc @VitalyFedyunin @ngimel @pearu @cpuhrsch @amjames @bhosmer", "label": [{"id": 679952992, "node_id": "MDU6TGFiZWw2Nzk5NTI5OTI=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20performance", "name": "module: performance", "color": "f7e101", "default": false, "description": "Issues related to performance, either of kernel code or framework glue"}, {"id": 679954154, "node_id": "MDU6TGFiZWw2Nzk5NTQxNTQ=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20sparse", "name": "module: sparse", "color": "f7e101", "default": false, "description": "Related to torch.sparse"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1492701675, "node_id": "MDU6TGFiZWwxNDkyNzAxNjc1", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20half", "name": "module: half", "color": "f7e101", "default": false, "description": "Related to float16 half-precision floats"}, {"id": 3769217376, "node_id": "LA_kwDOA-j9z87gqbFg", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20sparse", "name": "release notes: sparse", "color": "2A7626", "default": false, "description": "release notes category"}]}
{"number": 88064, "title": "add mixed data type support for LayerNorm backward on CPU", "time": "2022-10-31T03:03:16Z", "body": "### Motivation\r\nAmp provides convenience methods for mixed precision. If users use amp to run bfloat16 models, torch.autocast will keep module parameters in acc dtype which will leave gamma and beta in float while input/output will be in bfloat16. The same goes for backward: parameters are in float, and X & dX & dY are in bfloat16.\r\nMixed data type support for LayerNorm backward is also needed for model training with LayerNorm.\r\n\r\n### Testing\r\nSingle socket (icx, 32cores):\r\n| shape | fp32 forward (ms) | bf16 forward (ms) | mix forward (ms) | fp32 backward (ms) | bf16 backward (ms) | mix backward (ms) |\r\n| -- | -- | -- | -- | -- | -- | -- |\r\n| (1, 8, 16) | 0.012 | 0.012 | 0.012 | 0.071 | 0.065 | 0.062 |\r\n| (8, 8, 16) | 0.015 | 0.014 | 0.015 | 0.074 | 0.070 | 0.063 |\r\n| (32, 8, 16) | 0.062 | 0.016 | 0.016 | 0.073 | 0.073 | 0.072 |\r\n| (64, 128, 56, 56) | 2.467 | 0.907 | 0.0897 | 12.993 | 7.603 | 7.777 |\r\n| (64, 128, 256, 256) | 48.904 | 25.589 | 25.472 | 343.992 | 183.133 | 188.222 |\r\n\r\nSingle core(icx):\r\n| shape | fp32 forward (ms) | bf16 forward (ms) | mix forward (ms) | fp32 backward (ms) | bf16 backward (ms) | mix backward (ms) |\r\n| -- | -- | -- | -- | -- | -- | -- |\r\n| (1, 8, 16) | 0.012 | 0.012 | 0.012 | 0.050 | 0.050 | 0.050 |\r\n| (8, 8, 16) | 0.014 | 0.014 | 0.014 | 0.052 | 0.054 | 0.053 |\r\n| (32, 8, 16) | 0.034 | 0.019 | 0.018 | 0.059 | 0.067 | 0.066 |\r\n| (64, 128, 56, 56) | 66.791| 17.725 | 19.799 | 119.431 | 106.123 | 107.446 |\r\n| (64, 128, 256, 256) | 1542.477 | 402.132 | 527.044 | 3019.437 | 2336.318 | 2448.320 |\r\n\r\ncc @VitalyFedyunin @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3769207236, "node_id": "LA_kwDOA-j9z87gqYnE", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20nn", "name": "release notes: nn", "color": "29E07F", "default": false, "description": "release notes category"}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 88015, "title": "[mta][foreach] Implement fused adamw", "time": "2022-10-28T22:01:43Z", "body": "\r\nrelated: https://github.com/pytorch/pytorch/issues/68041, https://github.com/pytorch/pytorch/issues/71274, https://github.com/pytorch/pytorch/issues/80167\r\npossibly related to https://github.com/pytorch/pytorch/issues/80595#issuecomment-1178519436\r\n\r\ncc @ptrblck @ngimel ", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 87981, "title": "[WIP] pre dispatch mode", "time": "2022-10-28T13:46:57Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #87981\n\n", "label": [{"id": 3769203231, "node_id": "LA_kwDOA-j9z87gqXof", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20composability", "name": "release notes: composability", "color": "A5282C", "default": false, "description": "release notes category"}, {"id": 3773060564, "node_id": "LA_kwDOA-j9z87g5FXU", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20new%20feature", "name": "topic: new feature", "color": "C32883", "default": false, "description": "topic category"}]}
{"number": 87897, "title": "[pytree] add ability to have different pytree functions for grad-like transforms", "time": "2022-10-27T19:47:55Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #87796\n* __->__ #87897\n\n", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 87885, "title": "[ao] public v private stubs and utils", "time": "2022-10-27T18:08:40Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #87885\n* #87883\n* #87521\n\nSummary: from utils, made _module_type_list, _func_list, _method_list,\n_get_combined_dict, _is_per_tensor, _is_per_channel, _getattr_from_fqn,\n_to_underlying_dtype, _get_swapped_custom_module_class,\n_activation_dtype, _weight_dtype, _activation_is_statically_quantized,\n_activation_is_dynamically_quantized, _activation_is_int8_quantized,\n_activation_is_int32_quantized, _weight_is_quantized,\n_weight_is_statically_quantized, _op_is_int8_dynamically_quantized,\n_get_qconfig_dtypes, _get_quant_type, _check_min_max_valid,\n_calculate_qmin_qmax, _has_no_children_ignoring_parametrizations\nprivate.\n\nfrom stubs just added __all__\n\nTest Plan: python test/test_public_bindings.py\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\nDifferential Revision: [D40814549](https://our.internmc.facebook.com/intern/diff/D40814549)", "label": [{"id": 1031340614, "node_id": "MDU6TGFiZWwxMDMxMzQwNjE0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20quantization", "name": "oncall: quantization", "color": "f7e101", "default": false, "description": "Quantization support in PyTorch"}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}]}
{"number": 87884, "title": "[ao] fixing public v private for nn util functions", "time": "2022-10-27T18:08:35Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #87885\n* __->__ #87884\n* #87883\n* #87521\n* #87520\n* #87519\n* #87518\n* #87517\n* #87516\n* #87515\n\nSummary: clearing up TODOs\n\nTest Plan: python test/test_public_bindings.py\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\nDifferential Revision: [D40814547](https://our.internmc.facebook.com/intern/diff/D40814547)", "label": [{"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}]}
{"number": 87883, "title": "[ao] fixing public v private for torch.ao.nn.X", "time": "2022-10-27T18:08:30Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #87883\n* #87521\n\nSummary: this mostly consisted of adding __all__ to files without them.\nA few functions in X.utils were made private too\n\nTest Plan: python test/test_public_bindings.py\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\nDifferential Revision: [D40814548](https://our.internmc.facebook.com/intern/diff/D40814548)\n\ncc @jerryzh168 @jianyuh @raghuramank100 @jamesr66a @vkuzo @jgong5 @Xia-Weiwen @leslie-fang-intel", "label": [{"id": 1031340614, "node_id": "MDU6TGFiZWwxMDMxMzQwNjE0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20quantization", "name": "oncall: quantization", "color": "f7e101", "default": false, "description": "Quantization support in PyTorch"}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}, {"id": 4726472156, "node_id": "LA_kwDOA-j9z88AAAABGbg93A", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20AO%20frontend", "name": "release notes: AO frontend", "color": "ededed", "default": false, "description": null}]}
{"number": 87863, "title": "WIP Add uint8 support for interpolate on channels_last (1, 3, H,W) CPU images, mode=bilinear, antialias=True", "time": "2022-10-27T15:13:03Z", "body": "This is heavily adapted / ported from PILLOW-SIMD.\r\n\r\nThis PR adds support for `uint8` images in the following case:\r\n- mode=bilinear, `antialias=True` -- support for other modes is possible\r\n- shape=(1, 3, H, W)  -- can extend to batch_size > 1. Not sure about channels yet.\r\n- layout = channels_last  --- we could easily support contiguous as well, since we have to copy (pack / unpack) the input anyway\r\n- device=CPU\r\n\r\nThis may sound restrictive, but it's not. This is exactly the setting in which torchvision' `Resize()` is used for training jobs.\r\n\r\nThis is still WIP with lots of TODOs, but it seems to be working decently. On the inputs I tried and comparing with torchvision's `Resize()` (which first converts uint8 to floats, runs `interpolate()`, and converts back to uint8), I'm getting ~3-5X speedup. It seems correct so far as the absolute difference in the outputs is never `> 1`, and only ~15% of the pixel values differ (by exactly 1).\r\n\r\nThis addresses https://github.com/pytorch/vision/issues/2289\r\n\r\n@vfdev-5 @mingfeima @fmassa I'd love your initial thoughts on this!\n\ncc @VitalyFedyunin @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}]}
{"number": 87855, "title": "Deduplicate c10 error and PyTorchError hierarchy", "time": "2022-10-27T11:14:58Z", "body": "Fixes #53370\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769208226, "node_id": "LA_kwDOA-j9z87gqY2i", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20cpp", "name": "release notes: cpp", "color": "8250CF", "default": false, "description": "release notes category"}]}
{"number": 87848, "title": "Use c10::irange in many places", "time": "2022-10-27T05:56:51Z", "body": "Switches to using `c10::irange` for `const` safety and appropriate data type deduction.", "label": [{"id": 3768809762, "node_id": "LA_kwDOA-j9z87go3ki", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20mobile", "name": "release notes: mobile", "color": "bfdadc", "default": false, "description": "release notes category"}]}
{"number": 87810, "title": "[dynamo] Refactor DynamicShapeVariable to be a top level VariableTracker", "time": "2022-10-26T19:23:44Z", "body": "cc @jansel @mlazos @soumith @yanboliang @penguinwu @anijain2305", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 87802, "title": "Data loader initialization performance improvements", "time": "2022-10-26T17:37:52Z", "body": "This PR accomplishes two objectives.\r\n1. When a multi-process data loader is created, the dataset is pickled explicitly once before we start creating processes. (It will be pickled again once per thread in https://github.com/python/cpython/blob/3.7/Lib/multiprocessing/popen_forkserver.py#L46 or https://github.com/python/cpython/blob/3.7/Lib/multiprocessing/popen_spawn_posix.py#L46, but that would be much faster with a single 'bytes' object instead of a potentially complex nested Python data structure.)\r\n2. If we are starting multiple subprocesses, we use an intermediate thread pool. The objective here is to allow initial dataset uploads https://github.com/python/cpython/blob/3.7/Lib/multiprocessing/popen_spawn_posix.py#L62 to happen concurrently instead of sequentially.\r\n\r\nThe two changes combined significantly improve the time it takes to construct the DataLoader in the case of a large complex dataset and a large number of num_workers. (I have a real-world use case, with num_workers=32, and pickled dataset size 2 GB, where these two changes bring the construction time down from ~30 minutes to less than 2 minutes.)", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3769215019, "node_id": "LA_kwDOA-j9z87gqagr", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20dataloader", "name": "release notes: dataloader", "color": "23169D", "default": false, "description": "release notes category"}]}
{"number": 87796, "title": "[functorch] Modules as pytrees", "time": "2022-10-26T16:45:18Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #87796\n* #87897\n\r\nAdds support for Modules as pytrees, allowing users to pass modules to functorch transforms. This PR:\r\n- modules as pytrees are a funky subject outside of functorch because of restrictions on nn.Modules, so we're limiting it as an experimental feature in functorch for now until we get some user feedback. In order to use it, a user will use a context manager around the vmaps/grads/etc that are supporting modules as pytrees\r\n- Adds pytree versions for all relevant test (relevant == test uses make_functional and doesn't do ensembling since that hasn't been figured out quite yet)\r\n\r\n", "label": [{"id": 3773060564, "node_id": "LA_kwDOA-j9z87g5FXU", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20new%20feature", "name": "topic: new feature", "color": "C32883", "default": false, "description": "topic category"}, {"id": 4395386783, "node_id": "LA_kwDOA-j9z88AAAABBfxHnw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20functorch", "name": "release notes: functorch", "color": "957DAE", "default": false, "description": "release notes category"}]}
{"number": 87773, "title": "Support only GLog >=0.6.0", "time": "2022-10-26T05:34:34Z", "body": "Fixes #58054\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 87767, "title": "[primTorch] Enable regex error testing for `masked_fill` ref", "time": "2022-10-26T04:03:42Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* __->__ #87767\n* #87766\n* #87765\n\n\n\ncc @ezyang @mruberry @ngimel @Lezcano @fdrocha", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4081012228, "node_id": "LA_kwDOA-j9z87zP04E", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20primTorch", "name": "module: primTorch", "color": "455561", "default": false, "description": ""}]}
{"number": 87766, "title": "[primTorch] Enable regex error testing for `cat` and related refs", "time": "2022-10-26T04:03:36Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #87767\n* __->__ #87766\n* #87765\n\n\n\ncc @ezyang @mruberry @ngimel @Lezcano @fdrocha", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4081012228, "node_id": "LA_kwDOA-j9z87zP04E", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20primTorch", "name": "module: primTorch", "color": "455561", "default": false, "description": ""}]}
{"number": 87719, "title": "Removed self for bacwkard for max_pool2d_with_indices (#85485).", "time": "2022-10-25T19:03:39Z", "body": "Fixes #85485.\r\n\r\nNot fully completed:\r\n- input.names() should be passed to the backward function too\r\n- changes in MPS untested and probably not fully correct\r\n\r\nPasses CPU/CUDA tests locally, more precisely `pytest test/test_ops_gradients.py -vk max_pool2d` reports 24 passed and 24 skipped tests.\n\ncc @jansel @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 4164781683, "node_id": "LA_kwDOA-j9z874PYZz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20mps", "name": "release notes: mps", "color": "1d76db", "default": false, "description": "Release notes category"}, {"id": 4417837777, "node_id": "LA_kwDOA-j9z88AAAABB1La0Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/mps", "name": "ciflow/mps", "color": "02EDA7", "default": false, "description": "Run MPS tests (subset of trunk)"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 87716, "title": "add torch deploy compatibility tests", "time": "2022-10-25T18:38:20Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #87716\n\n", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 87715, "title": "Change generated Triton kernel prefix to 'triton_kernel' to make it more distinguishable", "time": "2022-10-25T18:17:16Z", "body": "Current prefix is `kernel_` which is too generic to assert that it exists in the profiler trace. Changing this prefix to something more distinguishable.\r\n\n\ncc @jansel @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 87709, "title": "Pass in Storage Offset For Input Tensors", "time": "2022-10-25T17:23:27Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #87709\n\r\nFix for https://github.com/pytorch/torchdynamo/issues/1734\n\ncc @jansel @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 87689, "title": "[MPS] Add support for aten::nansum for MPS backend", "time": "2022-10-25T12:44:18Z", "body": "`nansum` now supported on mps devices\r\n\r\nFixes #86809 \r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 4164781683, "node_id": "LA_kwDOA-j9z874PYZz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20mps", "name": "release notes: mps", "color": "1d76db", "default": false, "description": "Release notes category"}, {"id": 4417837777, "node_id": "LA_kwDOA-j9z88AAAABB1La0Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/mps", "name": "ciflow/mps", "color": "02EDA7", "default": false, "description": "Run MPS tests (subset of trunk)"}]}
{"number": 87687, "title": "Skip ao_sparsity TestComposability for missing FBGEMM", "time": "2022-10-25T10:55:36Z", "body": "Those tests (from test_ao_sparsity) require FBGEMM which may not be available. So add the skip decorator.\r\n\r\nFixes #87364\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 87683, "title": "error: \"caffe2::cuda\" is ambiguous", "time": "2022-10-25T09:08:15Z", "body": "Fixes #87681\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 87678, "title": "check cuda conv binary model accuracy", "time": "2022-10-25T07:21:39Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #87678\n\n\n\ncc @jansel @lezcano @fdrocha @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}]}
{"number": 87662, "title": "Consistent meta reg across dispatcher, py-dispatcher, active_meta_table", "time": "2022-10-25T02:37:07Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #87662\n\n", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 87656, "title": "Run static_runtime_test under TSAN mode", "time": "2022-10-25T00:42:27Z", "body": "This should prevent https://github.com/pytorch/pytorch/pull/74869 which broke the internal `static_runtime_cpptest` under tsan mode from happening.\r\n\r\nI also remove some deprecated xenial v.s. focal check, now that we have asan, tsan, and whatnot running in the newer focal.", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4614675625, "node_id": "LA_kwDOA-j9z88AAAABEw5cqQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/test-config/tsan", "name": "test-config/tsan", "color": "16922B", "default": false, "description": ""}]}
{"number": 87646, "title": "fix as_strided_scatter_backward", "time": "2022-10-24T22:44:32Z", "body": "as_strided_scatter's derivative formula was broken - instead of making a \"mask\" of 1's and 0's, it would effectively make a mask of 1's and uninitialized memory.\r\n\r\nFixes https://github.com/pytorch/pytorch/issues/88105\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #88198\n* #88230\n* #88197\n* #87647\n* __->__ #87646\n* #87255\n\r\n\r\n\r\ncc @albanD", "label": [{"id": 2510927053, "node_id": "MDU6TGFiZWwyNTEwOTI3MDUz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Reverted", "name": "Reverted", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769210091, "node_id": "LA_kwDOA-j9z87gqZTr", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20python_frontend", "name": "release notes: python_frontend", "color": "87E01C", "default": false, "description": "release notes category"}, {"id": 3773061952, "node_id": "LA_kwDOA-j9z87g5FtA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20bug%20fixes", "name": "topic: bug fixes", "color": "A44870", "default": false, "description": "topic category"}]}
{"number": 87631, "title": "TEST", "time": "2022-10-24T20:30:24Z", "body": "TEST\r\n", "label": [{"id": 3618084153, "node_id": "LA_kwDOA-j9z87Xp5U5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/binaries", "name": "ciflow/binaries", "color": "245567", "default": false, "description": "Trigger all binary build and upload jobs on the PR"}]}
{"number": 87623, "title": "Add support for aten::erfinv.out for MPS backend", "time": "2022-10-24T19:11:19Z", "body": "Fixes #86808\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 4164781683, "node_id": "LA_kwDOA-j9z874PYZz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20mps", "name": "release notes: mps", "color": "1d76db", "default": false, "description": "Release notes category"}, {"id": 4417837777, "node_id": "LA_kwDOA-j9z88AAAABB1La0Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/mps", "name": "ciflow/mps", "color": "02EDA7", "default": false, "description": "Run MPS tests (subset of trunk)"}]}
{"number": 87593, "title": "Avoid cuda stubs libraries being RPATHed", "time": "2022-10-24T11:33:45Z", "body": "This is a continuation of #68912 from @casparvl\r\n\r\nIn short I put the code from @casparvl into a function to not duplicate it. The new function `link_cuda_libraries` is a drop-in replacement for `target_link_libraries` which calls the latter and does the check and manual-rpath-setting on the libraries.\r\nI also made it to work on CMake prior to 3.20 by using `get_filename_component` instead of `cmake_path`\r\n\r\nFixes #35418\r\n\r\n@kumpera in the other PR you mentioned that some changes should/need to be split off. How would this need to look like? The changes depend on each other: The 3 CMakeLists.txt require the new module/file in the `cmake` folder and the `test_torch.py` will fail without those changes.", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 87586, "title": "optimize gather performance for gnn usage on CPU", "time": "2022-10-24T04:52:42Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #83727\n* __->__ #87586\n* #82703\n\r\nOn classic pyg user case for message passing, `gather` has `index` tensor in a broadcasted shape, e.g. with shape `5000, 128` and stride `[1, 0]`. That indicated gather is done on each row of the self tensor. The current implementation will try to parallel on the inner dimension which is bad performance for CPU and unable to be vectorized. \r\n\r\nThis PR addressed this use case and optimize in a similar manner to index_select, parallel on outer dimension of `index` and do vectorized copy on inner dimension.\r\n\r\nPerformance benchmarking on Xeon Icelake single socket on `GCN`: the `gather` reduced from `150.787ms` to `10.926ms`, after this optimization, `gather` will no longer be the major bottleneck for training of GNN models when `EdgeIndex` is in COO format.\r\n\r\nfor more details, please refer to https://github.com/pyg-team/pytorch_geometric/issues/4891#issuecomment-1288423705\n\ncc @VitalyFedyunin @jgong5 @XiaobingSuper @sanchitintel @ashokei @jingxu10", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3200025338, "node_id": "MDU6TGFiZWwzMjAwMDI1MzM4", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel%20priority", "name": "intel priority", "color": "4E4EF8", "default": false, "description": "matters to intel architecture from performance wise"}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}, {"id": 4718445952, "node_id": "LA_kwDOA-j9z88AAAABGT3FgA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20gnn", "name": "release notes: gnn", "color": "AC65AD", "default": false, "description": "gnn related optimizations"}]}
{"number": 87582, "title": "Add support for aten::remainder.Tensor_out for MPS backend", "time": "2022-10-24T01:54:59Z", "body": "Fixes #86806", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 4164781683, "node_id": "LA_kwDOA-j9z874PYZz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20mps", "name": "release notes: mps", "color": "1d76db", "default": false, "description": "Release notes category"}, {"id": 4417837777, "node_id": "LA_kwDOA-j9z88AAAABB1La0Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/mps", "name": "ciflow/mps", "color": "02EDA7", "default": false, "description": "Run MPS tests (subset of trunk)"}]}
{"number": 87578, "title": "[pytorch] add specialization for fmsub/float16", "time": "2022-10-23T17:35:51Z", "body": "Differential Revision: D40623912\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}]}
{"number": 87572, "title": "MPS: Argsort op", "time": "2022-10-23T09:07:14Z", "body": "Fixes #86991\r\n\r\nI did the things below following [this explanation](https://github.com/pytorch/pytorch/issues/86991#issuecomment-1284330692)\r\n\r\n1. Register `argsort_stable_mps` in native_functions.yaml\r\n2. Create a new file Sorting.mm\r\n3. Add support for `argsort_stable_mps` in Sorting.mm\r\n4. Add a test to test_mps.py\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 4164781683, "node_id": "LA_kwDOA-j9z874PYZz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20mps", "name": "release notes: mps", "color": "1d76db", "default": false, "description": "Release notes category"}, {"id": 4417837777, "node_id": "LA_kwDOA-j9z88AAAABB1La0Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/mps", "name": "ciflow/mps", "color": "02EDA7", "default": false, "description": "Run MPS tests (subset of trunk)"}]}
{"number": 87562, "title": "Implement `torch._foreach_lerp`", "time": "2022-10-23T01:06:07Z", "body": "As per title.\r\n\r\n- [ ] ~~Q: Do we want `torch._foreach_lerp.ScalarList` as well?~~\r\n- [ ] ~~we might want to have `ATen/native/cuda/lerp.cuh` and include it in `ATen/native/cuda/Lerp.cu` and `ATen/native/cuda/ForeachTernaryOp.cu`~~\r\n\r\nRelated:\r\n- https://github.com/pytorch/pytorch/issues/58833\r\n- https://github.com/pytorch/pytorch/issues/71683\r\n\r\ncc @vadimkantorov @ptrblck ", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3769214458, "node_id": "LA_kwDOA-j9z87gqaX6", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20foreach_frontend", "name": "release notes: foreach_frontend", "color": "96F26C", "default": false, "description": "release notes category"}]}
{"number": 87547, "title": "[nn] fix: full_backward_hook called for Module returning dict, list, etc", "time": "2022-10-22T10:08:45Z", "body": "Fixes https://github.com/pytorch/pytorch/issues/87540\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3769207236, "node_id": "LA_kwDOA-j9z87gqYnE", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20nn", "name": "release notes: nn", "color": "29E07F", "default": false, "description": "release notes category"}]}
{"number": 87529, "title": "Address a TODO in MPS testing now that it's possible", "time": "2022-10-22T00:22:57Z", "body": null, "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4417837777, "node_id": "LA_kwDOA-j9z88AAAABB1La0Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/mps", "name": "ciflow/mps", "color": "02EDA7", "default": false, "description": "Run MPS tests (subset of trunk)"}]}
{"number": 87521, "title": "[ao] quantize.py fixing public v private", "time": "2022-10-21T22:07:16Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #87883\n* __->__ #87521\n\nSummary: made _register_activation_post_process_hook, _add_observer,\n_get_unique_devices_, _get_observer_dict private\n\nTest Plan: python test/test_public_bindings.py\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\nDifferential Revision: [D40709277](https://our.internmc.facebook.com/intern/diff/D40709277)", "label": [{"id": 1031340614, "node_id": "MDU6TGFiZWwxMDMxMzQwNjE0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20quantization", "name": "oncall: quantization", "color": "f7e101", "default": false, "description": "Quantization support in PyTorch"}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}]}
{"number": 87492, "title": "[reland][dynamo] use optimizers correctly in benchmarking", "time": "2022-10-21T18:30:00Z", "body": "Reland https://github.com/pytorch/pytorch/pull/87311\r\n\r\nmlazos: updated to use SGD to not add a bunch of additional memory allocations (like Adam)\r\n\r\ncc @jansel @lezcano @fdrocha @mlazos @soumith @voznesenskym @yanboliang", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 87483, "title": "run test_decomp in serial", "time": "2022-10-21T17:16:53Z", "body": "it leads to a lot of cuda mem leaks\r\n\r\ntragically, it takes 60 minutes on windows...", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 87444, "title": "aten.squeeze.default and aten.squeeze.dim decomp", "time": "2022-10-21T06:28:55Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": []}
{"number": 87418, "title": "[WIP] Dynamo traces through nn.modules", "time": "2022-10-20T22:35:56Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #87418\n\nThis causes dynamo to inline the code for nn.module.__call__ rather than\nrecording call_module nodes in the graph it will then hand to AOT to trace.\n\nNote: AOT would trace through __call__ anyway, so this has no effect on the IR level\nfor downstream compilation/optimization.  Instead, it could cause modules that could\nbe traced wholesale into a dynamo graph to now cause graphbreaks, if they contain\nunsupported behavior inside.\n\nWe would probably need to support an escape list of nn.modules that need to be captured\nas nodes before we could land this, pending more investigation.\n\nThis change makes nn.module tracing match how user modules get traced, so in some ways\nit's arguably a better policy.\n\ncc @jansel @lezcano @fdrocha @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}, {"id": 4705034454, "node_id": "LA_kwDOA-j9z88AAAABGHEg1g", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20dynamo", "name": "release notes: dynamo", "color": "b60205", "default": false, "description": ""}]}
{"number": 87414, "title": "[Vulkan] implement abs", "time": "2022-10-20T22:30:45Z", "body": null, "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3769206412, "node_id": "LA_kwDOA-j9z87gqYaM", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20vulkan", "name": "release notes: vulkan", "color": "23DD30", "default": false, "description": "release notes category"}]}
{"number": 87362, "title": "Add Decomposition for aten::mult.left_t", "time": "2022-10-20T13:55:57Z", "body": "Adding support for aten::mult.left_t, if any changes need to be made, let me know.", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 87349, "title": "Adding QuantizedCPU for zero_ operation", "time": "2022-10-20T05:15:54Z", "body": "Test Plan: buck test caffe2/test:quantization -- test_qzero_\n\nDifferential Revision: D40502571\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}]}
{"number": 87341, "title": "[WIP] shared memory no pooling", "time": "2022-10-20T01:00:01Z", "body": "This version of cudagraphs which uses one stroage for each set of inputs fails for the following repro https://gist.github.com/eellison/d39f28791f7faa9ff1e4f7b5f6575ba0 and I'm really sure why.\r\n", "label": [{"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}]}
{"number": 87334, "title": "[dont-land][dynamo][fsdp] reset flat params during dynamo bytecode tracing", "time": "2022-10-20T00:00:26Z", "body": "cc @jansel @lezcano @fdrocha @wconstab this should unblock for the flat params error\r\n\r\nas title, this is hack and not suitable for landing.", "label": [{"id": 3769221312, "node_id": "LA_kwDOA-j9z87gqcDA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(fsdp)", "name": "release notes: distributed (fsdp)", "color": "C1F551", "default": false, "description": "release notes category"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}]}
{"number": 87315, "title": "[PyTorch Edge] Set training for module only", "time": "2022-10-19T20:08:57Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #87315\n\nUpdate previous recursive logic.\n\nContinue setting training attribute only if the slot is an object and a module.\n\nFor the corresponding JIT module, they get the module list first and set module one by one. there is method to get all modules iteratively, instead of recursively.\n\nThis change patch one fix to set training attribute for `model_f269583363.ptl`. Another patch is needed, because current lite interpreter doesn't have the correct type when loading object with setstate.\n\nDifferential Revision: [D29310952](https://our.internmc.facebook.com/intern/diff/D29310952/)", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768809762, "node_id": "LA_kwDOA-j9z87go3ki", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20mobile", "name": "release notes: mobile", "color": "bfdadc", "default": false, "description": "release notes category"}]}
{"number": 87310, "title": "[WIP] fix abs meta", "time": "2022-10-19T18:53:50Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #87310\n\n", "label": []}
{"number": 87284, "title": "Re-enabled 2 Metaprogramming tests on Windows", "time": "2022-10-19T13:22:43Z", "body": "With C++17 these tests are not failing\r\n\r\nFixes #25161\r\n\r\nDepends on https://github.com/pytorch/pytorch/pull/85969\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 87275, "title": "Added return code to the method compute_q8gemm_prepacked_sparse_dq", "time": "2022-10-19T08:59:33Z", "body": "Summary:\nChanged the return type of the method to return pytorch_qnnp_status.\nIn case of failure, replaced the assert(false) with return pytorch_qnnp_status_invalid_parameter.\nand returns pytorch_qnnp_status_success in all other 'happy' scenarios.\n\nTest Plan:\nbuck2 test //caffe2/test:ao -- TestQlinearPackedParams\n\nhttps://pxl.cl/2h9ks\n\nReviewed By: kimishpatel\n\nDifferential Revision: D40473999\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}]}
{"number": 87270, "title": "disable zlib-style API for miniz", "time": "2022-10-19T04:56:13Z", "body": "Summary: zlib-style API seems not used. Disable it to save startup size\n\nTest Plan:\nbuck test xplat/feed_ads/fam_fl_predictor:fam_fl_predictor_test\n\nsandcastle\n\nDifferential Revision: D40501784\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 87269, "title": "Link to torchbench userbenchmark frontend", "time": "2022-10-19T04:01:39Z", "body": "RUN_TORCHBENCH: nvfuser\r\n\r\nFixes #ISSUE_NUMBER\r\n", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 87259, "title": "Reopen inductor ut test_dense_mask_index_cpu and test_expanded_reduction_cpu", "time": "2022-10-19T02:49:30Z", "body": "Solve issue https://github.com/pytorch/torchdynamo/issues/1697.\r\nReopen \r\n```\r\ntest_dense_mask_index_cpu (__main__.CpuTests)\r\ntest_expanded_reduction_cpu (__main__.CpuTests)\r\n```\r\n\r\nThe failures for these two UTs is caused by different reduce order for \"sum\".\r\n\r\nFor example, below code will get difference result for difference order of sum:\r\n```python\r\nimport torch\r\nsize = 100000\r\na  = torch.randn(size)\r\nb = torch.randn(3)\r\nfor i in range(10):\r\n    c = torch.zeros((1, ))\r\n    order = torch.randperm(size)\r\n    for j in range(size):\r\n        c += (a[order[j]] * b[2])\r\n    print(c.item())\r\n```\r\noutput:\r\n```\r\n1.209582805633545\r\n1.2103385925292969\r\n1.2097705602645874\r\n1.2099699974060059\r\n1.2103685140609741\r\n1.2103989124298096\r\n1.210781216621399\r\n1.210976243019104\r\n1.2101386785507202\r\n1.2098314762115479\r\n```\r\n\r\n\r\nThe generated CPP code cannot guarantee a fixed reduce order for \"sum\" with\r\n```\r\n #pragma omp for reduction(+:var)\r\n```\r\nor guarantee exactly same reduce order for aten::sum (which is the reference to compare in UT).\r\n\r\nWe can increase the tolerance here.\r\n\r\n\r\n\r\n\r\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @jansel @lezcano @fdrocha", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}]}
{"number": 87230, "title": "Add symint support for select, unfold, unfold_backward", "time": "2022-10-18T18:45:03Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #87230\n\n", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 87217, "title": "[inductor] Run opinfo with all samples", "time": "2022-10-18T17:13:01Z", "body": "Fixes #ISSUE_NUMBER\r\n\n\ncc @jansel @mlazos @soumith @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @lezcano @fdrocha", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}]}
{"number": 87183, "title": "Assorted decomposition fixes", "time": "2022-10-18T08:09:54Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #87182\n* __->__ #87183\n* #87205\n* #87204\n* #87203\n\n\n\ncc @VitalyFedyunin @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @ezyang @mruberry @ngimel @Lezcano @fdrocha @peterbell10 @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @Guobing-Chen @chunyuan-w @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773061952, "node_id": "LA_kwDOA-j9z87g5FtA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20bug%20fixes", "name": "topic: bug fixes", "color": "A44870", "default": false, "description": "topic category"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4081012228, "node_id": "LA_kwDOA-j9z87zP04E", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20primTorch", "name": "module: primTorch", "color": "455561", "default": false, "description": ""}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 87182, "title": "Add a test for decompositions that decomposes all the operations as much as possible", "time": "2022-10-18T08:09:49Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #87182\n* #87183\n* #87205\n* #87204\n* #87203\n\r\nThis will enable a more thorough testing of the decompositions than the\r\none just provided by OpInfos.\n\ncc @ezyang @mruberry @ngimel @Lezcano @fdrocha @peterbell10", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2484356711, "node_id": "MDU6TGFiZWwyNDg0MzU2NzEx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20testing", "name": "module: testing", "color": "fbca04", "default": false, "description": "Issues related to the torch.testing module (not tests)"}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}, {"id": 4081012228, "node_id": "LA_kwDOA-j9z87zP04E", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20primTorch", "name": "module: primTorch", "color": "455561", "default": false, "description": ""}]}
{"number": 87176, "title": "Update decomp for upsample_bilinear2d_vec", "time": "2022-10-18T07:07:33Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #87176\r\n\r\nThis new implementation fuses multiple index operations into one. ", "label": []}
{"number": 87151, "title": "improving packaging error messaging", "time": "2022-10-17T23:36:16Z", "body": "Summary: Packaging error are hard to debug but this show a path from the root module to the offending import\n\nTest Plan: N2650406\n\nDifferential Revision: D40390262\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 3769202179, "node_id": "LA_kwDOA-j9z87gqXYD", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20package/deploy", "name": "release notes: package/deploy", "color": "EC030C", "default": false, "description": "release notes category"}]}
{"number": 87120, "title": "[autoreload][rfc] pybind fixes to prevent reinitialization of extensions", "time": "2022-10-17T19:42:47Z", "body": "Summary: This is a Meta-internal thing that is a no-op for external users.\n\nTest Plan: Let the Github actions run.\n\nDifferential Revision: D40371442\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 3769218887, "node_id": "LA_kwDOA-j9z87gqbdH", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(rpc)", "name": "release notes: distributed (rpc)", "color": "E308B2", "default": false, "description": "release notes category"}]}
{"number": 87116, "title": "[PrimTorch] Implement batch_norm and instance_norm", "time": "2022-10-17T19:17:06Z", "body": "Add `batch_norm` and `instance_norm` references\r\nSplit from https://github.com/pytorch/pytorch/pull/81191", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 87115, "title": "Set maximum 1536 threads per SM on sm_87 and sm_89", "time": "2022-10-17T19:16:36Z", "body": "When compiling pytorch with CUDA 11.8 and sm_89 support, I received numerous ptxas warnings, such as\r\n\r\n```\r\nptxas warning : Value of threads per SM for entry _ZN2at6native38_GLOBAL__N__7d852361_6_RNN_cu_08bf8c056kernel17lstm_cell_forwardIffiLi2EEEvNS_4cuda6detail10TensorInfoIT_T1_EES9_S9_S9_S9_S9_S9_S9_S8_S8_ is out of range. .minnctapersm will be ignored\r\n```\r\n\r\nPer the [CUDA docs](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications), capability 8.7 should also have a maximum of 1536 threads per SM. Capability 8.9 is conspicuously missing from this list, but running deviceQuery from the CUDA samples shows it to also be 1536.\r\n\r\n```\r\nDevice 0: \"NVIDIA GeForce RTX 4090\"\r\n  CUDA Driver Version / Runtime Version          11.8 / 11.8\r\n  CUDA Capability Major/Minor version number:    8.9\r\n  Total amount of global memory:                 24256 MBytes (25433735168 bytes)\r\nMapSMtoCores for SM 8.9 is undefined.  Default to use 128 Cores/SM\r\nMapSMtoCores for SM 8.9 is undefined.  Default to use 128 Cores/SM\r\n  (128) Multiprocessors, (128) CUDA Cores/MP:    16384 CUDA Cores\r\n  GPU Max Clock rate:                            2535 MHz (2.54 GHz)\r\n  Memory Clock rate:                             10501 Mhz\r\n  Memory Bus Width:                              384-bit\r\n  L2 Cache Size:                                 75497472 bytes\r\n  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\r\n  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\r\n  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\r\n  Total amount of constant memory:               65536 bytes\r\n  Total amount of shared memory per block:       49152 bytes\r\n  Total shared memory per multiprocessor:        102400 bytes\r\n  Total number of registers available per block: 65536\r\n  Warp size:                                     32\r\n  Maximum number of threads per multiprocessor:  1536\r\n  Maximum number of threads per block:           1024\r\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\r\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\r\n  Maximum memory pitch:                          2147483647 bytes\r\n  Texture alignment:                             512 bytes\r\n  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)\r\n  Run time limit on kernels:                     Yes\r\n  Integrated GPU sharing Host Memory:            No\r\n  Support host page-locked memory mapping:       Yes\r\n  Alignment requirement for Surfaces:            Yes\r\n  Device has ECC support:                        Disabled\r\n  Device supports Unified Addressing (UVA):      Yes\r\n  Device supports Managed Memory:                Yes\r\n  Device supports Compute Preemption:            Yes\r\n  Supports Cooperative Kernel Launch:            Yes\r\n  Supports MultiDevice Co-op Kernel Launch:      Yes\r\n\r\n```", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}]}
{"number": 87083, "title": "Use Fake Tensor For Inductor Shape Prop", "time": "2022-10-17T15:48:31Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}]}
{"number": 87058, "title": "Allow inputs to be complex view tensors for Inductor CUDA graph", "time": "2022-10-17T03:57:18Z", "body": "Fixes https://github.com/pytorch/torchdynamo/issues/1620.\r\n\n\ncc @jansel @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305", "label": [{"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 87057, "title": "Optimize find_split_dim for reduction", "time": "2022-10-17T02:49:14Z", "body": "Add the check if the rounded columns of multiples of 128 bytes are enough to be parallelized when the selected dim is the dim with contiguous stride. \n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 87053, "title": "optimize upsample bilinear perf for NCHW memory format on CPU", "time": "2022-10-17T01:48:44Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* __->__ #87053\r\n\r\n\r\n### Addressing `upsample` or `interpolate` performance from https://github.com/pytorch/vision/issues/6619\r\n\r\nThis PR is targeting at improving `upsample` on mode `bilinear` with `antialias` turned on for NCHW, which is commonly used at resizing input images with C=1 or C=3.\r\n\r\n* `bilinear` with antialias off won't be affected (this is not requested from https://github.com/pytorch/vision/issues/6619 so I did not optimize it, actually antialias off would be easier to implement).\r\n* parallel on dimensions of `{n, c, h}` and vectorize on `{w}`.\r\n* pre-calculate `idx_min`, `idx_size` and `weights`: we can compute weights on the fly for antialias off but it's better to pre calculate the weights for antialias on since the process is quite complexed.\r\n* most part in the original kernel won't be vectorized on GCC, the current kernel manually vectorize with `at::vec::Vectorized<>`.\r\n* bottleneck would be `gather` (roughly 50% of the time), so lower the gain size accordingly.\r\n* cover only `float32` and `float64` at the moment (optimizing `bfloat16` for this one doesn't make much sense, so ignore `bfloat16`).\r\n* channels last won't be affected.\r\n\r\nThe following benchmark show performance with torch and PIL on single core. Multi core will have even larger perf improvement.\r\n\r\n#### This is system config:\r\n```\r\nTorch version: 1.14.0a0+git1bf079d\r\nTorch config: PyTorch built with:\r\n  - GCC 8.3\r\n  - C++ Version: 201402\r\n  - Intel(R) oneAPI Math Kernel Library Version 2021.3-Product Build 20210617 for Intel(R) 64 architecture applications\r\n  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)\r\n  - OpenMP 201511 (a.k.a. OpenMP 4.5)\r\n  - LAPACK is enabled (usually provided by MKL)\r\n  - NNPACK is enabled\r\n  - CPU capability usage: AVX2\r\n  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=/opt/rh/devtoolset-8/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=OFF, USE_CUDNN=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF,\r\n```\r\n\r\n#### This is before:\r\n```\r\nNum threads: 1\r\n[---------------------------- Downsampling (bilinear): torch.Size([1, 1, 906, 438]) -> (320, 196) ----------------------------]\r\n                                 |  Reference, PIL 9.2.0, mode: L  |  1.14.0a0+git1bf079d cpu  |  Reference, PIL 9.2.0, mode: F\r\n1 threads: --------------------------------------------------------------------------------------------------------------------\r\n       contiguous torch.float32  |              585.9              |           1094.0          |              1293.7\r\n\r\nTimes are in microseconds (us).\r\n\r\n[---------------------------- Downsampling (bilinear): torch.Size([1, 1, 906, 438]) -> (460, 220) ----------------------------]\r\n                                 |  Reference, PIL 9.2.0, mode: L  |  1.14.0a0+git1bf079d cpu  |  Reference, PIL 9.2.0, mode: F\r\n1 threads: --------------------------------------------------------------------------------------------------------------------\r\n       contiguous torch.float32  |              840.8              |           1429.5          |              1635.7\r\n\r\nTimes are in microseconds (us).\r\n\r\n[----------------------------- Downsampling (bilinear): torch.Size([1, 1, 906, 438]) -> (120, 96) ----------------------------]\r\n                                 |  Reference, PIL 9.2.0, mode: L  |  1.14.0a0+git1bf079d cpu  |  Reference, PIL 9.2.0, mode: F\r\n1 threads: --------------------------------------------------------------------------------------------------------------------\r\n       contiguous torch.float32  |              210.9              |           756.8           |              965.5\r\n\r\nTimes are in microseconds (us).\r\n\r\n[---------------------------- Downsampling (bilinear): torch.Size([1, 1, 906, 438]) -> (1200, 196) ---------------------------]\r\n                                 |  Reference, PIL 9.2.0, mode: L  |  1.14.0a0+git1bf079d cpu  |  Reference, PIL 9.2.0, mode: F\r\n1 threads: --------------------------------------------------------------------------------------------------------------------\r\n       contiguous torch.float32  |               2.1               |            3.6            |               4.2\r\n\r\nTimes are in milliseconds (ms).\r\n\r\n[---------------------------- Downsampling (bilinear): torch.Size([1, 1, 906, 438]) -> (120, 1200) ---------------------------]\r\n                                 |  Reference, PIL 9.2.0, mode: L  |  1.14.0a0+git1bf079d cpu  |  Reference, PIL 9.2.0, mode: F\r\n1 threads: --------------------------------------------------------------------------------------------------------------------\r\n       contiguous torch.float32  |              505.8              |           853.8           |              1121.4\r\n\r\nTimes are in microseconds (us).\r\n\r\n[-------------------- Downsampling (bilinear): torch.Size([1, 3, 906, 438]) -> (320, 196) ---------------------]\r\n                                                  |  Reference, PIL 9.2.0, mode: RGB  |  1.14.0a0+git1bf079d cpu\r\n1 threads: -----------------------------------------------------------------------------------------------------\r\n      channels_first contiguous torch.float32     |                2.9                |            3.2\r\n      channels_last non-contiguous torch.float32  |                2.9                |            4.2\r\n\r\nTimes are in milliseconds (ms).\r\n\r\n[-------------------- Downsampling (bilinear): torch.Size([1, 3, 906, 438]) -> (460, 220) ---------------------]\r\n                                                  |  Reference, PIL 9.2.0, mode: RGB  |  1.14.0a0+git1bf079d cpu\r\n1 threads: -----------------------------------------------------------------------------------------------------\r\n      channels_first contiguous torch.float32     |                3.7                |            4.2\r\n      channels_last non-contiguous torch.float32  |                3.7                |            6.0\r\n\r\nTimes are in milliseconds (ms).\r\n\r\n[--------------------- Downsampling (bilinear): torch.Size([1, 3, 906, 438]) -> (120, 96) ---------------------]\r\n                                                  |  Reference, PIL 9.2.0, mode: RGB  |  1.14.0a0+git1bf079d cpu\r\n1 threads: -----------------------------------------------------------------------------------------------------\r\n      channels_first contiguous torch.float32     |                1.8                |            2.2\r\n      channels_last non-contiguous torch.float32  |                1.8                |            2.4\r\n\r\nTimes are in milliseconds (ms).\r\n\r\n[-------------------- Downsampling (bilinear): torch.Size([1, 3, 906, 438]) -> (1200, 196) --------------------]\r\n                                                  |  Reference, PIL 9.2.0, mode: RGB  |  1.14.0a0+git1bf079d cpu\r\n1 threads: -----------------------------------------------------------------------------------------------------\r\n      channels_first contiguous torch.float32     |                9.7                |            10.7\r\n      channels_last non-contiguous torch.float32  |                9.7                |            14.6\r\n\r\nTimes are in milliseconds (ms).\r\n\r\n[-------------------- Downsampling (bilinear): torch.Size([1, 3, 906, 438]) -> (120, 1200) --------------------]\r\n                                                  |  Reference, PIL 9.2.0, mode: RGB  |  1.14.0a0+git1bf079d cpu\r\n1 threads: -----------------------------------------------------------------------------------------------------\r\n      channels_first contiguous torch.float32     |                2.3                |            2.4\r\n      channels_last non-contiguous torch.float32  |                2.4                |            4.1\r\n\r\nTimes are in milliseconds (ms).\r\n```\r\n\r\n#### This is after:\r\n```\r\nNum threads: 1\r\n[---------------------------- Downsampling (bilinear): torch.Size([1, 1, 906, 438]) -> (320, 196) ----------------------------]\r\n                                 |  Reference, PIL 9.2.0, mode: L  |  1.14.0a0+git1bf079d cpu  |  Reference, PIL 9.2.0, mode: F\r\n1 threads: --------------------------------------------------------------------------------------------------------------------\r\n       contiguous torch.float32  |              580.4              |           558.9           |              1360.5\r\n\r\nTimes are in microseconds (us).\r\n\r\n[---------------------------- Downsampling (bilinear): torch.Size([1, 1, 906, 438]) -> (460, 220) ----------------------------]\r\n                                 |  Reference, PIL 9.2.0, mode: L  |  1.14.0a0+git1bf079d cpu  |  Reference, PIL 9.2.0, mode: F\r\n1 threads: --------------------------------------------------------------------------------------------------------------------\r\n       contiguous torch.float32  |              841.6              |           641.7           |              1634.8\r\n\r\nTimes are in microseconds (us).\r\n\r\n[----------------------------- Downsampling (bilinear): torch.Size([1, 1, 906, 438]) -> (120, 96) ----------------------------]\r\n                                 |  Reference, PIL 9.2.0, mode: L  |  1.14.0a0+git1bf079d cpu  |  Reference, PIL 9.2.0, mode: F\r\n1 threads: --------------------------------------------------------------------------------------------------------------------\r\n       contiguous torch.float32  |              210.3              |           515.3           |              944.5\r\n\r\nTimes are in microseconds (us).\r\n\r\n[---------------------------- Downsampling (bilinear): torch.Size([1, 1, 906, 438]) -> (1200, 196) ---------------------------]\r\n                                 |  Reference, PIL 9.2.0, mode: L  |  1.14.0a0+git1bf079d cpu  |  Reference, PIL 9.2.0, mode: F\r\n1 threads: --------------------------------------------------------------------------------------------------------------------\r\n       contiguous torch.float32  |               2.1               |            1.5            |               4.4\r\n\r\nTimes are in milliseconds (ms).\r\n\r\n[---------------------------- Downsampling (bilinear): torch.Size([1, 1, 906, 438]) -> (120, 1200) ---------------------------]\r\n                                 |  Reference, PIL 9.2.0, mode: L  |  1.14.0a0+git1bf079d cpu  |  Reference, PIL 9.2.0, mode: F\r\n1 threads: --------------------------------------------------------------------------------------------------------------------\r\n       contiguous torch.float32  |              505.4              |           762.3           |              1105.1\r\n\r\nTimes are in microseconds (us).\r\n\r\n[-------------------- Downsampling (bilinear): torch.Size([1, 3, 906, 438]) -> (320, 196) ---------------------]\r\n                                                  |  Reference, PIL 9.2.0, mode: RGB  |  1.14.0a0+git1bf079d cpu\r\n1 threads: -----------------------------------------------------------------------------------------------------\r\n      channels_first contiguous torch.float32     |                2.9                |            1.6\r\n      channels_last non-contiguous torch.float32  |                3.0                |            4.4\r\n\r\nTimes are in milliseconds (ms).\r\n\r\n[-------------------- Downsampling (bilinear): torch.Size([1, 3, 906, 438]) -> (460, 220) ---------------------]\r\n                                                  |  Reference, PIL 9.2.0, mode: RGB  |  1.14.0a0+git1bf079d cpu\r\n1 threads: -----------------------------------------------------------------------------------------------------\r\n      channels_first contiguous torch.float32     |                3.7                |            1.8\r\n      channels_last non-contiguous torch.float32  |                3.9                |            6.2\r\n\r\nTimes are in milliseconds (ms).\r\n\r\n[--------------------- Downsampling (bilinear): torch.Size([1, 3, 906, 438]) -> (120, 96) ---------------------]\r\n                                                  |  Reference, PIL 9.2.0, mode: RGB  |  1.14.0a0+git1bf079d cpu\r\n1 threads: -----------------------------------------------------------------------------------------------------\r\n      channels_first contiguous torch.float32     |                1.8                |            1.5\r\n      channels_last non-contiguous torch.float32  |                1.9                |            2.4\r\n\r\nTimes are in milliseconds (ms).\r\n\r\n[-------------------- Downsampling (bilinear): torch.Size([1, 3, 906, 438]) -> (1200, 196) --------------------]\r\n                                                  |  Reference, PIL 9.2.0, mode: RGB  |  1.14.0a0+git1bf079d cpu\r\n1 threads: -----------------------------------------------------------------------------------------------------\r\n      channels_first contiguous torch.float32     |                10.2               |             4.5\r\n      channels_last non-contiguous torch.float32  |                 9.6               |            14.6\r\n\r\nTimes are in milliseconds (ms).\r\n\r\n[-------------------- Downsampling (bilinear): torch.Size([1, 3, 906, 438]) -> (120, 1200) --------------------]\r\n                                                  |  Reference, PIL 9.2.0, mode: RGB  |  1.14.0a0+git1bf079d cpu\r\n1 threads: -----------------------------------------------------------------------------------------------------\r\n      channels_first contiguous torch.float32     |                2.4                |            2.3\r\n      channels_last non-contiguous torch.float32  |                2.4                |            4.2\r\n\r\nTimes are in milliseconds (ms).\r\n```", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 87008, "title": "Fix DFS infinite loop", "time": "2022-10-15T00:35:29Z", "body": "The new partitioner reaches max recursive limit in Python when partitioning BERT-base. This PR reduces unnecessary graph traversal and therefore reduces recursion to a runnable level.", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}, {"id": 3773064149, "node_id": "LA_kwDOA-j9z87g5GPV", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20developer%20feature", "name": "topic: developer feature", "color": "A78C18", "default": false, "description": "Not end-user facing but still impact people that compile from source, develop into pytorch, etc."}, {"id": 4347693568, "node_id": "LA_kwDOA-j9z88AAAABAySKAA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20fx.passes", "name": "module: fx.passes", "color": "0e8a16", "default": false, "description": "Optimization passes written in FX (don't forget to select a more specific label)"}]}
{"number": 87004, "title": "[onnx] Add warning for variable batch size in RNN export", "time": "2022-10-14T21:54:29Z", "body": null, "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3768821874, "node_id": "LA_kwDOA-j9z87go6hy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20onnx", "name": "release notes: onnx", "color": "B4EB93", "default": false, "description": "torch.onnx related changes that should show up in the release notes"}]}
{"number": 86994, "title": "[skip ci][WIP] utility functions for (nt, nt) broadcasting and (nt, t) broadcasting (fw-only)", "time": "2022-10-14T21:26:37Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* __->__ #86994\n\n", "label": []}
{"number": 86986, "title": "[xnnpack][lite-int][graph-build] torchscript -> xnnpack graph", "time": "2022-10-14T19:49:24Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #86986\n* #86981\n* #86980\n\nThis point we perform conversion for Torchscript IR to XNNPack graph. Currently we only support converting Add Nodes and fp32 tensor values.\n\nAs a caveat, we are not building this at runtime. So for testing we just run the xnn graph once ahead of time and with sample inputs and forward it to execute. This is only for testing, and will be changed in a later diff. This will allow us to check that graph creation is sound.\n\nDifferential Revision: [D39838851](https://our.internmc.facebook.com/intern/diff/D39838851/)", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 86969, "title": "[vulkan] Add option for printing GPU timestamp info in speed_benchmark_torch", "time": "2022-10-14T15:58:47Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #86972\n* #86971\n* __->__ #86969\n\nAdds some functionality in `QueryPool` to print out collected GPU timestamps in a format that can be displayed by AIBench, and add the ability to print this out in `speed_benchmark_torch`.\n\nDifferential Revision: [D40266073](https://our.internmc.facebook.com/intern/diff/D40266073/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D40266073/)!", "label": [{"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769206412, "node_id": "LA_kwDOA-j9z87gqYaM", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20vulkan", "name": "release notes: vulkan", "color": "23DD30", "default": false, "description": "release notes category"}]}
{"number": 86949, "title": "[ONNX] Generate diagnostic source files during build", "time": "2022-10-14T00:03:37Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #86949\n\r\nAdd the generation of diagnostics rules source code into build.\r\n\r\n- [x] Fix bazel build\r\n- [x] Fix cmake for unnecessary regenerate/rebuild\n\nDifferential Revision: [D40638423](https://our.internmc.facebook.com/intern/diff/D40638423)", "label": [{"id": 693805995, "node_id": "MDU6TGFiZWw2OTM4MDU5OTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20onnx", "name": "module: onnx", "color": "f7e101", "default": false, "description": "Related to torch.onnx"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768821874, "node_id": "LA_kwDOA-j9z87go6hy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20onnx", "name": "release notes: onnx", "color": "B4EB93", "default": false, "description": "torch.onnx related changes that should show up in the release notes"}, {"id": 3837551288, "node_id": "LA_kwDOA-j9z87kvGK4", "url": "https://api.github.com/repos/pytorch/pytorch/labels/onnx-needs-import", "name": "onnx-needs-import", "color": "547451", "default": false, "description": "This PR is related to ONNX, but touches files outside of merge rule patterns, and hence needs import"}, {"id": 4183081985, "node_id": "MDU6TGFiZWw0MTgzMDgxOTg1", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20build", "name": "topic: build", "color": "0e8a16", "default": false, "description": null}]}
{"number": 86947, "title": "Fixed ListType::isSubtypeOfExt to have correct comparison for nested TensorType", "time": "2022-10-13T23:45:01Z", "body": "### Description\r\nFixed `ListType::isSubtypeOfExt` method to have correct comparison for nested TensorType.\r\n\r\n### Details\r\nOriginally `ListType::isSubtypeOfExt` method relies on `Type::isSubtypeOfExt(rhs_, why_not)` which invokes the equality of `ListType` objects which leads to nested element types comparison. So there is a case when it doesn't work correctly:\r\n```cpp\r\n// Empty TensorType\r\nauto tt = TensorType::get();\r\n\r\n// Modified TensorType\r\nauto tt_modified = c10::TensorType::create(\r\n      at::kFloat,\r\n      at::kCPU,\r\n      c10::SymbolicShape(std::vector<c10::optional<int64_t>>({1, 49})),\r\n      std::vector<c10::Stride>(\r\n          {c10::Stride{2, true, 1},\r\n           c10::Stride{1, true, 1},\r\n           c10::Stride{0, true, c10::nullopt}}),\r\n      false);\r\n\r\ntt_modified->isSubtypeOf(*tt); // Returns true\r\n\r\n// But if we put these tensor types inside ListType\r\nauto lt = ListType::create(tt);\r\nauto lt_modified = ListType::create(tt_modified);\r\nlt_modified->isSubtypeOf(*lt); // Returns false because it invokes *tt == *tt_modified\r\n```\r\nChanges in PR help to align the behaviour of `TensorType::isSubtypeOfExt` and `ListType::isSubtypeOfExt`.\r\n\r\n@antoniojkim @ke1337", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 86946, "title": "[vulkan] Enable 2D texture types and use them for convolution weights/biases", "time": "2022-10-13T22:50:48Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\nDifferential Revision: [D40363112](https://our.internmc.facebook.com/intern/diff/D40363112/)", "label": [{"id": 3769206412, "node_id": "LA_kwDOA-j9z87gqYaM", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20vulkan", "name": "release notes: vulkan", "color": "23DD30", "default": false, "description": "release notes category"}]}
{"number": 86945, "title": "[vulkan] Add option for printing GPU timestamp info in speed_benchmark_torch", "time": "2022-10-13T22:50:44Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\nDifferential Revision: [D40266073](https://our.internmc.facebook.com/intern/diff/D40266073/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D40266073/)!", "label": [{"id": 3769206412, "node_id": "LA_kwDOA-j9z87gqYaM", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20vulkan", "name": "release notes: vulkan", "color": "23DD30", "default": false, "description": "release notes category"}]}
{"number": 86912, "title": "[FSDP] Introduce optim_state_dict() and load_optim_state_dict()", "time": "2022-10-13T17:07:59Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #86912\n\nCurrent FSDP saving and loading optimizer state_dict involves several different APIs. This PR aims to consolidate the usage and make the APIs looks like FSDP.state_dict()/FSDP.load_state_dict() by introducing FSDP.optim_state_dict() and FSDP.load_optim_state_dict().\n\nDifferential Revision: [D40083733](https://our.internmc.facebook.com/intern/diff/D40083733/)", "label": [{"id": 3769221312, "node_id": "LA_kwDOA-j9z87gqcDA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(fsdp)", "name": "release notes: distributed (fsdp)", "color": "C1F551", "default": false, "description": "release notes category"}]}
{"number": 86858, "title": "AOTAutograd and make_fx uses the new format of decomp_table", "time": "2022-10-13T00:28:36Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #86858\n* #86857\n\n", "label": [{"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}]}
{"number": 86847, "title": "[TVM] Add Normalize to C2 Frontend", "time": "2022-10-12T23:12:18Z", "body": "Summary: As title states, add TVM support for Normalize when compiling\n\nTest Plan:\n__Command__\n```\nbuck2 test mode/opt-split-dwarf //caffe2/caffe2/fb/tvm:test_tvm_transform\n```\n__Result__\n```\nBuck UI:      https://www.internalfb.com/buck2/b157c764-3ce0-4372-9b2d-74db50d0f931\nTest Session: https://www.internalfb.com/intern/testinfra/testrun/8725724395520327\nRE: reSessionID-8e229f26-b969-4d2f-b570-e14f9beed568  Up: 63 MiB  Down: 10 GiB\nJobs completed: 174220. Time elapsed: 170.6s. Cache hits: 95%. Commands: 75677 (cached: 71791, rem\nTests finished: Pass 31. Fail 0. Fatal 0. Skip 4. 0 builds failed\n```\n\nDifferential Revision: D40281703\n\n", "label": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false, "description": ""}, {"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}]}
{"number": 86751, "title": "Remove workaround in FusedAdam for CUDA11.3 CI jobs", "time": "2022-10-11T23:46:16Z", "body": "Moving implementations in the other files to FusedAdamKernel.cu\r\n\r\nRelated: https://github.com/pytorch/pytorch/pull/86540", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3768824578, "node_id": "LA_kwDOA-j9z87go7MC", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20cuda", "name": "release notes: cuda", "color": "91275E", "default": false, "description": "release notes category"}]}
{"number": 86742, "title": "[ONNX] Bump ONNX Runtime version to 1.13.0 RC for validation", "time": "2022-10-11T22:13:08Z", "body": null, "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 86740, "title": "Use tile size of (1, 1) for mali g52", "time": "2022-10-11T21:51:37Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #86740\n* #86739\n* #86738\n* #86737\n\nDifferential Revision: [D40280335](https://our.internmc.facebook.com/intern/diff/D40280335/)", "label": [{"id": 3769206412, "node_id": "LA_kwDOA-j9z87gqYaM", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20vulkan", "name": "release notes: vulkan", "color": "23DD30", "default": false, "description": "release notes category"}]}
{"number": 86738, "title": "[Pytorch][vulkan] Generate shader with parameters", "time": "2022-10-11T21:51:25Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #86740\n* #86739\n* __->__ #86738\n* #86737\n\nParametsr such as tile size and weight type and format is embedded within the\nshader code. This is used to generate ShaderInfo.\n\nFor now we will maintain both ShaderSrc and ShaderInfo so as to transition from\nVK_KERNEL to VK_SHADER incremental. Otherwise we will have to switch multiple\nof them at the same time.\n\nDifferential Revision: [D40280338](https://our.internmc.facebook.com/intern/diff/D40280338/)", "label": [{"id": 3769206412, "node_id": "LA_kwDOA-j9z87gqYaM", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20vulkan", "name": "release notes: vulkan", "color": "23DD30", "default": false, "description": "release notes category"}]}
{"number": 86736, "title": "[Pytorch][Vulkan] Update spv generation script to embed shader parameters", "time": "2022-10-11T21:47:59Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\nThis diffs adds shader parameters such as tile size, weight storage type and\nformat to the generated spv.cpp file.\nThis is used in ShaderInfo struct that ops such as convolution will use to\ndetermine, the workgroup size  and how to pack weights.\n\nDifferential Revision: [D40280337](https://our.internmc.facebook.com/intern/diff/D40280337/)", "label": [{"id": 3769206412, "node_id": "LA_kwDOA-j9z87gqYaM", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20vulkan", "name": "release notes: vulkan", "color": "23DD30", "default": false, "description": "release notes category"}]}
{"number": 86728, "title": "[DO NOT REVIEW] Upstream bump", "time": "2022-10-11T21:16:02Z", "body": "TODO: add usual description for code bump\r\n\r\nFixes #86717 \r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 86722, "title": "Resubmit FSDP hybrid shard", "time": "2022-10-11T20:24:51Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #86722\n\nAdd a new sharding strategy `hybrid_shard` to FSDP. This sharding strategy shards across the node and replicates between nodes. This means that allreduce is done between nodes and _reduce_scatter is within nodes. This can help save communication volume as FSDP related collectives happen only within a node.\n\nImplementation:\n- hybrid_shard should be consistent across all FSDP modules contained within the root. Unlike other sharding strategies we don't support mixing and matching hybrid_shard yet.\n- User is expected to pass in the appropriate inter-node process group\n- FSDP internals generate the cross-node process groups. For example, if we have node0: 0, 1 node1: 0,1, node2: 0, 1, there will be 2 inter-node process groups: containing [(node0 local rank 0), (node1 local rank 0), ...] and so forth.\n\nDifferential Revision: [D40277048](https://our.internmc.facebook.com/intern/diff/D40277048/)", "label": [{"id": 3769221312, "node_id": "LA_kwDOA-j9z87gqcDA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(fsdp)", "name": "release notes: distributed (fsdp)", "color": "C1F551", "default": false, "description": "release notes category"}]}
{"number": 86715, "title": "[WIP] functorch testing for ROCM", "time": "2022-10-11T19:07:08Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #86715\n\n", "label": [{"id": 1078897659, "node_id": "MDU6TGFiZWwxMDc4ODk3NjU5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20rocm", "name": "module: rocm", "color": "f7e101", "default": false, "description": "AMD GPU support for Pytorch"}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 86706, "title": "[JIT] Frozen Graph Linear-BatchNormNd Folding", "time": "2022-10-11T18:19:17Z", "body": "This PR adds linear-batchnormNd folding for JIT frozen graphs. \r\n\r\n**Performance benchmark**\r\nA preliminary benchmark with a simple model of linear+bn1d tested on first socket, physical cores of skylake machine. \r\n\r\n**FP32, JIT** \r\nwithout linear-bn folding\r\n![Screenshot (1368)](https://user-images.githubusercontent.com/93151422/195168944-cfc5b920-bc82-4be1-a221-d194c8fa6c18.png)\r\n\r\nwith linear-bn folding\r\n![Screenshot (1367)](https://user-images.githubusercontent.com/93151422/195168926-267b0515-45a1-4f08-922d-c150845199ae.png)\r\n", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 86692, "title": "Updated tryConvertToType to use more precise types", "time": "2022-10-11T15:09:01Z", "body": "### Description\r\nUpdate `tryConvertToType` to use more precise types during conversion to `NumberType`.\r\n\r\n### Details\r\nThis problem with `NumberType` was found in torch-mlir during PyTorch LTC to JIT graph conversion when we lower Scalar operation to Constant with tensor output type. In cases when Constant has a consumer which expects scalar literal type, it triggers (tensor to scalar) type alignment in PyTorch. The alignment works well, and from JIT perspective `ScalarImplicit` with `NumberType` is a valid operation. But from backends perspective we still have to convert `NumberType` Pythonism to something more precise (like `IntImplicit` or `FloatImplicit`) which takes compilation time. So the best option is to change `tryConvertToType` to produce more precise output type during conversion from tensor to number type.\r\n\r\n@antoniojkim @ke1337 ", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 86687, "title": "Add typecasting for gelu backward kernel (#86673)", "time": "2022-10-11T13:05:15Z", "body": "Fixes #86673\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768824578, "node_id": "LA_kwDOA-j9z87go7MC", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20cuda", "name": "release notes: cuda", "color": "91275E", "default": false, "description": "release notes category"}]}
{"number": 86648, "title": "[ONNX] Avoid copying constant in scalar_type_analysis pass", "time": "2022-10-10T23:39:02Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #86648\n* #86716\n\r\nTo prevent unnecessarily increasing model size due to duplicating constants in graph.\r\nFollow up work is needed to apply CSE on graph, in case the cast nodes are constant\r\nfolded, and re-introducing duplicated constants.\r\nThis issue was discovered in #86627. ~~This PR also adds unit test coverage for scope\r\ninformation of nodes when they are altered by CSE and related passes.~~\r\n\r\nEdit: Convert to draft now that it appears some legacy caffe2 tests are breaking, which might\r\ndepend on the behavior of copying constants.\r\n", "label": [{"id": 693805995, "node_id": "MDU6TGFiZWw2OTM4MDU5OTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20onnx", "name": "module: onnx", "color": "f7e101", "default": false, "description": "Related to torch.onnx"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3768821874, "node_id": "LA_kwDOA-j9z87go6hy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20onnx", "name": "release notes: onnx", "color": "B4EB93", "default": false, "description": "torch.onnx related changes that should show up in the release notes"}, {"id": 3773061952, "node_id": "LA_kwDOA-j9z87g5FtA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20bug%20fixes", "name": "topic: bug fixes", "color": "A44870", "default": false, "description": "topic category"}]}
{"number": 86647, "title": "build: Add -Werror=shadow to CMAKE_CXX_FLAGS", "time": "2022-10-10T22:39:03Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* __->__ #86647\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 86591, "title": "[WIP] Add support for VS2022 Windows build.", "time": "2022-10-10T13:50:32Z", "body": "This is just WIP PR to check if pipelines will fail or pass.\r\n\r\nCloses #87695\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769204372, "node_id": "LA_kwDOA-j9z87gqX6U", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20releng", "name": "release notes: releng", "color": "EED552", "default": false, "description": "release notes category"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}, {"id": 4077587778, "node_id": "LA_kwDOA-j9z87zCw1C", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/nightly", "name": "ciflow/nightly", "color": "bfdadc", "default": false, "description": "Trigger all jobs we run nightly (nightly.yml)"}]}
{"number": 86565, "title": "[torch] Implement fast version of min/max", "time": "2022-10-09T23:41:54Z", "body": "Summary: Add non IEEE conformant implementations of max/min operations that follow std::max()/std::min()\n\nTest Plan: CI\n\nDifferential Revision: D40215266\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}]}
{"number": 86564, "title": "update fbgemm commit ID in PyTorch", "time": "2022-10-09T23:26:57Z", "body": "Differential Revision: D40216348\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 86550, "title": "Add CPU specific info.", "time": "2022-10-09T13:42:52Z", "body": "On platforms without AVX512 BFloat16 instruction torch.autocast('cpu', enabled=True) works slower than torch.autocast('cpu', enabled=False).\r\n\r\nFixes #ISSUE_NUMBER\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}]}
{"number": 86541, "title": "[Profiler][Trivial] Small style and safety fixes", "time": "2022-10-09T04:22:44Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\nI noticed a couple abbreviations in the new optimizer capture code that are worth expanding. I also made the RawTensorMetadata a bit safer.\n\nDifferential Revision: [D40210702](https://our.internmc.facebook.com/intern/diff/D40210702/)", "label": [{"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 86460, "title": "Testing to remove ref", "time": "2022-10-07T15:36:33Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4077622962, "node_id": "LA_kwDOA-j9z87zC5ay", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/binaries_conda", "name": "ciflow/binaries_conda", "color": "3FC2AC", "default": false, "description": "Trigger binary build and upload jobs for conda on the PR"}]}
{"number": 86400, "title": "try building on no gpu 3.8 machines", "time": "2022-10-06T19:12:48Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #86400\n* #86341\n* #86155\n\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 86383, "title": "Consistent geometric distributions", "time": "2022-10-06T18:32:43Z", "body": "Fixes #85375\r\n\r\nThis is a WIP. The idea is to add a `kwarg` for changing the range of output random numbers. So far the changes have been introduced to `torch.distributions.Geometric`.", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 86341, "title": "Try directly buidling and using deploy", "time": "2022-10-05T23:57:27Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #86341\n* #86155\n\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 86327, "title": "MultiThreaded FileSystemWriter for distributed checkpointing.", "time": "2022-10-05T21:33:28Z", "body": "This adds two knobs to FileSystemWriter: thread_count and per_thread_copy_ahead.\r\n\r\nThose two lead to up to 50% performance improvement on 32 GPUs workloads on AWS.", "label": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20distributed", "name": "oncall: distributed", "color": "f7e101", "default": false, "description": "Add this issue/PR to distributed oncall triage queue"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 86323, "title": "cla check", "time": "2022-10-05T21:11:41Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 86299, "title": "[not4land] Added CuSparseLt kernels using custom class", "time": "2022-10-05T19:01:30Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #86299\n* #85835\n* #83989\n* #84759\n\nSummary:\nStill WIP.", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 86248, "title": "turn fake tensor on by default in aotautograd", "time": "2022-10-05T00:39:24Z", "body": null, "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}]}
{"number": 86238, "title": "[Skip in Diff-Train] Revert commits causing internal merge conflicts", "time": "2022-10-04T21:30:03Z", "body": "Reverting the following commits as they were causing internal merge conflicts..\r\n\r\n- https://github.com/pytorch/pytorch/commit/686555b663077b40f28bc88adc049e64035046b4\r\n- https://github.com/pytorch/pytorch/commit/a4dca9822dfabcdbd1b36a12c013764f2af87613\r\n- https://github.com/pytorch/pytorch/commit/bc6dc8d271d6cf4d0ae381077f59fc7bb7cf024d\r\n- https://github.com/pytorch/pytorch/commit/0b363c5c5c1832820466b7768b353db121809018\r\n\r\nI will notify the authors of these commits to re-land their changes in a separate PR. \r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769217376, "node_id": "LA_kwDOA-j9z87gqbFg", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20sparse", "name": "release notes: sparse", "color": "2A7626", "default": false, "description": "release notes category"}, {"id": 4417837777, "node_id": "LA_kwDOA-j9z88AAAABB1La0Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/mps", "name": "ciflow/mps", "color": "02EDA7", "default": false, "description": "Run MPS tests (subset of trunk)"}]}
{"number": 86201, "title": "fixing cuda and cpu differing error msgs", "time": "2022-10-04T11:44:36Z", "body": "Fixes [#85712](https://github.com/pytorch/pytorch/issues/85712)\r\n\r\nRemade this PR because of CLA issues.\r\n\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 86196, "title": "Fix non-tensor values ​​in batch used for pipeline parallelization", "time": "2022-10-04T08:28:09Z", "body": "# Motivation\r\n\r\nFix bugs in pipeline parallelization to support non-tensor values ​​in batch.\r\n\r\n# Modification\r\n\r\n1.  In `Copy` autograd function, add logic to check if it is a tensor.\r\n2. Changed to check whether it is a tensor first in the `profile` process for layer partitioning.", "label": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20distributed", "name": "oncall: distributed", "color": "f7e101", "default": false, "description": "Add this issue/PR to distributed oncall triage queue"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3769220454, "node_id": "LA_kwDOA-j9z87gqb1m", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(pipeline)", "name": "release notes: distributed (pipeline)", "color": "53B5EF", "default": false, "description": "release notes category"}]}
{"number": 86182, "title": "[ONNX] Auto test based on OpInfo", "time": "2022-10-04T01:51:31Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #86146\n* __->__ #86182\n\r\n\r\nThis change introduces a mechanism to test onnx export based on sample inputs registered in OpInfo, similar to how MPS and other components of pytorch are tested. It provides test coverage on ops and dtypes previously unattainable with manually created test models. This is the best way for us to discover gaps in the exporter support, especially for ops with partial existing support.\r\n\r\nThis test is adapted from https://github.com/pytorch/pytorch/blob/master/test/test_mps.py\r\n\r\nThis PR also\r\n\r\n- Update sqrt to support integer inputs to match pytorch behavior\r\n- Add pytest-subtests for unittest subtests support in the new test file\r\n\r\nI only enabled very few ops: `t`, `ceil` and `sqrt` because otherwise too many things will fail due to (1) unsupported dtypes in the exporter (2) unimplemented dtype support in onnxruntime (3) unexpected input to verification.verify. \r\n\r\nSubsequent PRs should improve `verification.verify` first for it to accept any legal input to a pytorch model, then incrementally fix the symbolic functions to enable more test cases.\r\n\r\nFixes #85363\r\nDesign #88118", "label": [{"id": 693805995, "node_id": "MDU6TGFiZWw2OTM4MDU5OTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20onnx", "name": "module: onnx", "color": "f7e101", "default": false, "description": "Related to torch.onnx"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768821874, "node_id": "LA_kwDOA-j9z87go6hy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20onnx", "name": "release notes: onnx", "color": "B4EB93", "default": false, "description": "torch.onnx related changes that should show up in the release notes"}, {"id": 3773064149, "node_id": "LA_kwDOA-j9z87g5GPV", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20developer%20feature", "name": "topic: developer feature", "color": "A78C18", "default": false, "description": "Not end-user facing but still impact people that compile from source, develop into pytorch, etc."}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 86158, "title": "[WIP] Testing conda nightly windows", "time": "2022-10-03T21:29:50Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4077622962, "node_id": "LA_kwDOA-j9z87zC5ay", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/binaries_conda", "name": "ciflow/binaries_conda", "color": "3FC2AC", "default": false, "description": "Trigger binary build and upload jobs for conda on the PR"}]}
{"number": 86156, "title": "[MPS] Raise error for dot int64 input", "time": "2022-10-03T21:22:15Z", "body": "int64 input not supported in dot op", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 4164781683, "node_id": "LA_kwDOA-j9z874PYZz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20mps", "name": "release notes: mps", "color": "1d76db", "default": false, "description": "Release notes category"}, {"id": 4417837777, "node_id": "LA_kwDOA-j9z88AAAABB1La0Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/mps", "name": "ciflow/mps", "color": "02EDA7", "default": false, "description": "Run MPS tests (subset of trunk)"}]}
{"number": 86155, "title": "Add torch::deploy tests to CI", "time": "2022-10-03T21:19:06Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #86341\n* __->__ #86155\n\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 86146, "title": "[ONNX] Update floor_divide behavior to match eager execution ", "time": "2022-10-03T19:07:45Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #86146\n* #86182\n\r\n- Fixes https://github.com/pytorch/pytorch/issues/78442\r\n- Change floor_divide to round down to match eager. The old behavior for floor_divide (which was actually trunc divide) is deprecated and removed in eager mode. (See https://github.com/pytorch/pytorch/issues/78442)\r\n- Remove division logic in opset 10 because it is duplicated from opset 9.\r\n- Add tests to `remainder`, `div`, `floor_divide` and `true_divide`", "label": [{"id": 693805995, "node_id": "MDU6TGFiZWw2OTM4MDU5OTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20onnx", "name": "module: onnx", "color": "f7e101", "default": false, "description": "Related to torch.onnx"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3768821874, "node_id": "LA_kwDOA-j9z87go6hy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20onnx", "name": "release notes: onnx", "color": "B4EB93", "default": false, "description": "torch.onnx related changes that should show up in the release notes"}, {"id": 3773058107, "node_id": "LA_kwDOA-j9z87g5Ew7", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20bc_breaking", "name": "topic: bc_breaking", "color": "c2e0c6", "default": false, "description": "topic category"}, {"id": 3773061952, "node_id": "LA_kwDOA-j9z87g5FtA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20bug%20fixes", "name": "topic: bug fixes", "color": "A44870", "default": false, "description": "topic category"}]}
{"number": 86089, "title": "Add some documentation to deploy.h", "time": "2022-10-03T03:36:29Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #86095\n* __->__ #86089\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}]}
{"number": 86082, "title": "Fix Error Messages", "time": "2022-10-02T22:46:37Z", "body": "Potentially Fixes #85712 \r\n- Standardized error messages for each differing situation\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 86064, "title": "CVE-2007-4559 Patch", "time": "2022-10-02T09:40:09Z", "body": "# Patching CVE-2007-4559\n\nHi, we are security researchers from the Advanced Research Center at [Trellix](https://www.trellix.com). We have began a campaign to patch a widespread bug named CVE-2007-4559. CVE-2007-4559 is a 15 year old bug in the Python tarfile package. By using extract() or extractall() on a tarfile object without sanitizing input, a maliciously crafted .tar file could perform a directory path traversal attack. We found at least one unsantized extractall() in your codebase and are providing a patch for you via pull request. The patch essentially checks to see if all tarfile members will be extracted safely and throws an exception otherwise. We encourage you to use this patch or your own solution to secure against CVE-2007-4559. Further technical information about the vulnerability can be found in this [blog](https://www.trellix.com/en-us/about/newsroom/stories/research/tarfile-exploiting-the-world.html).\n\nIf you have further questions you may contact us through this projects lead researcher [Kasimir Schulz](mailto:kasimir.schulz@trellix.com).\n", "label": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false, "description": ""}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 85995, "title": "Add `ncclRemoteError` and `ncclInProgress`", "time": "2022-09-30T20:53:24Z", "body": "As per title, add the values which NCCL has added to `ncclResult_t`\r\n\r\nrel:\r\n- https://github.com/pytorch/pytorch/pull/85887#discussion_r984827298\r\n- https://github.com/pytorch/pytorch/pull/85782\n\ncc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu", "label": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20distributed", "name": "oncall: distributed", "color": "f7e101", "default": false, "description": "Add this issue/PR to distributed oncall triage queue"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769218061, "node_id": "LA_kwDOA-j9z87gqbQN", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(c10d)", "name": "release notes: distributed (c10d)", "color": "EE86D4", "default": false, "description": "release notes category"}]}
{"number": 85976, "title": "Add A Permutted Strides Input To Unary Opinfos", "time": "2022-09-30T15:51:27Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #85976\n\r\nAdds a mechanism to create tensors with a permutted stride in `make_tensor`, and then adds a channels last tensor input to a couple of sample generation mechanisms `sample_inputs_elementwise_unary` and `generate_elementwise_unary_tensors`. \r\n\r\nI think in a subsequent test we should add a OpInfo test that asserts for every operator we're testing we are generating a permutted stride input. \r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}]}
{"number": 85955, "title": "To support XPU profiler with Kineto", "time": "2022-09-30T06:54:15Z", "body": "As Kineto's changes start to support extend profilers to register, we made some following changes to PyTorch's profiler shim layel. This patch only affects c source codes related to kineto profiler which will add the support of XPU profiler with some APIs into kineto_shim and initializations.\r\n\r\nSigned-off-by: Xunsong, Huang <xunsong.huang@intel.com>\r\n\r\nFixes #ISSUE_NUMBER\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 85948, "title": "[DO_NOT_MERGE]pin tf branch", "time": "2022-09-30T02:19:07Z", "body": "this is to test tf ci build", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 85947, "title": "Add exception for build_variables.bzl in .gitignore", "time": "2022-09-30T01:54:32Z", "body": "Fixes #80431\r\nThe regular expression of .gitignore accidentally matches build_variables.bzl, which in turn causes .dockerignore to match it.\r\nAn exception needs to be set for build_variables.bzl.", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 85945, "title": "Delete torch::deploy from pytorch core", "time": "2022-09-30T01:17:17Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #85945\n\nAs we have migrated torch::deploy over to https://github.com/pytorch/multipy, we can now delete it from pytorch core as ongoing development will happen there.\n\nThis PR was created due to syncing issues with https://github.com/pytorch/pytorch/pull/85443 which is where the review history can be found.\n\nDifferential Revision: [D39933787](https://our.internmc.facebook.com/intern/diff/D39933787/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D39933787/)!\n\ncc @ezyang @SherlockNoMad @soumith @EikanWang @jgong5 @wenzhe-nrv", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769204372, "node_id": "LA_kwDOA-j9z87gqX6U", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20releng", "name": "release notes: releng", "color": "EED552", "default": false, "description": "release notes category"}, {"id": 4347715449, "node_id": "LA_kwDOA-j9z88AAAABAyTfeQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fx", "name": "fx", "color": "ededed", "default": false, "description": null}, {"id": 4443556792, "node_id": "LA_kwDOA-j9z88AAAABCNtLuA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/skip-pr-sanity-checks", "name": "skip-pr-sanity-checks", "color": "BED7D5", "default": false, "description": ""}]}
{"number": 85942, "title": "delete torch::deploy from pytorch code (but non internal)", "time": "2022-09-29T23:50:22Z", "body": "cc @ezyang @SherlockNoMad @soumith @EikanWang @jgong5 @wenzhe-nrv", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769204372, "node_id": "LA_kwDOA-j9z87gqX6U", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20releng", "name": "release notes: releng", "color": "EED552", "default": false, "description": "release notes category"}, {"id": 4347715449, "node_id": "LA_kwDOA-j9z88AAAABAyTfeQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fx", "name": "fx", "color": "ededed", "default": false, "description": null}]}
{"number": 85941, "title": "delete torch::deploy from pytorch code (but non internal)", "time": "2022-09-29T23:46:27Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #85941\n\n\n\ncc @ezyang @SherlockNoMad @soumith @EikanWang @jgong5 @wenzhe-nrv", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769204372, "node_id": "LA_kwDOA-j9z87gqX6U", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20releng", "name": "release notes: releng", "color": "EED552", "default": false, "description": "release notes category"}, {"id": 4347715449, "node_id": "LA_kwDOA-j9z88AAAABAyTfeQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fx", "name": "fx", "color": "ededed", "default": false, "description": null}]}
{"number": 85930, "title": "register efficient zero decomp", "time": "2022-09-29T21:39:28Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #85930\n* #85921\n* #85920\n\r\nThis should just be done as a compiler optimization (it is already in inductor), and we don't have any handling of efficientzero in the lowering stack.\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 85926, "title": "Enable max.unary_out", "time": "2022-09-29T21:22:21Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #85926\n\n", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 2404913419, "node_id": "MDU6TGFiZWwyNDA0OTEzNDE5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Merged", "name": "Merged", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2510927053, "node_id": "MDU6TGFiZWwyNTEwOTI3MDUz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Reverted", "name": "Reverted", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 85913, "title": "Improve readability in ranged assertions", "time": "2022-09-29T19:18:56Z", "body": "Changes\r\n```\r\nassert(x >= 0 && x < 20)\r\n```\r\nto\r\n```\r\nassert(0 <= x && x < 20)\r\n```\r\nin order to better demonstrate that `x` must be in the range `[0, 20)`.", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1078897659, "node_id": "MDU6TGFiZWwxMDc4ODk3NjU5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20rocm", "name": "module: rocm", "color": "f7e101", "default": false, "description": "AMD GPU support for Pytorch"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769217376, "node_id": "LA_kwDOA-j9z87gqbFg", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20sparse", "name": "release notes: sparse", "color": "2A7626", "default": false, "description": "release notes category"}]}
{"number": 85911, "title": "Reland: Update `amax/amin/norm/count_nonzero` signatures with `int[*]? dim`", "time": "2022-09-29T19:11:09Z", "body": "Changes `dim` arg to use `int[*]?` type for the following functions in `native_funcitons.yaml`:\r\n* `amax`\r\n* `amin`\r\n* `norm`\r\n* `frobenius_norm`\r\n* `native_norm`\r\n* `count_nonzero`\r\n\r\nPart of #29137\r\n\r\nReland of #83300\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel @mcarilli @ptrblck @leslie-fang-intel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1972560798, "node_id": "MDU6TGFiZWwxOTcyNTYwNzk4", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20amp%20(automated%20mixed%20precision)", "name": "module: amp (automated mixed precision)", "color": "f7e101", "default": false, "description": "autocast"}, {"id": 2406316143, "node_id": "MDU6TGFiZWwyNDA2MzE2MTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20reductions", "name": "module: reductions", "color": "f7e101", "default": false, "description": ""}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3212089274, "node_id": "MDU6TGFiZWwzMjEyMDg5Mjc0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/with-ssh", "name": "with-ssh", "color": "127AF1", "default": false, "description": ""}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769215850, "node_id": "LA_kwDOA-j9z87gqatq", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20linalg_frontend", "name": "release notes: linalg_frontend", "color": "B7C4BC", "default": false, "description": "release notes category"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}, {"id": 4417837777, "node_id": "LA_kwDOA-j9z88AAAABB1La0Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/mps", "name": "ciflow/mps", "color": "02EDA7", "default": false, "description": "Run MPS tests (subset of trunk)"}]}
{"number": 85904, "title": "[WIP][Testing CI] use same path as symbolic", "time": "2022-09-29T17:19:35Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #85904\n\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 85901, "title": "Delete torch::deploy from pytorch core", "time": "2022-09-29T16:42:33Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #85901\n\nAs we have migrated torch::deploy over to https://github.com/pytorch/multipy, we can now delete it from pytorch core as ongoing development will happen there.\n\nThis PR was created due to syncing issues with https://github.com/pytorch/pytorch/pull/85443 which is where the review history can be found.\n\nDifferential Revision: [D39933787](https://our.internmc.facebook.com/intern/diff/D39933787/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D39933787/)!\n\ncc @ezyang @SherlockNoMad @soumith @EikanWang @jgong5 @wenzhe-nrv", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769204372, "node_id": "LA_kwDOA-j9z87gqX6U", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20releng", "name": "release notes: releng", "color": "EED552", "default": false, "description": "release notes category"}, {"id": 4347715449, "node_id": "LA_kwDOA-j9z88AAAABAyTfeQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fx", "name": "fx", "color": "ededed", "default": false, "description": null}, {"id": 4443556792, "node_id": "LA_kwDOA-j9z88AAAABCNtLuA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/skip-pr-sanity-checks", "name": "skip-pr-sanity-checks", "color": "BED7D5", "default": false, "description": ""}]}
{"number": 85900, "title": "[ONNX] Upload code coverage to codecov", "time": "2022-09-29T16:32:17Z", "body": "TODO: Limit the report scope to torch.onnx", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1447309924, "node_id": "MDU6TGFiZWwxNDQ3MzA5OTI0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/better-engineering", "name": "better-engineering", "color": "94f76a", "default": false, "description": "Relatively self-contained tasks for better engineering contributors"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 85897, "title": "Switch to magma backend for a couple tests only on ROCm", "time": "2022-09-29T15:51:37Z", "body": "Fixes #ISSUE_NUMBER\r\n\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport", "label": [{"id": 1078897659, "node_id": "MDU6TGFiZWwxMDc4ODk3NjU5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20rocm", "name": "module: rocm", "color": "f7e101", "default": false, "description": "AMD GPU support for Pytorch"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769215850, "node_id": "LA_kwDOA-j9z87gqatq", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20linalg_frontend", "name": "release notes: linalg_frontend", "color": "B7C4BC", "default": false, "description": "release notes category"}, {"id": 4804263862, "node_id": "LA_kwDOA-j9z88AAAABHls_tg", "url": "https://api.github.com/repos/pytorch/pytorch/labels/rocm", "name": "rocm", "color": "B3FC7B", "default": false, "description": "This tag is for PRs from ROCm team"}]}
{"number": 85885, "title": "Add a missing CUDA kernel launch check", "time": "2022-09-29T04:42:38Z", "body": "All kernel launches should be checked for success; this one wasn't.\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel @ngimel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1076930055, "node_id": "MDU6TGFiZWwxMDc2OTMwMDU1", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cuda", "name": "module: cuda", "color": "f7e101", "default": false, "description": "Related to torch.cuda, and CUDA support in general"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3768824578, "node_id": "LA_kwDOA-j9z87go7MC", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20cuda", "name": "release notes: cuda", "color": "91275E", "default": false, "description": "release notes category"}, {"id": 3773061335, "node_id": "LA_kwDOA-j9z87g5FjX", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20improvements", "name": "topic: improvements", "color": "22499E", "default": false, "description": "topic category"}, {"id": 3773061952, "node_id": "LA_kwDOA-j9z87g5FtA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20bug%20fixes", "name": "topic: bug fixes", "color": "A44870", "default": false, "description": "topic category"}, {"id": 3773064149, "node_id": "LA_kwDOA-j9z87g5GPV", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20developer%20feature", "name": "topic: developer feature", "color": "A78C18", "default": false, "description": "Not end-user facing but still impact people that compile from source, develop into pytorch, etc."}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 85880, "title": "Fused SDP runtime dispatch", "time": "2022-09-29T01:46:04Z", "body": "# Summary \r\nThis shows the run mechanism for dispatching to a custom backend kernel or defaulting to the composite version for SDP. Currently we short circuit for composite but in the future this will dispatch to the appropriate kernel.\n\ncc @cpuhrsch @jbschlosser @bhosmer @mikaylagawarecki @erichan1", "label": [{"id": 1537268693, "node_id": "MDU6TGFiZWwxNTM3MjY4Njkz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20nestedtensor", "name": "module: nestedtensor", "color": "f7e101", "default": false, "description": "NestedTensor tag see issue #25032"}, {"id": 1900929949, "node_id": "MDU6TGFiZWwxOTAwOTI5OTQ5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20transformer/mha", "name": "oncall: transformer/mha", "color": "f7e101", "default": false, "description": "Issues related to Transformers and MultiheadAttention"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 4251729749, "node_id": "LA_kwDOA-j9z879bD9V", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20nested%20tensor", "name": "release notes: nested tensor", "color": "13B071", "default": false, "description": "Changes that have a direct impact on nested tensors"}]}
{"number": 85875, "title": "update func test", "time": "2022-09-29T00:56:15Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #85844\n* __->__ #85875\n* #85681\n* #82601\n* #82602\n* #82771\n* #83265\n\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 85865, "title": "[xnnpack][lite-int][graph-build] graph passes and op checking", "time": "2022-09-28T23:01:57Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\nBeginning of building the xnnpack graph from the torchscript IR. We first massage the torchscript graph using a few graph passes that perform things such as unused self argument removal and constant propagation.\nThis also performs tracing for us so that the model does not have to be prepped by tracing before being lowered by us.\n\nThe other check we perform is through the torchscript IR to identify any nodes that are not lowerable/supported, and throwing an error to spit out the specific nodes that are not lowerable.\n\nDifferential Revision: [D39838338](https://our.internmc.facebook.com/intern/diff/D39838338/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D39838338/)!\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 85859, "title": "[CUBLAS][TF32] Change cuBLAS TF32 environment variable to be initialization only", "time": "2022-09-28T22:00:18Z", "body": "CC @ptrblck @xwang233 ", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 85857, "title": "Add bmm path for nested matmul", "time": "2022-09-28T21:58:13Z", "body": "Registered a function that does this \r\n\r\nFor two nested 4D inputs of shape `self [N, n_heads, *_1, head_dim]`, `mat2 [N, n_heads, head_dim, *_2]`,\r\n\r\n1. view them as 3D nested tensors `self [N * n_heads, *_1, head_dim]`, `mat2 [N * n_heads, head_dim, *_2]`,\r\n2. call bmm\r\n3. view the output as `mat2 [N * n_heads, *_1, *_2]`,\r\n\r\nThis function is called in nested matmul if inputs are 4D, on CUDA and do not require gradients, a follow-up would be to add backward support\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack):\r\n* __->__ #85857\r\n\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 85849, "title": "Update autograd.grad to call the next node's prehook before capture", "time": "2022-09-28T21:04:15Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #90105\n* __->__ #85849\n* #88357\n\r\nAddresses: https://github.com/pytorch/pytorch/issues/35802\r\n\r\n## Updated hooks behavior:\r\n\r\n### Captured gradients for `.grad`\r\nWhen running `.grad`, asking for the gradient for an input tensor (not necessarily a leaf) captures the \"grad out\" passed to the node corresponding to that tensor. With this PR, pre-hooks of that node are applied before the gradient is captured for `.grad` (post-hooks still aren't applied). Note that for .backward, it is slightly different as the accumulate grad's post hooks is also called (this was unchanged). This change is particularly useful when one wants to register a hook on a parameter, and want that hook to still be applied when using `.grad`.\r\n\r\n### Pre-hooks registered to leaf tensor\r\n\r\nWith this PR hooks registered to leaf tensor will now change the input you receive in the post hook registered to accumulate grad.\r\n\r\n### Ordering of pre-hooks \r\n\r\nSome context first - previously hooks registered to tensors were always pre-hooks in that they are executed BEFORE the corresponding node. and take a single gradient. On the other hand hooks that are registered directly to the grad_fn, which take an entire tuple of gradient outputs are always post-hooks in that they are executed AFTER the corresponding node.\r\n\r\nhttps://github.com/pytorch/pytorch/pull/83226 introduces a new API to register pre-hooks that can be directly registered to grad_fn as well. So now there are now two types of pre-hooks. We'll call the ones registered to tensors \"tensor pre-hooks\" and the ones registered to grad_fn just \"pre-hooks\"\r\n\r\nDue to the way hooks are currently implemented, tensor pre-hooks and pre-hooks are fired in groups, all tensor pre-hooks fire before all pre-hooks (or vice-versa). Within each group, hooks that are registered first are called first.\r\n\r\nNow we run into the question of how the two groups as whole should be ordered. Though I don't think the order itself matters too much, a well-defined ordering should exist that users can rely on. In this PR I order pre-hooks before tensor pre-hooks because of interactions with retain_grad. See comment below for more discussion.\r\n\r\n### Retains grad \r\n\r\nTBD\r\n\r\n## Implications for distributed engine\r\n \r\n### No longer need to run pre-hooks in capture hooks\r\n\r\nThe distributed engine's capture hooks execute the pre-hooks of the next node. Since we've update those pre-hooks to be called, the capture hooks no longer need to do that.\r\n\r\n### Update in accumulate grad\r\n\r\nAccumulate grad node no longer needs to call tensor pre-hooks manually, because we moved that to be executed along with the rest of the pre-hooks.\r\n\r\n\r\n", "label": [{"id": 846310180, "node_id": "MDU6TGFiZWw4NDYzMTAxODA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20bc-breaking", "name": "module: bc-breaking", "color": "f7e101", "default": false, "description": "Related to a BC-breaking change"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3769211609, "node_id": "LA_kwDOA-j9z87gqZrZ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20autograd", "name": "release notes: autograd", "color": "9433A1", "default": false, "description": "release notes category"}, {"id": 3773058107, "node_id": "LA_kwDOA-j9z87g5Ew7", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20bc_breaking", "name": "topic: bc_breaking", "color": "c2e0c6", "default": false, "description": "topic category"}]}
{"number": 85844, "title": "[test] make optional<c10::SymInt> const ref", "time": "2022-09-28T20:25:18Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* __->__ #85844\n* #85875\n* #85681\n* #82601\n* #82602\n* #82771\n* #83265\n\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 85835, "title": "[not4land] spmma2 cuSPARSELt benchmarking", "time": "2022-09-28T18:56:04Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #86299\n* __->__ #85835\n* #83989\n* #84759\n\nSummary:\nTook\nhttps://github.com/NVIDIA/CUDALibrarySamples/tree/master/cuSPARSELt/spmma2\nand integrated it as an aten function in PyTorch for benchmarking\npurposes. Made some modifications to the example (e.g., adding\nmatmulsearch) and some timing functions.\n\nThis PR is currently not intended to be landed. A future PR will\nrefractor the spmma2 example using custom class to expose the different\noperations (pruning, compression, matmul) in python.\n\nTODO: insert benchmark table here", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}]}
{"number": 85827, "title": "Revert D39619861: Multisect successfully blamed D39619861 for test or build failures", "time": "2022-09-28T17:56:30Z", "body": "Summary:\nThis diff is reverting D39619861 (https://github.com/pytorch/pytorch/commit/f0869cc8d095c9bdbcaca147ba52857932e7a743)\nD39619861 (https://github.com/pytorch/pytorch/commit/f0869cc8d095c9bdbcaca147ba52857932e7a743) has been identified to be causing the following test or build failures:\nTests affected:\n- https://www.internalfb.com/intern/test/281475047227156/\n\nHere's the Multisect link:\nhttps://www.internalfb.com/intern/testinfra/multisect/1295835\nHere are the tasks that are relevant to this breakage:\nT133041108: 2 tests started failing for oncall fbgemm_dev in the last 2 weeks\nWe're generating a revert to back out the changes in this diff, please note the backout may land if someone accepts it.\n\nTest Plan: NA\n\nDifferential Revision: D39883179\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 85823, "title": "[nn] allow upsample to take list", "time": "2022-09-28T17:26:12Z", "body": "Fixes #83244\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 85785, "title": "[JIT] update error message when a type cannot be inferred", "time": "2022-09-28T04:12:58Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* __->__ #85785\n\nThis error can be observed in non-tracing scenarios; the new error\nmessage is probably less confusing, although ideally it would be great\nif we could dump additional information about what's actually getting\nscripted at the time when this failure occurs.\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 85772, "title": "[pytorch] Fix docstring of clip_grad_norm_ and clip_grad_value_", "time": "2022-09-27T23:52:02Z", "body": "Summary: Fixing the types in the docstrings to match the function signatures\n\nTest Plan:\nN/A\n\nSandcastle run\n\nReviewed By: malfet\n\nDifferential Revision: D38324860\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}]}
{"number": 85763, "title": "Enable ninja installation for ROCm", "time": "2022-09-27T21:59:54Z", "body": "This PR specifies NINJA_VERSION for the explicit ROCm CI configs (missed in https://github.com/pytorch/pytorch/pull/87505)\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport", "label": [{"id": 1078897659, "node_id": "MDU6TGFiZWwxMDc4ODk3NjU5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20rocm", "name": "module: rocm", "color": "f7e101", "default": false, "description": "AMD GPU support for Pytorch"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}, {"id": 4804263862, "node_id": "LA_kwDOA-j9z88AAAABHls_tg", "url": "https://api.github.com/repos/pytorch/pytorch/labels/rocm", "name": "rocm", "color": "B3FC7B", "default": false, "description": "This tag is for PRs from ROCm team"}]}
{"number": 85762, "title": "Add ROCm merge rules", "time": "2022-09-27T21:59:40Z", "body": "Adds jeffdaily as approver needed to merge any changes to ROCm or HIP-related files in PyTorch\r\n\n\ncc @jeffdaily @sunway513 @pruthvistony @ROCmSupport", "label": [{"id": 1078897659, "node_id": "MDU6TGFiZWwxMDc4ODk3NjU5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20rocm", "name": "module: rocm", "color": "f7e101", "default": false, "description": "AMD GPU support for Pytorch"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4804263862, "node_id": "LA_kwDOA-j9z88AAAABHls_tg", "url": "https://api.github.com/repos/pytorch/pytorch/labels/rocm", "name": "rocm", "color": "B3FC7B", "default": false, "description": "This tag is for PRs from ROCm team"}]}
{"number": 85741, "title": "Debug", "time": "2022-09-27T19:03:24Z", "body": "This is for running tests on environments that do not have on my end. Please do not merge.", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}]}
{"number": 85727, "title": "[Do NOT Merge] Generate unused-typdef error", "time": "2022-09-27T16:54:27Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 85724, "title": "Fix caffe2 test", "time": "2022-09-27T16:42:36Z", "body": "Summary:\nthsi test is failing blaming my diff: https://www.internalfb.com/intern/test/562949956474510/\nfollowed  https://fb.workplace.com/groups/koski.users/permalink/5811736972171470/ suggestion\n\nTest Plan:\n```\n[aidana@devvm2509.cln0 ~/fbsource/fbcode (0ae71702c)]$ buck1 test mode/opt-split-dwarf //caffe2/caffe2/distributed:file_store_handler_ops_test -- --exact 'caffe2/caffe2/distributed:file_store_handler_ops_test - test_set_get (caffe2.caffe2.distributed.file_store_handler_op_test.TestFileStoreHandlerOp)' --run-disabled --jobs 18 --stress-runs 10 --record-results\n\nInvalidating internal cached state: Buck configuration options changed between invocations. This may cause slower builds.\n  Changed value //cache.cache_cas_host='buildinfra-buckcache-dc-l7-prod.inter...' (was 'buildinfra-buckcache-prod.internal.tf...')\nParsing buck files: finished in 31.9 sec\nCreating action graph: finished in 26.8 sec\nBuilding: finished in 44.7 sec (100%) 15449/15449 jobs, 0/15449 updated\n  Total time: 01:43.4 min\nMore details at https://www.internalfb.com/intern/buck/build/8df3039d-8d68-47e8-90d9-9c4c85382ef2\nBUILD SUCCEEDED\nTpx test run coordinator for Meta. See https://fburl.com/tpx for details.\nRunning with tpx session id: f17e661d-f6db-4743-82b5-b6201076b11d\nTrace available for this run at /tmp/tpx-20220927-065213.416126-f17e661d-f6db-4743-82b5-b6201076b11d/trace.log\nRemoteExecution session id: reSessionID-f17e661d-f6db-4743-82b5-b6201076b11d-tpx\nStarted reporting to test run: https://www.internalfb.com/intern/testinfra/testrun/5348024685406637\n    ✓ ListingSuccess: caffe2/caffe2/distributed:file_store_handler_ops_test : 2 tests discovered (2.238)\n    ✓ Pass: caffe2/caffe2/distributed:file_store_handler_ops_test - test_set_get (caffe2.caffe2.distributed.file_store_handler_op_test.TestFileStoreHandlerOp) (2.538)\n    ✓ Pass: caffe2/caffe2/distributed:file_store_handler_ops_test - test_set_get (caffe2.caffe2.distributed.file_store_handler_op_test.TestFileStoreHandlerOp) (2.734)\n    ✓ Pass: caffe2/caffe2/distributed:file_store_handler_ops_test - test_set_get (caffe2.caffe2.distributed.file_store_handler_op_test.TestFileStoreHandlerOp) (2.808)\n    ✓ Pass: caffe2/caffe2/distributed:file_store_handler_ops_test - test_set_get (caffe2.caffe2.distributed.file_store_handler_op_test.TestFileStoreHandlerOp) (2.808)\n    ✓ Pass: caffe2/caffe2/distributed:file_store_handler_ops_test - test_set_get (caffe2.caffe2.distributed.file_store_handler_op_test.TestFileStoreHandlerOp) (2.808)\n    ✓ Pass: caffe2/caffe2/distributed:file_store_handler_ops_test - test_set_get (caffe2.caffe2.distributed.file_store_handler_op_test.TestFileStoreHandlerOp) (2.809)\n    ✓ Pass: caffe2/caffe2/distributed:file_store_handler_ops_test - test_set_get (caffe2.caffe2.distributed.file_store_handler_op_test.TestFileStoreHandlerOp) (2.809)\n    ✓ Pass: caffe2/caffe2/distributed:file_store_handler_ops_test - test_set_get (caffe2.caffe2.distributed.file_store_handler_op_test.TestFileStoreHandlerOp) (2.810)\n    ✓ Pass: caffe2/caffe2/distributed:file_store_handler_ops_test - test_set_get (caffe2.caffe2.distributed.file_store_handler_op_test.TestFileStoreHandlerOp) (2.831)\n    ✓ Pass: caffe2/caffe2/distributed:file_store_handler_ops_test - test_set_get (caffe2.caffe2.distributed.file_store_handler_op_test.TestFileStoreHandlerOp) (2.810)\nSummary\n  Pass: 10\n  ListingSuccess: 1\nFinished test run: https://www.internalfb.com/intern/testinfra/testrun/5348024685406637\nIf you need help understanding your runs, please follow the wiki: https://fburl.com/posting_in_tpx_users\n```\n\nReviewed By: motanvfb\n\nDifferential Revision: D39822669\n\n\n\ncc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu", "label": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20distributed", "name": "oncall: distributed", "color": "f7e101", "default": false, "description": "Add this issue/PR to distributed oncall triage queue"}, {"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false, "description": ""}, {"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}]}
{"number": 85699, "title": "Add vector-Jacobian-products for a subset of nvFuser-supported prims; add backward support for nvprims", "time": "2022-09-27T11:37:51Z", "body": "NOTE: Currently, this PR is blocked until https://github.com/pytorch/pytorch/issues/85696 is resolved.\r\n\r\n`vjp_implementations[\"prim_name\"]` is a callable that computes vector-Jacobian products for given cotangents, forward results, and forward inputs. And this function is used in the `torch.autograd.Function.backward` method.\r\n\r\nHere's an example trace for the `torch.sigmoid` function:\r\n```py\r\nimport torch\r\nfrom torch._prims.context import NvfuserPrimsMode, TorchRefsMode\r\nfrom torch.fx.experimental.proxy_tensor import make_fx\r\n\r\na = torch.randn(3, 3, device='cuda', requires_grad=True)\r\ng = torch.ones_like(a)\r\n\r\ndef func(a):\r\n    c = torch.sigmoid(a)\r\n    return torch.autograd.grad(c, a, g)\r\n\r\n# torch._refs.sigmoid trace\r\nwith NvfuserPrimsMode(), TorchRefsMode():\r\n    make_fx(func)(a).graph.print_tabular()\r\n```\r\n```\r\nopcode         name                 target                     args                        kwargs\r\n-------------  -------------------  -------------------------  --------------------------  --------\r\nplaceholder    a_1                  a_1                        ()                          {}\r\ncall_function  neg                  nvprims.neg.default        (a_1,)                      {}\r\ncall_function  detach               aten.detach.default        (neg,)                      {}\r\ncall_function  exp                  nvprims.exp.default        (neg,)                      {}\r\ncall_function  detach_1             aten.detach.default        (exp,)                      {}\r\ncall_function  add                  nvprims.add.default        (1.0, exp)                  {}\r\ncall_function  detach_2             aten.detach.default        (add,)                      {}\r\ncall_function  div                  nvprims.div.default        (1.0, add)                  {}\r\ncall_function  detach_3             aten.detach.default        (div,)                      {}\r\nget_attr       _tensor_constant0    _tensor_constant0          ()                          {}\r\ncall_function  is_same_size         aten.is_same_size.default  (div, _tensor_constant0)    {}\r\ncall_function  detach_4             aten.detach.default        (detach_3,)                 {}\r\ncall_function  detach_5             aten.detach.default        (detach_3,)                 {}\r\nget_attr       _tensor_constant0_1  _tensor_constant0          ()                          {}\r\ncall_function  div_1                nvprims.div.default        (_tensor_constant0_1, add)  {}\r\nget_attr       _tensor_constant0_2  _tensor_constant0          ()                          {}\r\ncall_function  neg_1                nvprims.neg.default        (_tensor_constant0_2,)      {}\r\ncall_function  mul                  nvprims.mul.default        (neg_1, 1.0)                {}\r\ncall_function  pow_1                nvprims.pow.default        (add, -2)                   {}\r\ncall_function  mul_1                nvprims.mul.default        (mul, pow_1)                {}\r\ncall_function  detach_6             aten.detach.default        (detach_2,)                 {}\r\ncall_function  detach_7             aten.detach.default        (detach_2,)                 {}\r\ncall_function  view_of              nvprims.view_of.default    (mul_1,)                    {}\r\ncall_function  view_of_1            nvprims.view_of.default    (mul_1,)                    {}\r\ncall_function  detach_8             aten.detach.default        (detach_1,)                 {}\r\ncall_function  detach_9             aten.detach.default        (detach_1,)                 {}\r\ncall_function  mul_2                nvprims.mul.default        (view_of_1, detach_9)       {}\r\ncall_function  detach_10            aten.detach.default        (detach,)                   {}\r\ncall_function  detach_11            aten.detach.default        (detach,)                   {}\r\ncall_function  neg_2                nvprims.neg.default        (mul_2,)                    {}\r\noutput         output               output                     ((neg_2,),)                 {}\r\n```\r\n\r\n`aten.detach` and `aten.is_same_size` appear in the trace that are called somewhere in the autograd engine code but other than that the trace consists of `nvprims` calls.\r\n\r\nThanks to @rdspring1 for implementing several VJP functions!\r\n\n\ncc @kevinstephano @jjsjann123 @ezyang @mruberry @ngimel @Lezcano @fdrocha @peterbell10", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3997471357, "node_id": "LA_kwDOA-j9z87uRJJ9", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20nvfuser", "name": "module: nvfuser", "color": "9CA380", "default": false, "description": ""}, {"id": 4081012228, "node_id": "LA_kwDOA-j9z87zP04E", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20primTorch", "name": "module: primTorch", "color": "455561", "default": false, "description": ""}]}
{"number": 85690, "title": "[WIP] Fix path issue in torchgen.gen_backend_stubs", "time": "2022-09-27T04:15:25Z", "body": "### Issue Description\r\nThe path in `gen_backend_stubs` was the PyTorch source tree instead of the torchgen package which fails out of PyTorch source tree.\r\nAll the required file are mirrored to the `packaged` director of the torchgen. \r\nFix the path to the correct location for extension to generate the code with `gen_backend_stubs`\r\n\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 85681, "title": "aot_autograd: add assert for functional-only graph", "time": "2022-09-27T00:03:00Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #85844\n* #85875\n* __->__ #85681\n* #82601\n* #82602\n* #82771\n* #83265\n\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 85662, "title": "cpp_backtrace with filename and lineno", "time": "2022-09-26T21:07:36Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #85662\n\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}]}
{"number": 85643, "title": "Convert unbatched inputs to batch with BS=1, and Make MHA symbolically traceable", "time": "2022-09-26T17:54:09Z", "body": "Summary: Make MHA symbolically traceable\n\nTest Plan: sandcastle\n\nDifferential Revision: D39790429\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 85641, "title": "WIP OpTracker", "time": "2022-09-26T17:15:13Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 85623, "title": "Update persons_of_interest.rst for torchvision, torch.hub, torchRL", "time": "2022-09-26T09:42:50Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #85623\n\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}]}
{"number": 85612, "title": "[primTorch] Minor improvements to doc and impl of `gaussian_nll_loss`", "time": "2022-09-25T18:03:13Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* __->__ #85612\n\nFixes #53392.", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4081012228, "node_id": "LA_kwDOA-j9z87zP04E", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20primTorch", "name": "module: primTorch", "color": "455561", "default": false, "description": ""}]}
{"number": 85576, "title": "Enabling Transformer fast path for not batch_first", "time": "2022-09-23T22:27:51Z", "body": "Summary: The fast path for the `forward()` method in `MultiheadAttention` only accepted `batch_first = True`. This diff enables fast path for `batch_first=False` as well.\n\nTest Plan:\nAdded unit test for fast path for both values of `batch_first` producing identical outputs.\n\n`buck test mode/dev-nosan //caffe2/torch/nn/modules/tests:test_activation` passed\n\nReviewed By: mikekgfb\n\nDifferential Revision: D39669982\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}]}
{"number": 85543, "title": "CSAN support for new dispatch hook", "time": "2022-09-23T14:17:21Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #85543\n* #85469\n* #85468\n* #85160\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 85537, "title": "Remove unused code from old Caffe2 iOS library", "time": "2022-09-23T08:26:16Z", "body": "I believe this is all dead code. The caffe2/mobile directory, which is a parent of the directory removed in this PR, is only added to build when INTERN_BUILD_MOBILE is false. But the contents of this directory are only linked when IOS is true. There is no build where both these conditions are met (INTERN_BUILD_MOBILE is set to IOS or ANDROID)\r\n\r\nI believe this is code that used to be linked into the Caffe2 ios library, which no longer exists.\r\n", "label": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false, "description": ""}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4443556792, "node_id": "LA_kwDOA-j9z88AAAABCNtLuA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/skip-pr-sanity-checks", "name": "skip-pr-sanity-checks", "color": "BED7D5", "default": false, "description": ""}]}
{"number": 85536, "title": "[skip ci] nested_tensor pybind constructor", "time": "2022-09-23T07:26:04Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* __->__ #85536\n\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 85530, "title": "Remove unused build option INTERN_BUILD_ATEN_OPS", "time": "2022-09-23T06:38:38Z", "body": "This option was set to false in the caffe2 mobile runtime, however that build no longer exists and this option is set to true in all remaining builds. It should be safe to remove this option and associated conditionals and simplify out buildfiles a bit.", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 85501, "title": "Enrich error context for package exporter", "time": "2022-09-22T20:25:52Z", "body": "Summary:\nThe exception now include module name failing the check:\n\n  PytorchModelPackagerException:\n  * Module is a C extension module. torch.package supports Python modules only.\n    _swigfaiss_gpu\n      Context: module_name: _swigfaiss_gpu filename: /mnt/xarfuse/uid-233664/c36dd00e-seed-nspid4026531836_cgpid57478-ns-4026531840/_swigfaiss_gpu.so\n    _swigfaiss\n      Context: module_name: _swigfaiss filename: /mnt/xarfuse/uid-233664/c36dd00e-seed-nspid4026531836_cgpid57478-ns-4026531840/_swigfaiss.so\n\nTest Plan: N2531121\n\nDifferential Revision: D39743995\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769202179, "node_id": "LA_kwDOA-j9z87gqXYD", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20package/deploy", "name": "release notes: package/deploy", "color": "EC030C", "default": false, "description": "release notes category"}]}
{"number": 85496, "title": "[FSDP] Refactor comm hooks directory", "time": "2022-09-22T19:29:57Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #85496\n* #85495\n* #86343\n* #86337\n* #86336\n* #86331\n\r\nMove FSDP comm hooks into FSDP folder, expose the reduced precision\r\nhooks in torch.distributed.fsdp.default_comm_hooks namespace. Also a few small refactors.\r\n\r\nDifferential Revision: [D39743042](https://our.internmc.facebook.com/intern/diff/D39743042/)\n\ncc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu", "label": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20distributed", "name": "oncall: distributed", "color": "f7e101", "default": false, "description": "Add this issue/PR to distributed oncall triage queue"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769221312, "node_id": "LA_kwDOA-j9z87gqcDA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(fsdp)", "name": "release notes: distributed (fsdp)", "color": "C1F551", "default": false, "description": "release notes category"}, {"id": 4443556792, "node_id": "LA_kwDOA-j9z88AAAABCNtLuA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/skip-pr-sanity-checks", "name": "skip-pr-sanity-checks", "color": "BED7D5", "default": false, "description": ""}]}
{"number": 85495, "title": "[FSDP][BE][Docs] Expose helper classes", "time": "2022-09-22T19:17:36Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #85495\n\nPer title\n\nDifferential Revision: [D39742664](https://our.internmc.facebook.com/intern/diff/D39742664/)", "label": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20distributed", "name": "oncall: distributed", "color": "f7e101", "default": false, "description": "Add this issue/PR to distributed oncall triage queue"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3769221312, "node_id": "LA_kwDOA-j9z87gqcDA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(fsdp)", "name": "release notes: distributed (fsdp)", "color": "C1F551", "default": false, "description": "release notes category"}]}
{"number": 85489, "title": "[WIP] Testing rpath fix", "time": "2022-09-22T18:23:09Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3212089274, "node_id": "MDU6TGFiZWwzMjEyMDg5Mjc0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/with-ssh", "name": "with-ssh", "color": "127AF1", "default": false, "description": ""}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4077623621, "node_id": "LA_kwDOA-j9z87zC5lF", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/binaries_wheel", "name": "ciflow/binaries_wheel", "color": "AD47B4", "default": false, "description": "Trigger binary build and upload jobs for wheel on the PR"}]}
{"number": 85488, "title": "[ONNX] Refactor symbolic helper to use the dispatcher", "time": "2022-09-22T18:03:05Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #85488\n* #85487\n* #85486\n* #85350\n* #85349\n\n", "label": [{"id": 693805995, "node_id": "MDU6TGFiZWw2OTM4MDU5OTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20onnx", "name": "module: onnx", "color": "f7e101", "default": false, "description": "Related to torch.onnx"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3768821874, "node_id": "LA_kwDOA-j9z87go6hy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20onnx", "name": "release notes: onnx", "color": "B4EB93", "default": false, "description": "torch.onnx related changes that should show up in the release notes"}]}
{"number": 85487, "title": "[ONNX] Use symbolic dispatcher to dispatch function calls", "time": "2022-09-22T18:03:02Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #85488\n* __->__ #85487\n* #85486\n* #85350\n* #85349\n\n", "label": [{"id": 693805995, "node_id": "MDU6TGFiZWw2OTM4MDU5OTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20onnx", "name": "module: onnx", "color": "f7e101", "default": false, "description": "Related to torch.onnx"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768821874, "node_id": "LA_kwDOA-j9z87go6hy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20onnx", "name": "release notes: onnx", "color": "B4EB93", "default": false, "description": "torch.onnx related changes that should show up in the release notes"}]}
{"number": 85486, "title": "[ONNX] Implement symbolic dispatcher", "time": "2022-09-22T18:02:58Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #85488\n* #85487\n* __->__ #85486\n* #85350\n* #85349\n\r\nDispatcher for #83787 . A pyi file will be created for external use in a separate PR.", "label": [{"id": 693805995, "node_id": "MDU6TGFiZWw2OTM4MDU5OTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20onnx", "name": "module: onnx", "color": "f7e101", "default": false, "description": "Related to torch.onnx"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1885770657, "node_id": "MDU6TGFiZWwxODg1NzcwNjU3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/needs%20design", "name": "needs design", "color": "ed84e2", "default": false, "description": ""}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3768821874, "node_id": "LA_kwDOA-j9z87go6hy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20onnx", "name": "release notes: onnx", "color": "B4EB93", "default": false, "description": "torch.onnx related changes that should show up in the release notes"}]}
{"number": 85472, "title": "[PolishTypo] Alisa->Alias, indivually->individually, jEverything->Everything", "time": "2022-09-22T14:07:24Z", "body": "Polish comment typo, `Alisa->Alias`, `indivually->individually`, `jEverything->Everything`", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 85469, "title": "Add CSAN dispatch key", "time": "2022-09-22T12:45:42Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #85543\n* __->__ #85469\n* #85468\n* #85160\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 85468, "title": "Add Python hook for kernel launch", "time": "2022-09-22T12:45:03Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #85543\n* #85469\n* __->__ #85468\n* #85160\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 85441, "title": "Use fallback approach for nested softmax (+backward) and bmm", "time": "2022-09-21T21:45:05Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* __->__ #85441\n\r\nFollow up would be to use masked_softmax ", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 85433, "title": "Add dim checks for internal `embedding_bag` functions", "time": "2022-09-21T20:17:02Z", "body": "Fixes #85213\r\n", "label": [{"id": 679955625, "node_id": "MDU6TGFiZWw2Nzk5NTU2MjU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20crash", "name": "module: crash", "color": "f7e101", "default": false, "description": "Problem manifests as a hard crash, as opposed to a RuntimeError"}, {"id": 1300891057, "node_id": "MDU6TGFiZWwxMzAwODkxMDU3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20error%20checking", "name": "module: error checking", "color": "f7e101", "default": false, "description": "Bugs related to incorrect/lacking error checking"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2409470727, "node_id": "MDU6TGFiZWwyNDA5NDcwNzI3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20embedding", "name": "module: embedding", "color": "f7e101", "default": false, "description": ""}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3769207236, "node_id": "LA_kwDOA-j9z87gqYnE", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20nn", "name": "release notes: nn", "color": "29E07F", "default": false, "description": "release notes category"}, {"id": 3773061952, "node_id": "LA_kwDOA-j9z87g5FtA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20bug%20fixes", "name": "topic: bug fixes", "color": "A44870", "default": false, "description": "topic category"}]}
{"number": 85424, "title": "Ensure batch size lower than datasize length", "time": "2022-09-21T18:10:44Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769215019, "node_id": "LA_kwDOA-j9z87gqagr", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20dataloader", "name": "release notes: dataloader", "color": "23169D", "default": false, "description": "release notes category"}]}
{"number": 85398, "title": "Fix thread-allocation in `_vec_log_softmax_lastdim`", "time": "2022-09-21T09:58:06Z", "body": "## Problem history\r\n\r\nThere seems to always have been a bug in `_vec_log_softmax_lastdim `.\r\nIn particular, there were two issues with it -\r\n\r\n#### Bug 1\r\n Before AVX512 support was added, `CHUNK_SIZE` had been heuristically chosen in `_vec_log_softmax_lastdim`:\r\n `CHUNK_SIZE = (128 / sizeof(scalar_t)) * Vec::size();`\r\n \r\nIt was  `256` for float32, bfloat16, and float16.\r\nWhen AVX512 support was added, `CHUNK_SIZE` became `512`.\r\n\r\nThe rationale behind determining `CHUNK_SIZE` has not been described, and seems flawed, since the number of OpenMP threads used currently depends upon it.\r\n\r\n#### Bug 2\r\n`grain_size` had been defined as `internal::GRAIN_SIZE / (16 * dim_size * CHUNK_SIZE)`\r\nSo, `grain_size` was usually 0, as it was `8 / (dim_size)`, so, it's always replaced by `CHUNK_SIZE`, viz. 256.\r\nSince `256` was always the `grain_size` for `at::parallel_for`, few threads were used in certain cases.\r\n\r\n#### Problem caused by bugs\r\nWith `outer_size` of say, 700, only 3 threads would have been used with AVX2, irrespective of the value of `dim_size`!\r\nWhen AVX512 support was added, since `CHUNK_SIZE` became `512`, only 2 threads were used if `outer_dim` was 700.\r\nIn the Transformers training example, `log_softmax` was computed on the last dim of a tensor of shape `(700, 23258)`.\r\nAVX512 thus appeared to be quite slower, cloaking the actual issue that even AVX2 performance for the kernel was quite poor due to inefficient work distribution amongst OpenMP threads.\r\n\r\n\r\n## Solution\r\nDistribute work more efficiently, which would result in higher performance for both AVX2 & AVX512 than now,\r\nand fixes the regression observed with AVX512 (AVX512 kernel would now be faster than its AVX2 counterpart).\r\n\r\n\r\n## Benchmarks\r\n\r\n##### Machine-config:\r\nIntel(R) Xeon(R) Platinum 8371HC CPU (Cooper Lake)\r\nOne socket of 26 physical cores was used.\r\nIntel OpenMP & tcmalloc were preloaded.\r\n\r\nExample of a command to run benchmark:\r\n`ATEN_CPU_CAPABILITY=avx512 KMP_AFFINITY=granularity=fine,verbose,compact,1,0 KMP_BLOCKTIME=1 KMP_SETTINGS=1 MKL_NUM_THREADS=28 OMP_NUM_THREADS=28 python3.8 -m pt.softmax_test --test_name LogSoftmax_N1024_seq_len23258_dim1_cpu`\r\n\r\nBenchmark | Old implementation time (us) | New implementation time (us) | Speedup\r\n-- | -- | -- | --\r\nLogSoftmax_N1024_seq_len23258_dim1_cpu AVX2 | 10596.908 | 2695.066 | 3.931966045\r\nLogSoftmax_N1024_seq_len23258_dim1_cpu  AVX512 | 18292.928 | 2623.015 | 6.974008155\r\nLogSoftmax_N700_seq_len23258_dim1_cpu  AVX2 | 10599.014 | 1808.043 | 5.862147084\r\nLogSoftmax_N700_seq_len23258_dim1_cpu  AVX512 | 11872.541 | 1699.102 | 6.987538712\r\n\r\n\r\ncc @VitalyFedyunin @jgong5 @mingfeima @XiaobingSuper @ashokei @jingxu10", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 85394, "title": "[NNC] enable bf16 for mkldnn prepack conv2d", "time": "2022-09-21T07:08:12Z", "body": "## Pitch\r\nEnable bf16 support for mkldnn prepack conv2d in NNC.\r\n\r\n## Performance\r\nThe BF16 conv performance has been evaluated in https://github.com/pytorch/pytorch/pull/82705.\r\n\r\n## Additional context\r\nThis PR depends on BF16 support in NNC: https://github.com/pytorch/pytorch/pull/84041.\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2510754463, "node_id": "MDU6TGFiZWwyNTEwNzU0NDYz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/NNC", "name": "NNC", "color": "e5678d", "default": false, "description": ""}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 85350, "title": "[ONNX] Use `symbolic_function_group` to dispatch function calls", "time": "2022-09-20T17:25:32Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #85488\n* #85487\n* #85486\n* __->__ #85350\n* #85349\n\r\nThis PR implements makes SymbolicFunctionGroup callable so we can dispatch right from it and avoid a get call and a None check.\r\n\r\nThis is possible because we (1) included the opset version in the graph context and (2) used a decorator to handel the legacy functions taking an additional SymbolicContext argument so all functions expect the same first argument (GraphContext) and we no need to inspect function annotations.\r\n\r\nThe next PR will create a general dispatching mechanism to replace conditionals in symbolic helper\r\n\r\nAlso changed\r\n\r\n- `min_supported()` now returns an optional int to handle cases when no functions exist in the function group. Updated _onnx_supported_ops to fix mypy errors because of this change.", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768821874, "node_id": "LA_kwDOA-j9z87go6hy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20onnx", "name": "release notes: onnx", "color": "B4EB93", "default": false, "description": "torch.onnx related changes that should show up in the release notes"}]}
{"number": 85324, "title": "Add support for map_location for backends", "time": "2022-09-20T05:29:48Z", "body": "In torch.load, map_location variable is not handled properly in case of other backend device like HPU.\r\n\r\nA new class _Mapper is created in _utils.py which stores the information of the map_location variable which is set by serialization.py. This class is used to get the correct device to be used for the tensors based on the set map_location variable.\r\n\r\nSigned-off-by: Sanju C Sudhakaran <scsudhakaran@habana.ai>\r\nSigned-off-by: Jeeja <jeejakp@habana.ai>\r\n\r\nFixes #ISSUE_NUMBER\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 85312, "title": "[ONNX] Quantization handler", "time": "2022-09-19T23:07:08Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #85312\n* #86130\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768821874, "node_id": "LA_kwDOA-j9z87go6hy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20onnx", "name": "release notes: onnx", "color": "B4EB93", "default": false, "description": "torch.onnx related changes that should show up in the release notes"}]}
{"number": 85310, "title": "Python stack tracing OD flow (part 2)", "time": "2022-09-19T22:45:03Z", "body": "Summary: set flag\n\nTest Plan:\nMUST pass CI tests since\n1) submodule updated (part 1)\n2) flag is set (part 2)\n\nReviewed By: robieta\n\nDifferential Revision: D39177683\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}]}
{"number": 85291, "title": "[vulkan] Fix timestamp queries to account for timestamp period and add reporting interface", "time": "2022-09-19T18:52:37Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #85291\n* #84973\n\nExample output: P528693117\n\nDifferential Revision: [D39446106](https://our.internmc.facebook.com/intern/diff/D39446106/)", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769206412, "node_id": "LA_kwDOA-j9z87gqYaM", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20vulkan", "name": "release notes: vulkan", "color": "23DD30", "default": false, "description": "release notes category"}]}
{"number": 85276, "title": "Fixed regression", "time": "2022-09-19T17:12:04Z", "body": "Summary: Fake tensors break many model import.\n\nTest Plan: Ran all the tests that were broken by the switch to fake tensors.\n\nDifferential Revision: D39623289\n\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 85272, "title": "OpInfo tests for torch.bmm: Strided, COO, CSR, CSC layouts.", "time": "2022-09-19T16:16:37Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #84843\n* __->__ #85272\n* #85271\n* #85270\n* #85269\n* #85268\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 85271, "title": "Add utility function torch.sparse._transpose_copy", "time": "2022-09-19T16:16:33Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #84843\n* #85272\n* __->__ #85271\n* #85270\n* #85269\n* #85268\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769217376, "node_id": "LA_kwDOA-j9z87gqbFg", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20sparse", "name": "release notes: sparse", "color": "2A7626", "default": false, "description": "release notes category"}]}
{"number": 85270, "title": "Add sample/reference inputs functions for bmm: strided and CSR/CSR layouts.", "time": "2022-09-19T16:16:28Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #84843\n* #85272\n* #85271\n* __->__ #85270\n* #85269\n* #85268\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 85269, "title": "Add reference_inputs_sparse_coo/csr/csc_func attributes to OpInfo", "time": "2022-09-19T16:16:24Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #84843\n* #85272\n* #85271\n* #85270\n* __->__ #85269\n* #85268\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 85268, "title": "Add utility function torch.sparse._to_layout", "time": "2022-09-19T16:16:19Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #84843\n* #85272\n* #85271\n* #85270\n* #85269\n* __->__ #85268\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769217376, "node_id": "LA_kwDOA-j9z87gqbFg", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20sparse", "name": "release notes: sparse", "color": "2A7626", "default": false, "description": "release notes category"}]}
{"number": 85202, "title": "[opinfo] conv3d", "time": "2022-09-17T10:28:52Z", "body": "Reference: #74613\r\n\r\nChunks covered in this PR:\r\n- Adds OpInfo for conv3d\r\n- Adds ErrorInputs for conv.{1, 2, 3}d\r\n\r\ncc @mruberry @kshitij12345!", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}]}
{"number": 85197, "title": "Csl/api2", "time": "2022-09-16T23:53:13Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 85174, "title": "embedding_backward", "time": "2022-09-16T19:21:51Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #85174\n* #85172\n* #85196\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 85173, "title": "allow compositeImplicitDispatch on symint kernels", "time": "2022-09-16T19:21:43Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #85196\n* #85174\n* __->__ #85173\n* #85172\n* #85171\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 85172, "title": "symintify embedding_dense_backward", "time": "2022-09-16T19:21:37Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #85174\n* __->__ #85172\n* #85196\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 85160, "title": "Change default keysets to be static and atomic", "time": "2022-09-16T16:40:09Z", "body": "This changes the default keysets to be mutable and global, as discussed. I'm using an atomic instead of locks for performance. \r\n\r\nThe operations on the keysets should be mostly thread-safe, however it is possible in theory that a thread can modify the keyset after another thread loads but before it stores its keyset, resulting in an overwrite. I'm relying on the keysets not being modified often and the callers synchronizing with each other. If needed, I can look into improving the safety.\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #85543\n* #85469\n* #85468\n* __->__ #85160\n\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 85119, "title": "[do not merge] Testing for dispatch error (update)", "time": "2022-09-15T22:22:00Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* **#85119 [do not merge] Testing for dispatch error (update)**\r\n* #85115 [do not merge] Testing for dispatch error\r\n\r\nThis PR works (at the cost of TensorList -> Vector conversion)\n\ncc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @kwen2501 @awgu", "label": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20distributed", "name": "oncall: distributed", "color": "f7e101", "default": false, "description": "Add this issue/PR to distributed oncall triage queue"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769218061, "node_id": "LA_kwDOA-j9z87gqbQN", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(c10d)", "name": "release notes: distributed (c10d)", "color": "EE86D4", "default": false, "description": "release notes category"}]}
{"number": 85115, "title": "[do not merge] Testing for dispatch error", "time": "2022-09-15T21:59:31Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* #85119 [do not merge] Testing for dispatch error (update)\r\n* **#85115 [do not merge] Testing for dispatch error**\r\n\r\n1. `ghstack checkout https://github.com/pytorch/pytorch/pull/85115`\r\n2. `python setup.py develop`\r\n3.  run the following script\r\n```python\r\nimport torch\r\nimport os\r\nimport torch.distributed as dist\r\n\r\nos.environ[\"MASTER_ADDR\"] = \"localhost\"\r\nos.environ[\"MASTER_PORT\"] = \"29500\"\r\nos.environ[\"RANK\"] = \"0\"\r\nos.environ[\"WORLD_SIZE\"] = \"1\"\r\nos.environ[\"TORCH_SHOW_CPP_STACKTRACES\"] = \"1\"\r\n\r\nif __name__ == \"__main__\":\r\n    dist.init_process_group(\"gloo\")\r\n    t = torch.zeros(2)\r\n    t_list = [torch.zeros(2) for _ in range(1)]\r\n    print(f\"{t_list=}\")\r\n    dist.all_gather(t_list, t)\r\n\r\n```\r\n\r\nErrors with\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"produce_dispatch_error_example.py\", line 16, in <module>\r\n    dist.all_gather(t_list, t)\r\n  File \"/fsx/users/howardhuang/work/pytorch/torch/distributed/distributed_c10d.py\", line 2188, in all_gather\r\n    work = default_pg.allgather([tensor_list], [tensor])\r\nNotImplementedError: There were no tensor arguments to this function (e.g., you passed an empty list of Tensors), but no fallback function is registered for schema c10d::allgather_.  This usually means that this function requires a non-empty list of Tensors, or that you (the operator writer) forgot to register a fallback function.  Available functions are [CPU, BackendSelect, Python, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, Tracer, AutocastCPU, AutocastCUDA, Batched, VmapMode, PythonTLSSnapshot, PythonDispatcher].\r\n\r\nCPU: registered at ../torch/csrc/distributed/c10d/Ops.cpp:414 [kernel]\r\n...\r\n```\r\n\r\nI believe the error is happening when the op gets called here: https://github.com/pytorch/pytorch/pull/85115/files#diff-a0bb1747a4de5846bd43d0ec684e4650298648a083243bcdaf590e2b1e747873R264-R265, but I am not sure how to debug further.\r\n\r\nInterestingly, if I change the definition in allgather of `input_tensors` from `const std::vector<at::Tensor>& input_tensors` -> `at::TensorList` as shown in https://github.com/pytorch/pytorch/pull/85119, then this script does not error out.\n\ncc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @kwen2501 @awgu", "label": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20distributed", "name": "oncall: distributed", "color": "f7e101", "default": false, "description": "Add this issue/PR to distributed oncall triage queue"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769218061, "node_id": "LA_kwDOA-j9z87gqbQN", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(c10d)", "name": "release notes: distributed (c10d)", "color": "EE86D4", "default": false, "description": "release notes category"}]}
{"number": 85071, "title": "Pin XLA for testing", "time": "2022-09-15T04:48:56Z", "body": "Pin XLA for testing", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 85069, "title": "Todo-Clang5-required-solved", "time": "2022-09-15T03:13:50Z", "body": "Clang5 is required for compilation. Removed the Todo comment.", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 85043, "title": "make autocast a per-backend dispatch key, add alias key", "time": "2022-09-14T21:05:51Z", "body": "More backends are trying to add autocast support (see ), so this PR attempts to make autocast a per-backend dispatch key, saving us ~3 key slots and also rationalizing our model a bit.\r\n\r\nI ended up making `Autocast` an alias key for AutocastCUDA + AutocastXLA (see the note).\r\n\r\nAlso, this is technically a bit BC breaking - see discussion in the comments.\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack):\r\n* __->__ #85043\r\n* #85029\r\n\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}]}
{"number": 85040, "title": "Add support for the DirectML Backend", "time": "2022-09-14T19:57:53Z", "body": "Add support for the DirectML Backend\r\n\r\nSummary:\r\nThis PR implements the necessary hooks/stubs/enums/etc for complete DirectML Backend integration. The actual backend implementation is currently private.\r\n\r\nDirect Machine Learning (DirectML) is a low-level API for machine learning. DirectML is supported by all DirectX 12-compatible hardware and is layered on top of the Compute Driver Model which is the common abstraction layer to provide consistent, reliable, and secure ML execution across all Windows hardware devices and vendors.\r\n\r\nWe have been working on this at Microsoft for the last few months, and are finally ready to contribute the PyTorch core changes upstream (nothing major or exciting, just the usual boilerplate for adding new backends).\r\n\r\nThe DirectML Backend will allow us to ferry torch ops into granular DirectML powered kernels that execute against any DX12-capacble GPU device.\r\n\r\n[RFC-0028: PyTorch-DirectML by Adele101 · Pull Request #46 · pytorch/rfcs (github.com)](https://github.com/pytorch/rfcs/pull/46)\r\n\r\n[PyTorch-DirectML RFC · Issue #84721 · pytorch/pytorch (github.com)](https://github.com/pytorch/pytorch/issues/84721)\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 84977, "title": "Update xla maintainer", "time": "2022-09-14T01:54:25Z", "body": null, "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 84957, "title": "[primTorch] Add new test modes for refs and aliases", "time": "2022-09-13T21:37:06Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* __->__ #84957\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4081012228, "node_id": "LA_kwDOA-j9z87zP04E", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20primTorch", "name": "module: primTorch", "color": "455561", "default": false, "description": ""}]}
{"number": 84953, "title": "Removed now unnecessary extra compile arg", "time": "2022-09-13T20:16:28Z", "body": "The `-Wno-missing-braces` argument was added in https://github.com/pytorch/pytorch/commit/67612cba09605b3a8abbc7c528def64d2cdba21c. However, since then, the issue with Clang appears to have been resolved (https://bugs.llvm.org/show_bug.cgi?id=21629.), making this extra complier argument unnecessary.\r\n\r\nNote: the fix is only implemented in Clang version 6.0.0 and greater.", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 84929, "title": "mul(sparse, sparse): extend autograd support to cases with broadcasted dense dims.", "time": "2022-09-13T14:32:00Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #84929\n* #83428\n* #83427\n\n", "label": [{"id": 679954154, "node_id": "MDU6TGFiZWw2Nzk5NTQxNTQ=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20sparse", "name": "module: sparse", "color": "f7e101", "default": false, "description": "Related to torch.sparse"}, {"id": 1076922545, "node_id": "MDU6TGFiZWwxMDc2OTIyNTQ1", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20autograd", "name": "module: autograd", "color": "f7e101", "default": false, "description": "Related to torch.autograd, and the autograd engine in general"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 84868, "title": "make _shape_as_tensor composite compliant", "time": "2022-09-12T16:37:15Z", "body": "`_shape_as_tensor` wasn't composite compliant, because it would call `at::tensor()` (which would muck directly with the data pointer).\r\n\r\nI fixed it by making it a primitive w.r.t. tracing (by making it CompositeExplicitAutograd). It also seems like a factory-function, so I gave it the same treatment as `*_like` ops with autograd. I confirmed that this preserves autograd behavior:\r\n```\r\n>>> a = torch.ones(2, 3, 4, requires_grad=True)\r\n>>> b = torch.ops.aten._shape_as_tensor(a)\r\n>>> b.requires_grad\r\nFalse\r\n```\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack):\n* __->__ #84868\n\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 84855, "title": "[TensorImpl] Make set_storage_keep_dtype virtual", "time": "2022-09-12T08:48:21Z", "body": "Summary:\r\n\r\nFor lazy tensor impl, set_storage_keep_dtype needs to be virtual to take care of dummy storage created for frontend tensors. For lazy tensor impl, simple move of storage doesn't work because it may not have been evaluated and for other book keeping that TensorImpl need to do, this function should be virtual.\r\n\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 84843, "title": "Support different NSE in batches of CSR and CSC tensors", "time": "2022-09-11T22:04:28Z", "body": "This PR enables batched CSR/CSC tensors that batches may have different NSE counts.\r\n\r\nFor instance, with the current master we have\r\n```python\r\n>>> a = torch.tensor([[[1, 2], [3, 4]], [[0, 12], [21, 0]]])\r\n>>> a.to_sparse_csr()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nRuntimeError: Expect the same number of specified elements per batch.\r\n```\r\nbecause the NSE of the first and second batches are different, 4 and 2, respectively.\r\n\r\nThis PR implements a strided-to-sparse-CSR/CSC conversion algorithm that supports CSR/CSC batches with different NSE counts. For instance:\r\n```python\r\n>>> a = torch.tensor([[[1, 2], [3, 4]], [[0, 12], [21, 0]]])\r\n>>> b = a.to_sparse_csr()\r\n>>> b\r\ntensor(crow_indices=tensor([[0, 2, 4],\r\n                            [0, 1, 2]]),\r\n       col_indices=tensor([[0, 1, 0, 1],\r\n                           [1, 0, 0, 0]]),\r\n       values=tensor([[ 1,  2,  3,  4],\r\n                      [12, 21,  0,  0]]), size=(2, 2, 2), nnz=4,\r\n       layout=torch.sparse_csr)\r\n>>> b[0]\r\ntensor(crow_indices=tensor([0, 2, 4]),\r\n       col_indices=tensor([0, 1, 0, 1]),\r\n       values=tensor([1, 2, 3, 4]), size=(2, 2), nnz=4,\r\n       layout=torch.sparse_csr)\r\n>>> b[1]\r\ntensor(crow_indices=tensor([0, 1, 2]),\r\n       col_indices=tensor([1, 0]),\r\n       values=tensor([12, 21]), size=(2, 2), nnz=2, layout=torch.sparse_csr)\r\n```\r\nthat is, if the NSE of a batch is smaller than the maximum NSE over all batches, the corresponding rows in `col_indices`/`values` are padded with zeros as placeholders. Algorithms on batched CSR/CSC tensors must not access the padded parts of these tensors, that is, the algorithms should use the last element of the corresponding `crow_indices` row as the NSE value rather than the value of `.values().shape[0]` that holds the maximum NSE over all batches.\r\n\r\nPerformance-wise, the strided-to-sparse-CSR/CSC conversion algorithms in master and in this PR, are roughly equivalent:\r\n```python\r\n# master branch:\r\nn [2]: a = torch.rand(10, 10, 1000, 1000)\r\n\r\nIn [3]: a = torch.where(a==0, 0.1, a)  # required for master, optional for the PR\r\n\r\nIn [4]: %timeit a.to_sparse_csr()\r\n2.25 s ± 9.84 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\r\n\r\nIn [5]: a_cuda = a.cuda()\r\n\r\nIn [6]: %timeit a_cuda.to_sparse_csr()\r\n55.2 ms ± 6.95 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\r\n```\r\n```python\r\n# this PR\r\nIn [2]: a = torch.rand(10, 10, 1000, 1000)\r\n\r\nIn [3]: a = torch.where(a==0, 0.1, a)  # required for master, optional for the PR\r\n\r\nIn [4]: %timeit a.to_sparse_csr()\r\n2.12 s ± 2.13 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\r\n\r\nIn [5]: a_cuda = a.cuda()\r\n\r\nIn [6]: %timeit a_cuda.to_sparse_csr(); torch.cuda.synchronize()\r\n47.2 ms ± 10.4 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\r\n```\r\nThe performance of `to_sparse_csr()` on CUDA tensors increased by 15% with this PR.\r\n \r\nA strided-to-sparse-BSR/BSC conversion with variable NSE support will be implemented as a follow-up.\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #84843\n* #85272\n* #85271\n* #85270\n* #85269\n* #85268\n\r\n\n\ncc @nikitaved @cpuhrsch @amjames @bhosmer", "label": [{"id": 679954154, "node_id": "MDU6TGFiZWw2Nzk5NTQxNTQ=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20sparse", "name": "module: sparse", "color": "f7e101", "default": false, "description": "Related to torch.sparse"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3769217376, "node_id": "LA_kwDOA-j9z87gqbFg", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20sparse", "name": "release notes: sparse", "color": "2A7626", "default": false, "description": "release notes category"}, {"id": 4888296239, "node_id": "LA_kwDOA-j9z88AAAABI117Lw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/no-stale", "name": "no-stale", "color": "782198", "default": false, "description": ""}]}
{"number": 84801, "title": "Map views => reshapes in aotautograd", "time": "2022-09-10T01:06:46Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #84801\n\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769204372, "node_id": "LA_kwDOA-j9z87gqX6U", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20releng", "name": "release notes: releng", "color": "EED552", "default": false, "description": "release notes category"}]}
{"number": 84791, "title": "ns for fx: fix comparison for quant model to reference model", "time": "2022-09-09T23:39:38Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#84791 ns for fx: fix comparison for quant model to reference model**\n\nSummary:\n\nWhile debugging PTQ accuracy issues with a model quantized for `fbgemm` where\nthe output of reference model does not match closely the output of quantized\nmodel, I noticed that Numeric Suite for FX currently does not match patterns\nin quantized models properly. For example if we have\n\n```\nx0 -> dq -> op -> q -> ...\n```\n\nBefore this PR, NS would insert a logger after the op inside the reference pattern:\n\n```\nx0 -> dq -> op -> log0 -> q -> ...\n```\n\nThis is not useful because to match quantized ops, we want to insert a logger\nafter the q.  This PR changes the matching to do\n\n```\nx0 -> dq -> op -> q -> log0 -> ...\n```\n\ninstead.\n\nNote: this only makes sense for op-based backends.  How this applies to TensorRT\nis out of scope for this PR - the entire architecture of NS may have to be changed\nto support that.\n\nTest plan:\n\n```\npython test/test_quantization.py -k test_simple_mod_reference\npython test/test_quantization.py -k test_simple_fun_reference\n```\n\ncc @ezyang @SherlockNoMad @soumith @EikanWang @jgong5 @wenzhe-nrv", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}, {"id": 4347715449, "node_id": "LA_kwDOA-j9z88AAAABAyTfeQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fx", "name": "fx", "color": "ededed", "default": false, "description": null}]}
{"number": 84789, "title": "[ONNX] Use optional op to keep None in results for ONNX internal tests", "time": "2022-09-09T22:57:14Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #84789\n* #83186\n* #90337\n\r\nAll this time, PyTorch and ONNX has different strategy for None in output. And in internal test, we flatten the torch outputs to see if the rest of them matched. However, this doesn't work anymore in scripting after Optional node is introduced, since some of None would be kept.\r\n\r\n#83184 forces script module to keep all Nones from Pytorch, but in ONNX, the model only keeps the ones generated with Optional node, and deletes those meaningless None.\r\n\r\nThis PR uses Optional node to keep those meaningless None in output as well, so when it comes to script module result comparison, Pytorch and ONNX should have the same amount of Nones.", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 693805995, "node_id": "MDU6TGFiZWw2OTM4MDU5OTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20onnx", "name": "module: onnx", "color": "f7e101", "default": false, "description": "Related to torch.onnx"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768821874, "node_id": "LA_kwDOA-j9z87go6hy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20onnx", "name": "release notes: onnx", "color": "B4EB93", "default": false, "description": "torch.onnx related changes that should show up in the release notes"}]}
{"number": 84759, "title": "Integrated cuSPARSELt", "time": "2022-09-09T17:33:29Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #86299\n* #85835\n* #83989\n* __->__ #84759\n\nSummary:\nThis PR integrates cuSPARSELt which will be used to implement sparse\ncuda linear ops\n\nDifferential Revision: [D39391953](https://our.internmc.facebook.com/intern/diff/D39391953)", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 84739, "title": "[MPS] native sgn implementation", "time": "2022-09-09T03:29:14Z", "body": "Implement sgn_out operator using metal\r\n\r\n~I couldn't build pytorch on my arm macbook, so I have had no way to actually test this.~\r\n\r\nUsage example:\r\n```\r\n>>> import torch\r\n>>> t = torch.tensor([3+4j, 7-24j, 0, 1+2j], device=\"mps\")\r\n>>> t.sgn()\r\ntensor([0.6000+0.8000j, 0.2800-0.9600j, 0.0000+0.0000j, 0.4472+0.8944j])\r\n```", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 4164781683, "node_id": "LA_kwDOA-j9z874PYZz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20mps", "name": "release notes: mps", "color": "1d76db", "default": false, "description": "Release notes category"}, {"id": 4417837777, "node_id": "LA_kwDOA-j9z88AAAABB1La0Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/mps", "name": "ciflow/mps", "color": "02EDA7", "default": false, "description": "Run MPS tests (subset of trunk)"}]}
{"number": 84738, "title": "[hotfix] fix build error from FuncTorch dispatch key", "time": "2022-09-09T03:23:08Z", "body": "Test Plan: CI ...?\n\nReviewed By: ezyang\n\nDifferential Revision: D39374319\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 84732, "title": "Update flake8 versions", "time": "2022-09-09T00:46:04Z", "body": "Bring open source flake8 versions closer to Meta-internal flake8.\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 84718, "title": "add alias to generate tensor with random uniform distribution", "time": "2022-09-08T21:28:17Z", "body": "Fixes issue #67321 \r\n\r\nAdded alias `torch.random.uniform(low, high, size) for initiating a uniformly distributed tensor.\r\n\r\ncc @pbelevich\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 84703, "title": "[WIP] Find Magma Test. Test only not merge", "time": "2022-09-08T15:30:59Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 4077622962, "node_id": "LA_kwDOA-j9z87zC5ay", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/binaries_conda", "name": "ciflow/binaries_conda", "color": "3FC2AC", "default": false, "description": "Trigger binary build and upload jobs for conda on the PR"}]}
{"number": 84677, "title": "[WIP] remove is_symbolic", "time": "2022-09-08T00:14:59Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 84671, "title": "Turn on throwing on data dep ops", "time": "2022-09-07T23:25:01Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #84447\n* __->__ #84671\n* #84432\n\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 84649, "title": "[ONNX] Make `_is_value` a TypeGuard", "time": "2022-09-07T17:11:00Z", "body": "Annotate `_is_value` to return a TypeGuard to signal to mypy type narrowing is performed. So that we can use `_is_value` in `_maybe_get_const`\r\n\r\nhttps://mypy.readthedocs.io/en/latest/type_narrowing.html#user-defined-type-guards", "label": [{"id": 693805995, "node_id": "MDU6TGFiZWw2OTM4MDU5OTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20onnx", "name": "module: onnx", "color": "f7e101", "default": false, "description": "Related to torch.onnx"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3768821874, "node_id": "LA_kwDOA-j9z87go6hy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20onnx", "name": "release notes: onnx", "color": "B4EB93", "default": false, "description": "torch.onnx related changes that should show up in the release notes"}, {"id": 3773061335, "node_id": "LA_kwDOA-j9z87g5FjX", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20improvements", "name": "topic: improvements", "color": "22499E", "default": false, "description": "topic category"}]}
{"number": 84594, "title": "[ONNX] Add col2im for opset 18", "time": "2022-09-06T20:02:21Z", "body": "Opset 18 will be used to introduce suport for ONNX's Col2Im-18 and resolve https://github.com/pytorch/pytorch/issues/84408\r\n\r\nDepends: https://github.com/pytorch/pytorch/pull/83201 (CI will fail until ONNX submodule is updated)\r\n\r\nas per Faith recommendation, this PR should be merged post ORT 1.13 only", "label": [{"id": 693805995, "node_id": "MDU6TGFiZWw2OTM4MDU5OTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20onnx", "name": "module: onnx", "color": "f7e101", "default": false, "description": "Related to torch.onnx"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768821874, "node_id": "LA_kwDOA-j9z87go6hy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20onnx", "name": "release notes: onnx", "color": "B4EB93", "default": false, "description": "torch.onnx related changes that should show up in the release notes"}, {"id": 3773061335, "node_id": "LA_kwDOA-j9z87g5FjX", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20improvements", "name": "topic: improvements", "color": "22499E", "default": false, "description": "topic category"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 84584, "title": "reshape in python", "time": "2022-09-06T18:02:43Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 84575, "title": "[AI Accelerators] GPU Unit test for at::_scaled_dot_product_self_attention", "time": "2022-09-06T15:59:59Z", "body": "Summary:\r\n- 1. Create a seperate folder for cuda unit test: fbcode/caffe2/test/RE_tests\r\n         - 2. Put (write) cuda unit test, TARGET file and dependencies in above folder\r\n         - 3. Update contbuild config file accordingly:\r\n         - 4. Opt out of GPU sections removal for CUDA (GPU) tests\r\n\r\nTest Plan:\r\n1. arc lint for all the newly added files (ok No lint issues):\r\n              fbcode/caffe2/test/RE_tests/test_cuda_sdpa.py\r\n              fbcode/caffe2/test/RE_tests/TARGETS\r\n              fbcode/contbuild/configs/pytorch_core_cuda\r\n           2. Remote test excution requesting CUDA:\r\n              buck2 test @//mode/dev-nosan -c test.external_runner=tpx :test_cuda_sdpa -- --use-remote-execution --force-tpx\r\n              10 tests passed: \r\n[\r\n](https://www.internalfb.com/intern/testinfra/testconsole/testrun/1688849987899027/)\r\n\r\n           3. Local test is skipped because config set it up to skip local test as CUDA doesn't work with ASAN:\r\n              buck2 test :test_cuda_sdpa\r\n              10 tests skipped:  [\r\n](https://www.internalfb.com/intern/testinfra/testconsole/testrun/3377699845391150/)\r\nDifferential Revision: D39276148\r\n\r\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 84572, "title": "Opinfo based testing of torch.bmm with Strided, COO, CSR, and CSC samples.", "time": "2022-09-06T15:35:56Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #84572\n* #84843\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769217376, "node_id": "LA_kwDOA-j9z87gqbFg", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20sparse", "name": "release notes: sparse", "color": "2A7626", "default": false, "description": "release notes category"}]}
{"number": 84569, "title": "[PyTorch] Improving efficiency of `torch.square` by calling `mul` instead of `pow`", "time": "2022-09-06T14:55:05Z", "body": "Summary:\n`x.square()` is significantly (~2x) slower than `x * x`. According to the profiling included below, this is due to relying on `pow`, despite there being a [special case for the power of two](https://github.com/pytorch/pytorch/blob/ce1b727e774c75f8e31b28ff5915851385c70dcf/aten/src/ATen/native/cpu/PowKernel.cpp#L52). This PR changes square to call `mul` instead of `pow` to close the performance gap.\n\nFor detailed benchmark results, see the following.\n```\n************************************************************************************\nsize =  ()\nx.square()\n---------------------  ------------  ------------  ------------  ------------  ------------  ------------\n                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls\n---------------------  ------------  ------------  ------------  ------------  ------------  ------------\n         aten::square        20.93%     448.000us       100.00%       2.140ms       2.090us          1024\n            aten::pow        71.59%       1.532ms        79.07%       1.692ms       1.652us          1024\n    aten::result_type         3.74%      80.000us         3.74%      80.000us       0.078us          1024\n             aten::to         3.74%      80.000us         3.74%      80.000us       0.078us          1024\n---------------------  ------------  ------------  ------------  ------------  ------------  ------------\nSelf CPU time total: 2.140ms\n\nx * x\n-------------  ------------  ------------  ------------  ------------  ------------  ------------\n         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls\n-------------  ------------  ------------  ------------  ------------  ------------  ------------\n    aten::mul       100.00%     354.000us       100.00%     354.000us       0.437us           810\n-------------  ------------  ------------  ------------  ------------  ------------  ------------\nSelf CPU time total: 354.000us\n\n************************************************************************************\nsize =  1\nx.square()\n---------------------  ------------  ------------  ------------  ------------  ------------  ------------\n                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls\n---------------------  ------------  ------------  ------------  ------------  ------------  ------------\n         aten::square        20.10%     160.000us       100.00%     796.000us       0.777us          1024\n            aten::pow        71.11%     566.000us        79.90%     636.000us       0.621us          1024\n    aten::result_type         4.15%      33.000us         4.15%      33.000us       0.032us          1024\n             aten::to         4.65%      37.000us         4.65%      37.000us       0.036us          1024\n---------------------  ------------  ------------  ------------  ------------  ------------  ------------\nSelf CPU time total: 796.000us\n\nx * x\n-------------  ------------  ------------  ------------  ------------  ------------  ------------\n         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls\n-------------  ------------  ------------  ------------  ------------  ------------  ------------\n    aten::mul       100.00%     388.000us       100.00%     388.000us       0.468us           829\n-------------  ------------  ------------  ------------  ------------  ------------  ------------\nSelf CPU time total: 388.000us\n\n************************************************************************************\nsize =  10\nx.square()\n---------------------  ------------  ------------  ------------  ------------  ------------  ------------\n                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls\n---------------------  ------------  ------------  ------------  ------------  ------------  ------------\n         aten::square        21.53%     174.000us       100.00%     808.000us       0.789us          1024\n            aten::pow        71.29%     576.000us        78.47%     634.000us       0.619us          1024\n    aten::result_type         4.21%      34.000us         4.21%      34.000us       0.033us          1024\n             aten::to         2.97%      24.000us         2.97%      24.000us       0.023us          1024\n---------------------  ------------  ------------  ------------  ------------  ------------  ------------\nSelf CPU time total: 808.000us\n\nx * x\n-------------  ------------  ------------  ------------  ------------  ------------  ------------\n         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls\n-------------  ------------  ------------  ------------  ------------  ------------  ------------\n    aten::mul       100.00%     385.000us       100.00%     385.000us       0.461us           835\n-------------  ------------  ------------  ------------  ------------  ------------  ------------\nSelf CPU time total: 385.000us\n\n************************************************************************************\nsize =  1000\nx.square()\n---------------------  ------------  ------------  ------------  ------------  ------------  ------------\n                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls\n---------------------  ------------  ------------  ------------  ------------  ------------  ------------\n         aten::square        19.60%     157.000us       100.00%     801.000us       0.782us          1024\n            aten::pow        72.28%     579.000us        80.40%     644.000us       0.629us          1024\n    aten::result_type         4.24%      34.000us         4.24%      34.000us       0.033us          1024\n             aten::to         3.87%      31.000us         3.87%      31.000us       0.030us          1024\n---------------------  ------------  ------------  ------------  ------------  ------------  ------------\nSelf CPU time total: 801.000us\n\nx * x\n-------------  ------------  ------------  ------------  ------------  ------------  ------------\n         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls\n-------------  ------------  ------------  ------------  ------------  ------------  ------------\n    aten::mul       100.00%     415.000us       100.00%     415.000us       0.499us           832\n-------------  ------------  ------------  ------------  ------------  ------------  ------------\nSelf CPU time total: 415.000us\n\n************************************************************************************\nsize =  1000000\nx.square()\n---------------------  ------------  ------------  ------------  ------------  ------------  ------------\n                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls\n---------------------  ------------  ------------  ------------  ------------  ------------  ------------\n         aten::square         0.39%     543.000us       100.00%     139.923ms     136.644us          1024\n            aten::pow        99.51%     139.235ms        99.61%     139.380ms     136.113us          1024\n    aten::result_type         0.06%      88.000us         0.06%      88.000us       0.086us          1024\n             aten::to         0.04%      57.000us         0.04%      57.000us       0.056us          1024\n---------------------  ------------  ------------  ------------  ------------  ------------  ------------\nSelf CPU time total: 139.923ms\n\nx * x\n-------------  ------------  ------------  ------------  ------------  ------------  ------------\n         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls\n-------------  ------------  ------------  ------------  ------------  ------------  ------------\n    aten::mul       100.00%     132.887ms       100.00%     132.887ms     129.772us          1024\n-------------  ------------  ------------  ------------  ------------  ------------  ------------\nSelf CPU time total: 132.887ms\n```\n\nTest Plan:\nbuck build //caffe2:ATen-cpu\nbuck2 test //caffe2/test:torch\n\nDifferential Revision: D39225491\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 84545, "title": "Add nvfuser support for prims.copy_to", "time": "2022-09-05T16:09:43Z", "body": "I use nvFuser's `aliasOutputToInput` here and since it implicitly adds outputs to the fusion, I need to drop those within Python.\r\n\r\nNow we can lower the batch_norm implementation from torch._decomp to nvprims(see `test_batch_norm_forward_nvprims`).\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel @kevinstephano @jjsjann123 @ezyang @mruberry @ngimel @Lezcano @fdrocha @peterbell10", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}, {"id": 3997471357, "node_id": "LA_kwDOA-j9z87uRJJ9", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20nvfuser", "name": "module: nvfuser", "color": "9CA380", "default": false, "description": ""}, {"id": 4081012228, "node_id": "LA_kwDOA-j9z87zP04E", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20primTorch", "name": "module: primTorch", "color": "455561", "default": false, "description": ""}]}
{"number": 84528, "title": "Enforce the type of counter_halflife to be int", "time": "2022-09-05T05:21:38Z", "body": "Differential Revision: D39251970\n\n", "label": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false, "description": ""}, {"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 84506, "title": "[WIP] [RFC] Naive nested tensor support in proxy tensor [Needs tests]", "time": "2022-09-03T00:12:45Z", "body": "```\r\na, b = torch.tensor([0.5, 0.5]), torch.tensor([0.33, 0.33])\r\n\r\ndef foo(x, y):\r\n    nt = torch.nested_tensor([x, y])\r\n    return nt * nt\r\nz = make_fx(foo)(a, b)\r\n```\r\nBefore:\r\n```\r\nRuntimeError: Internal error: NestedTensorImpl doesn't support sizes. Please file an issue on https://github.com/pytorch/nestedtensor\r\n```\r\nAfter:\r\n\r\n```\r\ndef forward(self, x_1, y_1):\r\n    nested_tensor = torch.ops.aten.nested_tensor.default([x_1, y_1]);  x_1 = y_1 = None\r\n    getitem = nested_tensor[0]\r\n    getitem_1 = nested_tensor[1];  nested_tensor = None\r\n    _tensor_constant0 = self._tensor_constant0\r\n    _tensor_constant0_1 = self._tensor_constant0\r\n    mul = torch.ops.aten.mul.Tensor(_tensor_constant0, _tensor_constant0_1);  _tensor_constant0 = _tensor_constant0_1 = None\r\n    getitem_2 = mul[0]\r\n    getitem_3 = mul[1];  mul = None\r\n    _tensor_constant1 = self._tensor_constant1\r\n    return _tensor_constant1\r\n```\n\ncc @ezyang @SherlockNoMad @soumith @EikanWang @jgong5 @wenzhe-nrv", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}, {"id": 4347715449, "node_id": "LA_kwDOA-j9z88AAAABAyTfeQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fx", "name": "fx", "color": "ededed", "default": false, "description": null}]}
{"number": 84504, "title": "Refine torch.fx's NVFuser backend.", "time": "2022-09-02T21:50:57Z", "body": "1. Avoid re-wrapping when using cached module. \r\n2. Avoid DecompositionInterpreter. Since decomposition happens before \"CapabilityBasedPartitioner\", NVFuser should get bigger regions in a graph.\r\n\r\n@SherlockNoMad, any comments?\r\n\n\ncc @ezyang @SherlockNoMad @soumith @EikanWang @jgong5 @wenzhe-nrv", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}, {"id": 4347715449, "node_id": "LA_kwDOA-j9z88AAAABAyTfeQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fx", "name": "fx", "color": "ededed", "default": false, "description": null}]}
{"number": 84467, "title": "optimize the performance of binary_kernel_reduce for welford using Ro…", "time": "2022-09-02T07:50:48Z", "body": "### Description\r\nOptimize the performance of binary_kernel_reduce for welford using RowwiseMoments for bfloat16 on CPU.\r\nThis PR depends on the optimization of RowwiseMoments https://github.com/pytorch/pytorch/pull/84404 and https://github.com/pytorch/pytorch/pull/84405\r\n\r\n### Testing\r\nsingle socket (28cores):\r\n```\r\nbefore: torch.Size([10, 128, 10, 124]) bf16: 0.000414 s\r\n        torch.Size([10, 128, 30, 124]) bf16: 0.00163 s\r\n            \r\nafter: torch.Size([10, 128, 10, 124]) bf16: 2.246e-05 s\r\n       torch.Size([10, 128, 30, 124]) bf16: 6.811e-05 s\r\n```\r\nsingle core:\r\n```\r\nbefore: torch.Size([10, 128, 10, 124]) bf16: 0.00980 s\r\n        torch.Size([10, 128, 30, 124]) bf16: 0.0398 s\r\n            \r\nafter: torch.Size([10, 128, 10, 124]) bf16: 0.000310 s\r\n       torch.Size([10, 128, 30, 124]) bf16: 0.00127 s\r\n```", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 84466, "title": "nnc: support hardswish fusion with conv and linear", "time": "2022-09-02T07:26:52Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #84466\n* #84255\n* #84401\n* #84400\n* #84254\n\n\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 84447, "title": "[WIP] remove proxy tensor constant tracking now that fake tensor does it", "time": "2022-09-01T23:54:05Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #84447\n* #84671\n* #84432\n\n\n\ncc @ezyang @SherlockNoMad @soumith @EikanWang @jgong5 @wenzhe-nrv", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}, {"id": 4347715449, "node_id": "LA_kwDOA-j9z88AAAABAyTfeQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fx", "name": "fx", "color": "ededed", "default": false, "description": null}]}
{"number": 84436, "title": "Prime number", "time": "2022-09-01T18:46:52Z", "body": "## Prime number\r\n\r\n```Python\r\nprime_number(n, *, out=None) -> Tensor\r\n```\r\n\r\n$n^{\\textup{th}}$ prime number. \r\n\r\nParameters:\r\n\r\nn ([Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – the input tensor. `n` must have an integral [dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch-dtype).\r\n\r\nKeyword Arguments:\r\n\r\nout ([Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor), optional) – the output tensor.\r\n\r\n### Checklist\r\n\r\n- [x] structured kernel (or have a good reason not to be)\r\n- [x] jiterated\r\n- [x] documented via docstring \r\n- [x] updated `docs/source/special.rst`\r\n- [x] exposed on the `torch.special` namespace\r\n- [x] function and out variant (2*N new entries there) added to native_functions.yaml\r\n- [x] tested via an OpInfo that includes a ref\r\n- [x] functions for the cpp API in csrc/api/include (optional but should be consistent)\r\n- [x] derivatives.yaml contain the right enty for each function (N new entries)\r\n- [x] nothing is added to the public API allowlist\r\n- [x] Sanity check the c++ headers to only have minimal includes\r\n- [x] per-operator headers are being used and no large includes like ATen.h or Function.h", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3769208226, "node_id": "LA_kwDOA-j9z87gqY2i", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20cpp", "name": "release notes: cpp", "color": "8250CF", "default": false, "description": "release notes category"}]}
{"number": 84403, "title": "caffe2: set `supports_python_dlopen = True` on libtorch", "time": "2022-09-01T07:09:52Z", "body": "Summary:\nLike it says in the title. This library ends up depended on by Python and\nexposed as a root anyway. I pulled this out of D39104634 since it needs a\nGithub PR to land, so I'll land it afterwards.\n\nNote: this is a better way to do what D37946374 (https://github.com/pytorch/pytorch/commit/0933c037e7e5d306d3ba537dcbe3192ccd060f34) does. That diff excluded all of\nlibtorch's deps from Omnibus linking (hence the timeouts). This diff, in\ncontrast, only excluded libtorch itself (which most of the time it already is\nimplicitly by virtue of being depended on by a Python dep, but not necessarily\nalways).\n\nContext on this attribute: D39104635\n\nReviewed By: ndmitchell\n\nDifferential Revision: D39168231\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 84401, "title": "nnc: enable bf16 for mkldnn prepack conv2d", "time": "2022-09-01T05:50:08Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #84466\n* #84255\n* __->__ #84401\n* #84400\n* #84254\n\n", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 84400, "title": "nnc: support aten::_convolution when it is Conv2d", "time": "2022-09-01T05:42:46Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #84466\n* #84255\n* #84401\n* __->__ #84400\n* #84254\n\n\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 84394, "title": "sym_offset", "time": "2022-09-01T03:55:57Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #84394\n* #84393\n* #84392\n\nStore storage_offset_ on TensorImpl directly as SymInt\n\nUnlike numel, which is a derived quantity, storage offset is canonical,\nso it is simplest if we store it correctly.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nfinal fixes\n\nremove dead code\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3768809762, "node_id": "LA_kwDOA-j9z87go3ki", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20mobile", "name": "release notes: mobile", "color": "bfdadc", "default": false, "description": "release notes category"}]}
{"number": 84391, "title": "Implement embedding with NNC IR", "time": "2022-09-01T03:31:44Z", "body": "This PR re-implemented aten::embedding with native NNC IR, which should provide better performance and fusion potential. The existing embedding has been renamed with externalcall suffix.\r\n\r\nA new generic infrastructure function is implemented for indirect-indexing related Ops. It helps generating the overall loop-nest for the Op and indirect-indexing related logic, while leaving Op-specific logic to be defined by Op with injected function. E.x.: take a 2D indices and a 3D target (to be indirect indexing), and the 2nd dim is the dim to be indirect-indexing. This new infrastructure function will generate below lowering statement, while each Op can specify its specific logic with a lambda function as the inner-most loop body.\r\n```\r\nfor i : size_i\r\n   for j : size_j\r\n     x = indices[i, j]\r\n     for m : size_m\r\n       for n : size_n\r\n         innerStmt by Op with innerStmtFunc(idxingTarget[m, x, n], [i, j, m, n]) \r\n```\r\n\r\naten::embedding is implemented based on above new function very easily.\r\n\r\nThis PR has been tested and verified with unit case as below:\r\n```\r\nimport time\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nwarm_up_num = 10\r\nrun_num = 200000\r\n\r\nclass EmbeddingModel(nn.Module):\r\n  def __init__(self):\r\n    super(EmbeddingModel,self).__init__()\r\n    self.embedding = nn.Embedding(10000, 300)\r\n  def forward(self,x):\r\n    x = self.embedding(x)\r\n    x = x + x\r\n    x = x * x\r\n    return x\r\n\r\nif __name__ =='__main__':\r\n  torch.manual_seed(0)\r\n  model = EmbeddingModel()\r\n  model.eval()\r\n  print(model)\r\n\r\n  jit_model = torch.jit.script(model)\r\n  jit_model = torch.jit.freeze(jit_model)\r\n  print(\"[INFO]: Before fusion\")\r\n  print(jit_model.graph)\r\n\r\n  input_tensor = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\r\n\r\n  print(\"Warming up ...\")\r\n  with torch.no_grad():\r\n    for i in range(warm_up_num):\r\n      warmup_ts = time.time()\r\n      out = jit_model(input_tensor)\r\n      warmup_duration = (time.time() - warmup_ts)*1000000\r\n      print(f\"  round {i} : {warmup_duration} us\")\r\n\r\n  print(\"\")\r\n  print(\"Official run and benchmarking...\")\r\n  with torch.no_grad():\r\n    start_ts = time.time()\r\n    for i in range(run_num):\r\n      out = jit_model(input_tensor)\r\n    end_ts = time.time()\r\n  print(f\"Latency: {(end_ts-start_ts)/run_num*1000000} us\")\r\n```\r\n\r\nWith this PR, embedding will be pulled into NNC fusion group as below:\r\n```\r\n%x.26 : Float(2, 4, 300, strides=[1200, 300, 1], requires_grad=0, device=cpu) = aten::embedding(%self.embedding.weight, %x.1, %padding_idx.41, %2, %2)\r\n%x.22 : Float(2, 4, 300, strides=[1200, 300, 1], requires_grad=0, device=cpu) = aten::add(%x.26, %x.26, %1)\r\n%x.18 : Float(2, 4, 300, strides=[1200, 300, 1], requires_grad=0, device=cpu) = aten::mul(%x.22, %x.22)\r\n```\r\nAnd related NNC IR (original) are generated as below:\r\n```\r\nafter fuse{\r\n  for (int64_t i = 0ll; i < 2ll; i++) {\r\n    for (int64_t j = 0ll; j < 4ll; j++) {\r\n      int64_t ind_idx = tx_1[i, j];\r\n      for (int64_t k_2 = 0ll; k_2 < 300ll; k_2++) {\r\n        aten_mul[i, j, k_2] = ((const_self_embedding_weight[ind_idx, k_2]) + (const_self_embedding_weight[ind_idx, k_2])) * ((const_self_embedding_weight[ind_idx, k_2]) + (const_self_embedding_weight[ind_idx, k_2]));\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nThe performance of this PR (embedding+add+mul all fused in NNC) vs. before this PR (add+mul fused in NNC, embedding as aten call) under JIT mode is as below table (run above unit test with numactl to set CPU Cores on a CPX 4x8380H server), which shows about 80% ~ more than 200% performance benefit.\r\n<html>\r\n<body>\r\n<!--StartFragment--><div ccp_infra_version='3' ccp_infra_timestamp='1661742578237' ccp_infra_user_hash='526604988' ccp_infra_copy_id='c1817984-ca7b-456b-b62a-28ac3a7321f1' data-ccp-timestamp='1661742578237'><html><head><meta name=ProgId content=Excel.Sheet><meta name=Generator content=\"Microsoft Excel 15\"></head><body link=\"#0563C1\" vlink=\"#954F72\">\r\n\r\nExecution CPU | Vector DIM | Word Num | Indices | Pytorch (us) | Pytorch + NNC Embedding (us) | Perf Improvement\r\n-- | -- | -- | -- | -- | -- | --\r\n1 | 16 | 16 | [16] | 9.72 | 4.72 | 105.93%\r\n1 | 16 | 16 | [128] | 10.47 | 5.69 | 84.01%\r\n1 | 16 | 16 | [256, 256] | 570.15 | 199.63 | 185.60%\r\n1 | 16 | 10000 | [16] | 10.02 | 4.76 | 110.50%\r\n1 | 16 | 10000 | [128] | 10.41 | 5.79 | 79.79%\r\n1 | 16 | 10000 | [256, 256] | 762.89 | 268.16 | 184.49%\r\n1 | 128 | 100 | [16] | 10.36 | 5.08 | 103.94%\r\n1 | 128 | 100 | [128] | 13.57 | 5.82 | 133.16%\r\n1 | 128 | 100 | [256, 256] | 27898.13 | 13461.12 | 107.25%\r\n1 | 128 | 10000 | [16] | 10.38 | 5.13 | 102.34%\r\n1 | 128 | 10000 | [128] | 13.51 | 6.15 | 119.67%\r\n1 | 128 | 10000 | [256, 256] | 29312.62 | 14779.03 | 98.34%\r\n1 | 512 | 10000 | [16] | 11.41 | 5.29 | 115.69%\r\n1 | 512 | 10000 | [128] | 44.72 | 10.04 | 345.42%\r\n1 | 512 | 10000 | [256, 256] | 122491.56 | 62629.17 | 95.58%\r\n4 | 128 | 10000 | [16] | 10.45 | 4.96 | 110.69%\r\n4 | 128 | 10000 | [128] | 13.78 | 5.99 | 130.05%\r\n4 | 128 | 10000 | [256, 256] | 9771.33 | 4973.41 | 96.47%\r\n4 | 512 | 10000 | [16] | 11.58 | 5.54 | 109.03%\r\n4 | 512 | 10000 | [128] | 27.39 | 8.96 | 205.69%\r\n4 | 512 | 10000 | [256, 256] | 40666.72 | 20633.8 | 97.09%\r\n\r\n</body></html></div><!--EndFragment-->\r\n</body>\r\n</html>\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 84381, "title": "[Easy] spelling fix", "time": "2022-08-31T23:41:53Z", "body": "Just ran into this error message, I thought I'd send out a quick fix.\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 84369, "title": "Ignore return value of function declared with 'warn_unused_result'", "time": "2022-08-31T20:16:13Z", "body": "Summary:\nIgnore return value of function declared with 'warn_unused_result'\n\nAddresses the following build failure that we get on some of our internal build environments:\ncaffe2/torch/csrc/deploy/environment.h:60:5: error: ignoring return value of function declared with 'warn_unused_result' attribute [-Werror,-Wunused-result] system(rmCmd.c_str());\n\nTest Plan: Successful build\n\nDifferential Revision: D39181069\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 84352, "title": "Harmonic number", "time": "2022-08-31T14:50:37Z", "body": "## Harmonic number\r\n\r\n```Python\r\nharmonic_number(n, *, out=None) -> Tensor\r\n```\r\n\r\n$n^{\\textup{th}}$ harmonic number. \r\n\r\nParameters:\r\n\r\nn ([Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)) – the input tensor. `n` must have an integral [dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch-dtype).\r\n\r\nKeyword Arguments:\r\n\r\nout ([Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor), optional) – the output tensor.\r\n\r\n### Checklist\r\n\r\n- [x] structured kernel (or have a good reason not to be)\r\n- [x] jiterated\r\n- [x] documented via docstring \r\n- [x] updated `docs/source/special.rst`\r\n- [x] exposed on the `torch.special` namespace\r\n- [x] function and out variant (2*N new entries there) added to native_functions.yaml\r\n- [x] tested via an OpInfo that includes a ref\r\n- [x] functions for the cpp API in csrc/api/include (optional but should be consistent)\r\n- [x] derivatives.yaml contain the right enty for each function (N new entries)\r\n- [x] nothing is added to the public API allowlist\r\n- [x] Sanity check the c++ headers to only have minimal includes\r\n- [x] per-operator headers are being used and no large includes like ATen.h or Function.h", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3769211609, "node_id": "LA_kwDOA-j9z87gqZrZ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20autograd", "name": "release notes: autograd", "color": "9433A1", "default": false, "description": "release notes category"}]}
{"number": 84343, "title": "Add more security checks to fix crashing during torch::jit::load", "time": "2022-08-31T12:16:17Z", "body": "These changes make loading models with torch::jit::load more stable. Proposed checks fixes multiple segmentation faults and heap buffer overflows that was found during fuzzing pytorch with [sydr-fuzz](https://github.com/ispras/oss-sydr-fuzz/tree/master/projects/pytorch).\r\n\r\nSome of the fixed bugs:\r\n1) Heap buffer overflow that leads to crash \r\n[crash-842314913bf1820ec19cddfbb7400ffdbb756920.zip](https://github.com/pytorch/pytorch/files/9461316/crash-842314913bf1820ec19cddfbb7400ffdbb756920.zip)\r\n\r\n```\r\n  \"AsanReport\": [\r\n    \"==3751==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x619000033478 at pc 0x0000005f9bc3 bp 0x7fffffff1eb0 sp 0x7fffffff1ea8\\n\",\r\n    \"READ of size 4 at 0x619000033478 thread T0\\n\",\r\n    \"[Detaching after fork from child process 3762]\\n\",\r\n    \"    #0 0x5f9bc2 in c10::IValue::IValue(c10::IValue&&) /pytorch_fuzz/aten/src/ATen/core/ivalue.h:192:43\\n\",\r\n    \"    #1 0x9ecd0a7 in torch::jit::pop(std::vector<c10::IValue, std::allocator<c10::IValue> >&) /pytorch_fuzz/aten/src/ATen/core/stack.h:102:12\\n\",\r\n    \"    #2 0x9ecd0a7 in torch::jit::Unpickler::readInstruction() /pytorch_fuzz/torch/csrc/jit/serialization/unpickler.cpp:380:17\\n\",\r\n    \"    #3 0x9ecafc7 in torch::jit::Unpickler::run() /pytorch_fuzz/torch/csrc/jit/serialization/unpickler.cpp:226:27\\n\",\r\n    \"    #4 0x9ecac62 in torch::jit::Unpickler::parse_ivalue() /pytorch_fuzz/torch/csrc/jit/serialization/unpickler.cpp:183:3\\n\",\r\n    \"    #5 0x9e45996 in torch::jit::unpickle(std::function<unsigned long (char*, unsigned long)>, std::function<c10::StrongTypePtr (c10::QualifiedName const&)>, c10::ArrayRef<at::Tensor>, c10::Type::SingletonOrSharedTypePtr<c10::Type> (*)(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)) /pytorch_fuzz/torch/csrc/jit/serialization/pickle.cpp:127:20\\n\",\r\n    \"    #6 0x9e4626d in torch::jit::unpickle(char const*, unsigned long, std::function<c10::StrongTypePtr (c10::QualifiedName const&)>, c10::ArrayRef<at::Tensor>, c10::Type::SingletonOrSharedTypePtr<c10::Type> (*)(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)) /pytorch_fuzz/torch/csrc/jit/serialization/pickle.cpp:137:10\\n\",\r\n```\r\n\r\n2) Segmentation fault\r\n[crash-e690c58718e88921350562f0b4d9180938145d77.zip](https://github.com/pytorch/pytorch/files/9461331/crash-e690c58718e88921350562f0b4d9180938145d77.zip)\r\n\r\n```\r\n \"AsanReport\": [\r\n    \"==3744==ERROR: AddressSanitizer: SEGV on unknown address (pc 0x000009122754 bp 0x7fffffff5290 sp 0x7fffffff5270 T0)\\n\",\r\n    \"==3744==The signal is caused by a READ memory access.\\n\",\r\n    \"==3744==Hint: this fault was caused by a dereference of a high value address (see register values below).  Disassemble the provided pc to learn which register was used.\\n\",\r\n    \"[Detaching after fork from child process 3763]\\n\",\r\n    \"    #0 0x9122754 in c10::intrusive_ptr<torch::jit::Tree, c10::detail::intrusive_target_default_null_type<torch::jit::Tree> >::retain_() /pytorch_fuzz/c10/util/intrusive_ptr.h:269:54\\n\",\r\n    \"    #1 0x9127929 in c10::intrusive_ptr<torch::jit::Tree, c10::detail::intrusive_target_default_null_type<torch::jit::Tree> >::intrusive_ptr(c10::intrusive_ptr<torch::jit::Tree, c10::detail::intrusive_target_default_null_type<torch::jit::Tree> > const&) /pytorch_fuzz/c10/util/intrusive_ptr.h:352:5\\n\",\r\n    \"    #2 0x9127929 in torch::jit::Expr::Expr(c10::intrusive_ptr<torch::jit::Tree, c10::detail::intrusive_target_default_null_type<torch::jit::Tree> > const&) /pytorch_fuzz/torch/csrc/jit/frontend/tree_views.h:269:49\\n\",\r\n    \"    #3 0x91b1bbb in torch::jit::Maybe<torch::jit::Expr>::get() const /pytorch_fuzz/torch/csrc/jit/frontend/tree_views.h:211:12\\n\",\r\n    \"    #4 0x92a8f74 in torch::jit::ScriptTypeParser::parseClassConstant(torch::jit::Assign const&) /pytorch_fuzz/torch/csrc/jit/frontend/script_type_parser.cpp:461:41\\n\",\r\n    \"    #5 0x9e1c09b in torch::jit::SourceImporterImpl::importClass(c10::QualifiedName const&, torch::jit::ClassDef const&, bool) /pytorch_fuzz/torch/csrc/jit/serialization/import_source.cpp:549:34\\n\",\r\n    \"    #6 0x9e13f00 in torch::jit::SourceImporterImpl::importNamedType(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, torch::jit::ClassDef const&) /pytorch_fuzz/torch/csrc/jit/serialization/import_source.cpp:288:5\\n\",\r\n    \"    #7 0x9e11fbc in torch::jit::SourceImporterImpl::findNamedType(c10::QualifiedName const&) /pytorch_fuzz/torch/csrc/jit/serialization/import_source.cpp:140:5\\n\",\r\n```\r\n\r\n3) Unhandled out of bounds access in a vector\r\n[crash-ccd524e7ba19a37982dd91e0d6fc06bb26dd0b10.zip](https://github.com/pytorch/pytorch/files/9461367/crash-ccd524e7ba19a37982dd91e0d6fc06bb26dd0b10.zip)\r\n\r\n```\r\n  \"AsanReport\": [\r\n    \"==3792== ERROR: libFuzzer: deadly signal\\n\",\r\n    \"[Detaching after fork from child process 3809]\\n\",\r\n    \"    #0 0x59cc11 in __sanitizer_print_stack_trace /llvm-project/compiler-rt/lib/asan/asan_stack.cpp:87:3\\n\",\r\n    \"    #1 0x511547 in fuzzer::PrintStackTrace() /llvm-project/compiler-rt/lib/fuzzer/FuzzerUtil.cpp:210:5\\n\",\r\n    \"    #2 0x4f7753 in fuzzer::Fuzzer::CrashCallback() /llvm-project/compiler-rt/lib/fuzzer/FuzzerLoop.cpp:233:3\\n\",\r\n    \"    #3 0x7ffff7c6741f  (/lib/x86_64-linux-gnu/libpthread.so.0+0x1441f)\\n\",\r\n    \"    #4 0x7ffff7a8700a in __libc_signal_restore_set /build/glibc-SzIz7B/glibc-2.31/signal/../sysdeps/unix/sysv/linux/internal-signals.h:86:3\\n\",\r\n    \"    #5 0x7ffff7a8700a in raise /build/glibc-SzIz7B/glibc-2.31/signal/../sysdeps/unix/sysv/linux/raise.c:48:3\\n\",\r\n    \"    #6 0x7ffff7a66858 in abort /build/glibc-SzIz7B/glibc-2.31/stdlib/abort.c:79:7\\n\",\r\n    \"    #7 0x7ffff7e73910  (/lib/x86_64-linux-gnu/libstdc++.so.6+0x9e910)\\n\",\r\n    \"    #8 0x7ffff7e7f38b  (/lib/x86_64-linux-gnu/libstdc++.so.6+0xaa38b)\\n\",\r\n    \"    #9 0x7ffff7e7f3f6 in std::terminate() (/lib/x86_64-linux-gnu/libstdc++.so.6+0xaa3f6)\\n\",\r\n    \"    #10 0x7ffff7e7f6a8 in __cxa_throw (/lib/x86_64-linux-gnu/libstdc++.so.6+0xaa6a8)\\n\",\r\n    \"    #11 0x7ffff7e763aa  (/lib/x86_64-linux-gnu/libstdc++.so.6+0xa13aa)\\n\",\r\n    \"    #12 0x6aeedf in std::vector<c10::IValue, std::allocator<c10::IValue> >::_M_range_check(unsigned long) const /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/stl_vector.h:1073:4\\n\",\r\n    \"    #13 0x9ecd66c in torch::jit::Unpickler::readInstruction() /pytorch_fuzz/torch/csrc/jit/serialization/unpickler.cpp\\n\",\r\n    \"    #14 0x9ecafc7 in torch::jit::Unpickler::run() /pytorch_fuzz/torch/csrc/jit/serialization/unpickler.cpp:226:27\\n\",\r\n    \"    #15 0x9ecac62 in torch::jit::Unpickler::parse_ivalue() /pytorch_fuzz/torch/csrc/jit/serialization/unpickler.cpp:183:3\\n\",\r\n```\r\n\r\nSome other crashes found by fuzzer:\r\n[crash-0cab888cbd1e9fea92ab6ddeadf40b958b87d62b.zip](https://github.com/pytorch/pytorch/files/9461406/crash-0cab888cbd1e9fea92ab6ddeadf40b958b87d62b.zip)\r\n[crash-04c9ba8e3b0f15028fd0fb0ed014fd352e182a1d.zip](https://github.com/pytorch/pytorch/files/9461407/crash-04c9ba8e3b0f15028fd0fb0ed014fd352e182a1d.zip)\r\n[crash-422ad8c3a3472980ba751f4c7f79cf2b53e49927.zip](https://github.com/pytorch/pytorch/files/9461408/crash-422ad8c3a3472980ba751f4c7f79cf2b53e49927.zip)\r\n", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 84341, "title": "Fix floordiv warning.", "time": "2022-08-31T11:41:11Z", "body": "Fixes ##84340\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 84320, "title": "[WIP] decomposition for getitem", "time": "2022-08-31T00:45:15Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #84320\n\neverything except bool tensor handling\n\ngetindex passing test/test_index.py\n\npatch getitem stuff", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 84319, "title": "Fix placeholder's target name.", "time": "2022-08-31T00:38:05Z", "body": "When a placeholder's target name conflicts with another node\r\noutput, the placeholder should use a new name as its\r\ntarget. Alternative solution is sorting `nodes` before running a graph\r\nso that all placeholders are executed as early as possible.\r\nSee #84311's for details.\r\n\r\nFixes #84311\r\n\n\ncc @ezyang @SherlockNoMad @soumith @EikanWang @jgong5 @wenzhe-nrv", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}, {"id": 4347715449, "node_id": "LA_kwDOA-j9z88AAAABAyTfeQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fx", "name": "fx", "color": "ededed", "default": false, "description": null}]}
{"number": 84306, "title": "Complete Carlson elliptic integral, $R_{F}\\left(x, y\\right)$", "time": "2022-08-30T20:42:55Z", "body": null, "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3769211609, "node_id": "LA_kwDOA-j9z87gqZrZ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20autograd", "name": "release notes: autograd", "color": "9433A1", "default": false, "description": "release notes category"}]}
{"number": 84295, "title": "Adding sym_offset", "time": "2022-08-30T19:14:45Z", "body": "Fixes #ISSUE_NUMBER\r\n\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3769211609, "node_id": "LA_kwDOA-j9z87gqZrZ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20autograd", "name": "release notes: autograd", "color": "9433A1", "default": false, "description": "release notes category"}]}
{"number": 84275, "title": "[Profiler][Minor] Cleanup collection.cpp and replace some asserts with `SOFT_ASSERT`", "time": "2022-08-30T16:05:33Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #85161\n* #84276\n* __->__ #84275\n* #84274\n\nAs part of the reliability effort, I am pulling `TORCH_INTERNAL_ASSERT`s out of post processing and replacing them with `SOFT_ASSERT`s so we can gracefully recover and not crash training jobs when profiling is triggered.\n\nDifferential Revision: [D39108643](https://our.internmc.facebook.com/intern/diff/D39108643/)", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}]}
{"number": 84274, "title": "[Profiler][Testing] Add `torch/testing/_internal/profiler_utils.py` and set `_soft_assert_raises` on every test.", "time": "2022-08-30T16:05:28Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #85161\n* #84276\n* #84275\n* __->__ #84274\n\nNow that profiler can recover from exceptions in post processing we can make SOFT_ASSERT error in unit tests. There is a lot of duplicated trace processing code that we use in testing, and given that we want to beef up testing (particularly for less common cases like `__torch_function__` and `__torch_dispatch__` it makes sense to start factoring them into standalone utils.\n\nDifferential Revision: [D39108650](https://our.internmc.facebook.com/intern/diff/D39108650/)", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 84255, "title": "nnc: support linear-eltwise fusion", "time": "2022-08-30T06:24:43Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #84466\n* __->__ #84255\n* #84401\n* #84400\n* #84254\n\n\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 84254, "title": "nnc: add more fusion of elementwise OPs with conv ", "time": "2022-08-30T06:24:34Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #84466\n* #84255\n* #84401\n* #84400\n* __->__ #84254\n\nadd conv eltwise fusion for sigmoid and tanh\n\nrefactor the code to support more eltwise OPs\n\nadd conv eltwise fusion for leaky_relu, hardtanh, gelu and clamp\n\nadd UT for for leaky_relu, hardtanh, gelu and clamp\n\nrefactor UT to define eltwise OP list only once\n\nremove unused variable\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 84251, "title": "Fix the thread synchronization problem.", "time": "2022-08-30T05:28:31Z", "body": "Fixes #84202\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768824578, "node_id": "LA_kwDOA-j9z87go7MC", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20cuda", "name": "release notes: cuda", "color": "91275E", "default": false, "description": "release notes category"}]}
{"number": 84250, "title": "Warn on use of global constructors", "time": "2022-08-30T04:49:07Z", "body": "To prevent further reverts similar to https://github.com/pytorch/pytorch/pull/82040#issuecomment-1229503604\r\n\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 84246, "title": "Symbolic shapes", "time": "2022-08-30T03:00:18Z", "body": "cc @VitalyFedyunin @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @Guobing-Chen @chunyuan-w @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire @jansel @lezcano @fdrocha", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3769211609, "node_id": "LA_kwDOA-j9z87gqZrZ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20autograd", "name": "release notes: autograd", "color": "9433A1", "default": false, "description": "release notes category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 84240, "title": "[WIP][NVFUSER][DO NOT REVIEW] smoke test", "time": "2022-08-30T00:06:17Z", "body": "cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2510754463, "node_id": "MDU6TGFiZWwyNTEwNzU0NDYz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/NNC", "name": "NNC", "color": "e5678d", "default": false, "description": ""}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 84230, "title": "add a backward exception", "time": "2022-08-29T22:16:15Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #84230\n* #84207\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}]}
{"number": 84226, "title": "updates", "time": "2022-08-29T21:49:53Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}]}
{"number": 84224, "title": "[c10d] Introduce new_sub_group function.", "time": "2022-08-29T20:56:52Z", "body": "new_sub_group behaves like new_group except that it only needs to be called\r\non the member ranks.\r\n\r\nThis addresses both scalability and composability problems associated with new_group.\r\n\r\nI did a quick benchmark of creating 3 PGs per rank using both functions and perf is the following:\r\n\r\nnew_group:\r\n| World Size | Time (in secs) |\r\n| --- | ----------- |\r\n| 4 | 0.12 |\r\n| 8 | 0.25 |\r\n| 16 | 0.51 |\r\n| 32 | 0.87 |\r\n| 64 | 1.50 |\r\n| 128 | 2.87 |\r\n\r\n\r\nnew_sub_group:\r\n| World Size | Time (in secs) |\r\n| --- | ----------- |\r\n| 4 | 0.05 |\r\n| 8 | 0.04 |\r\n| 16 | 0.03 |\r\n| 32 | 0.03 |\r\n| 64 | 0.04 |\r\n| 128 | 0.04 |\r\n\r\nScaling for new_group is sub linear because the number of process groups created as a multiple of world_size decreases as we go up. It's 6 with world_size 4 and 192 with world_size 128.\r\n\r\nScaling for new_sub_group is constant as the number of store barriers executer per rank remains constant at 3.\r\n\r\nSetup:\r\n\r\n1 AWS host, backend gloo.\r\n\r\n\r\nFixes #81291\r\n", "label": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20distributed", "name": "oncall: distributed", "color": "f7e101", "default": false, "description": "Add this issue/PR to distributed oncall triage queue"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3769218061, "node_id": "LA_kwDOA-j9z87gqbQN", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(c10d)", "name": "release notes: distributed (c10d)", "color": "EE86D4", "default": false, "description": "release notes category"}, {"id": 3773060564, "node_id": "LA_kwDOA-j9z87g5FXU", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20new%20feature", "name": "topic: new feature", "color": "C32883", "default": false, "description": "topic category"}, {"id": 3773062847, "node_id": "LA_kwDOA-j9z87g5F6_", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20performance", "name": "topic: performance", "color": "AE9D13", "default": false, "description": "topic category"}]}
{"number": 84197, "title": "Remove unnecessary tensor copy in baddbmm_out_cuda_impl", "time": "2022-08-29T16:22:29Z", "body": "Summary: We only need to clone `result` if it's not contiguous.\n\nTest Plan:\n```\nCUBLASLT_LOG_LEVEL=5 buck run mode/opt //caffe2/test:linalg\n\n...\n\nRan 739 tests in 370.657s\n\nOK (skipped=53)\n```\n\nDifferential Revision: D39063652\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 84174, "title": "Create codeql.yml to leverage github Code Scanning", "time": "2022-08-27T22:41:28Z", "body": "Enables Code Scanning on Python and JS. It detects bugs like\r\n\r\nhttps://github.com/pytorch/pytorch/blob/1dabb51a16eb6cf81475efecb1d39c4683af50fb/benchmarks/distributed/rpc/rl/launcher.py#L99\r\n\r\nand others.\n\ncc @seemethere @malfet @pytorch/pytorch-dev-infra", "label": [{"id": 1300896147, "node_id": "MDU6TGFiZWwxMzAwODk2MTQ3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20ci", "name": "module: ci", "color": "f7e101", "default": false, "description": "Related to continuous integration"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 84166, "title": "[data utils] pin_memory returns list or tuple accurately", "time": "2022-08-27T12:57:25Z", "body": "Fixes #48419\r\n\r\n#### Background\r\n\r\nMy data (node schema for a graph) comes in the format `NodeSchema(node_id='node_id', float_tensor_features=['embedding_features'], int_tensor_features=(), int_categorical_tensor_features=(), single_text_features=(), labels=(), set_name=None, node_type='node_type', sample_weight=None, other_columns=())`. \r\n\r\nWhen we pin it to memory, as part of our dataloader,  we find an issue: all our schema properties are changes from tuples to lists.\r\nThis is problematic as we sometimes have to do certain operations that will no longer work. e.g:\r\n\r\n```\r\n>>> ['b'] + ('a',)   # without fix\r\nTypeError: can only concatenate list (not \"tuple\") to list\r\n>>> tuple(['b']) + ('a',) # fixed\r\n('b', 'a')\r\n```\r\n#### Issue\r\n\r\nThe type returned by the method is not the same as the type that goes in - tuples become lists.\r\n\r\n#### Fix\r\n\r\nSimply add another condition that catches the tuple and does the same thing but returning a tuple instead of a list.\r\n\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 84159, "title": "Remove .item calls from indexing", "time": "2022-08-27T00:11:35Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #84159\n* #84322\n* #84114\n\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 84143, "title": "DO NOT MERGE, testing no_desired_cuda", "time": "2022-08-26T20:37:22Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #84143\n* #84142\n\r\nTesting changes from https://github.com/pytorch/builder/pull/1113\r\n\r\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3618084153, "node_id": "LA_kwDOA-j9z87Xp5U5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/binaries", "name": "ciflow/binaries", "color": "245567", "default": false, "description": "Trigger all binary build and upload jobs on the PR"}]}
{"number": 84142, "title": "ci: Remove DESIRED_CUDA env variable", "time": "2022-08-26T20:37:17Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #84143\n* __->__ #84142\n\nThis is a legacy environment variable that we shouldn't use anymore\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 84139, "title": "Add __all__ typing to torch.distributed.", "time": "2022-08-26T20:17:09Z", "body": "Depends on https://github.com/pytorch/pytorch/pull/84119/", "label": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20distributed", "name": "oncall: distributed", "color": "f7e101", "default": false, "description": "Add this issue/PR to distributed oncall triage queue"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 84133, "title": "some refactoring", "time": "2022-08-26T18:06:07Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 84131, "title": "[NOT FOR lAND] Temp commit to allow tracing of on device quant", "time": "2022-08-26T17:53:34Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #84131\n* #83807\n* #83742\n* #83571\n* #83570\n* #83569\n* #83568\n\nmethods via bundled inputs\ng\n\nSummary:\n\nTest Plan:\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\nDifferential Revision: [D39062963](https://our.internmc.facebook.com/intern/diff/D39062963)\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}]}
{"number": 84129, "title": "[Fix] Use the skip table to exclude aten decomposition", "time": "2022-08-26T17:41:24Z", "body": "This loop appears twice... We should only keep the later one because the later one honors the skip table `aten2aten_decomp_skips `.\n\ncc @ezyang @SherlockNoMad @soumith @EikanWang @jgong5 @wenzhe-nrv", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}, {"id": 4347715449, "node_id": "LA_kwDOA-j9z88AAAABAyTfeQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fx", "name": "fx", "color": "ededed", "default": false, "description": null}]}
{"number": 84124, "title": "trigonometric special functions", "time": "2022-08-26T15:53:14Z", "body": "# Trigonometric Special Functions\r\n\r\n## cos_pi\r\n\r\n```Python\r\ncos_pi(z, *, out=None) -> Tensor\r\n```\r\n\r\nCosine of $\\pi z$.\r\n\r\n## cosh_pi\r\n\r\n```Python\r\ncosh_pi(z, *, out=None) -> Tensor\r\n```\r\n\r\nHyperbolic cosine of $\\pi z$.\r\n\r\n## sin_pi\r\n\r\n```Python\r\nsin_pi(z, *, out=None) -> Tensor\r\n```\r\n\r\nSine of $\\pi z$.\r\n\r\n## sinc_pi\r\n\r\n```Python\r\nsinc_pi(z, *, out=None) -> Tensor\r\n```\r\n\r\nSinc of $\\pi z$.\r\n\r\n## sinh_pi\r\n\r\n```Python\r\nsinh_pi(z, *, out=None) -> Tensor\r\n```\r\n\r\nHyperbolic sine of $\\pi z$.\r\n\r\n## sinhc \r\n\r\n```Python\r\nsinhc(z, *, out=None) -> Tensor\r\n```\r\n\r\nHyperbolic sinc of $z$.\r\n\r\n## sinhc_pi\r\n\r\n```Python\r\nsinhc_pi(z, *, out=None) -> Tensor\r\n```\r\n\r\nHyperbolic sinc of $\\pi z$.\r\n\r\n## tan_pi\r\n\r\n```Python\r\ntan_pi(z, *, out=None) -> Tensor\r\n```\r\n\r\nTangent of $\\pi z$.\r\n\r\n## tanh_pi\r\n\r\n```Python\r\ntanh_pi(z, *, out=None) -> Tensor\r\n```\r\n\r\nHyperbolic tangent of $\\pi z$.", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3769211609, "node_id": "LA_kwDOA-j9z87gqZrZ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20autograd", "name": "release notes: autograd", "color": "9433A1", "default": false, "description": "release notes category"}]}
{"number": 84115, "title": "[test_nn] move the remaining pooling tests to test/nn/test_pooling", "time": "2022-08-26T11:38:31Z", "body": "Ref: #63085", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 84104, "title": "Enable maxpool_2d in NNC", "time": "2022-08-26T02:45:53Z", "body": "This PR implemented maxpool2d for NNC with external call.\r\n \r\n- maxpool2d NNC lowering function implementation and lowering path enabling:\r\n```\r\ntorch/csrc/jit/tensorexpr/operators/reduction.cpp\r\ntorch/csrc/jit/tensorexpr/operators/reduction.h\r\ntorch/csrc/jit/tensorexpr/lowerings.cpp\r\n```\r\n\r\n- maxpool2d NNC external call implementations for both default and out version\r\n```\r\ntorch/csrc/jit/tensorexpr/external_functions.cpp\r\ntorch/csrc/jit/tensorexpr/codegen.cpp\r\n```\r\n\r\n- add into TE fuser reduce group\r\n```\r\ntorch/csrc/jit/passes/tensorexpr_fuser.cpp\r\n```\r\n\r\n- Add related test case for quantization and non-quantization scenarios:\r\n```\r\ntest/cpp/tensorexpr/test_quantization.cpp\r\n```\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2510754463, "node_id": "MDU6TGFiZWwyNTEwNzU0NDYz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/NNC", "name": "NNC", "color": "e5678d", "default": false, "description": ""}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 84100, "title": "[ROCm] Enable test_filtering_env_var", "time": "2022-08-26T01:31:07Z", "body": "The test \"test_filtering_env_var\" requires CPU runner along with GPU.\r\nHence enable both runners for ROCm\r\n\r\nSigned-off-by: Jagadish Krishnamoorthy <jagdish.krishna@gmail.com>\r\n\r\nEnables the test disabled by #56178\r\n\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport", "label": [{"id": 1078897659, "node_id": "MDU6TGFiZWwxMDc4ODk3NjU5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20rocm", "name": "module: rocm", "color": "f7e101", "default": false, "description": "AMD GPU support for Pytorch"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 4804263862, "node_id": "LA_kwDOA-j9z88AAAABHls_tg", "url": "https://api.github.com/repos/pytorch/pytorch/labels/rocm", "name": "rocm", "color": "B3FC7B", "default": false, "description": "This tag is for PRs from ROCm team"}]}
{"number": 84087, "title": "OpInfo: Add outputs_uninitialized property", "time": "2022-08-25T22:33:20Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #84087\n\nCurrently all empty-like functions manually skip lots of OpInfo tests\nbecause their tensor ouputs are uninitialized and so can't be compared\nwith `assertEqual`. Instead, this adds an `assertOutputsMatch` method\nwhich will check the `outputs_uninitialized` property and if it's set\nit will only compare tensor meta-data as if the tensor was a\nmeta-device tensor.\n\nTo support the metadata-only comparison, I've split the\n`TensorLikePair` class into `TensorMetaPair` for comparing attributes\nand `TensorLikePair` which is derived from `TensorMetaPair` and adds\nthe value-comparisons.", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 84086, "title": "Remove guards from Python stack tracing OD flow", "time": "2022-08-25T21:34:09Z", "body": "Summary:\nDiff for removing #ifdef guard from Python stack tracing option on on-demand flow.\n1. Point a new Kineto submodule to compile in CIs with kineto.submodule.txt\n2. The new Kineto submodule has 'withStack' inside #ifdef USE_KINETO_MIN_CHANGE\n3. This diff removed #ifdef from codes\n4. but we still need another diff to deprecate 'USE_KINETO_MIN_CHANGE' from TARGETS and .bzl files.\n\nTest Plan:\nlaunch a python test case with the following command for on-demand flow:\necho -e \"PYTHON_STACK_TRACE=true\" > /tmp/scott_kineto.conf && dyno gputrace --gputrace_duration 300ms --gpuconf /tmp/scott_kineto.conf\n\nDifferential Revision: D38963630\n\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 84077, "title": "[ROCm] add case for FP32MatMulPattern skip property", "time": "2022-08-25T19:11:38Z", "body": "TF32 is not supported on ROCm and hence the torch/profiler/_pattern_matcher.py FP32MatMulPattern should return False for ROCm instead of checking the results of torch.cuda.get_arch_list().  Depending on the gfx arch running the test, test_profiler.py's test_profiler_fp32_matmul_pattern (__main__.TestExperimentalUtils) will fail otherwise.\r\n\r\nSigned-off-by: Jagadish Krishnamoorthy <jagdish.krishna@gmail.com>\r\n\r\n\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport", "label": [{"id": 1078897659, "node_id": "MDU6TGFiZWwxMDc4ODk3NjU5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20rocm", "name": "module: rocm", "color": "f7e101", "default": false, "description": "AMD GPU support for Pytorch"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}, {"id": 4804263862, "node_id": "LA_kwDOA-j9z88AAAABHls_tg", "url": "https://api.github.com/repos/pytorch/pytorch/labels/rocm", "name": "rocm", "color": "B3FC7B", "default": false, "description": "This tag is for PRs from ROCm team"}]}
{"number": 84036, "title": "Optimize transpose copy on CPU using fbgemm transpose", "time": "2022-08-25T03:19:39Z", "body": "### Description\r\nOptimize transpose copy on CPU using fbgemm transpose\r\n\r\n### Testing\r\nsingle socket (28cores):\r\n```\r\nbefore: torch.Size([10, 128, 10, 124]) -> torch.Size([10, 128, 124, 10]) fp32: 4.819e-05 s; bf16: 4.846e-05 s\r\n        torch.Size([10, 128, 30, 124]) -> torch.Size([10, 128, 124, 30]) fp32: 0.000171 s; bf16: 0.000129 s\r\n            \r\nafter: torch.Size([10, 128, 10, 124]) -> torch.Size([10, 128, 124, 10])  fp32: 2.439e-05 s; bf16: 2.152e-05 s\r\n        torch.Size([10, 128, 30, 124]) -> torch.Size([10, 128, 124, 30]) fp32: 0.000132 s; bf16: 3.916e-05 s\r\n```\r\nsingle core:\r\n```\r\nbefore: torch.Size([10, 128, 10, 124]) -> torch.Size([10, 128, 124, 10]) fp32: 0.00109 s;  bf16: 0.00103 s\r\n        torch.Size([10, 128, 30, 124]) -> torch.Size([10, 128, 124, 30]) fp32: 0.00339 s; bf16: 0.00295 s\r\n            \r\nafter: torch.Size([10, 128, 10, 124]) -> torch.Size([10, 128, 124, 10]) fp32: 0.000566  s; bf16: 0.000382 s\r\n        torch.Size([10, 128, 30, 124]) -> torch.Size([10, 128, 124, 30]) fp32: 0.00282 s; bf16: 0.000999 s\r\n```\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 84025, "title": "[quant] perchannel quant in the convTranspose", "time": "2022-08-24T22:43:50Z", "body": "The per channel quantization was disabled because of ambiguity for\r\nthe case where `groups > 1`. However, for the cases where there is only\r\na single group, we can run the transposed convolution, and raise an\r\nerror otherwise.\r\n\r\nFixes #ISSUE_NUMBER\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 84024, "title": "Suppress unused parameters in PyTorch (#73821)", "time": "2022-08-24T22:16:42Z", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/73821\n\nDifferential Revision: D34631600\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 84020, "title": "[MRS-Inf] Deepcopy to avoid errors from inplace operators (#1245)", "time": "2022-08-24T21:49:38Z", "body": "Summary: Pull Request resolved: https://github.com/pytorch/FBGEMM/pull/1245\n\nReviewed By: houseroad\n\nDifferential Revision: D38716708\n\n\n\ncc @ezyang @SherlockNoMad @soumith @EikanWang @jgong5 @wenzhe-nrv", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}, {"id": 4347715449, "node_id": "LA_kwDOA-j9z88AAAABAyTfeQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fx", "name": "fx", "color": "ededed", "default": false, "description": null}]}
{"number": 84004, "title": "Ability to add comments to merge_rules.json", "time": "2022-08-24T18:17:16Z", "body": "Fixes #83441\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 84001, "title": "Enable record_stack_traces for make_fx", "time": "2022-08-24T17:37:46Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #84001\n\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 4347715449, "node_id": "LA_kwDOA-j9z88AAAABAyTfeQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fx", "name": "fx", "color": "ededed", "default": false, "description": null}]}
{"number": 84000, "title": "[WIP] symintifying reshape ", "time": "2022-08-24T17:34:47Z", "body": "Fixes #ISSUE_NUMBER\r\n\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 83997, "title": "Add x86_64 intrinsics guard", "time": "2022-08-24T17:18:13Z", "body": "Test Plan: Run tests\n\nDifferential Revision: D38964321\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}]}
{"number": 83989, "title": "[not4land] Added sparse mm using cusparselt", "time": "2022-08-24T15:11:06Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #86299\n* #85835\n* __->__ #83989\n* #84759\n\n\n\nDifferential Revision: [D39393372](https://our.internmc.facebook.com/intern/diff/D39393372)", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3769217376, "node_id": "LA_kwDOA-j9z87gqbFg", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20sparse", "name": "release notes: sparse", "color": "2A7626", "default": false, "description": "release notes category"}]}
{"number": 83962, "title": "[Profiler] Microbenchmarks for tuned components", "time": "2022-08-24T05:13:10Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #83962\n\nSimple Google benchmark for some of the new low overhead data structures. For both a simple int and a larger struct (in this case an array of four ints) AppendOnlyList is markedly faster than the STL containers, with an optimal chunk size of ~512-1024. I also did a test with writes to multiple containers to look at cache contention; everything is a bit slower, but the general trend holds so I don't think cache contention is an issue. And finally, the wall clocks are quite clear that `std::chrono` is slow and `__rdtsc` is almost free.\n\nDifferential Revision: [D34883031](https://our.internmc.facebook.com/intern/diff/D34883031/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D34883031/)!", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 83944, "title": "[ONNX] Opset 17 STFT support", "time": "2022-08-23T22:18:23Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #83944\r\n\r\n#81075 \r\n\r\nTODO: \r\n- Split complex input into real\r\n- Test\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 83897, "title": "[NNC] support conv hardswish fusion", "time": "2022-08-23T07:21:04Z", "body": "## Pitch\r\nEnable Conv-HardSwish fusion in NNC.\r\n\r\n## Performance\r\nThe performance has been evaluated in https://github.com/pytorch/pytorch/pull/82705.\r\n\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel @mingfeima @XiaobingSuper @ashokei @jingxu10", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2510754463, "node_id": "MDU6TGFiZWwyNTEwNzU0NDYz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/NNC", "name": "NNC", "color": "e5678d", "default": false, "description": ""}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 83896, "title": "[NNC] enable fusion of Linear with elementwise OPs", "time": "2022-08-23T07:20:37Z", "body": "## Pitch\r\nEnable Linear-Eltwise fusion in NNC.\r\n\r\n## Description\r\nThe code change is similar to https://github.com/pytorch/pytorch/pull/77157 which has enabled conv2d related fusions. This PR adds a fusion pass to fuse Linear with elementwise OP for TE subgraph. This pass will insert prepack and packed run ops for linear and enable fusion of linear with elementwise OPs. The fused packed run ops is implemented via external call in NNC.\r\n\r\nThe below elementwise fusion with Linear is added in this PR:\r\n- relu\r\n- sigmoid\r\n- tanh\r\n- leaky_relu\r\n- hardtanh\r\n- gelu\r\n- clamp\r\n\r\n## Code structure\r\n\r\nNNC integration of fused linear-eltwise OP via external call is located in:\r\n```\r\ntorch/csrc/jit/tensorexpr/kernel.cpp\r\n\r\ntorch/csrc/jit/tensorexpr/operators/matmul.h\r\ntorch/csrc/jit/tensorexpr/operators/matmul.cpp\r\n\r\ntorch/csrc/jit/tensorexpr/lowerings.cpp\r\ntorch/csrc/jit/tensorexpr/external_functions.cpp\r\n```\r\n\r\nFused linear OP implementation is done in:\r\n```\r\naten/src/ATen/native/mkldnn/LinearPrepack.h\r\naten/src/ATen/native/mkldnn/LinearPrepack.cpp\r\n```\r\n\r\nThis PR has the dependency on https://github.com/pytorch/pytorch/pull/83249. I've separated the changes of https://github.com/pytorch/pytorch/pull/83249 in one commit here https://github.com/pytorch/pytorch/commit/c5b24227d3c9ded70ddb268a622986497cc4182e.\r\n\r\n## UT\r\n```\r\ntest/test_mkldnn_fusion.py\r\n```\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 83883, "title": "[WIP] OOM callback with alloc trace cache", "time": "2022-08-23T02:42:08Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #83883\n\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 83882, "title": "[deploy][inference] Load extra_files of package in cpp (needed for saved requests and warmup)", "time": "2022-08-23T01:25:12Z", "body": "Summary:\n- Adds a method to the package struct to enable reading of data saved in \"extra_files\" of the package\n- D38378610 **[torchrec] Allow storing binary data in extra_files** - added how to save and read the extra files in python\n- For warmup, I need a way to save the requests, which can only be read as binary in python and the extra_files provides a convenient way to do so\n\nTest Plan: Tested that I can save a recordio file in python as binary, and read it as a string in cpp\n\nDifferential Revision: D38914842\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 83874, "title": "[PyTorch][caffe2] Migrate to CAFFE_{DECLARE,DEFINE}_KNOWN_TYPE within caffe2/", "time": "2022-08-22T22:04:03Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #83874\n\nPrevious diff added these macros. We should use them where appropriate so as not to leave bad examples laying around.\n\nDifferential Revision: [D38923357](https://our.internmc.facebook.com/intern/diff/D38923357/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D38923357/)!", "label": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20distributed", "name": "oncall: distributed", "color": "f7e101", "default": false, "description": "Add this issue/PR to distributed oncall triage queue"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}]}
{"number": 83873, "title": "[test only]", "time": "2022-08-22T21:55:59Z", "body": "Differential Revision: D38922822\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 83862, "title": "[PyTorch] Fix default arg for MKL thread count", "time": "2022-08-22T18:22:42Z", "body": "Summary:\nThe `caffe2_mkl_num_threads` arg controls how many threads the [MKL library](https://www.intel.com/content/www/us/en/develop/documentation/get-started-with-mkl-for-dpcpp/top.html) will use when doing matrix multiplication and other linear algebra. It's described like this:\n> The number of mkl threads. 0 to use default value.\n\nHowever, the code that parses it is buggy. Setting it to 0 does *not* make it use the default value (which would be the number of CPU cores). Instead, it gets set to 1, which disables threading.\n\nThis diff fixes the logic so that MKL will be threaded by default; threading can still be disabled if desired by setting the arg to 1 explicitly.\n\nThis causes **a ≈ 7x speedup** for matrix multiplication inside of PyTorch on apps that use the default args.\n\nDifferential Revision: D38911802\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 83850, "title": "[functorch] add size correctness test", "time": "2022-08-22T15:59:06Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #83850\n\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 83830, "title": "Reset torchdynamo before running individual tests", "time": "2022-08-22T03:46:46Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #83830\n\nThis ensures that tests are more isolated against each other\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 83816, "title": "Enable xdoctest runner in CI for real this time", "time": "2022-08-21T05:40:32Z", "body": "Builds on #83317 and enables running the doctests. Just need to figure out what is causing the failures.\n\ncc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu", "label": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20distributed", "name": "oncall: distributed", "color": "f7e101", "default": false, "description": "Add this issue/PR to distributed oncall triage queue"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769204372, "node_id": "LA_kwDOA-j9z87gqX6U", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20releng", "name": "release notes: releng", "color": "EED552", "default": false, "description": "release notes category"}]}
{"number": 83812, "title": "[primTorch] Add refs for diff and trapezoid", "time": "2022-08-21T02:02:20Z", "body": "This PR adds refs for:\r\n* torch.diff\r\n* torch.trapezoid", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 83806, "title": "Test onnx quantized tensor as arg", "time": "2022-08-20T16:19:56Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 83796, "title": "Make ProcessGroupNCCL work.wait() respect timeout ", "time": "2022-08-20T10:01:36Z", "body": "Fixes #83486 \r\nIn the `synchronizeInternal` function, the `timedOut` function does not take a parameter and instead defaults to checking the timeout period to `opTimeout_`.\r\nhttps://github.com/pytorch/pytorch/blob/9e1daf764419fb0b57c66dedce486e067cdd6be0/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp#L449\r\nhttps://github.com/pytorch/pytorch/blob/9e1daf764419fb0b57c66dedce486e067cdd6be0/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp#L534\r\nThis PR adds an overloaded version of the `timedOut` function that takes `timeout` as a parameter and checks if the object has timedout based on that.\r\n\n\ncc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu", "label": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20distributed", "name": "oncall: distributed", "color": "f7e101", "default": false, "description": "Add this issue/PR to distributed oncall triage queue"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769218061, "node_id": "LA_kwDOA-j9z87gqbQN", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(c10d)", "name": "release notes: distributed (c10d)", "color": "EE86D4", "default": false, "description": "release notes category"}]}
{"number": 83792, "title": "[primTorch] Add ref for kron", "time": "2022-08-20T03:56:41Z", "body": "This PR adds a ref implementation for torch.kron. ", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 83786, "title": "asyncio increase throughput", "time": "2022-08-20T00:25:27Z", "body": "Summary:\r\nThis diffs add a check in the fetcher, that if the dataset to be fetched has a function \"getitems\" then use it for fetching a batch of elements, as oppose to one by one. This is benefical for io bounded usage.\r\n\r\nDifferential Revision: D38876984\r\n\r\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 83784, "title": "fx quant: fix issue with FX quant for x.view(x.size(...), ...)", "time": "2022-08-19T23:09:30Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#83784 fx quant: fix issue with FX quant for x.view(x.size(...), ...)**\n\nSummary:\n\nhttps://github.com/pytorch/pytorch/issues/83658 reported that ops\nfollowed by a certain pattern of `view` and `size` ops were not quantized\ncorrectly by FX graph mode quantization.\n\nBefore this PR, the \"size\" op was in the \"op shares qparams with input\"\ncategory, and the code assumed that the input of this op has the same dtype\nas its output. This led to incorrectly propagating the `int` dtype\nas the output of whichever op was preceding the `view` op, which in turn\nmade that op blocklisted from quantization.\n\nThe fix is to create a new category of ops which work on different dtypes\nof tensors but are not observed.  This PR does so for `size`, and also for\n`shape` since it works the same way.\n\nTest plan:\n\n```\npython test/test_quantization.py -k test_linear_view_size\n```\n\ncc @ezyang @SherlockNoMad @soumith @EikanWang @jgong5 @wenzhe-nrv", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}, {"id": 4347715449, "node_id": "LA_kwDOA-j9z88AAAABAyTfeQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fx", "name": "fx", "color": "ededed", "default": false, "description": null}]}
{"number": 83779, "title": "[wip] dispatchable custom_vjp prototype", "time": "2022-08-19T22:12:29Z", "body": null, "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 83772, "title": "[WIP] OOM callback", "time": "2022-08-19T21:32:50Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #83772\n\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 83767, "title": "Test enable flatbuffers", "time": "2022-08-19T20:19:15Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #83767\n* #83766\n* #83605\n* #82829\n* #82828\n\nDifferential Revision: [D38554124](https://our.internmc.facebook.com/intern/diff/D38554124/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D38554124/)!", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 83766, "title": "Test disable flatbuffers", "time": "2022-08-19T20:19:10Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #83767\n* __->__ #83766\n* #83605\n* #82829\n* #82828\n\nDifferential Revision: [D38543806](https://our.internmc.facebook.com/intern/diff/D38543806/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D38543806/)!\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768809762, "node_id": "LA_kwDOA-j9z87go3ki", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20mobile", "name": "release notes: mobile", "color": "bfdadc", "default": false, "description": "release notes category"}]}
{"number": 83765, "title": "[bootcamp][Pytorch]Unit tests for NestedTensor_softmax", "time": "2022-08-19T20:15:57Z", "body": "Summary:\nUnit tests for pytorch _nested_tensor_softmax_ operator.\n\nThe unit tests run cpu/gpu implementations of _nested_tensor_softmax_ depending on devices available.\n\nTest Plan:\narc lint\n    buck test mode/dev-nosan //caffe2/test:nested -- test_nested_tensor_softmax\n\n# CPU impl test:\n{F761843855}\n\n# GPU impl test:\n\nFor test purposes I changed device to cuda temporarily as the device list in the test doesn't include cuda somehow. Below are the test results (the test name still says cpu since the input parameter for device is still cpu but is changed to cuda inside the function).\n\n{F761843395}\n\nThe GPU implementation didn't pass the test cases. This is probably because NestedTensor_softmax_dropout_cuda applied a mask on top, which might achieve different softmax values. The implementation is also temporary, which requires more investigation.\n\nNotice that the attn_mask is ported to the device of query in order for gpu implementation to run.\n\nDifferential Revision: D38791384\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 83755, "title": "quant: remove incorrect integer cast on histogram observer endpoints", "time": "2022-08-19T18:43:20Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#83755 quant: remove incorrect integer cast on histogram observer endpoints**\n\nSummary:\n\nIn https://github.com/pytorch/pytorch/pull/45630 a cast to int was added\nto endpoints of histogram calculation, to make mypy not complain. This is a bug\nbecause we want the histogram endpoints to be in the same dtype as the original\ntensor. Reverting this part of the original PR.\n\nNote: this may not have significant numerical impact, as the\n`HistogramObserver._non_linear_param_search` function is resilient to\nincorrect histogram endpoints and still gives reasonable results for handcrafted\ntensors which are likely to have integer casts of histogram endpoints\ngive incorrect histograms. Because of this, I did not add an additional test case\nto save time, and the main benefit will be code clarity.  If the\n`_non_linear_param_search` function changes in the future, this may need to be\nrevisited.\n\nTest plan:\n\n```\npython test/test_quantization.py -k histogram\n```", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 83752, "title": "[do not land] testing skip-pr-sanity-checks", "time": "2022-08-19T18:25:20Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* __->__ #83752\n* #83751\n\n\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}, {"id": 4443556792, "node_id": "LA_kwDOA-j9z88AAAABCNtLuA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/skip-pr-sanity-checks", "name": "skip-pr-sanity-checks", "color": "BED7D5", "default": false, "description": ""}]}
{"number": 83739, "title": "Add nvfuser support for refs.reshape", "time": "2022-08-19T16:00:55Z", "body": "In addition, nvFuser support is added for prims.split_dim, prims.collapse_view, prims.reshape.\r\n\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel @kevinstephano @jjsjann123 @ezyang @mruberry @ngimel @Lezcano @fdrocha @peterbell10", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}, {"id": 3997471357, "node_id": "LA_kwDOA-j9z87uRJJ9", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20nvfuser", "name": "module: nvfuser", "color": "9CA380", "default": false, "description": ""}, {"id": 4081012228, "node_id": "LA_kwDOA-j9z87zP04E", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20primTorch", "name": "module: primTorch", "color": "455561", "default": false, "description": ""}]}
{"number": 83736, "title": "Support slice.Tensor for nested tensors (forward only)", "time": "2022-08-19T15:27:18Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #83550\n* __->__ #83736\n\r\nAdds NT support for `slice.Tensor` on regular dimensions. This is used in `narrow`, `chunk`, `split`, and `split_with_sizes`, so we get those for (nearly) free. Note that the optional (start, end) arg handling is factored out into a helper function `get_slice_range()` in `TensorShape.h` for use in the normal and nested impls.\r\n\r\nTODO:\r\n* Backwards impl\r\n* Hook up to indexing operator for better advanced slicing support on the Python side\n\nDifferential Revision: [D39157966](https://our.internmc.facebook.com/intern/diff/D39157966)\n\ncc @cpuhrsch @bhosmer @drisspg @mikaylagawarecki", "label": [{"id": 1537268693, "node_id": "MDU6TGFiZWwxNTM3MjY4Njkz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20nestedtensor", "name": "module: nestedtensor", "color": "f7e101", "default": false, "description": "NestedTensor tag see issue #25032"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 4251729749, "node_id": "LA_kwDOA-j9z879bD9V", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20nested%20tensor", "name": "release notes: nested tensor", "color": "13B071", "default": false, "description": "Changes that have a direct impact on nested tensors"}]}
{"number": 83727, "title": "port spmm_reduce to pytorch and optimize it on CPU", "time": "2022-08-19T07:07:07Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* __->__ #83727\n* #87586\n* #82703\n\r\n### Motivation of this PR\r\n\r\nThis patch is to migrate `spmm_reduce` from `torch-sparse` (a 3rd party dependency for PyG) to `torch`, which is a response to the initial proposal for fusion of **Gather, Apply Scatter** in Message Passing of GNN inference/training. https://github.com/pytorch/pytorch/issues/71300\r\n\r\n**GAS** is the major step for Message Passing, the behavior of **GAS** can be classified into 2 kinds depending on the storage type of `EdgeIndex` which records the connections of nodes:\r\n\r\n* COO: the hotspot is `scatter_reduce`\r\n* CSR: the hotspot is `spmm_reduce`\r\n\r\nThe reduce type can be choose from: \"max\", \"mean\", \"max\",  \"min\".\r\n\r\n`spmm_reduce` is registered under the TensorTypeId of `SparseCsrCPU`, and this operator requires an internal interface `_spmm_reduce` which has dual outputs:\r\n* `out` - the actual output\r\n* `arg_out` - records output indices in the non zero elements if the reduce type is \"max\" or \"min\", this is only useful for training. So for inference, it will not be calculated.\r\n\r\n\r\n### Performance\r\n\r\nBenchmark on GCN for obgn-products on Xeon single socket, the workload is improved by `4.3x` with this patch.\r\n\r\nPerformance benefit for training will be bigger, the original backward impl for `sum|mean` is sequential; the original backward impl for `max|min` is not fused.\r\n\r\n\r\n#### before:\r\n```\r\n-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls\r\n-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n       torch_sparse::spmm_sum        97.09%       56.086s        97.09%       56.088s        6.232s             9\r\n                 aten::linear         0.00%      85.000us         1.38%     795.485ms      88.387ms             9\r\n                 aten::matmul         0.00%      57.000us         1.38%     795.260ms      88.362ms             9\r\n                     aten::mm         1.38%     795.201ms         1.38%     795.203ms      88.356ms             9\r\n                   aten::relu         0.00%      50.000us         0.76%     440.434ms      73.406ms             6\r\n              aten::clamp_min         0.76%     440.384ms         0.76%     440.384ms      73.397ms             6\r\n                   aten::add_         0.57%     327.801ms         0.57%     327.801ms      36.422ms             9\r\n            aten::log_softmax         0.00%      23.000us         0.10%      55.503ms      18.501ms             3\r\n```\r\n\r\n#### after\r\n```\r\n-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls\r\n-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n               aten::spmm_sum        87.35%       11.826s        87.36%       11.827s        1.314s             9\r\n                 aten::linear         0.00%      92.000us         5.87%     794.451ms      88.272ms             9\r\n                 aten::matmul         0.00%      62.000us         5.87%     794.208ms      88.245ms             9\r\n                     aten::mm         5.87%     794.143ms         5.87%     794.146ms      88.238ms             9\r\n                   aten::relu         0.00%      53.000us         3.35%     452.977ms      75.496ms             6\r\n              aten::clamp_min         3.35%     452.924ms         3.35%     452.924ms      75.487ms             6\r\n                   aten::add_         2.58%     348.663ms         2.58%     348.663ms      38.740ms             9\r\n                 aten::argmax         0.42%      57.473ms         0.42%      57.475ms      14.369ms             4\r\n            aten::log_softmax         0.00%      22.000us         0.39%      52.605ms      17.535ms             3\r\n```\n\ncc @VitalyFedyunin @jgong5 @XiaobingSuper @sanchitintel @ashokei @jingxu10", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3769217376, "node_id": "LA_kwDOA-j9z87gqbFg", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20sparse", "name": "release notes: sparse", "color": "2A7626", "default": false, "description": "release notes category"}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}, {"id": 4718445952, "node_id": "LA_kwDOA-j9z88AAAABGT3FgA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20gnn", "name": "release notes: gnn", "color": "AC65AD", "default": false, "description": "gnn related optimizations"}]}
{"number": 83725, "title": "[WIP] Some initial autograd tests", "time": "2022-08-19T06:06:13Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 83712, "title": "[DO_NOT_SUBMIT] Test all profiler tree tests", "time": "2022-08-19T00:03:35Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #83712\n\nChange names in order to bypass test disable logic.\n\nDifferential Revision: [D38849248](https://our.internmc.facebook.com/intern/diff/D38849248/)", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}]}
{"number": 83689, "title": "Special Functions", "time": "2022-08-18T17:51:43Z", "body": "Adds special functions operators that a user would expect (i.e., special functions available from Mathematica, MATLAB, or SciPy). All of the special functions defined for complex space have complex implementations. They are also written generically to enable them to be used across dtypes.", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 83677, "title": "[Future] Introduce Future::wait_for variant of wait that takes timeout argument.", "time": "2022-08-18T15:49:04Z", "body": "Future::wait doesn't support timeouts, which is a reliability problem when writing\r\nrobust distributed code.\r\n\r\nThis change introduce it on IValue::Future and expose it on the bindings as wait_for.\r\n\r\nThis is preferable to changing the semantics of wait by introducing an optional argument\r\nas timeouts would be expressed as exceptions instead of the return value which has poor\r\nusability given it's a common case that needs to be handled.\r\n\r\nThis is part of the work to improve Future enough such that it can replace ProcessGroup::Work.\r\n\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 83670, "title": "reinplace fix for overlapping memory", "time": "2022-08-18T14:33:22Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #83701\n* __->__ #83670\n* #83626\n* #83542\n* #83590\n\n\n\ncc @ezyang @SherlockNoMad @soumith @EikanWang @jgong5 @wenzhe-nrv", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}, {"id": 4347715449, "node_id": "LA_kwDOA-j9z88AAAABAyTfeQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fx", "name": "fx", "color": "ededed", "default": false, "description": null}]}
{"number": 83651, "title": "[WIP] remove inplace mul in SymInt", "time": "2022-08-18T03:35:37Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 83640, "title": "release the current symintnode in the move c-tor", "time": "2022-08-18T01:06:58Z", "body": "release the current symintnode in the move c-tor\r\n\r\nThis should fix the following memory leak:\r\n```\r\n  auto a = c10::make_intrusive<c10::SymIntNodeImpl>();\r\n  auto b = c10::make_intrusive<c10::SymIntNodeImpl>();\r\n  std::cout << \"ac = \" << a.use_count() << \" bc = \" << b.use_count() << std::endl;\r\n  auto as = a->toSymInt();\r\n  auto bs = b->toSymInt();\r\n  std::cout << \"ac = \" << a.use_count() << \" bc = \" << b.use_count() << std::endl;\r\n  std::cout << \"about to reassign SymInts\\n\";\r\n  as = bs;\r\n  std::cout << \"after reassignment\\n\";\r\n  std::cout << \"ac = \" << a.use_count() << \" bc = \" << b.use_count() << std::endl;\r\n```\r\n\r\n\r\n```\r\n[ RUN      ] TestIntrusive.Basic\r\nCreating SymIntNodeImpl 0x42ac330\r\nCreating SymIntNodeImpl 0x428cd80\r\nac = 1 bc = 1\r\nac = 2 bc = 2\r\nabout to reassign SymInts\r\nafter reassignment\r\nac = 2 bc = 3\r\nDestroying SymIntNodeImpl 0x428cd80\r\n```\r\n\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 83631, "title": "[WIP] Testing libiomp build", "time": "2022-08-17T22:42:07Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 4077622962, "node_id": "LA_kwDOA-j9z87zC5ay", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/binaries_conda", "name": "ciflow/binaries_conda", "color": "3FC2AC", "default": false, "description": "Trigger binary build and upload jobs for conda on the PR"}]}
{"number": 83630, "title": "Add lazy shape inference for celu op", "time": "2022-08-17T22:33:59Z", "body": "Add lazy shape inference for `celu` op", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 83618, "title": "[functorch] reclassifying max_unpool failures", "time": "2022-08-17T19:13:15Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #83619\n* __->__ #83618\n* #83614\n* #83612\n* #83178\n* #83597\n* #83530\n\nThey're expected because this variant of max_unpool is non-determnistic\n\nAlso removed a skip that seems to pass alright on my machine", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 83609, "title": "Verify conv input & parameters have the same dtype for all backends", "time": "2022-08-17T18:05:59Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* __->__ #83609\r\n\r\nFixes #83328", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 83605, "title": "Clean up dependancy for flatbuffer_loader", "time": "2022-08-17T16:50:25Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #83605\n\nDifferential Revision: [D38445936](https://our.internmc.facebook.com/intern/diff/D38445936/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D38445936/)!\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768809762, "node_id": "LA_kwDOA-j9z87go3ki", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20mobile", "name": "release notes: mobile", "color": "bfdadc", "default": false, "description": "release notes category"}]}
{"number": 83596, "title": "[not4land] Added per channel broadcast addition and scalar broad multiplication", "time": "2022-08-17T15:28:46Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #83989\n* __->__ #83596\n\nThere appears to be an issue with this PR with some of the test cases.\nTo run the test case, run\n```\npython test/test_quantization.py -k test_qconv2d_cudnn\n```\nfrom the main pytorch directory.\nThis test uses a single example consisting of conv_op(input, weight) *\nrequantize_op, where requantize_op uses the newly introduced by value\npassing feature, which seems to have introduced an accuracy issue, and\nthere's a mismatch between the reference answer (doesn't use cuDNN) and\ny_q (uses cuDNN)", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 83593, "title": "Tensorboard image summary speedup", "time": "2022-08-17T14:14:14Z", "body": "Scale only if necessary during image summary creation for tensorboard.\r\n\r\nFixes #83589\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1349952767, "node_id": "MDU6TGFiZWwxMzQ5OTUyNzY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20tensorboard", "name": "module: tensorboard", "color": "f7e101", "default": false, "description": ""}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 83550, "title": "Introduce SDP op to BT fastpath", "time": "2022-08-16T22:10:03Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #83550\n* #83736\n\r\nUse the `_scaled_dot_product_attention` op within `_native_multi_head_attention` instead of manual computation. This also allows us to stay in NT land longer. When we back the op with a fast kernel (flash attention or triton), BT will get a nice speedup as well.\r\n\r\nNote: The fused `_transform_bias_rescale_qkv` kernel is no longer used and can be removed in a follow-up PR.\r\n\r\nTODO:\r\n* Benchmarking", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 83546, "title": "Support more symintnode operations", "time": "2022-08-16T22:05:46Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 83544, "title": "[WIP] Remove SymIntNode code for mobile builds", "time": "2022-08-16T21:30:19Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}]}
{"number": 83525, "title": "Andysamfb patch 1", "time": "2022-08-16T17:26:38Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 83495, "title": "change the default warmup iterations to 11", "time": "2022-08-16T02:46:50Z", "body": "Summary: For CUDA graph, `DataDistributedParallel` needs 11 iterations for warm up, so we can warm up 11 iterations just to be safer?\n\nTest Plan: CI\n\nDifferential Revision: D38717891\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 83460, "title": "[fix] don't ignore sample_kwargs for UnaryUfuncInfo", "time": "2022-08-15T20:30:48Z", "body": "Fixes #82785\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 83416, "title": "[JIT] Support NamedTuple type inference", "time": "2022-08-15T11:32:06Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* __->__ #83416\n\nThis will allow scripting models that have a parameter that is a\nNamedTuple, but which is not explicitly annotated.\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 83348, "title": "add rematerialization code", "time": "2022-08-12T20:17:15Z", "body": "Fixes #ISSUE_NUMBER\r\n\n\ncc @ezyang @SherlockNoMad @soumith @EikanWang @jgong5 @wenzhe-nrv", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}, {"id": 4347715449, "node_id": "LA_kwDOA-j9z88AAAABAyTfeQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fx", "name": "fx", "color": "ededed", "default": false, "description": null}]}
{"number": 83308, "title": "[fx] Allow custom deepcopy behavior in tracer subclasses", "time": "2022-08-12T00:01:25Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* __->__ #83308\n\nIf a subclass of the tracer needs custom deepcopy handling, then it can override `deepcopy_special_behavior` to specify how to handle extra ops.\n\nAlternative 1: just have the subclass override `__deepcopy__` itself. Issue - if fx.Tracer adds a new attribute that needs special handling, then it breaks the deepcopy-ability of the subclass.\n\nAlternative 2: Have the subclass call `super().__deepcopy__()`, then handle its own special cases separately. Issue - super().__deepcopy__() will fail because the special case will still show up in self.__dict__.items().\n\nDifferential Revision: [D38643071](https://our.internmc.facebook.com/intern/diff/D38643071)\n\ncc @ezyang @SherlockNoMad @soumith @EikanWang @jgong5 @wenzhe-nrv", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}, {"id": 4347715449, "node_id": "LA_kwDOA-j9z88AAAABAyTfeQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fx", "name": "fx", "color": "ededed", "default": false, "description": null}]}
{"number": 83306, "title": "Add a workflow to cache third party dependencies on S3", "time": "2022-08-11T22:36:19Z", "body": "For the context, see #75703, pytorch/builder#1096.\r\n\r\nNote: depends on the docker image `pytorch/sync_s3_thirdparty_deps` from pytorch/builder#1096\r\n\r\nSummary of additions:\r\n* workflow config (based on pytorch/sync_s3_thirdparty_deps GH action)\r\n* S3 mapping config (sync_s3_cache.yml)\r\n", "label": [{"id": 2404913419, "node_id": "MDU6TGFiZWwyNDA0OTEzNDE5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Merged", "name": "Merged", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2510927053, "node_id": "MDU6TGFiZWwyNTEwOTI3MDUz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Reverted", "name": "Reverted", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 83298, "title": "[static-runtime] add microbenchmark for permute_copy op", "time": "2022-08-11T21:05:06Z", "body": "Test Plan: P521747402\n\nDifferential Revision: D38634961\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 83279, "title": "[JIT] Throw a better error for Tuple[UnsupportedType, ...] annotations", "time": "2022-08-11T18:24:03Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* __->__ #83279\n\nPreviously, this type of annotation would work as follows:\n\n```\n- try_ann_to_type(Tuple[UnsupportedTupe, ...]):\n    recursively calls for tuple members\n    - try_ann_to_type(UnsupportedType):\n        doesn't find a match, returns none\n    Tries to create a TupleType[parsed_types]\n    fails because TupleType[None, ...] fails\n```\n\nThis made it hard to debug where the error was actually coming from, because there were no Tuple[None, ...] annotations.\n\nThis change will improve the error message.\n\nDifferential Revision: [D38628458](https://our.internmc.facebook.com/intern/diff/D38628458)\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 83265, "title": "remove BC-guarantee for reinplace pass", "time": "2022-08-11T15:40:45Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #85844\n* #85875\n* #85681\n* #82601\n* #82602\n* #82771\n* __->__ #83265\n\n\n\ncc @ezyang @SherlockNoMad @soumith @EikanWang @jgong5 @wenzhe-nrv", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}, {"id": 4347715449, "node_id": "LA_kwDOA-j9z88AAAABAyTfeQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fx", "name": "fx", "color": "ededed", "default": false, "description": null}]}
{"number": 83262, "title": "[DO NOT MERGE, PROTOTYPE ONLY] Prototype a dummy operation with dispatched backends", "time": "2022-08-11T15:22:35Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#83262 [DO NOT MERGE, PROTOTYPE ONLY] Prototype a dummy operation with dispatched backends**\n* #83261 [2/N] [Dispatchable Collectives] Extract ProcessGroup::Work into a separate class and update references\n* #83260 [1/N] [Dispatchable Collectives] Create Backend class\n\r\n## Context\r\n\r\nWe will have `torch.distributed` automatically support device types without user creating extra PGs. The ProcessGroup base class will contain a list of backends to be invoked by dispatcher.  This will allow operations such as `dist.broadcast` to operate on both cpu and cuda tensors and we will route to their respective backends internally.\r\n\r\n## Example\r\n\r\n```python\r\n# GLOG_v=1 python script.py \r\nimport torch\r\nimport torch.distributed as dist\r\nimport os\r\n\r\nos.environ[\"MASTER_ADDR\"] = \"localhost\"\r\nos.environ[\"MASTER_PORT\"] = \"29500\"\r\nos.environ[\"RANK\"] = \"0\"\r\nos.environ[\"WORLD_SIZE\"] = \"1\"\r\nos.environ[\"TORCH_SHOW_CPP_STACKTRACES\"] = \"1\"\r\n\r\ndef main():\r\n    # Note: we are only specifying \"backend\" as \"nccl\" due to the current required argument,\r\n    # In future updates this backend string will be made optional\r\n    dist.init_process_group(\"nccl\")\r\n    print(\"finished creating process group\")\r\n    pg = dist._get_default_group()\r\n    t = [torch.ones(1)]\r\n    t_cuda = [torch.tensor([1, 2, 3], device=\"cuda:0\")]\r\n    num_eqs = 20\r\n    # Call broadcast on PG with different tensor device types\r\n    print(f\"{'=' * num_eqs} Calling Dummy Broadcast with tensor {t} {'=' * num_eqs}\")\r\n    pg._dummy_broadcast(t)\r\n    print(f\"{'=' * num_eqs} Calling Dummy Broadcast with tensor {t_cuda} {'=' * num_eqs}\")\r\n    pg._dummy_broadcast(t_cuda)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n## Output\r\n\r\nnote the different \"dummy_broadcast\" function execution based on tensor device\r\n```\r\nfinished creating process group\r\n==================== Calling Dummy Broadcast with tensor [tensor([1.])] ====================\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nI0811 08:19:45.877725 3539957 ProcessGroup.cpp:117] in _DummyBroadcast\r\nI0811 08:19:45.877765 3539957 ProcessGroup.cpp:126] calling op\r\nI0811 08:19:45.877794 3539957 ProcessGroup.cpp:92] in _dummy_broadcast_cpu_\r\nin DummyProcessGroupBackend::broadcast\r\n==================== Calling Dummy Broadcast with tensor [tensor([1, 2, 3], device='cuda:0')] ====================\r\nI0811 08:19:45.883888 3539957 ProcessGroup.cpp:117] in _DummyBroadcast\r\nI0811 08:19:45.883900 3539957 ProcessGroup.cpp:126] calling op\r\nI0811 08:19:45.883921 3539957 ProcessGroup.cpp:106] in _dummy_broadcast_cuda_\r\nin DummyProcessGroupBackend::broadcast\r\n```\r\n\r\n## Changes\r\n\r\n1. Introduce new `_dummy_broadcast` API in ProcessGroup pybind definition. This calls into `_DummyBroadcast` which is a collective implemented on ProcessGroup.cpp (Note all other collectives are defined as virtual methods)\r\n2. Register a new operation to the dispatcher called `dummy_broadcast_` in Ops.cpp, used custom classes for the schema definition\r\n3. Add the cpu_impl and cuda_impl for dummy broadcast as definitions in ProcessGroup\r\n4. Add a new backend mapping property for ProcessGroup\n\ncc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @kwen2501 @awgu", "label": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20distributed", "name": "oncall: distributed", "color": "f7e101", "default": false, "description": "Add this issue/PR to distributed oncall triage queue"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769218061, "node_id": "LA_kwDOA-j9z87gqbQN", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(c10d)", "name": "release notes: distributed (c10d)", "color": "EE86D4", "default": false, "description": "release notes category"}]}
{"number": 83258, "title": "Fix static initialization issue for static build (#83255)", "time": "2022-08-11T13:43:27Z", "body": "Fixes #83255\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 83253, "title": "Add new checks in CI system to verify the built linux conda wheel with cpu-cxx11-abi", "time": "2022-08-11T12:22:52Z", "body": "Add new checks in CI system to verify the built linux conda wheel with cpu-cxx11-abi\r\n\r\nThis PR is to validate https://github.com/pytorch/builder/pull/1095.\r\nCo-authored-by: Zhu Hong <hong.zhu@intel.com>\r\nCo-authored-by: Guo Yejun <yejun.guo@intel.com>\r\n\r\nFixes #ISSUE_NUMBER\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3200025338, "node_id": "MDU6TGFiZWwzMjAwMDI1MzM4", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel%20priority", "name": "intel priority", "color": "4E4EF8", "default": false, "description": "matters to intel architecture from performance wise"}, {"id": 3618084153, "node_id": "LA_kwDOA-j9z87Xp5U5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/binaries", "name": "ciflow/binaries", "color": "245567", "default": false, "description": "Trigger all binary build and upload jobs on the PR"}, {"id": 4077622962, "node_id": "LA_kwDOA-j9z87zC5ay", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/binaries_conda", "name": "ciflow/binaries_conda", "color": "3FC2AC", "default": false, "description": "Trigger binary build and upload jobs for conda on the PR"}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 83249, "title": "[NNC] add more fusion of elementwise OPs with conv", "time": "2022-08-11T09:56:55Z", "body": "## Pitch\r\nThis PR adds the below elementwise fusion for conv in NNC:\r\n\r\n- sigmoid\r\n- tanh\r\n- leaky_relu\r\n- hardtanh\r\n- gelu\r\n- clamp\r\n\r\n## Description\r\nExtend the graph rewrite and the fusion attribute map to support the below categories of elementwise OPs:\r\n\r\n- OPs that only have the activation tensor as input:\r\n  - `sigmoid(Tensor self) -> Tensor`\r\n  - `tanh(Tensor self) -> Tensor`\r\n- OPs that have activation tensor and scalar inputs:\r\n  - `leaky_relu(Tensor self, Scalar negative_slope=0.01) -> Tensor`\r\n  - `hardtanh(Tensor self, Scalar min_val=-1, Scalar max_val=1) -> Tensor`\r\n- OPs that have activation tensor and string inputs:\r\n  - `gelu(Tensor self, *, str approximate='none') -> Tensor`\r\n- OPs that have activation tensor and optional scalar inputs:\r\n  - `clamp(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor`\r\n \r\n### UT\r\n```\r\ntest/test_mkldnn_fusion.py\r\n```\r\n\r\n## Benchmark\r\nConfiguration\r\n- Measured on Ice Lake\r\n- Jemalloc enabled\r\n- batch_size = 1\r\n- Channels Last format\r\n\r\n### Single thread:\r\n<html xmlns:v=\"urn:schemas-microsoft-com:vml\"\r\nxmlns:o=\"urn:schemas-microsoft-com:office:office\"\r\nxmlns:x=\"urn:schemas-microsoft-com:office:excel\"\r\nxmlns=\"http://www.w3.org/TR/REC-html40\">\r\n\r\n<head>\r\n\r\n<meta name=ProgId content=Excel.Sheet>\r\n<meta name=Generator content=\"Microsoft Excel 15\">\r\n<link id=Main-File rel=Main-File\r\nhref=\"file:///C:/Users/chunyuan/AppData/Local/Temp/msohtmlclip1/01/clip.htm\">\r\n<link rel=File-List\r\nhref=\"file:///C:/Users/chunyuan/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml\">\r\n\r\n</head>\r\n\r\n<body link=\"#0563C1\" vlink=\"#954F72\">\r\n\r\n\r\n\r\nshape | time (us)_no_fusion | time (us)_fusion | Gain\r\n-- | -- | -- | --\r\nConv+tanh: kernel=3, N=1, iC=64, H=56,   W=56, oC=64, stride=1, pad=1, dilates=1, g=1 | 2407.23 | 1541.44 | 35.97%\r\nConv+tanh: kernel=1, N=1, iC=256, H=56,   W=56, oC=512, stride=2, pad=0, dilates=1, g=1 | 3316.45 | 1759.5 | 46.95%\r\nConv+tanh: kernel=3, N=1, iC=256, H=56,   W=56, oC=256, stride=1, pad=1, dilates=1, g=32 | 5685.67 | 3250.43 | 42.83%\r\nConv+tanh: kernel=3, N=1, iC=512, H=56,   W=56, oC=512, stride=2, pad=1, dilates=1, g=32 | 4181.54 | 2644.02 | 36.77%\r\nConv+tanh: kernel=1, N=1, iC=64, H=56,   W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 1357.76 | 517.36 | 61.90%\r\nConv+tanh: kernel=1, N=1, iC=256, H=56,   W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 1768.34 | 915.53 | 48.23%\r\nConv+tanh: kernel=1, N=1, iC=256, H=56,   W=56, oC=256, stride=1, pad=0, dilates=1, g=1 | 6329.62 | 3394.09 | 46.38%\r\nConv+tanh: kernel=1, N=1, iC=512, H=28,   W=28, oC=512, stride=1, pad=0, dilates=1, g=1 | 4412.08 | 2850.87 | 35.38%\r\nConv+sigmoid: kernel=3, N=1, iC=64, H=56,   W=56, oC=64, stride=1, pad=1, dilates=1, g=1 | 1822.93 | 1536.77 | 15.70%\r\nConv+sigmoid: kernel=1, N=1, iC=256,   H=56, W=56, oC=512, stride=2, pad=0, dilates=1, g=1 | 2445.69 | 1754.33 | 28.27%\r\nConv+sigmoid: kernel=3, N=1, iC=256,   H=56, W=56, oC=256, stride=1, pad=1, dilates=1, g=32 | 3956.79 | 3342.7 | 15.52%\r\nConv+sigmoid: kernel=3, N=1, iC=512,   H=56, W=56, oC=512, stride=2, pad=1, dilates=1, g=32 | 3237.87 | 2654.81 | 18.01%\r\nConv+sigmoid: kernel=1, N=1, iC=64, H=56,   W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 805.88 | 517.2 | 35.82%\r\nConv+sigmoid: kernel=1, N=1, iC=256,   H=56, W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 1202.22 | 910.34 | 24.28%\r\nConv+sigmoid: kernel=1, N=1, iC=256,   H=56, W=56, oC=256, stride=1, pad=0, dilates=1, g=1 | 4618.22 | 3377.29 | 26.87%\r\nConv+sigmoid: kernel=1, N=1, iC=512,   H=28, W=28, oC=512, stride=1, pad=0, dilates=1, g=1 | 3529.09 | 2862.32 | 18.89%\r\nConv+LeakyRelu: kernel=3, N=1, iC=64,   H=56, W=56, oC=64, stride=1, pad=1, dilates=1, g=1 | 1700.15 | 1459.02 | 14.18%\r\nConv+LeakyRelu: kernel=1, N=1, iC=256,   H=56, W=56, oC=512, stride=2, pad=0, dilates=1, g=1 | 2133.19 | 1612.48 | 24.41%\r\nConv+LeakyRelu: kernel=3, N=1, iC=256,   H=56, W=56, oC=256, stride=1, pad=1, dilates=1, g=32 | 3448.26 | 2670.56 | 22.55%\r\nConv+LeakyRelu: kernel=3, N=1, iC=512,   H=56, W=56, oC=512, stride=2, pad=1, dilates=1, g=32 | 3000.81 | 2498.86 | 16.73%\r\nConv+LeakyRelu: kernel=1, N=1, iC=64,   H=56, W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 685.06 | 446.12 | 34.88%\r\nConv+LeakyRelu: kernel=1, N=1, iC=256,   H=56, W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 1077.33 | 840.69 | 21.97%\r\nConv+LeakyRelu: kernel=1, N=1, iC=256,   H=56, W=56, oC=256, stride=1, pad=0, dilates=1, g=1 | 4064.71 | 3101.89 | 23.69%\r\nConv+LeakyRelu: kernel=1, N=1, iC=512,   H=28, W=28, oC=512, stride=1, pad=0, dilates=1, g=1 | 3233.11 | 2711.76 | 16.13%\r\nConv+hardtanh: kernel=3, N=1, iC=64,   H=56, W=56, oC=64, stride=1, pad=1, dilates=1, g=1 | 1701.53 | 1460.08 | 14.19%\r\nConv+hardtanh: kernel=1, N=1, iC=256,   H=56, W=56, oC=512, stride=2, pad=0, dilates=1, g=1 | 2137.1 | 1609.5 | 24.69%\r\nConv+hardtanh: kernel=3, N=1, iC=256,   H=56, W=56, oC=256, stride=1, pad=1, dilates=1, g=32 | 3548.07 | 2654.61 | 25.18%\r\nConv+hardtanh: kernel=3, N=1, iC=512,   H=56, W=56, oC=512, stride=2, pad=1, dilates=1, g=32 | 2999.23 | 2507.39 | 16.40%\r\nConv+hardtanh: kernel=1, N=1, iC=64,   H=56, W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 686.75 | 443.09 | 35.48%\r\nConv+hardtanh: kernel=1, N=1, iC=256,   H=56, W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 1076.85 | 840.71 | 21.93%\r\nConv+hardtanh: kernel=1, N=1, iC=256,   H=56, W=56, oC=256, stride=1, pad=0, dilates=1, g=1 | 4070.51 | 3099.9 | 23.84%\r\nConv+hardtanh: kernel=1, N=1, iC=512,   H=28, W=28, oC=512, stride=1, pad=0, dilates=1, g=1 | 3198.08 | 2691.94 | 15.83%\r\nConv+geluApproximateNone: kernel=3, N=1,   iC=64, H=56, W=56, oC=64, stride=1, pad=1, dilates=1, g=1 | 2325.84 | 1565.3 | 32.70%\r\nConv+geluApproximateNone: kernel=1, N=1,   iC=256, H=56, W=56, oC=512, stride=2, pad=0, dilates=1, g=1 | 3031.34 | 1811.86 | 40.23%\r\nConv+geluApproximateNone: kernel=3, N=1,   iC=256, H=56, W=56, oC=256, stride=1, pad=1, dilates=1, g=32 | 5171.4 | 3483.27 | 32.64%\r\nConv+geluApproximateNone: kernel=3, N=1,   iC=512, H=56, W=56, oC=512, stride=2, pad=1, dilates=1, g=32 | 3941.8 | 2716.11 | 31.09%\r\nConv+geluApproximateNone: kernel=1, N=1,   iC=64, H=56, W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 1265.2 | 543.45 | 57.05%\r\nConv+geluApproximateNone: kernel=1, N=1,   iC=256, H=56, W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 1694.72 | 940.23 | 44.52%\r\nConv+geluApproximateNone: kernel=1, N=1,   iC=256, H=56, W=56, oC=256, stride=1, pad=0, dilates=1, g=1 | 5830.62 | 3499.1 | 39.99%\r\nConv+geluApproximateNone: kernel=1, N=1,   iC=512, H=28, W=28, oC=512, stride=1, pad=0, dilates=1, g=1 | 4103.29 | 2917.37 | 28.90%\r\nConv+geluApproximateTanh: kernel=3, N=1,   iC=64, H=56, W=56, oC=64, stride=1, pad=1, dilates=1, g=1 | 2463.44 | 1565.52 | 36.45%\r\nConv+geluApproximateTanh: kernel=1, N=1,   iC=256, H=56, W=56, oC=512, stride=2, pad=0, dilates=1, g=1 | 3515.69 | 1803.82 | 48.69%\r\nConv+geluApproximateTanh: kernel=3, N=1,   iC=256, H=56, W=56, oC=256, stride=1, pad=1, dilates=1, g=32 | 6119.11 | 3520.61 | 42.47%\r\nConv+geluApproximateTanh: kernel=3, N=1,   iC=512, H=56, W=56, oC=512, stride=2, pad=1, dilates=1, g=32 | 4448.33 | 2803.28 | 36.98%\r\nConv+geluApproximateTanh: kernel=1, N=1,   iC=64, H=56, W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 1430.18 | 545.79 | 61.84%\r\nConv+geluApproximateTanh: kernel=1, N=1,   iC=256, H=56, W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 1842.78 | 943.23 | 48.81%\r\nConv+geluApproximateTanh: kernel=1, N=1,   iC=256, H=56, W=56, oC=256, stride=1, pad=0, dilates=1, g=1 | 6753.14 | 3526.2 | 47.78%\r\nConv+geluApproximateTanh: kernel=1, N=1,   iC=512, H=28, W=28, oC=512, stride=1, pad=0, dilates=1, g=1 | 4581.45 | 2951.88 | 35.57%\r\n\r\n\r\n\r\n</body>\r\n\r\n</html>\r\n\r\n\r\n\r\n### 4 threads:\r\n<html xmlns:v=\"urn:schemas-microsoft-com:vml\"\r\nxmlns:o=\"urn:schemas-microsoft-com:office:office\"\r\nxmlns:x=\"urn:schemas-microsoft-com:office:excel\"\r\nxmlns=\"http://www.w3.org/TR/REC-html40\">\r\n\r\n<head>\r\n\r\n<meta name=ProgId content=Excel.Sheet>\r\n<meta name=Generator content=\"Microsoft Excel 15\">\r\n<link id=Main-File rel=Main-File\r\nhref=\"file:///C:/Users/chunyuan/AppData/Local/Temp/msohtmlclip1/01/clip.htm\">\r\n<link rel=File-List\r\nhref=\"file:///C:/Users/chunyuan/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml\">\r\n\r\n</head>\r\n\r\n<body link=\"#0563C1\" vlink=\"#954F72\">\r\n\r\n\r\n\r\nshape | time (us)_no_fusion | time (us)_fusion | Gain\r\n-- | -- | -- | --\r\nConv+tanh: kernel=3, N=1, iC=64, H=56,   W=56, oC=64, stride=1, pad=1, dilates=1, g=1 | 586.07 | 420.98 | 28.17%\r\nConv+tanh: kernel=1, N=1, iC=256, H=56,   W=56, oC=512, stride=2, pad=0, dilates=1, g=1 | 765.55 | 430.55 | 43.76%\r\nConv+tanh: kernel=3, N=1, iC=256, H=56,   W=56, oC=256, stride=1, pad=1, dilates=1, g=32 | 1273.38 | 789.03 | 38.04%\r\nConv+tanh: kernel=3, N=1, iC=512, H=56,   W=56, oC=512, stride=2, pad=1, dilates=1, g=32 | 1073.27 | 719.35 | 32.98%\r\nConv+tanh: kernel=1, N=1, iC=64, H=56,   W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 262.88 | 95.37 | 63.72%\r\nConv+tanh: kernel=1, N=1, iC=256, H=56,   W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 392.27 | 227.78 | 41.93%\r\nConv+tanh: kernel=1, N=1, iC=256, H=56,   W=56, oC=256, stride=1, pad=0, dilates=1, g=1 | 1567.03 | 806.2 | 48.55%\r\nConv+tanh: kernel=1, N=1, iC=512, H=28,   W=28, oC=512, stride=1, pad=0, dilates=1, g=1 | 1079.88 | 747.73 | 30.76%\r\nConv+sigmoid: kernel=3, N=1, iC=64, H=56,   W=56, oC=64, stride=1, pad=1, dilates=1, g=1 | 443.88 | 419.77 | 5.43%\r\nConv+sigmoid: kernel=1, N=1, iC=256,   H=56, W=56, oC=512, stride=2, pad=0, dilates=1, g=1 | 468.36 | 427.21 | 8.79%\r\nConv+sigmoid: kernel=3, N=1, iC=256,   H=56, W=56, oC=256, stride=1, pad=1, dilates=1, g=32 | 777.16 | 776.97 | 0.02%\r\nConv+sigmoid: kernel=3, N=1, iC=512,   H=56, W=56, oC=512, stride=2, pad=1, dilates=1, g=32 | 773.1 | 732.8 | 5.21%\r\nConv+sigmoid: kernel=1, N=1, iC=64, H=56,   W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 120.24 | 95.76 | 20.36%\r\nConv+sigmoid: kernel=1, N=1, iC=256,   H=56, W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 250.95 | 226 | 9.94%\r\nConv+sigmoid: kernel=1, N=1, iC=256,   H=56, W=56, oC=256, stride=1, pad=0, dilates=1, g=1 | 879.55 | 801.58 | 8.86%\r\nConv+sigmoid: kernel=1, N=1, iC=512,   H=28, W=28, oC=512, stride=1, pad=0, dilates=1, g=1 | 782.4 | 745.92 | 4.66%\r\nConv+LeakyRelu: kernel=3, N=1, iC=64,   H=56, W=56, oC=64, stride=1, pad=1, dilates=1, g=1 | 407.49 | 396.2 | 2.77%\r\nConv+LeakyRelu: kernel=1, N=1, iC=256,   H=56, W=56, oC=512, stride=2, pad=0, dilates=1, g=1 | 406.09 | 384.21 | 5.39%\r\nConv+LeakyRelu: kernel=3, N=1, iC=256,   H=56, W=56, oC=256, stride=1, pad=1, dilates=1, g=32 | 607.71 | 593.08 | 2.41%\r\nConv+LeakyRelu: kernel=3, N=1, iC=512,   H=56, W=56, oC=512, stride=2, pad=1, dilates=1, g=32 | 732.3 | 716.45 | 2.16%\r\nConv+LeakyRelu: kernel=1, N=1, iC=64,   H=56, W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 82.59 | 71.94 | 12.90%\r\nConv+LeakyRelu: kernel=1, N=1, iC=256,   H=56, W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 216.97 | 204.84 | 5.59%\r\nConv+LeakyRelu: kernel=1, N=1, iC=256,   H=56, W=56, oC=256, stride=1, pad=0, dilates=1, g=1 | 789.29 | 714.72 | 9.45%\r\nConv+LeakyRelu: kernel=1, N=1, iC=512,   H=28, W=28, oC=512, stride=1, pad=0, dilates=1, g=1 | 726.41 | 703.36 | 3.17%\r\nConv+hardtanh: kernel=3, N=1, iC=64,   H=56, W=56, oC=64, stride=1, pad=1, dilates=1, g=1 | 407.6 | 396.96 | 2.61%\r\nConv+hardtanh: kernel=1, N=1, iC=256,   H=56, W=56, oC=512, stride=2, pad=0, dilates=1, g=1 | 407.28 | 382.26 | 6.14%\r\nConv+hardtanh: kernel=3, N=1, iC=256,   H=56, W=56, oC=256, stride=1, pad=1, dilates=1, g=32 | 603.98 | 590.46 | 2.24%\r\nConv+hardtanh: kernel=3, N=1, iC=512,   H=56, W=56, oC=512, stride=2, pad=1, dilates=1, g=32 | 745.39 | 715.81 | 3.97%\r\nConv+hardtanh: kernel=1, N=1, iC=64,   H=56, W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 84.23 | 72.47 | 13.96%\r\nConv+hardtanh: kernel=1, N=1, iC=256,   H=56, W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 215.92 | 205.01 | 5.05%\r\nConv+hardtanh: kernel=1, N=1, iC=256,   H=56, W=56, oC=256, stride=1, pad=0, dilates=1, g=1 | 786.32 | 715.2 | 9.04%\r\nConv+hardtanh: kernel=1, N=1, iC=512,   H=28, W=28, oC=512, stride=1, pad=0, dilates=1, g=1 | 726.05 | 703.44 | 3.11%\r\nConv+geluApproximateNone: kernel=3, N=1,   iC=64, H=56, W=56, oC=64, stride=1, pad=1, dilates=1, g=1 | 540.83 | 428.36 | 20.80%\r\nConv+geluApproximateNone: kernel=1, N=1,   iC=256, H=56, W=56, oC=512, stride=2, pad=0, dilates=1, g=1 | 668.43 | 446.6 | 33.19%\r\nConv+geluApproximateNone: kernel=3, N=1,   iC=256, H=56, W=56, oC=256, stride=1, pad=1, dilates=1, g=32 | 1144.99 | 849.36 | 25.82%\r\nConv+geluApproximateNone: kernel=3, N=1,   iC=512, H=56, W=56, oC=512, stride=2, pad=1, dilates=1, g=32 | 1001.79 | 755.24 | 24.61%\r\nConv+geluApproximateNone: kernel=1, N=1,   iC=64, H=56, W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 215.58 | 101.46 | 52.94%\r\nConv+geluApproximateNone: kernel=1, N=1,   iC=256, H=56, W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 347.16 | 235.95 | 32.03%\r\nConv+geluApproximateNone: kernel=1, N=1,   iC=256, H=56, W=56, oC=256, stride=1, pad=0, dilates=1, g=1 | 1408.63 | 836.89 | 40.59%\r\nConv+geluApproximateNone: kernel=1, N=1,   iC=512, H=28, W=28, oC=512, stride=1, pad=0, dilates=1, g=1 | 982.54 | 763.92 | 22.25%\r\nConv+geluApproximateTanh: kernel=3, N=1,   iC=64, H=56, W=56, oC=64, stride=1, pad=1, dilates=1, g=1 | 615.4 | 433.58 | 29.55%\r\nConv+geluApproximateTanh: kernel=1, N=1,   iC=256, H=56, W=56, oC=512, stride=2, pad=0, dilates=1, g=1 | 825.42 | 463.36 | 43.86%\r\nConv+geluApproximateTanh: kernel=3, N=1,   iC=256, H=56, W=56, oC=256, stride=1, pad=1, dilates=1, g=32 | 1416.24 | 911.38 | 35.65%\r\nConv+geluApproximateTanh: kernel=3, N=1,   iC=512, H=56, W=56, oC=512, stride=2, pad=1, dilates=1, g=32 | 1163.87 | 715.14 | 38.55%\r\nConv+geluApproximateTanh: kernel=1, N=1,   iC=64, H=56, W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 293.41 | 108.72 | 62.95%\r\nConv+geluApproximateTanh: kernel=1, N=1,   iC=256, H=56, W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 419.72 | 240.78 | 42.63%\r\nConv+geluApproximateTanh: kernel=1, N=1,   iC=256, H=56, W=56, oC=256, stride=1, pad=0, dilates=1, g=1 | 1656.42 | 854.6 | 48.41%\r\nConv+geluApproximateTanh: kernel=1, N=1,   iC=512, H=28, W=28, oC=512, stride=1, pad=0, dilates=1, g=1 | 1143.01 | 775.27 | 32.17%\r\n\r\n\r\n\r\n</body>\r\n\r\n</html>\r\n\r\n\r\n\r\n### 1 socket (32 cores):\r\n\r\n<html xmlns:v=\"urn:schemas-microsoft-com:vml\"\r\nxmlns:o=\"urn:schemas-microsoft-com:office:office\"\r\nxmlns:x=\"urn:schemas-microsoft-com:office:excel\"\r\nxmlns=\"http://www.w3.org/TR/REC-html40\">\r\n\r\n<head>\r\n\r\n<meta name=ProgId content=Excel.Sheet>\r\n<meta name=Generator content=\"Microsoft Excel 15\">\r\n<link id=Main-File rel=Main-File\r\nhref=\"file:///C:/Users/chunyuan/AppData/Local/Temp/msohtmlclip1/01/clip.htm\">\r\n<link rel=File-List\r\nhref=\"file:///C:/Users/chunyuan/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml\">\r\n\r\n</head>\r\n\r\n<body link=\"#0563C1\" vlink=\"#954F72\">\r\n\r\n\r\n\r\nshape | time (us)_no_fusion | time (us)_fusion | Gain\r\n-- | -- | -- | --\r\nConv+tanh: kernel=3, N=1, iC=64, H=56,   W=56, oC=64, stride=1, pad=1, dilates=1, g=1 | 127.55 | 104.88 | 17.77%\r\nConv+tanh: kernel=1, N=1, iC=256, H=56,   W=56, oC=512, stride=2, pad=0, dilates=1, g=1 | 143.31 | 94.4 | 34.13%\r\nConv+tanh: kernel=3, N=1, iC=256, H=56,   W=56, oC=256, stride=1, pad=1, dilates=1, g=32 | 369.44 | 130.5 | 64.68%\r\nConv+tanh: kernel=3, N=1, iC=512, H=56,   W=56, oC=512, stride=2, pad=1, dilates=1, g=32 | 211.54 | 169.37 | 19.93%\r\nConv+tanh: kernel=1, N=1, iC=64, H=56,   W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 75.53 | 45.35 | 39.96%\r\nConv+tanh: kernel=1, N=1, iC=256, H=56,   W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 91.14 | 62.77 | 31.13%\r\nConv+tanh: kernel=1, N=1, iC=256, H=56,   W=56, oC=256, stride=1, pad=0, dilates=1, g=1 | 235.74 | 137.03 | 41.87%\r\nConv+tanh: kernel=1, N=1, iC=512, H=28,   W=28, oC=512, stride=1, pad=0, dilates=1, g=1 | 189.23 | 140.28 | 25.87%\r\nConv+sigmoid: kernel=3, N=1, iC=64, H=56,   W=56, oC=64, stride=1, pad=1, dilates=1, g=1 | 98.24 | 88.93 | 9.48%\r\nConv+sigmoid: kernel=1, N=1, iC=256,   H=56, W=56, oC=512, stride=2, pad=0, dilates=1, g=1 | 105.12 | 88 | 16.29%\r\nConv+sigmoid: kernel=3, N=1, iC=256,   H=56, W=56, oC=256, stride=1, pad=1, dilates=1, g=32 | 252.74 | 251.59 | 0.46%\r\nConv+sigmoid: kernel=3, N=1, iC=512,   H=56, W=56, oC=512, stride=2, pad=1, dilates=1, g=32 | 223.13 | 180.2 | 19.24%\r\nConv+sigmoid: kernel=1, N=1, iC=64, H=56,   W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 44.93 | 42.42 | 5.59%\r\nConv+sigmoid: kernel=1, N=1, iC=256,   H=56, W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 63.61 | 60.25 | 5.28%\r\nConv+sigmoid: kernel=1, N=1, iC=256,   H=56, W=56, oC=256, stride=1, pad=0, dilates=1, g=1 | 158.42 | 135.56 | 14.43%\r\nConv+sigmoid: kernel=1, N=1, iC=512,   H=28, W=28, oC=512, stride=1, pad=0, dilates=1, g=1 | 147.82 | 139.43 | 5.68%\r\nConv+LeakyRelu: kernel=3, N=1, iC=64,   H=56, W=56, oC=64, stride=1, pad=1, dilates=1, g=1 | 89.88 | 85.6 | 4.76%\r\nConv+LeakyRelu: kernel=1, N=1, iC=256,   H=56, W=56, oC=512, stride=2, pad=0, dilates=1, g=1 | 88.5 | 79.18 | 10.53%\r\nConv+LeakyRelu: kernel=3, N=1, iC=256,   H=56, W=56, oC=256, stride=1, pad=1, dilates=1, g=32 | 198.14 | 100.09 | 49.49%\r\nConv+LeakyRelu: kernel=3, N=1, iC=512,   H=56, W=56, oC=512, stride=2, pad=1, dilates=1, g=32 | 159.42 | 145.96 | 8.44%\r\nConv+LeakyRelu: kernel=1, N=1, iC=64,   H=56, W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 41.86 | 35.36 | 15.53%\r\nConv+LeakyRelu: kernel=1, N=1, iC=256,   H=56, W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 67.19 | 52.18 | 22.34%\r\nConv+LeakyRelu: kernel=1, N=1, iC=256,   H=56, W=56, oC=256, stride=1, pad=0, dilates=1, g=1 | 166.35 | 124.43 | 25.20%\r\nConv+LeakyRelu: kernel=1, N=1, iC=512,   H=28, W=28, oC=512, stride=1, pad=0, dilates=1, g=1 | 139.47 | 130.29 | 6.58%\r\nConv+hardtanh: kernel=3, N=1, iC=64,   H=56, W=56, oC=64, stride=1, pad=1, dilates=1, g=1 | 92.12 | 81.74 | 11.27%\r\nConv+hardtanh: kernel=1, N=1, iC=256,   H=56, W=56, oC=512, stride=2, pad=0, dilates=1, g=1 | 87.87 | 80.13 | 8.81%\r\nConv+hardtanh: kernel=3, N=1, iC=256,   H=56, W=56, oC=256, stride=1, pad=1, dilates=1, g=32 | 139.39 | 101.07 | 27.49%\r\nConv+hardtanh: kernel=3, N=1, iC=512,   H=56, W=56, oC=512, stride=2, pad=1, dilates=1, g=32 | 174.47 | 146.47 | 16.05%\r\nConv+hardtanh: kernel=1, N=1, iC=64,   H=56, W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 43.69 | 34.81 | 20.33%\r\nConv+hardtanh: kernel=1, N=1, iC=256,   H=56, W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 67.65 | 53.39 | 21.08%\r\nConv+hardtanh: kernel=1, N=1, iC=256,   H=56, W=56, oC=256, stride=1, pad=0, dilates=1, g=1 | 180.4 | 122.88 | 31.88%\r\nConv+hardtanh: kernel=1, N=1, iC=512,   H=28, W=28, oC=512, stride=1, pad=0, dilates=1, g=1 | 180.22 | 129.91 | 27.92%\r\nConv+geluApproximateNone: kernel=3, N=1,   iC=64, H=56, W=56, oC=64, stride=1, pad=1, dilates=1, g=1 | 140.1 | 85.07 | 39.28%\r\nConv+geluApproximateNone: kernel=1, N=1,   iC=256, H=56, W=56, oC=512, stride=2, pad=0, dilates=1, g=1 | 163.15 | 87.89 | 46.13%\r\nConv+geluApproximateNone: kernel=3, N=1,   iC=256, H=56, W=56, oC=256, stride=1, pad=1, dilates=1, g=32 | 390.41 | 132.65 | 66.02%\r\nConv+geluApproximateNone: kernel=3, N=1,   iC=512, H=56, W=56, oC=512, stride=2, pad=1, dilates=1, g=32 | 229.57 | 155.16 | 32.41%\r\nConv+geluApproximateNone: kernel=1, N=1,   iC=64, H=56, W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 71.01 | 39.96 | 43.73%\r\nConv+geluApproximateNone: kernel=1, N=1,   iC=256, H=56, W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 95.58 | 55.09 | 42.36%\r\nConv+geluApproximateNone: kernel=1, N=1,   iC=256, H=56, W=56, oC=256, stride=1, pad=0, dilates=1, g=1 | 290.14 | 137.89 | 52.47%\r\nConv+geluApproximateNone: kernel=1, N=1,   iC=512, H=28, W=28, oC=512, stride=1, pad=0, dilates=1, g=1 | 232.39 | 136.23 | 41.38%\r\nConv+geluApproximateTanh: kernel=3, N=1,   iC=64, H=56, W=56, oC=64, stride=1, pad=1, dilates=1, g=1 | 151.82 | 89.56 | 41.01%\r\nConv+geluApproximateTanh: kernel=1, N=1,   iC=256, H=56, W=56, oC=512, stride=2, pad=0, dilates=1, g=1 | 185.89 | 95.09 | 48.85%\r\nConv+geluApproximateTanh: kernel=3, N=1,   iC=256, H=56, W=56, oC=256, stride=1, pad=1, dilates=1, g=32 | 392.33 | 187.3 | 52.26%\r\nConv+geluApproximateTanh: kernel=3, N=1,   iC=512, H=56, W=56, oC=512, stride=2, pad=1, dilates=1, g=32 | 238.26 | 156.25 | 34.42%\r\nConv+geluApproximateTanh: kernel=1, N=1,   iC=64, H=56, W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 78.53 | 40.02 | 49.04%\r\nConv+geluApproximateTanh: kernel=1, N=1,   iC=256, H=56, W=56, oC=64, stride=1, pad=0, dilates=1, g=1 | 102.51 | 56.49 | 44.89%\r\nConv+geluApproximateTanh: kernel=1, N=1,   iC=256, H=56, W=56, oC=256, stride=1, pad=0, dilates=1, g=1 | 300.9 | 138.6 | 53.94%\r\nConv+geluApproximateTanh: kernel=1, N=1,   iC=512, H=28, W=28, oC=512, stride=1, pad=0, dilates=1, g=1 | 232.61 | 138.24 | 40.57%\r\n\r\n\r\n\r\n</body>\r\n\r\n</html>\r\n\r\n\r\n\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2510754463, "node_id": "MDU6TGFiZWwyNTEwNzU0NDYz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/NNC", "name": "NNC", "color": "e5678d", "default": false, "description": ""}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3200025338, "node_id": "MDU6TGFiZWwzMjAwMDI1MzM4", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel%20priority", "name": "intel priority", "color": "4E4EF8", "default": false, "description": "matters to intel architecture from performance wise"}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 83233, "title": "Enable xpu distributed data parallel (DDP) in a device-agnostic way", "time": "2022-08-11T06:43:28Z", "body": "* Support DDP both in xpu and cuda with minor code changes.\r\n* Pytorch has supported xpu registration (_register_device_module (in torch/__init__.py), and we use _get_available_device_type (in torch/_utils.py) to get DDP device type.\r\n* Use DDP device type to get the ddp_torch_module (torch.cuda or torch.xpu), replacing the hard code of xpu.\r\n* Now we only support cuda-like devices, and not consider xla.\r\n\r\nCo-authored-by: Zhu Hong <hong.zhu@intel.com>\r\nCo-authored-by: Lu Chengjun <chengjun.lu@intel.com>\r\n\r\nFixes #ISSUE_NUMBER\r\n", "label": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20distributed", "name": "oncall: distributed", "color": "f7e101", "default": false, "description": "Add this issue/PR to distributed oncall triage queue"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 83201, "title": "[ONNX] Update ONNX version", "time": "2022-08-10T21:05:53Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* #84789\r\n* #83186\r\n* __->__ #83201\r\n\r\nWhy:\r\n\r\nONNX had mismatch checker usage between cpp and python and it's later fixed by https://github.com/onnx/onnx/pull/4386. And since `torch.onnx.export` is using cpp checker for graph-level check with older version of ONNX,this improvement should be added. Also, this version bump enables #83186 \r\n\r\nUpdated 12/5/2022:\r\nThis PR includes ONNX 1.13.0 pre-release (https://github.com/onnx/onnx/tree/rel-1.13.0)", "label": [{"id": 693805995, "node_id": "MDU6TGFiZWw2OTM4MDU5OTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20onnx", "name": "module: onnx", "color": "f7e101", "default": false, "description": "Related to torch.onnx"}, {"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1303456553, "node_id": "MDU6TGFiZWwxMzAzNDU2NTUz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20mkldnn", "name": "module: mkldnn", "color": "f7e101", "default": false, "description": "Related to Intel IDEEP/MKL-DNN (mkldnn) integration"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3768821874, "node_id": "LA_kwDOA-j9z87go6hy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20onnx", "name": "release notes: onnx", "color": "B4EB93", "default": false, "description": "torch.onnx related changes that should show up in the release notes"}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 3837551288, "node_id": "LA_kwDOA-j9z87kvGK4", "url": "https://api.github.com/repos/pytorch/pytorch/labels/onnx-needs-import", "name": "onnx-needs-import", "color": "547451", "default": false, "description": "This PR is related to ONNX, but touches files outside of merge rule patterns, and hence needs import"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 83200, "title": "Handle end time invariant violation", "time": "2022-08-10T20:57:52Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #83200\n\nWhen we encounter an event's end_time less than start_time, we want it to not crash.\n\nDifferential Revision: [D38586420](https://our.internmc.facebook.com/intern/diff/D38586420/)", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 83199, "title": "[Testing] Use CUDAToolkit in tensorpipe", "time": "2022-08-10T20:57:36Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #83199\n* #76104\n* #82695\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}]}
{"number": 83186, "title": "[ONNX] Add full checker mode in torch.onnx.export", "time": "2022-08-10T16:54:27Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* #84789\r\n* __->__ #83186\r\n* #90337\r\n\r\nFix #82589\r\nWhy:\r\n1. **full_check** works in `onnx::checker::check_model` function as it turns on **strict_mode** in `onnx::shape_inference::InferShapes()` which I think that was the intention of this part of code.\r\n2. **strict_mode** catches failed shape type inference (invalid ONNX model from onnx perspective) and ONNXRUNTIME can't run these invalid models, as ONNXRUNTIME actually rely on ONNX shape type inference to optimize ONNX graph. Why we don't set it True for default? >>> some of existing users use other platform, such as caffe2 to run ONNX model which doesn't need valid ONNX model to run.\r\n3. This PR doesn't change the original behavior of `check_onnx_proto`, but add a warning message for those models which can't pass strict shape type inference, saying the models would fail on onnxruntime.", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 693805995, "node_id": "MDU6TGFiZWw2OTM4MDU5OTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20onnx", "name": "module: onnx", "color": "f7e101", "default": false, "description": "Related to torch.onnx"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3768821874, "node_id": "LA_kwDOA-j9z87go6hy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20onnx", "name": "release notes: onnx", "color": "B4EB93", "default": false, "description": "torch.onnx related changes that should show up in the release notes"}, {"id": 3773061335, "node_id": "LA_kwDOA-j9z87g5FjX", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20improvements", "name": "topic: improvements", "color": "22499E", "default": false, "description": "topic category"}]}
{"number": 83172, "title": "[Quant][fx][resubmit] Remove dequant-quant around getitem", "time": "2022-08-10T13:43:54Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #83172\n\nSummary: https://github.com/pytorch/pytorch/issues/82480 saw\nunnecessary dequant-quant pairs around the getitem op, which led\nto significant slowdowns. This commit simply removes this pair in\nthe lowering step, since getitem already handles quantized inputs.\n\nTest Plan:\npython test/test_quantization.py TestQuantizeFxOps.test_getitem_no_dequant_quant\n\nReviewers: jerryzh168\n\nSubscribers: jerryzh168, supriyar\n\nTasks: https://github.com/pytorch/pytorch/issues/82480\n\nDifferential Revision: [D38657297](https://our.internmc.facebook.com/intern/diff/D38657297)\n\ncc @jerryzh168 @jianyuh @raghuramank100 @jamesr66a @vkuzo @jgong5 @Xia-Weiwen @leslie-fang-intel @ezyang @SherlockNoMad @soumith @EikanWang @wenzhe-nrv", "label": [{"id": 1031340614, "node_id": "MDU6TGFiZWwxMDMxMzQwNjE0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20quantization", "name": "oncall: quantization", "color": "f7e101", "default": false, "description": "Quantization support in PyTorch"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}, {"id": 3773061335, "node_id": "LA_kwDOA-j9z87g5FjX", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20improvements", "name": "topic: improvements", "color": "22499E", "default": false, "description": "topic category"}, {"id": 4347715449, "node_id": "LA_kwDOA-j9z88AAAABAyTfeQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fx", "name": "fx", "color": "ededed", "default": false, "description": null}]}
{"number": 83171, "title": "Enable QNNPACK for POWER/VSX (ppc64le)", "time": "2022-08-10T13:25:59Z", "body": "This PR enables QNNPACK for POWER/VSX (ppc64le).\r\n\r\nToday, when one tries to execute a quantized model with Convolutional Layers and/or Fully Connected Layers on any POWER architecture, the inference fails and the following error message is shown:\r\n\r\n```\r\n RuntimeError: Didn't find engine for operation quantized::linear_prepack NoQEngine\r\n```\r\n\r\nThis PR adds a QEngine (QNNPACK) for POWER to allow the execution of models with the layers mentioned above using either Dynamic or Static quantization.\r\n\r\nSee below the list of operators that have been added to this PR:\r\n- q8conv (Convolution)\r\n- q8dwconv (Depthwise Convolution)\r\n- q8gavgpool (Global Average Pooling)\r\n- q8gemm (GEMM)\r\n- q8vadd\r\n- u8clamp (ReLU/ReLu6)\r\n- u8maxpool (Max Pooling)\r\n- u8rmax\r\n- x8zip (Channel Shuffle)\r\n\r\nThe operators above have been tested on POWER8, POWER9 and POWER10 through the unittests available in QNNPACK and also with the following computer vision models:\r\n- AlexNet\r\n- DensetNet121 and 201\r\n- InceptionV3\r\n- ResNet18 and 50\r\n- ShuffleNetV2\r\n- SqueezeNet\r\n- VGG11 and 19\r\n\r\nThanks.", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 83145, "title": "Generate SLSA signature for tarball release", "time": "2022-08-10T05:11:13Z", "body": "Hi,\r\n\r\nI'm reaching out on behalf of the [Open Source Security Foundation (openssf.org)](https://openssf.org/). We work on improving the security of critical open source projects like yours. \r\n\r\nTogether with [GitHub](https://github.blog/2022-04-07-slsa-3-compliance-with-github-actions/), we designed a free, easy-to-use method of code signing. It will help your users verify that your tarball was created from your repository’s [workflow](https://github.com/pytorch/pytorch/blob/master/.github/workflows/create_release.yml) and not altered by anyone. It’s just a few lines of code, but it will make the release asset more secure against third-party tampering and attacks like [Codecov](https://about.codecov.io/apr-2021-post-mortem/) and [CTX](https://sockpuppets.medium.com/how-i-hacked-ctx-and-phpass-modules-656638c6ec5e).\r\n\r\nThis PR shows how to add this seamless code signing to your workflow. You don’t have to be a cryptography expert or learn complicated tools and [verification](https://github.com/slsa-framework/slsa-verifier/blob/main/README.md#verification-of-provenance) is simple for your users.  \r\n\r\nYou can read more on the [SLSA blog](https://slsa.dev/blog/2022/06/slsa-github-workflows). Please reach out if you have any questions!\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 83134, "title": "API for checking is_canonical", "time": "2022-08-10T01:11:08Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #83134\n\n\n\ncc @ezyang @soumith @EikanWang @jgong5 @wenzhe-nrv", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}, {"id": 4347715449, "node_id": "LA_kwDOA-j9z88AAAABAyTfeQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fx", "name": "fx", "color": "ededed", "default": false, "description": null}]}
{"number": 83123, "title": "Expose _module_to_run_forward as an overridable variable", "time": "2022-08-09T22:32:39Z", "body": "Summary:\nContext:\n\nWe want to be able to use DistributedDataParallel in a composable way, e.g. after applying this paralleization scheme - the model properties (including methods on the model) should remain intact.\n\nThe current DDP strategy wraps the module, and adds a \"module.\" prefix to everything. To get around this, we want to be able to not replace a model with a DDP'd wrapper module, but just mock out the model w/ the DDP model's forward.\n\nHowever, in order to do this, we have to do a three way swap, and need to be able to override the base impl forward.\n\nThis change shouldn't change any existing DDP behavior.\n\nTest Plan: unit tests\n\nDifferential Revision: D38553452\n\n\n\ncc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu", "label": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20distributed", "name": "oncall: distributed", "color": "f7e101", "default": false, "description": "Add this issue/PR to distributed oncall triage queue"}, {"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 83094, "title": "[WIP] codegenning forward symint overloads", "time": "2022-08-09T18:59:11Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 83039, "title": "Remat benchmark new", "time": "2022-08-08T23:32:17Z", "body": "Fixes #ISSUE_NUMBER\r\n\n\ncc @ezyang @SherlockNoMad @soumith @EikanWang @jgong5 @wenzhe-nrv", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}, {"id": 4347715449, "node_id": "LA_kwDOA-j9z88AAAABAyTfeQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fx", "name": "fx", "color": "ededed", "default": false, "description": null}]}
{"number": 83037, "title": "Using a custom c10::SymtInt caster; missed a case in numel", "time": "2022-08-08T23:08:22Z", "body": "The original custom caster PR was missing a use of `is_symint_node` which was introduced later in the `numel` PR.", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 83014, "title": "POC: make torch_function handling in C++ more structured", "time": "2022-08-08T19:58:28Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #74727\n* __->__ #83014\n* #74728\n\nThis is one idea for preventing `should_skip_torch_function` from\nbeing called multiple times inadvertantly. The idea is that you\nimplement torch function with three lambdas,\n```\n  return with_torch_function(\n      /*checker=*/ [&](auto checker) {\n        return checker.has_torch_function(self);\n      },\n      /*handle_torch_function=*/ [&] {\n        return handle_torch_function(self, \"_is_view\", args);\n      },\n      /*default_impl=*/ [&] {\n        auto& self_ = THPVariable_Unpack(self);\n        return wrap(self_.is_view());\n      });\n```\n\nBy making `has_torch_function` the method of a class with a private\nconstructor, this means you can only access it from inside\n`with_torch_function` and so the structure is forced on you.", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769211609, "node_id": "LA_kwDOA-j9z87gqZrZ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20autograd", "name": "release notes: autograd", "color": "9433A1", "default": false, "description": "release notes category"}]}
{"number": 83011, "title": "Remove ENABLE_UPGRADERS macros", "time": "2022-08-08T19:28:28Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #83011\n\nThis macro was introduced so it's easy to switch back in case there is any issue. Remove it now since upgraders has been enabled for a long time, since last year.\n\n\n\nDifferential Revision: [D38512477](https://our.internmc.facebook.com/intern/diff/D38512477/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D38512477/)!\n\nDifferential Revision: [D38512477](https://our.internmc.facebook.com/intern/diff/D38512477)\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 82997, "title": "Implement refs.var as a real reference", "time": "2022-08-08T17:00:45Z", "body": "### Description\r\nThis PR removes the use of `prims.var` in the implementation of the `var` reference because there's no need for `var` to be a primitive.\r\n\r\n### Testing\r\nNo new tests are needed.\r\n\n\ncc @ezyang @mruberry @ngimel @Lezcano @fdrocha @peterbell10", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 4081012228, "node_id": "LA_kwDOA-j9z87zP04E", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20primTorch", "name": "module: primTorch", "color": "455561", "default": false, "description": ""}]}
{"number": 82981, "title": "Revert \"Move doxygen generation to nightly only (#82573) (#82573)\"", "time": "2022-08-08T15:54:40Z", "body": "This reverts commit 19e13237d781ede29778d45a8e9172a26b88656e.\r\n\r\n### Description\r\nThe issue was corrected by increasing worker size\r\n\r\n### Testing\r\nPlease refer to: https://github.com/pytorch/pytorch/actions?query=workflow%3Anightly++\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 82975, "title": "[DataLoader] Add shuffling logic for MapDataPipe", "time": "2022-08-08T15:30:09Z", "body": "Fixes: https://github.com/pytorch/data/issues/718\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack):\n* **#82975 [DataLoader] Add shuffling logic for MapDataPipe**\n* #83202 [DataPipe] Convert MapDataPipe.shuffle to IterDataPipe\n\r\n## Changes:\r\n- Add shuffling logic for the graph of `DataPipe` with `MapDataPipe` at the end when using `DataLoader`\r\n  - Apply `shuffle` (argument from `DataLoader`)\r\n  - Assign new shuffle seed to each random `DataPipe` per iteration\r\n- When it's a `MapDataPipe` at the end of graph, we only need to run distributed sharding without counting the number of workers because index sampler will do the local sharding work.", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769215019, "node_id": "LA_kwDOA-j9z87gqagr", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20dataloader", "name": "release notes: dataloader", "color": "23169D", "default": false, "description": "release notes category"}, {"id": 3773061335, "node_id": "LA_kwDOA-j9z87g5FjX", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20improvements", "name": "topic: improvements", "color": "22499E", "default": false, "description": "topic category"}]}
{"number": 82967, "title": "getHistory Input generation order is not fixed so some Memdependency tests fails.", "time": "2022-08-08T14:20:32Z", "body": "Memdependency checks assume Input B/C or A/B have a particular order when getHistory() returns. This is not always true, this patches fixes such a rigid requirement reading what getHistory returns and making the appropriate checks from there.\r\n\r\nI see no reason why Input X on load history should be first or second to Input Y, this is why I posted this. If there is something I am missing and there is really a reason this should be otherwise please let me know as on Windows ARM64 sometimes the order of the Inputs gets swapped but everything else stays correct. \r\n\r\n### Description\r\n<!-- What did you change and why was it needed? -->\r\n\r\n### Issue\r\n<!-- Link to Issue ticket or RFP -->\r\n\r\n### Testing\r\n<!-- How did you test your change? -->\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 82958, "title": "Avoid external data namespace conflict [#36914]", "time": "2022-08-08T07:06:21Z", "body": "### Description\r\nWhen we have an external \"data\" namespace declared after import variant.h, the variant module uses the external \"data\" instead using self function.\r\n\r\nThis MR simplifies the code, working with an external \"data\" namespace declaration.\r\n\r\n### Issue\r\n[#36914]\r\n\r\n### Testing\r\nI did an minimal application that creates the data namespace and later I include the torch.h module. Like we can seen at the original issue.\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 82942, "title": "Test profiler on ARM", "time": "2022-08-07T04:33:23Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #82942\n\nJust saw that profiler tree tests had to be disabled, and figuring out if it's fundamental.\n\nDifferential Revision: [D38487443](https://our.internmc.facebook.com/intern/diff/D38487443/)", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}]}
{"number": 82916, "title": "[DO NOT MERGE] Testing add.Tensor kernel registered for AutogradNestedTensor in derivatives.yaml works", "time": "2022-08-05T22:09:16Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* __->__ #82916\n* #82801\n\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 82906, "title": "Introducing DependencyViewer for querying Node's depedency relationships", "time": "2022-08-05T20:28:15Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #82906\n\r\nThis PR refactor the dependency map builder from the CapabilityBasedPartitioner into a standalone component. \n\ncc @ezyang @soumith @EikanWang @jgong5 @wenzhe-nrv", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}, {"id": 4347715449, "node_id": "LA_kwDOA-j9z88AAAABAyTfeQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fx", "name": "fx", "color": "ededed", "default": false, "description": null}]}
{"number": 82896, "title": "Update deploy documentation", "time": "2022-08-05T18:00:23Z", "body": "Summary: Update deploy docs to use docker\n\nTest Plan: tested in OSS\n\nDifferential Revision: D38466869\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 82884, "title": "[typing] upgrade pyre version in `fbcode/caffe2` - batch 2", "time": "2022-08-05T14:58:48Z", "body": "Test Plan: Sandcastle\n\nDifferential Revision: D38447540\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 82869, "title": "Default permissions for torch.hub downloads", "time": "2022-08-05T05:04:55Z", "body": "### Description\r\nThe `download_url_to_file` function in torch.hub uses a temporary file to prevent overriding a local working checkpoint with a broken download.This temporary file is created using `NamedTemporaryFile`. However, since `NamedTemporaryFile` creates files with overly restrictive permissions (0600), the resulting download will not have default permissions and will not respect umask on Linux (since moving the file will retain the restrictive permissions of the temporary file). This is especially problematic when trying to share model checkpoints between multiple users as other users will not even have read access to the file.\r\n\r\nThe change in this PR fixes the issue by using custom code to create the temporary file without changing the permissions to 0600 (unfortunately there is no way to override the permissions behaviour of existing Python standard library code). This ensures that the downloaded checkpoint file correctly have the default permissions applied. If a user wants to apply more restrictive permissions, they can do so via usual means (i.e. by setting umask).\r\n\r\nSee these similar issues in other projects for even more context:\r\n* https://github.com/borgbackup/borg/issues/6400\r\n* https://github.com/borgbackup/borg/issues/6933\r\n* https://github.com/zarr-developers/zarr-python/issues/325\r\n\r\n### Issue\r\nhttps://github.com/pytorch/pytorch/issues/81297\r\n\r\n### Testing\r\nExtended the unit test `test_download_url_to_file` to also check permissions.\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 82844, "title": "draft for implementing softshrink ops", "time": "2022-08-04T21:34:42Z", "body": "Summary: implement softshrink ops based on Vulkan\n\nTest Plan: buck run //xplat/caffe2:pt_vulkan_api_test_binAppleMac\\#macosx-arm64\n\nDifferential Revision: D38432431\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 82816, "title": "Enable native_dropout/backward for ORT backend", "time": "2022-08-04T14:28:47Z", "body": "Hi all - new contributor to this project (and this area in general). I am working on expanding support for ONNX Runtime Eager mode. As ONNX directly supports the Dropout operator, we would like PyTorch to directly dispatch `dropout` to the high level ONNX Dropout operator, instead of decomposing to elementary operations.\r\n\r\nOne area I was hoping for guidance on - what is the best way to override the [dropout](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/native_functions.yaml#L244) operation? Is it better to implement the `native_dropout` kernel and indicate support for fused_kernel (as done here), or would it instead be better to implement `dropout` kernel directly, as it appears was recently done for `NestedTensorCPU` and `NestedTensorCUDA`? I am Interested in learning more about the tradeoffs here.\r\n\r\nWe initially tried to directly override the `dropout` operator, but it appears (per my understanding) that it is not possible for an out-of-tree backend to override an operator that is 1) backed by an `CompositeImplicitAutograd` kernel and 2) does not have a dedicated `Autograd` key (e.g. `AutograrORT`), as is the case with the `dropout` operator and ORT backend. Should we instead look at adding an `AutogradORT` entry? (if not for `dropout`, for other `CompositeImplicitAutograd` operators)? What would be the other implications of this approach (maybe it would be better to start a separate issue if this turns into a larger discussion, if there is not already existing documentation that I missed)?\r\n\r\nThanks!\r\n\r\n### Description\r\nIndicate that ONNX Runtime Eager Mode supports dropout operator[1], so\r\nthat operator can be called instead of default dropout kernel.\r\n\r\n1. https://github.com/onnx/onnx/blob/main/docs/Operators.md#Dropout\r\n\r\n### Testing\r\nIn a private build, I validated that `native_dropout` is dispatched to ORT backend. Will include link to changes that add support for `native_droput` to ONNX Runtime Eager Mode.\r\n\r\n/cc @souptc ", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 82806, "title": "Setting memory size to metadata `sizes` when unpickling qtensor", "time": "2022-08-04T08:14:22Z", "body": "### Description\r\nWhen unpickling a `qtensor`, the metadata `size` is unpickled after unpickling `TensorStorage`.   Since `QuantizedCPU` backed has not  dispatching for schema `- func: set_.source_Storage(Tensor(a!) self, Storage source) -> Tensor(a!)` , using `at::_empty_affine_quantized({}, options, 0, 0).set_(storage, 0, {}, {})`  to initialize storage would result in meaningless `sizes` and `strides` metadata.  Specifically , both the `strides` and `sizes` is `[]` in `TensorStorage` of the unpicked qtensor.\r\n\r\nWithout proper `size` metadata, trying to copy this unpicked tensor to `non-cpu` backend would fail (line 508 in `unpickler.cpp`). This is because the `copy` would use `size` info to create memory on target device. Passing a `[]` is meaningless.\r\n\r\n## Solution\r\nFirstly, we my need enable `- func: set_.source_Storage(Tensor(a!) self, Storage source) -> Tensor(a!)` dispatching in `QuantizedCPU` backend, this `set_` would infer a `size` metadat according to the `nbytes` and `dtype().itemsize()`.\r\n\r\nSecondly, in the `set_storage_quantized_`, we need to handle parameter `stride` has no elements. \n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}]}
{"number": 82804, "title": "Set o_scale of u8 qsigmoid to valid integer range", "time": "2022-08-04T07:00:11Z", "body": "### Description\r\nFor the output scale setting for u8 qtensor, we deem that `1/255` is more suitable.\r\nUsing `1/256` may resulting overflow, e.g. (value_f32=0.999, valut_uint8=255.744). Though using rounding can avoid overflow, from the view of methodology, `1/255` would ensure more accurate result. \r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 82777, "title": "optimization for zero_grad", "time": "2022-08-03T23:28:52Z", "body": "### Description\r\n\r\nThe original `zero_grad` calls `grad.zero_()` for every eligible tensor. There will be a lot of tiny cuda kernels in this way. I add a temporary list to save all eligible tensors and call a `torch._foreach_zero_()` which will call a single kernel to process all tensors. Even the kernel is called at the end of the big loop, the optimization removes a lot of `aten::fill` and cuda kernel launch overhead, so the final speedup is still about 3X at function level. \r\n\r\n\r\n\r\n### Testing\r\nThere are 44 models involved in TorchBench and 9 models obtained over 1.02X speedup for training at model level on A100. The speedup is up to 1.04X for timm_efficientnet.\r\n\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 82771, "title": "fix bernoulli_ for functionalization by making it a primitive wrt torch_dispatch", "time": "2022-08-03T23:23:12Z", "body": "Context: `bernoulli_.float` is currently a primitive op, but it's functional counterpart `bernoulli.p` is `CompositeImplicitAutograd`.\r\n\r\nThis is a problem when we're using functionalization on top of python tracing: functionalization will convert `bernoulli_.float` into `bernoulli.p`, but the decomposition for `bernoulli.p` will run before hitting python, and happily convert back into `bernoulli_.float`.\r\n\r\nThis doesn't seems to come up very often, but I think the right fix is to just make `bernoulli.p` a primitive w.r.t. the Python key too, by making it `CompositeExplicitAutograd`.\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack):\n* #85844\n* #85875\n* #85681\n* #82601\n* #82602\n* __->__ #82771\n* #83265\n\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 82770, "title": "try to build functorch in CI, not just for functorch tests", "time": "2022-08-03T23:23:08Z", "body": "@zou3519 I'll see how this goes (what are the odds that it just works 😃)\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #82602\n* #82771\n* #82601\n* __->__ #82770\n* #83265\n\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 82766, "title": "Fix Tensor.__format__ for subclasses (#82764)", "time": "2022-08-03T22:51:49Z", "body": "### Description\r\nAdds support for subclasses in Tensor.__format__.\r\n\r\n### Issue\r\n#82764 \r\n\r\n### Testing\r\nAdded unit test to test/test_subclass.py.\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 82753, "title": "[BE][CI] Rename .jenkins to .ci", "time": "2022-08-03T20:46:08Z", "body": "### Description\r\nWe've wanted to do this for a looong time. Let's eradicate confusion about what the .jenkins folder is and try to host our scripts in a more generic .ci folder.\r\n\r\nMost changes are a find and replace EXCEPT\r\n- deleted dirty.sh in both caffe2 and pytorch since it was not used anywhere\r\n- modified CONTRIBUTING.md to be more updated\r\n\r\nThe next steps:\r\n- rename jenkins user to pytorch-ci or something\r\n- move scripts from .circleci to here (e.g., docker)\r\n\r\n### Issue\r\nWe used to have an issue for this but it is so long ago that I could not find it.\r\n\r\n### Testing\r\nCI should not break.\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport", "label": [{"id": 1078897659, "node_id": "MDU6TGFiZWwxMDc4ODk3NjU5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20rocm", "name": "module: rocm", "color": "f7e101", "default": false, "description": "AMD GPU support for Pytorch"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769204372, "node_id": "LA_kwDOA-j9z87gqX6U", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20releng", "name": "release notes: releng", "color": "EED552", "default": false, "description": "release notes category"}, {"id": 4374488016, "node_id": "LA_kwDOA-j9z88AAAABBL1j0A", "url": "https://api.github.com/repos/pytorch/pytorch/labels/accept2ship", "name": "accept2ship", "color": "4068AD", "default": false, "description": ""}]}
{"number": 82723, "title": "Make MiscCheck test compilation only for Windows on ARM64 instead of compiling and executing.", "time": "2022-08-03T13:44:31Z", "body": "For Windows on ARM64 the test compiling and executing fails. Just compiling seems to be enough to check correctness. ", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 82721, "title": "Fix IdenticalTypesDifferentCus to check for a valid address on Windows.", "time": "2022-08-03T13:30:45Z", "body": "On Windows the address does not get printed as `0x` so this test always fails, This patch fixes this problem. ", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 82703, "title": "optimize scatte_add performance for gnn usage on CPU", "time": "2022-08-03T05:40:44Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #83727\n* #87586\n* __->__ #82703\n\r\n### Motivation of this PR\r\n\r\nThis PR is targeting at improving performance of `scatter_add` for GNN usage scenarios on PyG. Currently only CPU optimizations is covered.\r\n\r\n`Message Passing` is the major step in GNN learning which means exchanging/aggregating info between nodes. And from the perf point of view, if the `EdgeIndex` is stored as [2, num_edges], `scatter_reduce` would be a major perf hotspot on current pytorch implementations.\r\n\r\nTo be more specific, in the process of message passing, `scatter_add` is used in a very similar way as `index_select`, except that the `self` tensor is written into while `index_select` is only reading. Therefore, the `index` tensor passed to `scatter_add` is an expanded tensor on dim0, which means all the rest of dims would end up with the same value.\r\n\r\n### Algorithm\r\n\r\nCurrent impl on scatter would do parallel on the inner dims for such case which would cause bad perf: non-contiguous memory access pattern and non-vectorized.\r\n\r\nThis PR did sorting on the `index` to solve the write conflicts if we directly parallel on dim0. The algorithm is equivalent to:\r\n* convert memory format from `COO` to `CSR`\r\n* do spmm reduce\r\n\r\n### Perf improvement\r\n\r\nThe benchmark comes from https://github.com/pyg-team/pytorch_geometric/tree/master/examples, `python reddit.py` which runs model SAGE on dataset reddit.\r\n\r\nCPU type: Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz\r\n\r\n` aten::scatter_add_` has been reduced from **37.797s** to **5.989s**:\r\n\r\n* breakdown before\r\n```\r\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls\r\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n                                     aten::scatter_add_        49.00%       37.797s        49.00%       37.797s      41.445ms           912\r\n                                     aten::index_select        19.74%       15.223s        19.74%       15.227s       6.678ms          2280\r\n                                           aten::linear         0.01%       5.706ms        15.04%       11.602s      12.721ms           912\r\n                                            aten::addmm         6.62%        5.108s         7.92%        6.112s      13.403ms           456\r\n                                           aten::matmul         0.00%       2.339ms         7.10%        5.475s      12.006ms           456\r\n```\r\n\r\n* breakdown after\r\n```\r\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls\r\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------\r\n                                     aten::index_select        32.41%       14.677s        32.42%       14.681s       6.439ms          2280\r\n                                           aten::linear         0.01%       6.665ms        26.43%       11.968s      13.123ms           912\r\n                                            aten::addmm        11.76%        5.328s        13.76%        6.232s      13.667ms           456\r\n                                     aten::scatter_add_        13.22%        5.989s        13.22%        5.989s       6.566ms           912\r\n                                           aten::matmul         0.01%       2.303ms        12.63%        5.720s      12.543ms           456\r\n```\n\ncc @VitalyFedyunin @jgong5 @XiaobingSuper @sanchitintel @ashokei @jingxu10", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3200025338, "node_id": "MDU6TGFiZWwzMjAwMDI1MzM4", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel%20priority", "name": "intel priority", "color": "4E4EF8", "default": false, "description": "matters to intel architecture from performance wise"}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773062847, "node_id": "LA_kwDOA-j9z87g5F6_", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20performance", "name": "topic: performance", "color": "AE9D13", "default": false, "description": "topic category"}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}, {"id": 4718445952, "node_id": "LA_kwDOA-j9z88AAAABGT3FgA", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20gnn", "name": "release notes: gnn", "color": "AC65AD", "default": false, "description": "gnn related optimizations"}]}
{"number": 82695, "title": "Use FindCUDAToolkit to find cuda dependencies", "time": "2022-08-03T00:40:51Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #83199\n* #76104\n* __->__ #82695\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2404913419, "node_id": "MDU6TGFiZWwyNDA0OTEzNDE5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Merged", "name": "Merged", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2510927053, "node_id": "MDU6TGFiZWwyNTEwOTI3MDUz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Reverted", "name": "Reverted", "color": "ededed", "default": false, "description": null}, {"id": 3618084153, "node_id": "LA_kwDOA-j9z87Xp5U5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/binaries", "name": "ciflow/binaries", "color": "245567", "default": false, "description": "Trigger all binary build and upload jobs on the PR"}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768824578, "node_id": "LA_kwDOA-j9z87go7MC", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20cuda", "name": "release notes: cuda", "color": "91275E", "default": false, "description": "release notes category"}, {"id": 4183081985, "node_id": "MDU6TGFiZWw0MTgzMDgxOTg1", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20build", "name": "topic: build", "color": "0e8a16", "default": false, "description": null}]}
{"number": 82674, "title": "[ao][DO NOT MERGE][experimental] ModelReport DLRM test work", "time": "2022-08-02T21:36:11Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #82674\n\nSummary: This contains the info on the modified files used to test the\nModelReport API on the DLRM model. The README file gives a bit more of\nan overview of exactly how it was modified and what to run, but at a\nhigh level, the script file and the python file in the folder are\nmodified versions of originals in the DLRM repo, and all that needs to\nbe done is to clone the DLRM repo, train the model, and then run script\nwith the inference only flag to then quantize the model different ways\nto both generate the suggestions and how the model works.\n\nWhile attempting to test ModelReport API on the full model, i ran into\nthe issue where the embedding bags module is not completely traceable,\nas a result, it turned from an experiment to benchmark the ModelReport\nsuggestion based model to if we could get this work on just specific\nsubmodules of the model that were traceable instead, and after figuring\nout how to apply the fx workflow to a part of the model, we found that\nthe ModelReport API works well even when dealing with submodules of a\nlarger model.\n\nTest Plan:\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\ncc @ezyang @SherlockNoMad @soumith @EikanWang @jgong5 @wenzhe-nrv", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 4347715449, "node_id": "LA_kwDOA-j9z88AAAABAyTfeQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fx", "name": "fx", "color": "ededed", "default": false, "description": null}, {"id": 4726472156, "node_id": "LA_kwDOA-j9z88AAAABGbg93A", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20AO%20frontend", "name": "release notes: AO frontend", "color": "ededed", "default": false, "description": null}]}
{"number": 82665, "title": "Add CPP stacktrace on CUDA OOM", "time": "2022-08-02T19:56:38Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #82665\n\r\nWhen reporting CUDA OOM error, defaults to showing the C++ stacktrace to help with debugability in terms of when this happened / which operator caused it. \r\n\r\nHad to make separate macros for handling the torch errors based on whether CUDA is available or not, because you cannot have a define directive within another directive: https://stackoverflow.com/questions/10074520/why-compiler-complain-about-this-macro-declaration. Feel free to suggest better alternatives!", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 82609, "title": "Manual submodule update: kineto (#82609)", "time": "2022-08-01T21:20:06Z", "body": "Summary:\r\nThis is a manual pull request to update the first-party submodule for [pytorch/kineto](https://github.com/pytorch/kineto).\r\n\r\nNew submodule commit: https://github.com/pytorch/kineto/commit/0a2af89149ce2103281bfc995f0c250bd801c506\r\n\r\nTest Plan: Ensure that CI jobs succeed on GitHub before landing.\r\n\r\nDifferential Revision: D38322833\r\n\r\nPulled By: aaronenyeshi\r\n\r\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}, {"id": 4077587778, "node_id": "LA_kwDOA-j9z87zCw1C", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/nightly", "name": "ciflow/nightly", "color": "bfdadc", "default": false, "description": "Trigger all jobs we run nightly (nightly.yml)"}]}
{"number": 82602, "title": "FX pass to move input mutations into submodule", "time": "2022-08-01T20:38:02Z", "body": "When functionalization is turned on in AOT Autograd, we want to hide input mutations in the graph so that the backend compiler doesn't need to worry about seeing `copy_()` ops in the graph. This PR does that by hiding it in an opaque submodule.\r\n\r\nRight now this logic happens after the partitioning, and we're relying on partitioning to always leave the `copy_()` nodes in the forward graph (which... probably needs some more testing, but I think is fine?).\r\n\r\nI added light testing for this pass by including it in the existing `test_functionalization.py` tests, but I'm planning to try hooking this into the torchbench suite, which will let us get rid of this code: https://github.com/pytorch/torchdynamo/blob/5040d49795dde35f0112e27a6744015d44318deb/torchdynamo/optimizations/training.py#L59\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack):\n* #85844\n* #85875\n* #85681\n* #82601\n* __->__ #82602\n* #82771\n* #83265\n\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 82601, "title": "fix functionalize() input metadata mutations, migrate core test to use functionalize()", "time": "2022-08-01T20:37:58Z", "body": "Although this won't fully solve the \"input metadata mutations\" issue for AOTAutograd, there's no reason functionalization should break on input metadata mutations - this PR updates functionalization to use `as_strided_()` to mutate input metadata when necessary, similar to how it uses `copy_()` to handle input data mutations.\r\n\r\nI also switched the functionalization tests over to using the proper `functionalize()` from functorch, which resulted in a few expect test changes.\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack):\n* #85844\n* #85875\n* #85681\n* __->__ #82601\n* #82602\n* #82771\n* #83265\n\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 82586, "title": "NestedTensor Softmax", "time": "2022-08-01T17:35:42Z", "body": "Summary: Nested tensor softmax implementation for cuda, based on  mask compute followed by softmax\n\nTest Plan: unit test\n\nDifferential Revision: D38288107\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 82564, "title": "Consolidate distributed benchmark folders", "time": "2022-08-01T12:15:04Z", "body": "### Description\r\nMerged benchmarks into single directory\r\n\r\n### Issue\r\nhttps://github.com/pytorch/pytorch/issues/63403\r\n", "label": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20distributed", "name": "oncall: distributed", "color": "f7e101", "default": false, "description": "Add this issue/PR to distributed oncall triage queue"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 82551, "title": "[complex] add complex support in dropout and its variants", "time": "2022-07-31T18:29:38Z", "body": "Fixes: #80256\r\n\r\nAdds `complex.{64, 128}` dtype support in cpu and cuda, and `chalf` support in cuda for the following functions:\r\n\r\n- [x] Dropout\r\n- [x] Dropout2d\r\n- [x] Dropout3d\r\n- [x] AlphaDropout\r\n- [x] FeatureAlphaDropout\r\n- [x] OpInfo for AlphaDropout (I see it's missing)\r\n- [ ] Fix tests\r\n\r\ncc: @kshitij12345!", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 82506, "title": "Add subset sampling support for distributed settings.", "time": "2022-07-29T22:23:05Z", "body": "This is useful for training on a subset of data or splitting validation\r\nsets. The subset sampling only exists for non-distributed settings in\r\ntorch.utils.data.sampler.SubsetRandomSampler before this code.\r\n\r\n### Description\r\n<!-- What did you change and why was it needed? -->\r\n\r\nI have added subset sampling support to the 'torch.utils.data.distributed.DistributedSampler' class. Before this code, subset sampling was only possible in non-distributed settings. \r\n\r\nThis is useful for training on a subset of data or splitting validation sets. Since the DataLoader class only accept one sampler, this code is necessary for splitting data in distributed settings (among GPUs and using a subset of data rather than the whole data). \r\n\r\n### Issue\r\n<!-- Link to Issue ticket or RFP -->\r\n\r\nI did not submit any issues. But I needed this code for splitting my training data into the validation set. It was easy in non-distributed settings with 'torch.utils.data.sampler.SubsetRandomSampler'. But the only option for distributed settings was 'torch.utils.data.distributed.DistributedSampler', which is not supporting subsets. \r\n\r\n### Testing\r\n<!-- How did you test your change? -->\r\n\r\nThe code was inherited from a tested class that existed before. The rest of the code is tested with training a model on cifar100. I also tested different corner cases to make sure that the code always has expected behavior and in case of bad usage, it has asserts and provides explanatory exceptions. ", "label": [{"id": 1076923878, "node_id": "MDU6TGFiZWwxMDc2OTIzODc4", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dataloader", "name": "module: dataloader", "color": "f7e101", "default": false, "description": "Related to torch.utils.data.DataLoader and Sampler"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 82488, "title": "Fix Gumbel.cdf function", "time": "2022-07-29T18:03:16Z", "body": "Fix Gumbel.cdf function.\r\n\r\n### Description\r\nWhen transformed parameters is outside of the support of underlying Uniform distribution. This makes behavior of Gumbel.cdf consistent with other TransformedDistribution that pass value of validate_args to the base distribution.\r\n\r\n### Issue\r\nrunning `Gumbel(0.0,1.0,validate_args=False).cdf(20.0)` would cause `ValueError` exception from `_validate_sample` \r\n\r\n### Testing\r\nTest was added to the `test_distributions.py` to check if `Gumbel(0.0,1.0,validate_args=False).cdf(20.0)` successfully returns 1.0 ", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 82439, "title": "Re-enable dynamic_quant_ops", "time": "2022-07-28T22:43:27Z", "body": null, "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}]}
{"number": 82420, "title": "Upstream shape function for embedding_bag.padding_idx from Torch-MLIR", "time": "2022-07-28T18:16:43Z", "body": "### Description\r\n<!-- What did you change and why was it needed? --> Adding upstream shape function for embedding_bag.padding_idx\r\n\r\n### Issue\r\n<!-- Link to Issue ticket or RFP --> None\r\n\r\n### Testing\r\n<!-- How did you test your change? --> None\r\n\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 82294, "title": "[WIP]Use empty_quantized when pickling non-cpu qtensor", "time": "2022-07-27T04:40:36Z", "body": "# Motivation\r\nWhen use `torch.jit.save` to serialize a non-cpu model, the `pickler` would move tensors(e.g. weight/bias) to CPU firstly. The new CPU tensor is created through `at::empty`, which does not fit for quantized tensor.  \r\n\r\n# Solution\r\nCheck whether original tensor is a quantized one before creating CPU copy. If original tensor is a quantized tensor, use `at::empty_quantied` to create CPU copy instead.\r\n\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 82210, "title": "[Quantization] Allow fuse function to take custom config", "time": "2022-07-26T07:28:02Z", "body": "Summary: ``fuse_modules`` takes ``fuse_custom_config_dict`` as an argument but does not pass it to the lower level functions.\n\nTest Plan: Easy fix\n\nDifferential Revision: D38144060\n\n", "label": [{"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 82207, "title": "[WIP] Add the HierarchicalSoftmax activation function to torch.nn #634", "time": "2022-07-26T04:08:16Z", "body": "Working on adding HierarchicalSoftmax to torch.nn using the existing Softmax function as a guide. The original issue is #634. Initially there may be only one (default) option for tree creation. The current task list is:\r\n\r\n- [x] Add cloned HierarchicalSoftmax class to torch.nn.modules.activation (and __all__ export)\r\n- [x] Add HierarchicalSoftmax import to torch.nn.modules.__init__ (and __all__ export)\r\n- [x] Add hierarchical_softmax function to torch.nn.functional\r\n- [x] Add static Huffman tree generation function\r\n- [ ] Add unit tests for Huffman tree\r\n- [ ] Update docstrings to define base/optional functionality\r\n- [ ] Update C++ kernel (optional)\r\n\r\nNotes:\r\n- hierarchical_softmax in torch.nn.functional uses the same unary, dim, and dtype checks as softmax\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769204372, "node_id": "LA_kwDOA-j9z87gqX6U", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20releng", "name": "release notes: releng", "color": "EED552", "default": false, "description": "release notes category"}]}
{"number": 82158, "title": "WIP Remove named tensors", "time": "2022-07-25T20:15:07Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #82158\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769215850, "node_id": "LA_kwDOA-j9z87gqatq", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20linalg_frontend", "name": "release notes: linalg_frontend", "color": "B7C4BC", "default": false, "description": "release notes category"}, {"id": 4417837777, "node_id": "LA_kwDOA-j9z88AAAABB1La0Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/mps", "name": "ciflow/mps", "color": "02EDA7", "default": false, "description": "Run MPS tests (subset of trunk)"}]}
{"number": 82154, "title": "[Profiler] Defer recording startup python events", "time": "2022-07-25T19:52:58Z", "body": "Summary:\nhttps://github.com/pytorch/pytorch/issues/81559\n- Bypass recording startup calls, instead store their TraceKey and start_time\n- Push stored startup calls into 'enters' (set_start_frames)\n- Try best-effort inference of system TID for startup calls (populate())\n\nTest Plan:\nLunch a workload that has python modules:\nbuck build mode/opt kineto/libkineto/fb/integration_tests:pytorch_resnet_integration_test\n./buck-out/gen/kineto/libkineto/fb/integration_tests/pytorch_resnet_integration_test.par\n\nDifferential Revision: D38029112\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}]}
{"number": 82143, "title": "Rematerialization on arbitrary graphs, mincut", "time": "2022-07-25T18:52:22Z", "body": "### Description\r\nAdd a rematerialization algorithm that analyze an fx graph and modify it to rematerialize some nodes such that the memory reads and writes between fusion groups are reduced. In this implementation, the fusion groups are NVFuser fusion groups, but it can be easily changed to support other fusion groups. \r\n\r\nRefactored partitioners.py to use the common functions in utitlities.py. The functions are just moved, they are not changed.\r\n\r\n### Issue\r\n<!-- Link to Issue ticket or RFP -->\r\n\r\n### Testing\r\nThere's a unittest file included in this PR. \r\n\r\n`pytest functorch/test/test_remat_mincut.py`\r\n\r\nMore tests and benchmarks on torchbench graphs in https://github.com/pytorch/functorch/pull/925 \r\n\r\nPassing ` pytest functorch/test/test_pythonkey.py -k TestPartitioning`\n\ncc @ezyang @SherlockNoMad @soumith @EikanWang @jgong5 @wenzhe-nrv", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}, {"id": 4347715449, "node_id": "LA_kwDOA-j9z88AAAABAyTfeQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fx", "name": "fx", "color": "ededed", "default": false, "description": null}]}
{"number": 82138, "title": "[JIT] [Draft] Test time difference increases from various appraoches to processing alias information", "time": "2022-07-25T18:24:40Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #82138\n* #82029\n* #81917\n* #81916\n* #81837\n* #81836\n\n\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 81942, "title": "[TEST] Test CI with nondeterministic ops", "time": "2022-07-21T22:48:28Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #81942\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 81862, "title": "FIX make sure we import the correct object from multiprocessing", "time": "2022-07-21T10:12:54Z", "body": "Fixes #44687.\r\n\r\nThe issue was that the Process object is not the one from the `_default_context` which should be `loky` when nesting `loky` calls.\r\n\r\nThis is a revamp of #53282 that was reverted because it broke some other tests.\r\nHow can I run the failing tests so I can see why this breaks?\r\n\r\ncc @VitalyFedyunin @janeyx99 ", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}]}
{"number": 81852, "title": "add mixed data type support for GroupNorm", "time": "2022-07-21T03:13:53Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* __->__ #81852\n* #81851\n* #84405\n* #84404\n\n1. If user uses amp to run bfloat16 models, `torch.autocast` will\nkeep module paramters in acc dtype which will leave `gamma` and`beta`\nin float while input/output will be in bfloat16.\n\n2. If user explicitly cast the model to bfloat16,\nthe input/output and gamma/beta will all be in bfloat16.\n\ncc @VitalyFedyunin @jgong5 @XiaobingSuper @sanchitintel @ashokei @jingxu10", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769207236, "node_id": "LA_kwDOA-j9z87gqYnE", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20nn", "name": "release notes: nn", "color": "29E07F", "default": false, "description": "release notes category"}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 81848, "title": "Fix is_channels_last_ issue for channels last memory format", "time": "2022-07-21T03:00:19Z", "body": "Fixes #78611 Reshape tensors witch are channels_last will get unexpected stride. The corresponding flags is_channels_last_contiguous_ will be set to true and is_channels_last_ is set to false.", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 81831, "title": "[Reland 2] Add should_traverse_fn to torch.fx.node.map_aggregate", "time": "2022-07-20T23:54:57Z", "body": "Differential Revision: D38019057\r\n\r\n\n\ncc @ezyang @SherlockNoMad @soumith @EikanWang @jgong5 @wenzhe-nrv", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}, {"id": 4347715449, "node_id": "LA_kwDOA-j9z88AAAABAyTfeQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fx", "name": "fx", "color": "ededed", "default": false, "description": null}]}
{"number": 81803, "title": "[ao][sparsity] adding several sparse quantized layers", "time": "2022-07-20T19:51:11Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #81803\n* #81802\n\nSummary: This PR adds sparse+quantized versions of the following quantized layers\n\nlinear_relu\nlinear_relu (qat)\nlinear (qat)\n\nalso updated the default sparse+quantized mappings to utilize these new\nlayers and also updated the qat linear layer to maintain the\nsparse_params which also solves the problem for the intrinsic qat layers.\n\nTest Plan: python test/test_ao_sparsity.py TestQuantizedSparseKernels\npython test/test_ao_sparsity.py TestQuantizedIntrinsicSparseKernels\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 81787, "title": "[JIT] Run CI tests with SchemaCheckMode", "time": "2022-07-20T18:01:55Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #81787\n* #81917\n* #81916\n* #81837\n* #81836\n* #81786\n* #81785\n* #81784\n* #81783\n* #81782\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 81763, "title": "Dispatch the auxiliary frobenius_norm and nuclear_norm to better implementations and deprecate them", "time": "2022-07-20T12:41:57Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #81763\n* #81762\n* #84624\n\nThese functions will be legacy functions. We deprecate them, but we also\ntake this chance to dispatch to a more efficient and consistent implementation.\nDoing so should help writing a conversion rule for these to be able to\nremove them once and for all", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1504141947, "node_id": "MDU6TGFiZWwxNTA0MTQxOTQ3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20linear%20algebra", "name": "module: linear algebra", "color": "f7e101", "default": false, "description": "Issues related to specialized linear algebra operations in PyTorch; includes matrix multiply matmul"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}, {"id": 3773059130, "node_id": "LA_kwDOA-j9z87g5FA6", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20deprecation", "name": "topic: deprecation", "color": "2824A8", "default": false, "description": "topic category"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}, {"id": 4081012228, "node_id": "LA_kwDOA-j9z87zP04E", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20primTorch", "name": "module: primTorch", "color": "455561", "default": false, "description": ""}]}
{"number": 81762, "title": "Remove overload at::frobenius_norm(const Tensor&)", "time": "2022-07-20T12:41:53Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #81763\n* __->__ #81762\n* #84624\n\nThis function is an auxiliary function for `torch.norm`. This particular\noverload was not even used or tested. I hope it's not used internally\neither. If it is, we can simply drop this PR\n\ncc @mcarilli @ptrblck @leslie-fang-intel @jgong5", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1972560798, "node_id": "MDU6TGFiZWwxOTcyNTYwNzk4", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20amp%20(automated%20mixed%20precision)", "name": "module: amp (automated mixed precision)", "color": "f7e101", "default": false, "description": "autocast"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3769215850, "node_id": "LA_kwDOA-j9z87gqatq", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20linalg_frontend", "name": "release notes: linalg_frontend", "color": "B7C4BC", "default": false, "description": "release notes category"}]}
{"number": 81698, "title": "Turn on TorchDispatchMode for crossref testing.", "time": "2022-07-19T13:28:00Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #81698\n* #81697\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 81658, "title": "Enable IWYU on c10/util", "time": "2022-07-18T20:55:59Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #81658\n* #81657\n* #81656\n\nFor the most part, this is the output of running\n```\nlintrunner c10/util/* -a\n```\n\nThere were some manual tweaks required, particularly for\nmacro-dependent includes which weren't in the `#ifdef` blocks.", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 81657, "title": "Prepare for using IWYU on c10/util", "time": "2022-07-18T20:55:56Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #81658\n* __->__ #81657\n* #81656\n\nThis does a few things:\n- moves the main header file to first in the list of the includes,\n  e.g. foo.cpp includes foo.h first.\n- otherwise, if the names differ, I've added\n  `// IWYU pragma: associated` to link them.\n- Add `// IWYU pragma: export` for headers that shouldn't need to be\n  re-included in the file consuming the header.", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 81656, "title": "WIP: Integrate include-what-you-use with lintrunner", "time": "2022-07-18T20:55:52Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #81658\n* #81657\n* __->__ #81656\n\r\nSee #77165\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 81630, "title": "[vulkan] benchmarking of quantized ops", "time": "2022-07-18T13:44:53Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #81630\n\nbenchmarking of quantized ops through regular testing and checking time, however doesn't account for extra overhead\nresults below:\n{F753441917}\n\nDifferential Revision: [D37923483](https://our.internmc.facebook.com/intern/diff/D37923483/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D37923483/)!", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 81629, "title": "[distributed] Self-adaptive `send()`/`recv()` pair", "time": "2022-07-18T09:24:54Z", "body": "Currently, the Point-to-point communication pair\r\n`torch.distributed.send()` and `torch.distributed.recv()`\r\nrequires the user to provide `recv()` a tensor of the same\r\n`shape` and `dtype` with that in corresponding `send()`.\r\n\r\nIn practice, the shape and dtype of src tensor in `send()` can be hard\r\nor just inconvenient to know, or they changes during time.\r\n\r\nThis PR implements `adaptive_send()`/`adaptive_recv()` pair.\r\n`adaptive_recv()` requires no input tensor as container, but returns a\r\nnew tensor whose `shape` and `dtype` is decided by the src tensor sent\r\nby `adaptive_send()`.\r\n\r\nRelated issue: #81545\r\n\r\nSigned-off-by: Shengjiang QUAN <qsj287068067@126.com>\r\n\r\nFixes #81545\r\n\n\ncc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu", "label": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20distributed", "name": "oncall: distributed", "color": "f7e101", "default": false, "description": "Add this issue/PR to distributed oncall triage queue"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769218061, "node_id": "LA_kwDOA-j9z87gqbQN", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(c10d)", "name": "release notes: distributed (c10d)", "color": "EE86D4", "default": false, "description": "release notes category"}]}
{"number": 81596, "title": "[BC-Breaking] Separate `stream_id`, `device_index`, and `device_type` in `pack` and `unpack` for `Streams`", "time": "2022-07-16T01:36:57Z", "body": "#75854\r\n\r\nA naive attempt at working around the limitations of using a single 64-bit integer to pack `stream_id`, `device_index`, and `device_type`.\r\n\r\nStills needs sanity checks, testing, and minimization of BC-breaking changes.\r\n\r\nCurrently a Holder for the `StreamData3` struct is used for `IValue` compatibility. While doing this seems to work for `ivalue.h` and `ivalue_inl.h`, this doesn't seem to be naively working for the JIT CUDA stream wrapper? (Something about ambiguous calls if an `intrusive_ptr` to `c10::ivalue::StreamData3Holder` is used as the return type for `pack()`. It turns out that the methods required to access the fields for rematerializing a CUDA Stream are basically already present anyway, so `pack` is simply removed in the wrapper for now and the methods to access the required fields are called directly.\r\n\r\nCC @ptrblck \n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}]}
{"number": 81490, "title": "[vulkan] implement quantized add", "time": "2022-07-14T19:07:41Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\nimplemented quantized add operator\n\nDifferential Revision: [D37491753](https://our.internmc.facebook.com/intern/diff/D37491753/)", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 81489, "title": "[vulkan] implement dequantize", "time": "2022-07-14T19:07:36Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\nfor T121382387\nimplemented dequantize function and corresponding shader\n\nDifferential Revision: [D37321171](https://our.internmc.facebook.com/intern/diff/D37321171/)", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 81488, "title": "[vulkan] implement quantize per tensor", "time": "2022-07-14T19:07:31Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\nfor T121382131\nimplement quantize_per_tensor function and corresponding shader\n\nDifferential Revision: [D37318641](https://our.internmc.facebook.com/intern/diff/D37318641/)", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 81487, "title": "[vulkan] add support for quantized tensors", "time": "2022-07-14T19:07:26Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\nfor T121381630\nadded support for int textures/quantized tensors\nwrote shaders for the above and had to modify certain files in the pytorch codebase to allow for vulkan tensors\n\nDifferential Revision: [D37314225](https://our.internmc.facebook.com/intern/diff/D37314225/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D37314225/)!", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 81429, "title": "csl/jank", "time": "2022-07-13T21:09:39Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}]}
{"number": 81427, "title": "csl/xla3", "time": "2022-07-13T20:46:07Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 81397, "title": "Update functional.py", "time": "2022-07-13T11:18:29Z", "body": "I've tried adding the required `dim` support.\r\nIs this how it is to be done?\r\nIf yes, I'll update the inline comments as well.\r\nThank You\r\n\r\nFixes #77256\r\n", "label": [{"id": 1076922764, "node_id": "MDU6TGFiZWwxMDc2OTIyNzY0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20nn", "name": "module: nn", "color": "f7e101", "default": false, "description": "Related to torch.nn"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 81379, "title": "Add memory_format overload for full, ones, zeros", "time": "2022-07-13T00:42:50Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #81379\n* #81380\n* #81378\n* #82241\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 81348, "title": "[WIP]change default tensor_adam to multi_tensor_adam", "time": "2022-07-12T19:03:44Z", "body": "I changed default tensor_adam to mutli_tensor_adam and would like to test its performance.\r\n\r\nTest plan: TorchBench CI\r\n\r\nRUN_TORCHBENCH: ALL", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 81302, "title": "[POC] functionalization w dynamic shapes E2E", "time": "2022-07-12T01:24:47Z", "body": "If you look at the test file, the output is:\r\n```\r\ndef f(x, y):\r\n    val = torch.mul(x, y)\r\n    val_view = val.view(val.shape)\r\n    val_view.mul_(x)\r\n    out = torch.cat([val, val])\r\n    if out.shape[0] * out.shape[1] > 20:\r\n        out = out.cos()\r\n    return out.expand(out.shape)\r\n\r\nfx_g = make_fx(functionalize(f, remove='mutations_and_views'))(torch.randn(5, 1), torch.randn(1, 5))\r\nfx_g.graph.eliminate_dead_code()\r\nfx_g.recompile()\r\nprint(fx_g.code)\r\nprint(fx_g.shape_env.guards)\r\n\r\n# prints...\r\n\r\ndef forward(self, x_1, y_1):\r\n    mul_tensor = torch.ops.aten.mul.Tensor(x_1, y_1);  y_1 = None\r\n    size_4 = mul_tensor.size(0)\r\n    size_5 = mul_tensor.size(1)\r\n    view_copy_sym_int = torch.ops.aten.view_copy.SymInt(mul_tensor, [size_4, size_5]);  mul_tensor = None\r\n    mul_tensor_1 = torch.ops.aten.mul.Tensor(view_copy_sym_int, x_1);  view_copy_sym_int = x_1 = None\r\n    view_copy_sym_int_1 = torch.ops.aten.view_copy.SymInt(mul_tensor_1, [size_4, size_5]);  mul_tensor_1 = size_4 = size_5 = None\r\n    cat_default = torch.ops.aten.cat.default([view_copy_sym_int_1, view_copy_sym_int_1]);  view_copy_sym_int_1 = None\r\n    cos_default = torch.ops.aten.cos.default(cat_default);  cat_default = None\r\n    size_18 = cos_default.size(0)\r\n    size_19 = cos_default.size(1)\r\n    expand_copy_sym_int = torch.ops.aten.expand_copy.SymInt(cos_default, [size_18, size_19]);  cos_default = size_18 = size_19 = None\r\n    return expand_copy_sym_int\r\n\r\n[(2*arg_1_0*arg_2_1 > 20, True)]\r\n```\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack):\n* __->__ #81302\n* #81301\n* #81238\n\nDifferential Revision: [D38594888](https://our.internmc.facebook.com/intern/diff/D38594888)", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 81301, "title": "get FunctionalTensorWrapper to propagate symints", "time": "2022-07-12T01:24:44Z", "body": "`FunctionalTensorWrapper` needs to know when it's dealing with ints vs. SymInts.\r\n\r\nI'm pretty sure the right way to do this is:\r\n```\r\nif (has_symbolic_sizes_strides())\r\n   set_sym_sizes_and_strides(..);\r\n} else {\r\n   set_sizes_and_strides(..);\r\n}\r\n```\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack):\n* #81302\n* __->__ #81301\n* #81238\n\nDifferential Revision: [](https://our.internmc.facebook.com/intern/diff/)", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 81293, "title": "Update batch norm to compute forward grads for saved_mean and saved_var when input requires grad", "time": "2022-07-11T23:38:47Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #81293\r\n\r\nThis change lets us get rid of the additional decomposition table specifically used for jvps\r\n\r\nI'm actually not completely sure we want to land this. Some downsides of the current approach:\r\n- The requires_grad-ness of the return lies about the backward differentiability of saved_mean and saved_var!\r\n   - a simple mitigation: add an assert when non-zero gradient is passed as grad_saved_mean/var\r\n   - how bad that is depends on how \"user-facing\" we consider this API\r\n- The decomposition and actual function diverges (we could either skip the test or update actual function also return a zero-filled tensor)\r\n\r\nSome alternatives:\r\n- Actually tell the truth about differentiability - update codegen to allow functions forward differentiable, but not backward differentiable functions\r\n   - This is not an entirely new concept actually because differentiability is not bidirectional in the case of detach\r\n- Actually make the gradients flow properly\r\n   - This is the cleaner solution, but requires a rewrite of the double backward formula.\r\n- Continue with the status quo:\r\n  - We'd have to stick with the current hack, but maybe it isn't so bad considering the alternatives.\r\n\r\nThis PR:\r\n- Gets rid of the additional decomposition table\r\n- Adds a bunch of random changes to the decomposition necessary to get it to JIT compile. I chose not to use the decomposition from before because it is missing subtle things like casting to the right intermediate dtype (I'm not sure if there are other differences).\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 81247, "title": "[ci] migrate sccache stats to s3", "time": "2022-07-11T16:16:38Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #81247\n\nDo the same thing where we upload to S3 and feed that to Rockset, which\nappears to be more reliable across the board.", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 81238, "title": "add aten::empty_strided.SymInt", "time": "2022-07-11T14:35:20Z", "body": "Adds `aten::empty_strided.SymInt`, which is needed to get functionalization to plumb symints properly.\r\n\r\nSpecifically, all of the functionalization kernels use `empty_strided` to generate meta tensors, that they use internally to figure out the correct sizes/strides of their outputs. I update that code (`to_meta()`) to use the new version, `empty_strided.SymInt`.\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack):\n* #81302\n* #81301\n* __->__ #81238\n\nDifferential Revision: [](https://our.internmc.facebook.com/intern/diff/)", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 81191, "title": "[primTorch] Implement batch, group, and instance norm references", "time": "2022-07-11T00:31:56Z", "body": "Add References:\r\n\r\n1. batch norm\r\n2. group norm\r\n3. instance norm\r\n\r\nDepends on:\r\n\r\n- expand https://github.com/pytorch/pytorch/pull/79820\r\n- repeat https://github.com/pytorch/pytorch/pull/81374\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 4081012228, "node_id": "LA_kwDOA-j9z87zP04E", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20primTorch", "name": "module: primTorch", "color": "455561", "default": false, "description": ""}]}
{"number": 81136, "title": "[bugfix] Parameters to ignore is not being fully respected", "time": "2022-07-08T20:39:39Z", "body": "Summary: I was experimenting with a model that had sharded tensors as parameters, and I set them to be ignored. However, DDP was still checking their device + some other sanitization logic. If we're ignoring them, then we shouldn't need to check\n\nTest Plan: unit tests\n\nDifferential Revision: D37726515\n\n\n\ncc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu", "label": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20distributed", "name": "oncall: distributed", "color": "f7e101", "default": false, "description": "Add this issue/PR to distributed oncall triage queue"}, {"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 81126, "title": "Remove _{fns} from torch, set correct module for linalg and special fns and add __all__ for other submodules", "time": "2022-07-08T18:33:51Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #81126\n* #80520\n\n", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 81011, "title": "[WIP] use __slots__ for module class", "time": "2022-07-06T22:37:46Z", "body": "Draft PR for CI", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 80990, "title": "[DONOTMERGE] [ROCm] Upgrade KFD on ROCm CI nodes to ROCm5.3", "time": "2022-07-06T19:34:55Z", "body": "Testing out the CI hosts with upgraded KFD\r\n\n\ncc @jeffdaily @sunway513 @pruthvistony @ROCmSupport", "label": [{"id": 1078897659, "node_id": "MDU6TGFiZWwxMDc4ODk3NjU5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20rocm", "name": "module: rocm", "color": "f7e101", "default": false, "description": "AMD GPU support for Pytorch"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}, {"id": 4804263862, "node_id": "LA_kwDOA-j9z88AAAABHls_tg", "url": "https://api.github.com/repos/pytorch/pytorch/labels/rocm", "name": "rocm", "color": "B3FC7B", "default": false, "description": "This tag is for PRs from ROCm team"}]}
{"number": 80859, "title": "[MPS] Fix incorrect contiguous operation before copy  ", "time": "2022-07-05T08:37:06Z", "body": "Currently, a contiguous operation is performed before copying a tensor from `cpu` to `mps`, which would break the default preserve memory copy strategy.\r\n\r\nLet's create a tensor with the following data with stride `(2, 1)`:\r\n```python\r\n[[0., 1.],\r\n [2., 3.],\r\n [4., 5.]]\r\n```\r\n\r\nThen, we derive a view from the above tensor so that the stride becomes `(1, 2)`:\r\n```python\r\n[[0., 2., 4.],\r\n [1., 3., 5.]])\r\n```\r\n\r\nAnd now if we incorrectly have the view layout contiguous before copying, and the stride `(1, 2)` is still applied after the copy, the resulting tensor will become the following, which is incorrect:\r\n\r\n```python\r\n[[0.0000, 4.0000, 3.0000],\r\n [2.0000, 1.0000, 5.0000]]\r\n```\r\n\r\nA repro can be found [here](https://github.com/pytorch/pytorch/issues/79383#issuecomment-1173474956).\r\n\r\n\r\nPartially `resolve` #79383\r\n\r\n@albanD @kulinseth Could you help add trunk label?", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 4121057428, "node_id": "LA_kwDOA-j9z871oliU", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20mps", "name": "module: mps", "color": "f7e101", "default": false, "description": "Related to Apple Metal Performance Shaders framework"}]}
{"number": 80854, "title": "Enhance OneDNN PostOp Fusion for Conv2d", "time": "2022-07-05T03:03:33Z", "body": "## Summary\r\nThis PR aims to support inplace op it TE partially. This can enable TE fusion for patterns like  \"at::conv, at::relu_\" with OneDNN post-op fusion.\r\n\r\n## Options\r\n\r\n### Option 1: Replace the in-place operator with the out-place operator\r\n - Pros: Easy to implement. Can re-use \"RemoveTensorMutation\" for replacement and the lowing function of outplace version. Will not have a performance penalty under \"fusion\" cases.\r\n - Cons: The fallback path is also outplaced version, which will have a performance penalty here. Sometimes we cannot safely replace the inplace op with its' outplace version.\r\n \r\n### Option 2: Lower the body in terms of in-place directly\r\n - Pros: More straightforward. Can support more scenarios.\r\n - Cons: Hard to develop. More engineer effort.\r\n\r\nWe choose **option 1** in this PR, and we can consider option 2 if we observed that in many real-world scenarios, options1 failed.\r\n\r\n## Implement Details\r\n![image](https://user-images.githubusercontent.com/54701539/179939197-7c2a2e84-ddec-4b40-90ec-4e1cd2870b5b.png)\r\n\r\nWe will pull inplace ops into TE fusion groups if the ops satisfy 2 conditionals:\r\n - This inplace ops can be safely replaced with its outplace version at the global graph. (Re-use the ability of RemoveTensorMutation)\r\n - The outplace version is supported by TE.\r\n\r\nTo achieve this, we extend the behavior of **Operator supported check** and **TryMerge**.\r\n### In **Operator supported check**. \r\nFor example, [isSupported](https://github.com/pytorch/pytorch/blob/23bdb570cf05f0cefdacdda5cbf73f58a2e574f4/torch/csrc/jit/passes/tensorexpr_fuser.cpp#L89). If an inplace op can be safely replaced with its outplace version. We will create an outplace node and send this node to pass the TE checks. After the checks are done, destroy the outplace node.\r\n - We choose to make a outplace node to pass the TE checks because we do not want directly added inplace op into the supported op list. Te does not **directly** support these inplace ops.\r\n - We will destroy the outplace node after **Operator supported check** is done because **Operator supported check** is not all checks for TE fusion, there are many following checks later and we do not know whether all following check will pass at this stage.\r\n\r\n\r\n### In **TryMerge**.\r\nHere we can know all checks are passed, we will replace an inplace op with its outplace version before the moment we merge the node into fusion groups.\r\n\r\n### Some details for safely replacement\r\nWhether an inplace op can be replaced safely depends on the behavior of RemoveTensorMutions.\r\n2 cases below will not be replaced.\r\n```python\r\ndef fn(a, b):\r\n    return a.relu_() + b.relu()\r\ndef fn(a, b):\r\n    c = a + b\r\n    return c.sigmoid().add(c.relu_())\r\n```\r\nOps like \\_\\_and\\_\\_/\\_\\_or\\_\\_. Their inplace version are \\_\\_iand\\_\\_/\\_\\_ior\\_\\_. These ops cannot be replaced by RemoveTensorMutaion. So we do not support them yet.\r\n\r\n## Unit test details\r\n - test_mkldnn_fusion.py\r\n\r\nUT covered ops\r\n```\r\nrelu, gelu, sigmoid, add, add+relu, tanh, hardswish, leaky_relu, hardtanh, clamp and their inplace version\r\n```\r\n\r\nLinear's post-op fusion and silu, elu, mish fusion for linear/conv depends on other PR. Will expand OneDNNPostopFusableWithGraph to support them after they are merged.\r\n\r\n\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2510754463, "node_id": "MDU6TGFiZWwyNTEwNzU0NDYz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/NNC", "name": "NNC", "color": "e5678d", "default": false, "description": ""}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3200025338, "node_id": "MDU6TGFiZWwzMjAwMDI1MzM4", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel%20priority", "name": "intel priority", "color": "4E4EF8", "default": false, "description": "matters to intel architecture from performance wise"}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 80583, "title": "Move the RemoveProfileNodesAndSpecializeTypes pass to the beginning o…", "time": "2022-06-30T01:51:07Z", "body": "## Motivation\r\nIt is found that some optimization passes cannot run successfully with extra prim::profile nodes in the graph in `ProfilingGraphExecutorImpl::runNoGradOptimizations`. (e.g: BatchMM or customPrePasses).\r\nAfter reviewing the code, it seems the `RemoveProfileNodesAndSpecializeTypes` is a common passes required for both TE and non-TE path.\r\n\r\n## Solution\r\nBy moving the RemoveProfileNodesAndSpecializeTypes from the TE path to the beginning of `ProfilingGraphExecutorImpl::runNoGradOptimizations` and making it a common passes for all other passes.\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 80484, "title": "DONT MERGE. Testing github export", "time": "2022-06-29T00:44:54Z", "body": "Summary: No Summary, no merge\n\nTest Plan: No Plan, no merge\n\nDifferential Revision: D37507501\n\n\n\ncc @ezyang @SherlockNoMad @soumith @EikanWang @jgong5 @wenzhe-nrv", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3267386230, "node_id": "MDU6TGFiZWwzMjY3Mzg2MjMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20fx", "name": "module: fx", "color": "ededed", "default": false, "description": ""}, {"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}]}
{"number": 80463, "title": "[vulkan] implement quantized add", "time": "2022-06-28T18:36:15Z", "body": "Summary: implemented quantized add operator\n\nTest Plan:\ntest cases for dif scale/zero point, dif shapes and generic test case\nbuck run //xplat/caffe2:pt_vulkan_quantized_api_test_binAppleMac\n\nDifferential Revision: D37491753\n\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 80448, "title": "Add experimental backward support for some torch._prims functions", "time": "2022-06-28T14:35:07Z", "body": "Current draft adds a new keyword argument `impl_vjp=` to `_make_prim` function.\r\n\r\n`impl_vjp` is a callable that computes vector-Jacobian products for given cotangents, forward results, and forward inputs. And this function is used in the `torch.autograd.Function.backward` method.\r\n\r\n`impl_vjp` function must exclusively use `torch._prims` operations because `TorchRefsMode` does nothing for backward calls.\r\n\r\nHere's an example trace for the `torch._refs.sigmoid` function:\r\n```py\r\nimport torch\r\nimport torch._refs\r\nfrom torch._prims.executor import TorchRefsMode\r\nfrom torch.fx.experimental.proxy_tensor import make_fx\r\n\r\na = torch.randn(3, 3, device='cuda', requires_grad=True\r\ng = torch.ones_like(a)\r\n\r\ndef func(a):\r\n    c = torch.sigmoid(a)\r\n    return torch.autograd.grad(c, a, g)\r\n```\r\n```py\r\n# torch.sigmoid trace\r\nprint(make_fx(func)(a).graph.print_tabular())\r\n\r\nopcode         name                      target                         args                                     kwargs\r\n-------------  ------------------------  -----------------------------  ---------------------------------------  --------\r\nplaceholder    a_1                       a_1                            ()                                       {}\r\ncall_function  sigmoid_default           aten.sigmoid.default           (a_1,)                                   {}\r\ncall_function  detach_default            aten.detach.default            (sigmoid_default,)                       {}\r\nget_attr       _tensor_constant0         _tensor_constant0              ()                                       {}\r\ncall_function  is_same_size_default      aten.is_same_size.default      (sigmoid_default, _tensor_constant0)     {}\r\ncall_function  detach_default_1          aten.detach.default            (detach_default,)                        {}\r\nget_attr       _tensor_constant0_1       _tensor_constant0              ()                                       {}\r\ncall_function  sigmoid_backward_default  aten.sigmoid_backward.default  (_tensor_constant0_1, detach_default_1)  {}\r\noutput         output                    output                         ((sigmoid_backward_default,),)           {}\r\n```\r\n\r\n```py\r\n# torch._refs.sigmoid trace\r\nwith TorchRefsMode.push():\r\n    print(make_fx(func)(a).graph.print_tabular())\r\n\r\nopcode         name                  target                     args                                kwargs\r\n-------------  --------------------  -------------------------  ----------------------------------  --------\r\nplaceholder    a_1                   a_1                        ()                                  {}\r\ncall_function  neg_default           prims.neg.default          (a_1,)                              {}\r\ncall_function  detach_default        aten.detach.default        (neg_default,)                      {}\r\ncall_function  exp_default           prims.exp.default          (neg_default,)                      {}\r\ncall_function  detach_default_1      aten.detach.default        (exp_default,)                      {}\r\ncall_function  add_default           prims.add.default          (1.0, exp_default)                  {}\r\ncall_function  detach_default_2      aten.detach.default        (add_default,)                      {}\r\ncall_function  div_default           prims.div.default          (1.0, add_default)                  {}\r\ncall_function  detach_default_3      aten.detach.default        (div_default,)                      {}\r\nget_attr       _tensor_constant0     _tensor_constant0          ()                                  {}\r\ncall_function  is_same_size_default  aten.is_same_size.default  (div_default, _tensor_constant0)    {}\r\ncall_function  detach_default_4      aten.detach.default        (detach_default_3,)                 {}\r\nget_attr       _tensor_constant0_1   _tensor_constant0          ()                                  {}\r\ncall_function  div_default_1         prims.div.default          (_tensor_constant0_1, add_default)  {}\r\nget_attr       _tensor_constant0_2   _tensor_constant0          ()                                  {}\r\ncall_function  neg                   prims.neg                  (_tensor_constant0_2,)              {}\r\ncall_function  mul_default           prims.mul.default          (neg, 1.0)                          {}\r\ncall_function  pow_default           prims.pow.default          (add_default, -2)                   {}\r\ncall_function  mul_default_1         prims.mul.default          (mul_default, pow_default)          {}\r\ncall_function  detach_default_5      aten.detach.default        (detach_default_2,)                 {}\r\ncall_function  detach_default_6      aten.detach.default        (detach_default_1,)                 {}\r\ncall_function  mul_default_2         prims.mul.default          (mul_default_1, detach_default_6)   {}\r\ncall_function  detach_default_7      aten.detach.default        (detach_default,)                   {}\r\ncall_function  neg_default_1         prims.neg.default          (mul_default_2,)                    {}\r\noutput         output                output                     ((neg_default_1,),)                 {}\r\n```\r\n`aten.detach` and `aten.is_same_size` appear in the trace that are called somewhere in the autograd engine code but other than that the trace consists of `prims` calls.\r\n\r\nGradcheck test passes:\r\n```py\r\na = torch.randn(3, 3, device='cuda', requires_grad=True, dtype=torch.float64)\r\nprint(torch.autograd.gradcheck(torch._refs.sigmoid, [a]))  # True\r\n```\r\n\r\nTODO:\r\n\r\n- [ ] tests\r\n- [ ] better save of args for backward\n\ncc @ezyang @mruberry @ngimel @Lezcano @fdrocha @peterbell10", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 4081012228, "node_id": "LA_kwDOA-j9z87zP04E", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20primTorch", "name": "module: primTorch", "color": "455561", "default": false, "description": ""}]}
{"number": 80445, "title": "Update release.md added statement about final tag", "time": "2022-06-28T13:11:48Z", "body": "Update release.md added statement about final tag\r\n\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 80360, "title": "Support many nested_tensor.unary_op", "time": "2022-06-27T17:47:01Z", "body": "In analogy to sparse tensor, we can support many nested tensor unary operations by redispatching the operation to the underlying data", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 80264, "title": "FX Partitioner: use \"all aten ops that decompose to prims\" as supported NVFuser ops", "time": "2022-06-25T01:25:59Z", "body": "Previously we were using the list taken from the torchscript\r\nintegration. But in this case it's actually more useful to just use the\r\nlist of ops that can decompose to prims, because\r\n\r\nTODO: test this to see how it performs with the existing testing.\n\ncc @ezyang @SherlockNoMad @soumith @EikanWang @jgong5 @wenzhe-nrv", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3267386230, "node_id": "MDU6TGFiZWwzMjY3Mzg2MjMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20fx", "name": "module: fx", "color": "ededed", "default": false, "description": ""}, {"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}]}
{"number": 80175, "title": "[vulkan] implement dequantize", "time": "2022-06-23T20:47:45Z", "body": "Summary:\nfor T121382387\nimplemented dequantize function and corresponding shader\n\nTest Plan:\nwrote unit tests and directly call dequantize function\nbuck run //xplat/caffe2:pt_vulkan_quantized_api_test_binAppleMac\n\nDifferential Revision: D37321171\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 80174, "title": "[vulkan] implement quantize per tensor", "time": "2022-06-23T20:46:44Z", "body": "Summary:\nfor T121382131\nimplement quantize_per_tensor function and corresponding shader\n\nTest Plan:\nwrote unit test and directly call function\nbuck run //xplat/caffe2:pt_vulkan_quantized_api_test_binAppleMac\n\nDifferential Revision: D37318641\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 80143, "title": "Fix CPUProfilingAllocator bug", "time": "2022-06-23T14:56:25Z", "body": "Formulating greedy allocation plan will use allocation tensor size as the map key. When there are multiple tensors with the same size, inserting it to the map will fail. This commit makes the freelist not point to only one offset, instead, to a list of offsets thus handle the same size tensor mapping issue.\r\n\r\nFixes #80142\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 80120, "title": "Do not save size and stride for compute nodes for lowering.", "time": "2022-06-23T04:10:33Z", "body": "Summary:\r\nAccesses to allocs should be based only on TVs, so JSON used\r\nfor lowering should not contain stride/shape in compute nodes.\r\nNext step is to ensure that lowering is done through TVs\r\nonly - this will mainly be done by chasing failures in the compile\r\ncaused by remove JSON data.\r\n\r\nDifferential Revision: D37335462\r\n\r\n\n\ncc @ezyang @SherlockNoMad @soumith @EikanWang @jgong5 @wenzhe-nrv", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3267386230, "node_id": "MDU6TGFiZWwzMjY3Mzg2MjMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20fx", "name": "module: fx", "color": "ededed", "default": false, "description": ""}, {"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}]}
{"number": 80109, "title": "[WIP] Allow FileBaton to recover from killed processes", "time": "2022-06-23T00:24:44Z", "body": "This happens regularly when multiprocessing is being used in metaseq\r\n\r\nNote that the same race as before can happen if the process is killed in between the creation of the file and writing the header into it. That section is hopefully small enough that this won't happen too often in practice.", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}]}
{"number": 80032, "title": "Expose set GPU memory fraction per process in CPP API.", "time": "2022-06-22T14:05:31Z", "body": "This PR exposes the TORCH.CUDA.SET_PER_PROCESS_MEMORY_FRACTION function that is already exposed in python\n\ncc @ngimel", "label": [{"id": 1076930055, "node_id": "MDU6TGFiZWwxMDc2OTMwMDU1", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cuda", "name": "module: cuda", "color": "f7e101", "default": false, "description": "Related to torch.cuda, and CUDA support in general"}, {"id": 1300909511, "node_id": "MDU6TGFiZWwxMzAwOTA5NTEx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20memory%20usage", "name": "module: memory usage", "color": "f7e101", "default": false, "description": "PyTorch is using more memory than it should, or it is leaking memory"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769208226, "node_id": "LA_kwDOA-j9z87gqY2i", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20cpp", "name": "release notes: cpp", "color": "8250CF", "default": false, "description": "release notes category"}]}
{"number": 80023, "title": "Ported more refs over to calling torch ops and fixed `nan_to_num`", "time": "2022-06-22T09:31:07Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #80023\n* #80021\n\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 80021, "title": "Fixed logsumexp ref for scalar case + made it a decomp", "time": "2022-06-22T09:15:46Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #80023\n* __->__ #80021\n\r\n<img width=\"816\" alt=\"image\" src=\"https://user-images.githubusercontent.com/6355099/174993307-b4502737-09fa-4038-b518-b82077fd0f96.png\">\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 79943, "title": "[vulkan] add support for quantized tensors", "time": "2022-06-21T15:43:06Z", "body": "Summary:\nfor T121381630\nadded support for int textures/quantized tensors\nwrote shaders for the above and had to modify certain files in the pytorch codebase to allow for vulkan tensors\n\nTest Plan:\nwrote unit tests in vulkan_quantized_api_test.cpp\ntested by directly calling copy_ to transfer from cpu to vulkan and vice versa for quantized tensors which would trigger these shaders accordingly\nbuck run //xplat/caffe2:pt_vulkan_quantized_api_test_binAppleMac\n\nDifferential Revision: D37314225\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 79801, "title": "csl/pytest-asan", "time": "2022-06-17T18:32:02Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 79799, "title": "Fix to preserve view relationships for Parameter.__deepcopy__()", "time": "2022-06-17T18:17:03Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* __->__ #79799\n\r\nFixes #79788\r\n\r\nBC-breaking notes:\r\n* Deepcopying involving parameters now preserves view relationships, breaking anyone who relies on deepcopy producing new, independent objects.\r\n* Callers of `Parameter.new_empty()` will now get a `Parameter` object instead of a `Tensor` object.", "label": [{"id": 846310180, "node_id": "MDU6TGFiZWw4NDYzMTAxODA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20bc-breaking", "name": "module: bc-breaking", "color": "f7e101", "default": false, "description": "Related to a BC-breaking change"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 79719, "title": "csl/pytest-proc", "time": "2022-06-16T16:40:45Z", "body": "https://hud.pytorch.org/hud/pytorch/pytorch/csl%2Fpytest-proc/1", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 79566, "title": "unittest profile", "time": "2022-06-14T21:29:02Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 79564, "title": "csl/pytest-threads", "time": "2022-06-14T21:18:58Z", "body": "[hud.pytorch.org/pr/79564](https://hud.pytorch.org/pr/79564)\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 4631339377, "node_id": "LA_kwDOA-j9z88AAAABFAyhcQ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20inductor", "name": "module: inductor", "color": "f9d0c4", "default": false, "description": ""}]}
{"number": 79462, "title": "Add backward for spdiags (cuda)", "time": "2022-06-13T21:57:12Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #79462\n* #79228\n* #78965\n\nPart of #70926", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 79442, "title": "[Static Runtime] Performance optimization for fork operation", "time": "2022-06-13T18:39:44Z", "body": "Summary:\n- StaticModule was being created at runtime which was adding overhead to the forked operation\n- Move staticModule creation to outside of runtime so that StaticRuntime instance can be created on top of same staticModule that is created once\n\nTest Plan:\n-  Local tests:\n1.1 buck test caffe2/benchmarks/static_runtime/fb:test_fb_operators\n1.2 buck test mode/opt caffe2/benchmarks/static_runtime:static_runtime_cpptest\n1.3 buck test mode/opt caffe2/test:static_runtime\n\n- Async execution of the subgraph is tested by adding pytorch profiler hooks on the StaticRuntime execution via below code. Async execution in threadpool is verfiied by checking trace\n\n        with profile(activities=[ProfilerActivity.CPU]) as prof:\n            static_runtime_module(inputs)\n        prof.export_chrome_trace(\"trace.json\")\n\n- Performance benefits in execution observed and verified by trace produced as shown above.\n\n3.1 Performance on base patch (wall time: ~1s)\n\n{F742082014}\n\n3.2 Performance after application of patch (wall time: ~60ms)\n\n{F742082211}\n\nDifferential Revision: D37114308\n\n\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 79409, "title": "Add new checks in CI system to verify the built linux pip wheel with cpu-cxx11-abi", "time": "2022-06-13T09:33:25Z", "body": "We added the linux pip wheel with cpu-cxx11-abi in pytorch/builder, see: https://github.com/pytorch/builder/pull/990 and https://github.com/pytorch/builder/pull/1023\r\n\r\nThe purpose of this PR is to add new checks in pytorch CI system to verify the linux pip wheel with cpu-cxx11-abi.\r\n\r\nCo-authored-by: Zhu Hong <hong.zhu@intel.com>\r\nCo-authored-by: Guo Yejun <yejun.guo@intel.com>\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3200025338, "node_id": "MDU6TGFiZWwzMjAwMDI1MzM4", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel%20priority", "name": "intel priority", "color": "4E4EF8", "default": false, "description": "matters to intel architecture from performance wise"}, {"id": 4077623621, "node_id": "LA_kwDOA-j9z87zC5lF", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/binaries_wheel", "name": "ciflow/binaries_wheel", "color": "AD47B4", "default": false, "description": "Trigger binary build and upload jobs for wheel on the PR"}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 79353, "title": "[bazel] Plumb all cc tests through rules.cc_test", "time": "2022-06-11T14:45:28Z", "body": "Fixes #79351\r\n\r\nAllows RBE users to have a central place for customizations of cc tests.\r\nSee issue for full context.", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 79285, "title": "csl/shard-go-br", "time": "2022-06-10T15:53:55Z", "body": "[hud.pytorch.org/pr/79285](https://hud.pytorch.org/pr/79285)", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 79234, "title": "clee2000/pytest-proc", "time": "2022-06-09T21:00:57Z", "body": "[hud.pytorch.org/pr/79234](https://hud.pytorch.org/pr/79234)", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 79228, "title": "Implement cuda kernel for torch.sparse.spdiags", "time": "2022-06-09T19:17:35Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #79462\n* __->__ #79228\n* #78965\n\nPart of #70926", "label": [{"id": 679954154, "node_id": "MDU6TGFiZWw2Nzk5NTQxNTQ=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20sparse", "name": "module: sparse", "color": "f7e101", "default": false, "description": "Related to torch.sparse"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 79194, "title": "Add shape function for split like operations.", "time": "2022-06-09T12:30:52Z", "body": "Added shape functions for `aten.split`, `aten.split.Tensor`, `aten.split.Sizes`\r\nand `aten.split_with_sizes`.\r\n\r\nSigned-Off-By: Prateek Gupta<prateek@nod-labs.com>\r\n\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 79059, "title": "clee2000/pytest-4", "time": "2022-06-07T20:51:00Z", "body": "[hud.pytorch.org/pr/79059](https://hud.pytorch.org/pr/79059)\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769204372, "node_id": "LA_kwDOA-j9z87gqX6U", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20releng", "name": "release notes: releng", "color": "EED552", "default": false, "description": "release notes category"}, {"id": 4417837777, "node_id": "LA_kwDOA-j9z88AAAABB1La0Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/mps", "name": "ciflow/mps", "color": "02EDA7", "default": false, "description": "Run MPS tests (subset of trunk)"}]}
{"number": 78976, "title": "clee2000/pytest-3", "time": "2022-06-06T22:24:17Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}]}
{"number": 78965, "title": "Add backward for spdiags (cpu) and enable autograd", "time": "2022-06-06T20:22:14Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #79462\n* #79228\n* __->__ #78965\n\n\n\ncc @nikitaved @pearu @cpuhrsch @bhosmer", "label": [{"id": 679954154, "node_id": "MDU6TGFiZWw2Nzk5NTQxNTQ=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20sparse", "name": "module: sparse", "color": "f7e101", "default": false, "description": "Related to torch.sparse"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769217376, "node_id": "LA_kwDOA-j9z87gqbFg", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20sparse", "name": "release notes: sparse", "color": "2A7626", "default": false, "description": "release notes category"}]}
{"number": 78825, "title": "clee2000/pytest-2", "time": "2022-06-03T17:07:59Z", "body": "Fixes #ISSUE_NUMBER\r\nhttps://hud.pytorch.org/pr/78825\r\nhttps://hud.pytorch.org/hud/pytorch/pytorch/clee2000%2Fpytest-2/1", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}]}
{"number": 78769, "title": "JIT SSA Add support for Stack", "time": "2022-06-02T21:48:54Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* __->__ #78769\n* #83629\n\n\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 78764, "title": "clee2000/pytest", "time": "2022-06-02T20:56:15Z", "body": "Fixes #ISSUE_NUMBER\r\n\r\n", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}]}
{"number": 78760, "title": "clee2000/run-parallel", "time": "2022-06-02T20:45:21Z", "body": "[hud.pytorch.org/pr/78760](https://hud.pytorch.org/pr/78760)", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 78645, "title": "Add HPU Activity Type", "time": "2022-06-01T19:06:15Z", "body": "    Summary: This change is part of Habana Device enablement in PyTorch.\r\n    Allows user to select HPU as profiling activity.\r\n\r\nFixes #ISSUE_NUMBER\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 78237, "title": "Justinchu/param quantize tests", "time": "2022-05-25T04:09:10Z", "body": "Fixes #ISSUE_NUMBER\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 78081, "title": "Enable SyncBatchNorm in CPU backend", "time": "2022-05-23T13:42:30Z", "body": "SyncBatchNorm is a feature in Pytorch that is currently supported only for GPU. It allows for calculation of batch statistics for Batch norm over multiple ranks during distributed training. By using SyncBatchNorm, training workload can scale-out over many ranks  without necessarily increasing global batch size and thereby epochs to convergence.\r\n\r\nThis PR is to enable SyncBatchNorm in CPU backend, to help Resnet50 training in CPU.", "label": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20distributed", "name": "oncall: distributed", "color": "f7e101", "default": false, "description": "Add this issue/PR to distributed oncall triage queue"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 77834, "title": "[ONNX] Add atan2 support in opset 9", "time": "2022-05-19T06:55:14Z", "body": "Fixes #51334", "label": [{"id": 693805995, "node_id": "MDU6TGFiZWw2OTM4MDU5OTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20onnx", "name": "module: onnx", "color": "f7e101", "default": false, "description": "Related to torch.onnx"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 77772, "title": "Use tensor overload for quantize_per_tensor in Quantize module", "time": "2022-05-18T18:48:03Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#77772**\n* #77771\n\nSummary:\nquantize_per_tensor has supported tensor arguments since\na95207dad4c6b970.  Use this overload to avoid a warning about casting\ntensor to float during tracing.\n\nTest Plan:\nAdded unit test that no longer throws a warning.\n\nReviewers:", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 77746, "title": "Allow external pytorch libraries to see kineto", "time": "2022-05-18T15:26:23Z", "body": "Summary: External activity profilers needs access to kineto to register.\r\nIn scenario when profilers are implemented as a pytorch plugins it becomes problematic,\r\nbecause kineto symbols are hidden.\r\n\r\nFixes #ISSUE_NUMBER\r\n", "label": [{"id": 778855555, "node_id": "MDU6TGFiZWw3Nzg4NTU1NTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20build", "name": "module: build", "color": "f7e101", "default": false, "description": "Build system issues"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 77486, "title": "Implement `soft_margin_loss` as dispatch-less composite kernel.", "time": "2022-05-14T14:11:58Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #77486\n* #77485\n* #77484\n\nThis PR makes `soft_margin_loss` kernel into a dispatch-less composite kernel.\nHere are the main changes:\n\n- `Loss.cpp`\n    - Removed `soft_margin_loss` function definitions\n- `composite/soft_margin_loss.h`\n    - Add the templated functions\n- `native_functions.yaml`\n    - Add the `composite` key to `soft_margin_loss`\n- `generated_ops.cpp`\n    - Replaced `at::native::soft_margin_loss` with `at::cpu::soft_margin_loss`\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3212089274, "node_id": "MDU6TGFiZWwzMjEyMDg5Mjc0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/with-ssh", "name": "with-ssh", "color": "127AF1", "default": false, "description": ""}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 77485, "title": "Implement `add.Scalar` as dispatch-less composite kernel.", "time": "2022-05-14T14:11:52Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #77486\n* __->__ #77485\n* #77484\n\nThis PR makes `add.Scalar` kernel into a dispatch-less composite kernel.\nHere are the main changes:\n\n- `BinaryOps.cpp`\n    - Removed `add` function definitions (only for `add.Scalar`)\n- `composite/add.h`\n    - Add the templated functions\n- `native_functions.yaml`\n    - Add the `composite` key to `add.Scalar`", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 77484, "title": "Generate non-structured dispatch-less composite kernels.", "time": "2022-05-14T14:11:47Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #77484\n\nFix #50953\n\nThis PR introduces changes in the codegen for generating dispatch-less composite\nkernels. Summarizing, the idea is to make use of templates as the namespace source for\ntensor operations. Then, we only have to generate a struct (the namespace) with the\nrequired operations.\n\nHere's a summary with the main changes in this PR:\n\n- `RegisterDispatchKey.cpp`\n    - Add `composite_headers` as code template variable\n- `model.py`\n    - Add `composite` as a key in `native_functions.yaml`\n        - Set of `OperatorName`, indicating the dependent operations\n    - Add `CompositeGraph` type alias\n    - Make `BackendIndex.get_kernel` return a new `BackendMetadata` for generated\n      dispatch-less composite kernels\n    - Add `BackendIndex.has_registered_kernel` to check whether a given tuple of\n      `NativeFunction` and `NativeFunctionsGroup` is registered to the given dispatch key\n        - Shortcut for dealing with both structured and unstructured kernels\n    - Add methods for generating the struct and kernel name of the generated dispatch-less\n      composite kernel\n        - Needed for returning the `BackendMetada`\n- `gen.py`\n    - Build the composite graph with `get_composite_graph`\n        - Representation of dependency for dispatch-less composite kernels\n        - Mapping of `OperatorName` with a list of tuples (one for each dependent kernel)\n          of `NativeFunction` and `NativeFunctionGroup`\n    - Collect a set of `#include <ATen/native/composite/op.h>` headers with\n      `get_composite_headers`\n- `register_dispatch_key.py`\n    - Add `composite_graph` as a field of `RegisterDispatchKey` class\n    - Generate the struct for that dispatch key\n- `native_functions.py`\n    - Skip the generation of `op_native.h` header for dispatch-less composite kernels\n    - They are already defined in their respective `ATen/native/composite/op.h` header\n\nDifferential Revision: [D36934643](https://our.internmc.facebook.com/intern/diff/D36934643)", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 77060, "title": "opitimze ConvTransposedND with mkldnn float32 and bfloat16 on CPU", "time": "2022-05-09T03:42:18Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #74023\n* #70897\n* __->__ #77060\n\r\nDifferential Revision: [D36377117](https://our.internmc.facebook.com/intern/diff/D36377117)\r\n\r\nThis patch is about enabling mkldnn on float32 and bfloat16 for transposed convolution on CPU path.", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 76869, "title": "improve cat performance on CPU", "time": "2022-05-05T02:48:06Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* __->__ #76869\r\n* #76868\r\n\r\nThis PR is targeting at further improving `torch.cat` performance on CPU.\r\n\r\nThe original implementation has a fast path for sequential case (1 core), and for parallel case it will:\r\n* if tensors from the input list have the same shape and stride, it will reuse TensorIterator;\r\n* if tensors from the input list list have different shape and stride, it will re-construct TensorIterator for each input tensor.\r\nBoth cases have multiple omp sessions which increase parallelization overhead and result in indirect memory access when writing output.\r\n\r\nBesides, when the input list is long but each input tensor is small, it will won't go the sequential fast path (result.numel() exceeds grain_size), and each TensorIterator will still go sequentially; so the parallel case will be even slower than the sequential case.\r\n\r\nThe kernels in this PR is explained in the notes, and briefly:\r\n* when outer_size == 1 (dim=0): parallel on ntensors if the input list is long enough; otherwise parallel on ninputs * input_dim_size;\r\n* when outer_size != 1(dim!=0): when inner_size == 1 or 2, manually vectorize it ; otherwise parallel on outer_size.\r\n\r\nAll kernels have single omp session, reduce creation of meta info as much as possible.\r\n\r\nThe following is benchmark result from torch operator_benchmark, Xeon 6248, dual sockets, 20 cores per socket, 2.5GHz.\r\n```bash\r\nnumactl --physcpubind=0 --membind=0 python -m pt.cat_test --omp_num_threads 1 --mkl_num_threads 1 --tag_filter ${tag_name}\r\n```\r\nUnit: us per iteration.\r\n\r\n### cat_test tag \"short\"\r\n\r\nName | 1C before | 1C after | 1C speedup | 4C before | 4C after | 4C speedup\r\n-- | -- | -- | -- | -- | -- | --\r\ncat_sizes(1,1,1)_N2_dim0_cpu | 2.164 | 2.222 | 97.39% | 2.194 | 2.232 | 98.30%\r\ncat_sizes(512,512,2)_N2_dim1_cpu | 315.138 | 315.725 | 99.81% | 91.817 | 88.024 | 104.31%\r\ncat_sizes(128,1024,2)_N2_dim1_cpu | 160.099 | 160.293 | 99.88% | 28.370 | 23.339 | 121.56%\r\n\r\n### cat_test tag \"long\"\r\n\r\nName | 1C before | 1C after | 1C speedup | 4C before | 4C after | 4C speedup\r\n-- | -- | -- | -- | -- | -- | --\r\ncat_sizes(1024,1024,2)_N2_dim0_cpu | 1599.037 | 1592.922 | 100.38% | 479.180 | 460.884 | 103.97%\r\ncat_sizes(1025,1023,2)_N2_dim1_cpu | 1827.472 | 1653.985 | 110.49% | 703.982 | 470.121 | 149.74%\r\ncat_sizes(1024,1024,2)_N2_dim2_cpu | 7267.629 | 1509.665 | 481.41% | 1259.040 | 444.630 | 283.17%\r\ncat_sizes[<function<lambda>at0x7efe675777a0>,111,65]_N5_dim0_cpu | 1105.257 | 995.440 | 111.03% | 436.243 | 336.996 | 129.45%\r\ncat_sizes[96,<function<lambda>at0x7efe67577680>,64]_N5_dim1_cpu | 812.477 | 789.520 | 102.91% | 239.786 | 218.246 | 109.87%\r\ncat_sizes[128,64,<function<lambda>at0x7efe4af81950>]_N5_dim2_cpu | 1242.302 | 1175.808 | 105.66% | 493.840 | 374.495 | 131.87%\r\ncat_sizes[<function<lambda>at0x7efe4af81b90>,32,64]_N50_dim0_cpu | 2073.221 | 1935.472 | 107.12% | 786.920 | 577.980 | 136.15%\r\ncat_sizes[32,<function<lambda>at0x7efe4af81c20>,64]_N50_dim1_cpu | 2018.251 | 1953.783 | 103.30% | 760.085 | 552.573 | 137.55%\r\ncat_sizes[33,65,<function<lambda>at0x7efe4af81cb0>]_N50_dim2_cpu | 3288.457 | 3135.313 | 104.88% | 1902.809 | 839.656 | 226.62%\r\n\r\n(*) the original sequential fast path is good enough except it did not specialize inner_size == 1 or 2.\r\n\r\n### cat_test tag \"multidim\"\r\n\r\nname | 1C before | 1C after | 1C speedup | 4C before | 4C after | 4C speedup\r\n-- | -- | -- | -- | -- | -- | --\r\ncat_sizes(64,32,4,16,32)_N2_dim2_cpu | 4282.034 | 4193.721 | 102.11% | 1251.783 | 1167.688 | 107.20%\r\ncat_sizes(16,32,4,16,32)_N8_dim2_cpu | 4432.973 | 4283.635 | 103.49% | 1225.981 | 1183.442 | 103.59%\r\ncat_sizes(9,31,5,15,33)_N17_dim4_cpu | 6308.275 | 6241.009 | 101.08% | 6514.760 | 1815.949 | 358.75%\r\n\r\n### cat_test tag \"manyinputs\"\r\n\r\nname | 1C before | 1C after | 1C speedup | 4C before | 4C after | 4C speedup\r\n-- | -- | -- | -- | -- | -- | --\r\ncat_sizes[<function<lambda>at0x7f87672d1d40>]_N100_dim0_cpu | 154.795 | 152.816 | 101.30% | 233.893 | 32.100 | 728.64%\r\ncat_sizes[<function<lambda>at0x7f87672d1dd0>]_N1000_dim0_cpu | 256.694 | 252.784 | 101.55% | 849.993 | 148.016 | 574.26%\r\ncat_sizes[<function<lambda>at0x7f87672d1e60>]_N2000_dim0_cpu | 360.112 | 352.116 | 102.27% | 1499.309 | 260.123 | 576.38%\r\ncat_sizes[<function<lambda>at0x7f87672d1ef0>]_N3000_dim0_cpu | 455.611 | 446.080 | 102.14% | 2107.840 | 368.567 | 571.90%\r\n\r\n### cat_test tag \"static_runtime\"\r\n\r\nname | 1C before | 1C after | 1C speedup | 4C before | 4C after | 4C speedup\r\n-- | -- | -- | -- | -- | -- | --\r\ncat_sizes[(1,160),(1,14)]_N-1_dim1_cpu | 2.096 | 2.101 | 99.76% | 2.134 | 2.174 | 98.16%\r\ncat_sizes[(1,20,40),(1,4,40),(1,5,40)]_N-1_dim1_cpu | 2.298 | 2.377 | 96.68% | 2.322 | 2.362 | 98.31%\r\ncat_sizes[(1,580),(1,174)]_N-1_dim1_cpu | 2.161 | 2.241 | 96.43% | 2.186 | 2.219 | 98.51%\r\ncat_sizes[(20,160),(20,14)]_N-1_dim1_cpu | 2.516 | 2.489 | 101.08% | 2.549 | 2.490 | 102.37%\r\ncat_sizes[(20,20,40),(20,4,40),(20,5,40)]_N-1_dim1_cpu | 4.100 | 4.134 | 99.18% | 4.102 | 4.048 | 101.33%\r\ncat_sizes[(20,580),(20,174)]_N-1_dim1_cpu | 3.557 | 3.670 | 96.92% | 3.601 | 3.721 | 96.78%\r\n\r\n\r\n### transformer shapes\r\n\r\nname | 1C before | 1C after | 1C speedup | 4C before | 4C after | 4C speedup\r\n-- | -- | -- | -- | -- | -- | --\r\ncat_sizes[(5, 8, 113,   64),(5,8,1,64)]_N-1_dim2 | 92.094 | 91.946 | 100.16% | 15.796 | 11.546 | 136.81%\r\n\r\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 76861, "title": "[WIP] Optionally avoid recordStream calls in ProcessGroupNCCL", "time": "2022-05-04T23:16:27Z", "body": "In big-NLP models with various types of model parallelism, some of our internal researchers (@azhurkevich, Eric Harper, Murat Guney), and our own @eqy, often encounter performance-killing allocator thrashing (repeated cudaMalloc/Free calls, even when the process as a whole doesn't go OOM).\r\n\r\nPlaying with `max_split_size_mb` and `garbage_collection_threshold` didn't help (imo the `garbage_collection_threshold` code is not useful anyway, it tries to extract a free lunch where there's none to be had).\r\n\r\nWhat DID help was commenting out all the recordStream calls in ProcessGroupNCCL. The thrashing behavior disappeared in every repro, presumably because recordStream prevents the allocator from repurposing allocations passed to collectives until those collectives finish from the host's perspective (which could take a while in the ideal scenario where CPU enqueueing runs far ahead of GPU execution).\r\n\r\nOf course, recordStreams were there for a reason: to prevent memory in user-facing stream(s) from being repurposed by the allocator while it's still in use by nccl side stream(s). Scenario 0 below sketches the potential problem. Scenario 1 sketches ProcessGroupNCCL's current (recordStream-based) guardrails.\r\n```python\r\n# Scenario 0, unsafe\r\n# \"later ops\" may use a's memory, and race with \"use a...\"\r\na = torch.rand(..., device=\"cuda\")\r\ns.wait_stream(torch.cuda.current_stream())\r\nwith torch.cuda.stream(s):\r\n    use a...\r\ndel a\r\nlater ops...\r\n\r\n# Scenario 1, safe\r\n# a.record_stream(s) ensures the allocator won't repurpose a for \"later ops...\"\r\n# until any kernels that could have used a in s finish from the host's perspective.\r\na = torch.rand(..., device=\"cuda\")\r\ns.wait_stream(torch.cuda.current_stream())\r\nwith torch.cuda.stream(s):\r\n    a.record_stream(s)\r\n    use a...\r\ndel a\r\nlater ops...\r\n```\r\n\r\nThis PR adds alternative stream-safety guardrails that don't rely on recordStream calls. Sketch:\r\n```python\r\n# Scenario 2, safe\r\n# We never call a.record_stream(s), but we don't release a for repurposing in\r\n# its creation stream until after we sync that creation stream with \"use a...\",\r\n# so there's no way \"later ops...\" can act on a's memory in a way that races with \"use a...\"\r\na = torch.rand(..., device=\"cuda\")\r\ns.wait_stream(torch.cuda.current_stream())\r\nwith torch.cuda.stream(s):\r\n    use a...\r\ntorch.cuda.current_stream().wait_stream(s)\r\ndel a\r\nlater ops...\r\n```\r\nI tried to port scenario 2 to each collective by holding live references to participating tensors until I'm sure the user-facing stream has been synced with the nccl side stream (ie, until im sure the user has wait()ed on the collective).\r\n\r\nBecause we're juggling razor blades here and it's hard to test, recordStream avoidance is off by default, and existing default (aka recordStream-based) behavior is unchanged. recordStream avoidance is enabled by setting envvar `NCCL_AVOID_RECORD_STREAMS=1`.\n\ncc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu", "label": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20distributed", "name": "oncall: distributed", "color": "f7e101", "default": false, "description": "Add this issue/PR to distributed oncall triage queue"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769218061, "node_id": "LA_kwDOA-j9z87gqbQN", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20distributed%20(c10d)", "name": "release notes: distributed (c10d)", "color": "EE86D4", "default": false, "description": "release notes category"}]}
{"number": 76828, "title": "Avoid copies in matmul", "time": "2022-05-04T18:08:13Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #76828\n\r\nWith this PR, matmul just folds a bmm into a mm o mv if and only if it\r\ncan achieve so without copying. We add tests for this to make sure that\r\nour algorithm to detect this is accurate.\r\n\r\nFor the cases where it was copying before see https://github.com/pytorch/pytorch/pull/75197#discussion_r843413208 https://github.com/pytorch/pytorch/pull/75197#discussion_r863489479 https://github.com/pytorch/pytorch/pull/75197#discussion_r863489805\r\n\r\nFixes https://github.com/pytorch/pytorch/issues/76702", "label": [{"id": 679952992, "node_id": "MDU6TGFiZWw2Nzk5NTI5OTI=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20performance", "name": "module: performance", "color": "f7e101", "default": false, "description": "Issues related to performance, either of kernel code or framework glue"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1504141947, "node_id": "MDU6TGFiZWwxNTA0MTQxOTQ3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20linear%20algebra", "name": "module: linear algebra", "color": "f7e101", "default": false, "description": "Issues related to specialized linear algebra operations in PyTorch; includes matrix multiply matmul"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769201219, "node_id": "LA_kwDOA-j9z87gqXJD", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20performance_as_product", "name": "release notes: performance_as_product", "color": "006b75", "default": false, "description": "release notes category"}, {"id": 3773062847, "node_id": "LA_kwDOA-j9z87g5F6_", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20performance", "name": "topic: performance", "color": "AE9D13", "default": false, "description": "topic category"}, {"id": 3936364626, "node_id": "LA_kwDOA-j9z87qoChS", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/periodic", "name": "ciflow/periodic", "color": "62A35B", "default": false, "description": "Trigger jobs ran periodically on master (periodic.yml) on the PR"}]}
{"number": 76489, "title": "Remove _log_softmax", "time": "2022-04-27T21:28:30Z", "body": "Fixes https://github.com/pytorch/pytorch/issues/76433\r\n\r\nWill also remove `_softmax`, and rename `_log_softmax_backward_data` and `_softmax_backward_data` in follow-up commits.\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769217376, "node_id": "LA_kwDOA-j9z87gqbFg", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20sparse", "name": "release notes: sparse", "color": "2A7626", "default": false, "description": "release notes category"}]}
{"number": 76242, "title": "Initial integration of ZenDNN as backend into PyTorch", "time": "2022-04-22T16:30:41Z", "body": "This PR covers following\r\n- Added ZenDNN as third-party library\r\n    - ZenDNN is a submodule for pytorch\r\n    - modified pytorch build infra to build with ZenDNN when USE_ZENDNN is set\r\n      to 1\r\n    - pytorch build reverts to default behavior if USE_ZENDNN environmental\r\n      variable is set to zero or unset\r\n- Added AOCL-blis as  dependency of ZenDNN\r\n    - AOCL-blis is a submodule for pytorch\r\n    - AOCL-blis is built if USE_ZENDNN is set to 1\r\n- Added ZenDNN as backend to pytorch\r\n    - added support for user level control to disable or enable zendnn backend\r\n- Added following changes\r\n    - added zendnn layout and ZendnnCPU dispatch key.\r\n    - tensor conversion between dense and zendnn layouts\r\n    - zendnn convolution as a new ConvBackend\r\n        - when built with zendnn (USE_ZENDNN=1) and gradient calculations are\r\n          disabled (torch.no_grad) convolution is computed using ZenDNN library)\r\n\t- backward propagation is not supported.\r\n\t- support is only added for fp32 data type\r\n- Added few unit tests for pytorch + zendnn\r\n\r\nSigned-off-by: Ratan Prasad <ratan.prasad2@amd.com>\r\n\r\nFixes #ISSUE_NUMBER\r\n", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1300918609, "node_id": "MDU6TGFiZWwxMzAwOTE4NjA5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20convolution", "name": "module: convolution", "color": "f7e101", "default": false, "description": "Problems related to convolutions (THNN, THCUNN, CuDNN)"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 76104, "title": "WIP: Use CUDAToolkit to find cupti", "time": "2022-04-20T13:29:14Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #83199\n* __->__ #76104\n* #82695\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 76093, "title": "[Draft] Removes named tensors", "time": "2022-04-20T05:36:32Z", "body": "To be written\n\ncc @zou3519", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1582187074, "node_id": "MDU6TGFiZWwxNTgyMTg3MDc0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20named%20tensor", "name": "module: named tensor", "color": "f7e101", "default": false, "description": "Named tensor support"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769215850, "node_id": "LA_kwDOA-j9z87gqatq", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20linalg_frontend", "name": "release notes: linalg_frontend", "color": "B7C4BC", "default": false, "description": "release notes category"}]}
{"number": 75863, "title": "Add new keys for Ascend NPU", "time": "2022-04-15T08:41:25Z", "body": "We need a key to register our out of tree backend: https://github.com/Ascend/pytorch,\r\nand the RFC is #76039 \n\ncc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20distributed", "name": "oncall: distributed", "color": "f7e101", "default": false, "description": "Add this issue/PR to distributed oncall triage queue"}, {"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 75786, "title": "use context manager for path extension in torch.hub", "time": "2022-04-14T06:59:49Z", "body": "We are using the idiom\r\n\r\n```py\r\nsys.path.insert(0, path)\r\n\r\n# do something\r\n\r\nsys.path.remove(path)\r\n```\r\n\r\nthree times in `torch.hub`. This is a textbook case for using a context manager. In addition, by using `try` / `finally` we can enforce the Python path is back in its original state even if the actual action raises an exception:\r\n\r\n```py\r\nimport sys\r\n\r\n\r\npath = \"/tmp\"\r\n\r\n# PR\r\ntry:\r\n    sys.path.insert(0, path)\r\n    try:\r\n        # Any exception raised while performing the actual functionality\r\n        raise Exception\r\n    finally:\r\n        sys.path.remove(path)\r\nexcept Exception:\r\n    assert path not in sys.path\r\n\r\n# main\r\ntry:\r\n    sys.path.insert(0, path)\r\n\r\n    # Any exception raised while performing the actual functionality\r\n    raise Exception\r\n\r\n    sys.path.remove(path)\r\nexcept Exception:\r\n    assert path in sys.path\r\n```\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1313583792, "node_id": "MDU6TGFiZWwxMzEzNTgzNzky", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20hub", "name": "module: hub", "color": "f7e101", "default": false, "description": ""}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 74898, "title": "improve sort cpu bfloat16 perf by directly comparing on acc_type", "time": "2022-03-29T07:24:39Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #76279\n* #74899\n* __->__ #74898\n* #74897\n\nDifferential Revision: [D37441442](https://our.internmc.facebook.com/intern/diff/D37441442)", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 74821, "title": "Add xpu path for distributed data parallel", "time": "2022-03-28T03:46:25Z", "body": "The target of this commit is to add the distributed data parallel (DDP) support for intel xpu device.\r\n\r\nWe add a xpu path in functions which \"Perform CPU to GPU copies in a background stream\" (\r\nto_map in distributed.py, _get_stream and forward in _functions.py).\r\n\r\nTo use the xpu path, ensure the \"hasattr(torch, \"xpu\") and torch.xpu.is_available()\" is True.\r\n\r\nCo-authored-by: Lu Chengjun <chengjun.lu@intel.com>\r\nCo-authored-by: Zhu Hong <hong.zhu@intel.com>\r\n\r\n\n\ncc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10", "label": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20distributed", "name": "oncall: distributed", "color": "f7e101", "default": false, "description": "Add this issue/PR to distributed oncall triage queue"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3200025338, "node_id": "MDU6TGFiZWwzMjAwMDI1MzM4", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel%20priority", "name": "intel priority", "color": "4E4EF8", "default": false, "description": "matters to intel architecture from performance wise"}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 74728, "title": "Skip only hop of __torch_function__ at a time", "time": "2022-03-25T00:07:35Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #74727\n* #83014\n* __->__ #74728\n\nCloses #55093\n\nThis changes `Tensor.__torch_function__` to skip only one level of\n`__torch_function__` calls. This is achieved by adding a new state variable\n`skip_next_torch_function` which is reset to false every time\n`has_torch_function` is checked, which works both in C++ and Python.\n\nIn the issue we discussed making `DisableTorchFunction` public, but with this\nyou can just write `super().__torch_function__(func, types, args, kwargs)`\nto achieve the intended effect whilst being clear that this is\nintended for use in `__torch_function__` implementations only.\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769211609, "node_id": "LA_kwDOA-j9z87gqZrZ", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20autograd", "name": "release notes: autograd", "color": "9433A1", "default": false, "description": "release notes category"}]}
{"number": 74727, "title": "Remove split functional wrapper", "time": "2022-03-25T00:07:32Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #74727\n* #83014\n* #74728\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2404913419, "node_id": "MDU6TGFiZWwyNDA0OTEzNDE5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Merged", "name": "Merged", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2510927053, "node_id": "MDU6TGFiZWwyNTEwOTI3MDUz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Reverted", "name": "Reverted", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 74039, "title": "[WIP] Implement complex support in DDP", "time": "2022-03-10T17:33:50Z", "body": "Summary:\nAdd Complex support for DDP.\n\nThis involved fixing a pair of issues:\n\nFirst, DDP usage broadcast_coalesced needs to be made complex aware.\nThis wasd one by introducing a wrapper in `torch.dist` like other comm primitives and using it instead.\n\nSecond, the reducer needed to be fixed. The main complication here is that ProcessGroup::Work exposes the result tensor and this is used by Reducers.\nSince we sneak a complex->real conversion before collectives, we need to convert the resulting tensor back to avoid copy tearing.\n\nFinally, this approach doesn't help custom hooks as they will need to deal with complex tensors by themselves.\n\nFixes https://github.com/pytorch/pytorch/issues/71613\n\nTest Plan: Automated tests to be added.\n\nDifferential Revision: D34790249\n\n", "label": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20distributed", "name": "oncall: distributed", "color": "f7e101", "default": false, "description": "Add this issue/PR to distributed oncall triage queue"}, {"id": 1661470543, "node_id": "MDU6TGFiZWwxNjYxNDcwNTQz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/fb-exported", "name": "fb-exported", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 74023, "title": "enable channels last 3d for Conv3d and ConvTranspose3d on mkldnn path", "time": "2022-03-10T07:33:49Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* __->__ #74023\n* #70897\n* #77060\n\r\nDifferential Revision: [D35782442](https://our.internmc.facebook.com/intern/diff/D35782442)\r\n\r\nAdd channels last 3d support for both Conv3d and ConvTransposed3d", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 73351, "title": "Change API type `Tensor?[]` for structured kernels.", "time": "2022-02-24T09:33:16Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #73351\n\nPartially fixes: #66328\n\n(basically, same as previous PR, but for `IOptTensorRefList`)\n\nThis PR:\n- adds support for `IOptTensorRefList` to the dispatcher for:\n  - computing the dispatch key\n  - boxing and unboxing `IOptTensorRefList`\n- modified the codegen for structured kernels:\n  - codegen APIs use `IOptTensorRefList` instead of `List<optional<Tensor>>`\n\n**Changes summary:**\n\n- Miscelaneous functions used by codegen'd functions (e.g. `FunctionalTensorWrapper.*`)\n- Dispatcher changes for handling `IOptTensorRefList` correctly (e.g. `DispatchKeyExtractor.h`)\n- Forward declarations of `IOptTensorRefList` (e.g. `MethodOperators.h`)\n- Signature changes of `at::index` due to the need of `const` inside `TensorBody.h`\n- Codegen changes, special casing structured kernels (e.g. `gen_variable_type.py`)\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel @ezyang @bhosmer @bdhirsh", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1862630176, "node_id": "MDU6TGFiZWwxODYyNjMwMTc2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20codegen", "name": "module: codegen", "color": "f7e101", "default": false, "description": "Issues related to the codegen for Aten and Autograd"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2583343429, "node_id": "MDU6TGFiZWwyNTgzMzQzNDI5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20structured%20kernels", "name": "module: structured kernels", "color": "ffc6f7", "default": false, "description": "Related to new structured kernels functionality"}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 72303, "title": "record_function: remove legacy internal operators", "time": "2022-02-03T23:10:50Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #72303\n* #76420\n\n", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3769205078, "node_id": "LA_kwDOA-j9z87gqYFW", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20fx", "name": "release notes: fx", "color": "5943DD", "default": false, "description": "release notes category"}]}
{"number": 71492, "title": "Refactor convolution_backward Miopen cases", "time": "2022-01-19T19:55:33Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#71492 Refactor convolution_backward Miopen cases**\n* #71491 Refactor convolution_backward's cudnn cases\n* #71490 Refactor convolution_backward's CudaDepthwise3d case\n* #71489 Refactor convolution_backward's CudaDepthwise2d case\n\nWe do not need to make the input contiguous unless it is necessary for\nbackward computation. The input is only necessary for backward\ncomputation when we're computing `grad_weight`; otherwise, to compute\ngrad_input or grad_bias the input is not used.\n\nDifferential Revision: [D33664695](https://our.internmc.facebook.com/intern/diff/D33664695)", "label": [{"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 71126, "title": "Implements rank-restricted SVD decomposition.", "time": "2022-01-10T21:47:07Z", "body": "This PR introduces a new function into the `linalg` namespace, `svd_rank_restricted`.\r\n\r\n`svd_rank_restricted` is a method designed to fix https://github.com/pytorch/pytorch/issues/69532. It is similar to `linalg.svd`, but it masks out subspaces corresponding to \"small\" singular values and backpropagates only through \"large\" subspaces.\r\n\r\nFixes https://github.com/pytorch/pytorch/issues/69532\n\ncc @jianyuh @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1504141947, "node_id": "MDU6TGFiZWwxNTA0MTQxOTQ3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20linear%20algebra", "name": "module: linear algebra", "color": "f7e101", "default": false, "description": "Issues related to specialized linear algebra operations in PyTorch; includes matrix multiply matmul"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769215850, "node_id": "LA_kwDOA-j9z87gqatq", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20linalg_frontend", "name": "release notes: linalg_frontend", "color": "B7C4BC", "default": false, "description": "release notes category"}]}
{"number": 70989, "title": "Remove deprecated torch.qr", "time": "2022-01-07T14:43:22Z", "body": "The time has come to remove deprecated linear algebra related functions. This PR removes `torch.qr`.\r\n\r\nFixes to torch/xla need to be merged before this PR:\r\n\r\n- [ ] https://github.com/pytorch/xla/pull/3273\n\ncc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1504141947, "node_id": "MDU6TGFiZWwxNTA0MTQxOTQ3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20linear%20algebra", "name": "module: linear algebra", "color": "f7e101", "default": false, "description": "Issues related to specialized linear algebra operations in PyTorch; includes matrix multiply matmul"}, {"id": 1659895945, "node_id": "MDU6TGFiZWwxNjU5ODk1OTQ1", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20deprecation", "name": "module: deprecation", "color": "f7e101", "default": false, "description": ""}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 70988, "title": "Remove deprecated torch.symeig", "time": "2022-01-07T14:34:06Z", "body": "The time has come to remove deprecated linear algebra related functions. This PR removes `torch.symeig`.\r\n\r\nFixes to torch/xla need to be merged before this PR:\r\n\r\n- [ ] https://github.com/pytorch/xla/pull/3272\n\ncc @jianyuh @nikitaved @pearu @mruberry @walterddr @xwang233 @Lezcano @mcarilli @ptrblck @leslie-fang-intel @jgong5 @IvanYashchuk", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1504141947, "node_id": "MDU6TGFiZWwxNTA0MTQxOTQ3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20linear%20algebra", "name": "module: linear algebra", "color": "f7e101", "default": false, "description": "Issues related to specialized linear algebra operations in PyTorch; includes matrix multiply matmul"}, {"id": 1659895945, "node_id": "MDU6TGFiZWwxNjU5ODk1OTQ1", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20deprecation", "name": "module: deprecation", "color": "f7e101", "default": false, "description": ""}, {"id": 1972560798, "node_id": "MDU6TGFiZWwxOTcyNTYwNzk4", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20amp%20(automated%20mixed%20precision)", "name": "module: amp (automated mixed precision)", "color": "f7e101", "default": false, "description": "autocast"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769215850, "node_id": "LA_kwDOA-j9z87gqatq", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20linalg_frontend", "name": "release notes: linalg_frontend", "color": "B7C4BC", "default": false, "description": "release notes category"}, {"id": 3773059130, "node_id": "LA_kwDOA-j9z87g5FA6", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20deprecation", "name": "topic: deprecation", "color": "2824A8", "default": false, "description": "topic category"}]}
{"number": 70979, "title": "Remove deprecated torch.cholesky", "time": "2022-01-07T10:50:11Z", "body": "The time has come to remove deprecated linear algebra related functions. This PR removes torch.cholesky.\r\n\r\nFixes to torch/xla need to be merged before this PR:\r\n\r\n- [ ] https://github.com/pytorch/xla/pull/3274\n\ncc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1504141947, "node_id": "MDU6TGFiZWwxNTA0MTQxOTQ3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20linear%20algebra", "name": "module: linear algebra", "color": "f7e101", "default": false, "description": "Issues related to specialized linear algebra operations in PyTorch; includes matrix multiply matmul"}, {"id": 1659895945, "node_id": "MDU6TGFiZWwxNjU5ODk1OTQ1", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20deprecation", "name": "module: deprecation", "color": "f7e101", "default": false, "description": ""}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 70978, "title": "Remove deprecated torch.chain_matmul", "time": "2022-01-07T10:28:05Z", "body": "The time has come to remove deprecated linear algebra related functions. This PR removes `torch.chain_matmul`.\r\n\r\nPrevious attempt: https://github.com/pytorch/pytorch/pull/69857.\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel @jianyuh @nikitaved @pearu @mruberry @walterddr @xwang233 @Lezcano @mcarilli @ptrblck @leslie-fang-intel @IvanYashchuk", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1504141947, "node_id": "MDU6TGFiZWwxNTA0MTQxOTQ3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20linear%20algebra", "name": "module: linear algebra", "color": "f7e101", "default": false, "description": "Issues related to specialized linear algebra operations in PyTorch; includes matrix multiply matmul"}, {"id": 1659895945, "node_id": "MDU6TGFiZWwxNjU5ODk1OTQ1", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20deprecation", "name": "module: deprecation", "color": "f7e101", "default": false, "description": ""}, {"id": 1972560798, "node_id": "MDU6TGFiZWwxOTcyNTYwNzk4", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20amp%20(automated%20mixed%20precision)", "name": "module: amp (automated mixed precision)", "color": "f7e101", "default": false, "description": "autocast"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3769215850, "node_id": "LA_kwDOA-j9z87gqatq", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20linalg_frontend", "name": "release notes: linalg_frontend", "color": "B7C4BC", "default": false, "description": "release notes category"}, {"id": 3773059130, "node_id": "LA_kwDOA-j9z87g5FA6", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20deprecation", "name": "topic: deprecation", "color": "2824A8", "default": false, "description": "topic category"}]}
{"number": 70897, "title": "add channels last support for slow_conv_transpose2d", "time": "2022-01-06T07:39:00Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #74023\n* __->__ #70897\n* #77060\n\r\nDifferential Revision: [D33571076](https://our.internmc.facebook.com/intern/diff/D33571076)\r\n\r\nThis patch is about enabling `channels last` support for the fallback ATen implementation for transposed convolution. So as to make sure with/without mkldnn will both have channels last support.", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 70873, "title": "Add `unique_indices` arg and update nondeterministic alerts for `scatter` and `scatter_add`", "time": "2022-01-05T22:53:56Z", "body": "Fixes #70583\r\n\n\ncc @mruberry @kurtamohler", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1389320984, "node_id": "MDU6TGFiZWwxMzg5MzIwOTg0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20determinism", "name": "module: determinism", "color": "f7e101", "default": false, "description": ""}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 70584, "title": "fix: clip-onnx spec 9 & 11", "time": "2022-01-03T18:13:40Z", "body": "clip-v6 and clip-v9 only support float(16/32/64) as inputs such that prior implementation will make exported `clamp(int_min, int_max)` fail in ONNXRuntime. \r\n\r\n![image](https://user-images.githubusercontent.com/38074777/147964382-23d8e173-8de4-4b26-be19-c165ee419b4a.png)\r\n\r\nTo fix it, use the \"cast-clip-cast\" pattern as a workaround.\r\n\r\ncc: @peterbell10 @BowenBao ", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 69967, "title": "Add mkl implementation for exponential on CPU", "time": "2021-12-15T07:21:49Z", "body": "### Description\r\nAdd mkl implementation for exponential on CPU to improve the performance of exponential.\r\n\r\n### Testing\r\ndata type: float32 \r\nsingle socket (28cores):\r\n```\r\nbefore: torch.Size([10, 128, 10, 124])  0.065 s\r\n        torch.Size([10, 128, 20, 124])  0.130 s\r\n            \r\nafter:  torch.Size([10, 128, 10, 124])  5.9e-05 s\r\n        torch.Size([10, 128, 20, 124])  0.000113 s\r\n```\r\nsingle core:\r\n```\r\nbefore: torch.Size([10, 128, 10, 124])  0.065 s\r\n        torch.Size([10, 128, 20, 124])  0.130 s \r\n\r\nafter:  torch.Size([10, 128, 10, 124])  0.00117 s\r\n        torch.Size([10, 128, 20, 124])  0.002347 s\r\n```\n\ncc @VitalyFedyunin @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10", "label": [{"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2404913419, "node_id": "MDU6TGFiZWwyNDA0OTEzNDE5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Merged", "name": "Merged", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2510927053, "node_id": "MDU6TGFiZWwyNTEwOTI3MDUz", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Reverted", "name": "Reverted", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3200025338, "node_id": "MDU6TGFiZWwzMjAwMDI1MzM4", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel%20priority", "name": "intel priority", "color": "4E4EF8", "default": false, "description": "matters to intel architecture from performance wise"}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 4195539126, "node_id": "LA_kwDOA-j9z876Eti2", "url": "https://api.github.com/repos/pytorch/pytorch/labels/intel", "name": "intel", "color": "f9d0c4", "default": false, "description": "This tag is for PR from Intel"}]}
{"number": 69624, "title": "Remove set_default_dtype call in jit_metaprogramming_utils.py", "time": "2021-12-08T18:52:54Z", "body": "Fixes #68972\r\n", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 67096, "title": "[caffe2] Export operators to c10 without including ATen/Tensor.h", "time": "2021-10-22T15:10:17Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #67096\n* #67094\n* #67093\n\nWith this change, the only caffe2 files that depend on `ATen/Tensor.h`\nare ones that directly use the ATen API. Specifically,\n```\n[\n  \"caffe2/CMakeFiles/torch_cuda_cpp.dir/contrib/aten/aten_op_gpu.cc.o\",\n  \"caffe2/CMakeFiles/torch_cpu.dir/core/tensor.cc.o\",\n  \"caffe2/CMakeFiles/torch_cuda_cpp.dir/operators/layer_norm_op.cu.o\",\n  \"caffe2/CMakeFiles/torch_cpu.dir/core/IValueInterface.cc.o\",\n  \"caffe2/CMakeFiles/cuda_tensor_interop_test.dir/__/aten/src/ATen/test/cuda_tensor_interop_test.cpp.o\",\n  \"caffe2/CMakeFiles/torch_cpu.dir/contrib/aten/aten_op.cc.o\",\n  \"caffe2/CMakeFiles/caffe2_pybind11_state_gpu.dir/python/pybind_state.cc.o\",\n  \"caffe2/CMakeFiles/torch_cpu.dir/operators/layer_norm_op.cc.o\",\n  \"caffe2/CMakeFiles/torch_cpu.dir/core/export_c10_op_to_caffe2.cc.o\",\n  \"caffe2/CMakeFiles/torch_cpu.dir/core/export_caffe2_op_to_c10.cc.o\",\n  \"caffe2/CMakeFiles/torch_cpu.dir/operators/enforce_finite_op.cc.o\",\n  \"caffe2/CMakeFiles/torch_cpu.dir/core/operator.cc.o\",\n  \"caffe2/CMakeFiles/tensor_interop_test.dir/__/aten/src/ATen/test/tensor_interop_test.cpp.o\",\n  \"caffe2/CMakeFiles/caffe2_pybind11_state.dir/python/pybind_state.cc.o\"\n]\n```\n\nDifferential Revision: [D32289810](https://our.internmc.facebook.com/intern/diff/D32289810)", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 67094, "title": "[caffe2] Remove IValue include from operator.h", "time": "2021-10-22T15:10:06Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #67096\n* __->__ #67094\n* #67093\n\nivalue.h includes Tensor.h, so creating a compilation barrier between\noperator.h and ivalue.h means non-exported caffe2 ops don't need to be\nrebuilt when developing PyTorch.\n\nDifferential Revision: [D32289812](https://our.internmc.facebook.com/intern/diff/D32289812)", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 67093, "title": "[caffe2] Remove OperatorBase::newstyle_outputs_", "time": "2021-10-22T15:09:58Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #67096\n* #67094\n* __->__ #67093\n\n`OperatorBase` maintains `output_tensors_` and `newstyle_outputs_`\nwhich hold the same list of tensors except one is\n`vector<caffe2::Tensor>` and the other is `List<at::Tensor>`.\n\nThis instead maintains only `output_tensors_` and handles the\nconversions inside of export_caffe2_op_to_c10.\n\nDifferential Revision: [D32289811](https://our.internmc.facebook.com/intern/diff/D32289811)", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3663721061, "node_id": "LA_kwDOA-j9z87aX_Jl", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/trunk", "name": "ciflow/trunk", "color": "574CBB", "default": false, "description": "Trigger trunk jobs on your pull request"}, {"id": 3773064655, "node_id": "LA_kwDOA-j9z87g5GXP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/topic:%20not%20user%20facing", "name": "topic: not user facing", "color": "B08798", "default": false, "description": "topic category"}]}
{"number": 66544, "title": "Fixes bias not working when not qkv has diff size", "time": "2021-10-13T08:51:55Z", "body": "Fixes #{issue number}\r\n", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 65365, "title": "Adds cudaMallocAsync as an alternative backend for the CUDA allocator", "time": "2021-09-20T19:30:51Z", "body": "### Benefits\r\n\r\nThe main benefit of adding cudaMallocAsync to Pytorch is \"ecosystem composability\":\r\nIt allows transparent, efficient co-use of GPU memory with other libraries in the same process that also use cudaMallocAsync.\r\n\r\n### User Exposure\r\n\r\nThis PR exposes cudaMallocAsync through the already-existing environment variable `PYTORCH_CUDA_ALLOC_CONF`, i.e.\r\n```\r\nexport PYTORCH_CUDA_ALLOC_CONF=backend:cudaMallocAsync or native\r\n```\r\n\r\n### Should we use default or Pytorch-private cudaMemPool_ts?\r\n\r\nOne goal of cudaMallocAsync is to facilitate memory sharing in scenarios where pytorch shares the GPU with other libraries in the same process that also use cudaMallocAsync under the hood. I see 3 options:\r\n\r\n1. Pytorch cudaMallocAsyncs from the (per-device) default pool.\r\n    - Pros: If other libraries also use the default pool, sharing memory is fast and efficient.\r\n    - Cons: Flags that Pytorch sets on the pool (e.g.  cudaMemPoolReuseAllowOpportunistic, cudaMemPoolAttrReleaseThreshold) silently affect the other libraries, and vice versa.\r\n2. Pytorch creates and cudaMallocAsyncs from its own private `cudaMemPool_t`s.\r\n    - Pros: Flags are sandboxed.\r\n    - Cons: Memory sharing with other libraries gets ugly. Different libraries hit memory high water marks at different times, so their pools want to reserve more memory at different times. Vivek Kini (a cudaMallocAsync author) says pools ARE allowed to steal unused reserved memory from other pools (such that the entire process hopefully won't go OOM), but it's slow: memory must be released to the system by pool A then reserved by pool B. This repeated rebalancing/inter-pool thrashing of reserved memory would be a difficult-to-diagnose, potentially nondeterministic performance hit.\r\n3. Allow the user to choose 1 or 2.\r\n\r\nThe PR currently implements 1. (Implementing 2 or 3 instead, ie, optionally creating per-device pools instead of using default pools, would be straightforward, so the choice between 1, 2, 3 should be motivated purely by high-level pros and cons.)\r\n\r\n### Internal (c10/cuda) interface\r\n\r\nEach allocator interface function in c10/cuda/CUDACachingAllocator.h calls a function pointer that's populated from either the `Native` namespace or `cudaMallocAsync` namespace at load time (static initialization).\r\n\r\nThis design is mainly motivated by not needing any thread-safety gunk in the hot path, which would be the case if, for example, the pointers were populated as function-static variables on first call (compare the code for \"interface() in https://godbolt.org/z/3GzaK65dv with the much simpler code for interface() in the load-time design, https://godbolt.org/z/crKGhb3o9).\r\n\r\nI think it also avoids deepening the current call chain, ie,  maintains existing inlining opportunities and doesn't add new out-of-line calls on the hot path. (I assumed \"existing opportunities\" does NOT include link-time inlining from LTO, which @malfet told me isn't used when linking lib10_cuda.so.)\r\n\r\n### Testing\r\n\r\nTesting will be a challenge. I propose we add one cuda CI build that enables cudaMallocAsync end to end, but first we need a CI build that uses cuda 11.4+. Afaik such a build doesn't exist yet.\r\n\r\n### Future goals (for followup PRs)\r\n\r\n#### IPC support for cudaMallocAsynced memory\r\n^\r\n\r\n#### Pluggable external allocators\r\n\r\nIn the future, pluggable external allocators could be enabled with the same approach (see https://github.com/pytorch/pytorch/issues/43144, https://github.com/rapidsai/rmm/issues/501) . For example, the user could say\r\n```\r\nexport PYTORCH_CUDA_ALLOC_CONF=backend:mylib.so\r\n```\r\nwhere `mylib.so` has symbols with the right signatures. The allocator API functions in CUDACachingAllocator.cpp could then set their static function pointers using dlopen and dlsym on `mylib.so` (huge security hole if anyone runs pytorch as a setuid process, but that's probably true for a million things in pytorch.)\r\n\r\ncc @ngimel", "label": [{"id": 1076930055, "node_id": "MDU6TGFiZWwxMDc2OTMwMDU1", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cuda", "name": "module: cuda", "color": "f7e101", "default": false, "description": "Related to torch.cuda, and CUDA support in general"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 62435, "title": "[jit] Add TORCH_API to [Scalar,Vector]AttributeValue", "time": "2021-07-29T20:51:27Z", "body": "Fixes failing dynamic_cast when building with clang and libc++abi.\r\n\r\n\r\n\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel", "label": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768810956, "node_id": "LA_kwDOA-j9z87go33M", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20jit", "name": "release notes: jit", "color": "6AEFB0", "default": false, "description": "release notes category"}]}
{"number": 57984, "title": "Add tests for bfloat16 math functions on CUDA", "time": "2021-05-10T21:00:00Z", "body": "I copy paste `cuda_half_test.cu` to `cuda_bfloat16_test.cu`  and change it to test bfloat16. It do find a few issues:\r\n- `CUDA_VERSION` sometimes doesn't work on c10 (I don't know the reason), I changed it to use `__CUDACC_VER_MAJOR__` instead\r\n- The `operator __nv_bfloat16()` of `c10::BFloat16` should not be explicit\r\n- PyTorch should be built with `-D__CUDA_NO_BFLOAT16_OPERATORS__` to be consistent with half behavior\r\n- There is a `assert(::abs(::atanh(Half(1.0)) - ::atanh(1.0f)) <= threshold);` in the test, this doesn't make sense, because `atanh(1)` is inf.", "label": [{"id": 1301347167, "node_id": "MDU6TGFiZWwxMzAxMzQ3MTY3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/triaged", "name": "triaged", "color": "006b75", "default": false, "description": "This issue has been looked at a team member, and triaged and prioritized into an appropriate module"}, {"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}]}
{"number": 57772, "title": "Deprecate torch.svd and change svd -> linalg_svd", "time": "2021-05-06T21:13:29Z", "body": "This PR adds a warning that `torch.svd` is deprecated, with @Lezcano's instructions from https://github.com/pytorch/pytorch/pull/57549.\r\nIn addition, all usage of the old svd function is replaced with a new one from `torch.linalg` module.\r\n\r\nThe PR is currently blocked by torch/xla.\r\nOnce torch/xla is able to work with `at::linalg_svd` the XLA CI build will be green.\n\ncc @jianyuh @nikitaved @pearu @mruberry @walterddr @xwang233 @Lezcano", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 1504141947, "node_id": "MDU6TGFiZWwxNTA0MTQxOTQ3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20linear%20algebra", "name": "module: linear algebra", "color": "f7e101", "default": false, "description": "Issues related to specialized linear algebra operations in PyTorch; includes matrix multiply matmul"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3769215850, "node_id": "LA_kwDOA-j9z87gqatq", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20linalg_frontend", "name": "release notes: linalg_frontend", "color": "B7C4BC", "default": false, "description": "release notes category"}]}
{"number": 55680, "title": "std/var: Require correction parameter always be given explicitly", "time": "2021-04-09T15:41:55Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #55680\n* #55679\n* #87118\n\nThis changes the default `None` correction and all overloads using the\nunbiased parameter result in an error.\n\nBC-breaking message:\n\n`torch.std`, `torch.std_mean`, `torch.var` and `torch.var_mean` now\nrequire the `correction` argument be passed in all function calls.\nThis means it cannot be left as defaulted and also the `unbiased`\noverloads cannot be used any more. To recover the old default, use\n`correction=1` which is equivalent to `unbiased=True`; or use\n`correction=0` for the same behavior as `unbiased=False`.", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}, {"id": 4417837777, "node_id": "LA_kwDOA-j9z88AAAABB1La0Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/mps", "name": "ciflow/mps", "color": "02EDA7", "default": false, "description": "Run MPS tests (subset of trunk)"}]}
{"number": 55679, "title": "std/var: Deprecate default `correction` value and `unbiased` argument", "time": "2021-04-09T15:41:50Z", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* #55680\r\n* __->__ #55679\r\n* #87118\r\n\r\nThe default correction value of 1 is incompatible with the array api\r\nwhich defaults to 0 correction. So, we should deprecate the existing\r\ndefault as the first step in changing the default without introducing\r\na silent BC-break.\r\n\r\nThis also deprecates the unbiased overloads entirely,\r\nfor better consistency with the array API.", "label": [{"id": 1392590051, "node_id": "MDU6TGFiZWwxMzkyNTkwMDUx", "url": "https://api.github.com/repos/pytorch/pytorch/labels/open%20source", "name": "open source", "color": "ededed", "default": false, "description": null}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}, {"id": 3769207236, "node_id": "LA_kwDOA-j9z87gqYnE", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20nn", "name": "release notes: nn", "color": "29E07F", "default": false, "description": "release notes category"}, {"id": 4417837777, "node_id": "LA_kwDOA-j9z88AAAABB1La0Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/mps", "name": "ciflow/mps", "color": "02EDA7", "default": false, "description": "Run MPS tests (subset of trunk)"}]}
{"number": 30253, "title": "Update the autograd notes in the doc", "time": "2019-11-21T20:45:07Z", "body": "Complete rewrite of the autograd notes.\r\n\r\n[current version](https://pytorch.org/docs/stable/notes/autograd.html)\n\ncc @ezyang @zou3519 @gqchen @pearu @nikitaved @soulitzer @Lezcano @Varal7", "label": [{"id": 1076922545, "node_id": "MDU6TGFiZWwxMDc2OTIyNTQ1", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20autograd", "name": "module: autograd", "color": "f7e101", "default": false, "description": "Related to torch.autograd, and the autograd engine in general"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 2695910552, "node_id": "MDU6TGFiZWwyNjk1OTEwNTUy", "url": "https://api.github.com/repos/pytorch/pytorch/labels/Stale", "name": "Stale", "color": "ededed", "default": false, "description": null}]}
{"number": 27167, "title": "[DO NOT DELETE OR MERGE] Postnightly release", "time": "2019-10-01T20:53:55Z", "body": "See #26817 for what this is\r\n\r\nFB only: Chronos job page: https://our.internmc.facebook.com/intern/chronos/job?smc=chronos_gp_admin_client&jobname=postnightly_trigger (click \"Run Now\" to manually trigger a nightly)\n\ncc @jerryzh168 @jianyuh @raghuramank100 @jamesr66a @vkuzo @jgong5 @Xia-Weiwen @leslie-fang-intel @VitalyFedyunin @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @mcarilli @ptrblck @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @Guobing-Chen @chunyuan-w @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @desertfire @jansel @lezcano @fdrocha", "label": [{"id": 1031340614, "node_id": "MDU6TGFiZWwxMDMxMzQwNjE0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20quantization", "name": "oncall: quantization", "color": "f7e101", "default": false, "description": "Quantization support in PyTorch"}, {"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1972560798, "node_id": "MDU6TGFiZWwxOTcyNTYwNzk4", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20amp%20(automated%20mixed%20precision)", "name": "module: amp (automated mixed precision)", "color": "f7e101", "default": false, "description": "autocast"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}, {"id": 3769204372, "node_id": "LA_kwDOA-j9z87gqX6U", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20releng", "name": "release notes: releng", "color": "EED552", "default": false, "description": "release notes category"}, {"id": 4417837777, "node_id": "LA_kwDOA-j9z88AAAABB1La0Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/mps", "name": "ciflow/mps", "color": "02EDA7", "default": false, "description": "Run MPS tests (subset of trunk)"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}, {"id": 4668973195, "node_id": "LA_kwDOA-j9z88AAAABFkrgiw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/inductor", "name": "ciflow/inductor", "color": "8E9C02", "default": false, "description": ""}]}
{"number": 26921, "title": "[DO NOT DELETE OR MERGE] Nightly release", "time": "2019-09-26T20:30:01Z", "body": "See https://github.com/pytorch/pytorch/issues/26817 for what this is\r\n\r\nFB only: Chronos job page: https://our.internmc.facebook.com/intern/chronos/job?smc=chronos_gp_admin_client&jobname=nightly_trigger (click \"Run Now\" to manually trigger a nightly)\n\ncc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @EikanWang @jgong5 @wenzhe-nrv @sanchitintel @malfet @seemethere @svekars @carljparker @bhosmer @smessmer @ljk53 @bdhirsh @jbschlosser @jerryzh168 @jianyuh @raghuramank100 @jamesr66a @vkuzo @Xia-Weiwen @leslie-fang-intel @albanD @mruberry @walterddr @kshitij12345 @saketh-are @VitalyFedyunin @mingfeima @XiaobingSuper @ashokei @jingxu10 @pytorch/pytorch-dev-infra @mcarilli @ptrblck @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @Guobing-Chen @chunyuan-w @zhuhaozhe @blzheng @jiayisunx @desertfire @jansel @lezcano @fdrocha", "label": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20distributed", "name": "oncall: distributed", "color": "f7e101", "default": false, "description": "Add this issue/PR to distributed oncall triage queue"}, {"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20jit", "name": "oncall: jit", "color": "c5def5", "default": false, "description": "Add this issue/PR to JIT oncall triage queue"}, {"id": 778855555, "node_id": "MDU6TGFiZWw3Nzg4NTU1NTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20build", "name": "module: build", "color": "f7e101", "default": false, "description": "Build system issues"}, {"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false, "description": ""}, {"id": 897287230, "node_id": "MDU6TGFiZWw4OTcyODcyMzA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20docs", "name": "module: docs", "color": "f7e101", "default": false, "description": "Related to our documentation, both in docs/ and docblocks"}, {"id": 912691507, "node_id": "MDU6TGFiZWw5MTI2OTE1MDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20internals", "name": "module: internals", "color": "f7e101", "default": false, "description": "Related to internal abstractions in c10 and ATen"}, {"id": 917150434, "node_id": "MDU6TGFiZWw5MTcxNTA0MzQ=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpp", "name": "module: cpp", "color": "f7e101", "default": false, "description": "Related to C++ API"}, {"id": 1031340614, "node_id": "MDU6TGFiZWwxMDMxMzQwNjE0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20quantization", "name": "oncall: quantization", "color": "f7e101", "default": false, "description": "Quantization support in PyTorch"}, {"id": 1076922764, "node_id": "MDU6TGFiZWwxMDc2OTIyNzY0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20nn", "name": "module: nn", "color": "f7e101", "default": false, "description": "Related to torch.nn"}, {"id": 1294499454, "node_id": "MDU6TGFiZWwxMjk0NDk5NDU0", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20cpu", "name": "module: cpu", "color": "f7e101", "default": false, "description": "CPU specific problem (e.g., perf, algorithm)"}, {"id": 1300896147, "node_id": "MDU6TGFiZWwxMzAwODk2MTQ3", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20ci", "name": "module: ci", "color": "f7e101", "default": false, "description": "Related to continuous integration"}, {"id": 1311350199, "node_id": "MDU6TGFiZWwxMzExMzUwMTk5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20pybind", "name": "module: pybind", "color": "f7e101", "default": false, "description": "Related to our Python bindings / interactions with other Python libraries"}, {"id": 1972560798, "node_id": "MDU6TGFiZWwxOTcyNTYwNzk4", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20amp%20(automated%20mixed%20precision)", "name": "module: amp (automated mixed precision)", "color": "f7e101", "default": false, "description": "autocast"}, {"id": 2467928730, "node_id": "MDU6TGFiZWwyNDY3OTI4NzMw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cla%20signed", "name": "cla signed", "color": "ededed", "default": false, "description": null}, {"id": 3768822607, "node_id": "LA_kwDOA-j9z87go6tP", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20quantization", "name": "release notes: quantization", "color": "A727FC", "default": false, "description": "release notes category"}, {"id": 3769204372, "node_id": "LA_kwDOA-j9z87gqX6U", "url": "https://api.github.com/repos/pytorch/pytorch/labels/release%20notes:%20releng", "name": "release notes: releng", "color": "EED552", "default": false, "description": "release notes category"}, {"id": 4417837777, "node_id": "LA_kwDOA-j9z88AAAABB1La0Q", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ciflow/mps", "name": "ciflow/mps", "color": "02EDA7", "default": false, "description": "Run MPS tests (subset of trunk)"}, {"id": 4631341851, "node_id": "LA_kwDOA-j9z88AAAABFAyrGw", "url": "https://api.github.com/repos/pytorch/pytorch/labels/module:%20dynamo", "name": "module: dynamo", "color": "e99695", "default": false, "description": ""}]}
